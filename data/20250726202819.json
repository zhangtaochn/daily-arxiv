{
  "2507.18262v1": {
    "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic\n  Grounding for Generalizable Robotic Manipulation",
    "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io.",
    "published": "2025-07-24T10:07:31Z",
    "updated": "2025-07-24T10:07:31Z",
    "id": "2507.18262v1",
    "authors": [
      "Chenyu Su",
      "Weiwei Shang",
      "Chen Qian",
      "Fei Zhang",
      "Shuang Cong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18262v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18262v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18262v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Multimodal Large Language Models (MLLMs) and Vision Foundation Models (VFMs) for robotic manipulation, which involves both multimodal integration and reasoning capabilities.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning",
      "VLA"
    ]
  },
  "2507.18252v1": {
    "title": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based\n  Reasoning",
    "summary": "Eye-tracking data reveals valuable insights into users' cognitive states but\nis difficult to analyze due to its structured, non-linguistic nature. While\nlarge language models (LLMs) excel at reasoning over text, they struggle with\ntemporal and numerical data. This paper presents a multimodal human-AI\ncollaborative framework designed to enhance cognitive pattern extraction from\neye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent\ngaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert\njudgment with LLM output to generate trust scores for behavioral\ninterpretations; and (3) a hybrid anomaly detection module combining LSTM-based\ntemporal modeling with LLM-driven semantic analysis. Our results across several\nLLMs and prompt strategies show improvements in consistency, interpretability,\nand performance, with up to 50% accuracy in difficulty prediction tasks. This\napproach offers a scalable, interpretable solution for cognitive modeling and\nhas broad potential in adaptive learning, human-computer interaction, and\neducational analytics.",
    "published": "2025-07-24T09:49:53Z",
    "updated": "2025-07-24T09:49:53Z",
    "id": "2507.18252v1",
    "authors": [
      "Dongyang Guo",
      "Yasmeen Abdrabou",
      "Enkeleda Thaqi",
      "Enkelejda Kasneci"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18252v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18252v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18252v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of eye-tracking data with LLM-based reasoning, which involves multimodal analysis and reasoning capabilities of LLMs. It also touches on the use of expert judgment and LLM output, indicating a focus on reasoning and multimodal approaches.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.18215v1": {
    "title": "Information Security Based on LLM Approaches: A Review",
    "summary": "Information security is facing increasingly severe challenges, and\ntraditional protection means are difficult to cope with complex and changing\nthreats. In recent years, as an emerging intelligent technology, large language\nmodels (LLMs) have shown a broad application prospect in the field of\ninformation security. In this paper, we focus on the key role of LLM in\ninformation security, systematically review its application progress in\nmalicious behavior prediction, network threat analysis, system vulnerability\ndetection, malicious code identification, and cryptographic algorithm\noptimization, and explore its potential in enhancing security protection\nperformance. Based on neural networks and Transformer architecture, this paper\nanalyzes the technical basis of large language models and their advantages in\nnatural language processing tasks. It is shown that the introduction of large\nlanguage modeling helps to improve the detection accuracy and reduce the false\nalarm rate of security systems. Finally, this paper summarizes the current\napplication results and points out that it still faces challenges in model\ntransparency, interpretability, and scene adaptability, among other issues. It\nis necessary to explore further the optimization of the model structure and the\nimprovement of the generalization ability to realize a more intelligent and\naccurate information security protection system.",
    "published": "2025-07-24T09:09:36Z",
    "updated": "2025-07-24T09:09:36Z",
    "id": "2507.18215v1",
    "authors": [
      "Chang Gong",
      "Zhongwen Li",
      "Xiaoqi Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18215v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18215v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18215v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in the field of information security, highlighting their role in various security tasks such as malicious behavior prediction, network threat analysis, and system vulnerability detection. The focus on LLMs and their technical basis aligns with the 'LLM' topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.18212v1": {
    "title": "Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with\n  Magnitude Compensation",
    "summary": "Layer pruning has emerged as a promising technique for compressing large\nlanguage models (LLMs) while achieving acceleration proportional to the pruning\nratio. In this work, we identify that removing any layer induces a significant\nmagnitude gap in hidden states, resulting in substantial performance\ndegradation. To address this issue, we propose Prune&Comp, a novel\nplug-and-play layer pruning scheme that leverages magnitude compensation to\nmitigate such gaps in a training-free manner. Specifically, we first estimate\nthe magnitude gap caused by layer removal and then eliminate this gap by\nrescaling the remaining weights offline, with zero runtime overhead incurred.\nWe further demonstrate the advantages of Prune&Comp through an iterative\npruning strategy. When integrated with an iterative prune-and-compensate loop,\nPrune&Comp consistently enhances existing layer pruning metrics. For instance,\nwhen 5 layers of LLaMA-3-8B are pruned using the prevalent block influence\nmetric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of the\noriginal model's question-answering performance, outperforming the baseline by\n4.01%.",
    "published": "2025-07-24T09:07:20Z",
    "updated": "2025-07-24T09:07:20Z",
    "id": "2507.18212v1",
    "authors": [
      "Xinrui Chen",
      "Hongxing Zhang",
      "Fanyi Zeng",
      "Yongxian Wei",
      "Yizhi Wang",
      "Xitong Ling",
      "Guanghao Li",
      "Chun Yuan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18212v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18212v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18212v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on layer pruning techniques for Large Language Models (LLMs) and proposes a novel method to mitigate performance degradation caused by pruning. This aligns with the topics of LLM research and model compression, which are relevant to the 'LLM' and 'Scaling' categories.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.18203v1": {
    "title": "Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to\n  Misinformation",
    "summary": "Instruction-tuning enhances the ability of large language models (LLMs) to\nfollow user instructions more accurately, improving usability while reducing\nharmful outputs. However, this process may increase the model's dependence on\nuser input, potentially leading to the unfiltered acceptance of misinformation\nand the generation of hallucinations. Existing studies primarily highlight that\nLLMs are receptive to external information that contradict their parametric\nknowledge, but little research has been conducted on the direct impact of\ninstruction-tuning on this phenomenon. In our study, we investigate the impact\nof instruction-tuning on LLM's susceptibility to misinformation. Our analysis\nreveals that instruction-tuned LLMs are significantly more likely to accept\nmisinformation when it is presented by the user. A comparison with base models\nshows that instruction-tuning increases reliance on user-provided information,\nshifting susceptibility from the assistant role to the user role. Furthermore,\nwe explore additional factors influencing misinformation susceptibility, such\nas the role of the user in prompt structure, misinformation length, and the\npresence of warnings in the system prompt. Our findings underscore the need for\nsystematic approaches to mitigate unintended consequences of instruction-tuning\nand enhance the reliability of LLMs in real-world applications.",
    "published": "2025-07-24T08:58:47Z",
    "updated": "2025-07-24T08:58:47Z",
    "id": "2507.18203v1",
    "authors": [
      "Kyubeen Han",
      "Junseo Jang",
      "Hongjin Kim",
      "Geunyeong Jeong",
      "Harksoo Kim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18203v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18203v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18203v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the impact of instruction-tuning on LLMs, specifically their susceptibility to misinformation, which is a key aspect of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.18181v1": {
    "title": "SpecASR: Accelerating LLM-based Automatic Speech Recognition via\n  Speculative Decoding",
    "summary": "Large language model (LLM)-based automatic speech recognition (ASR) has\nrecently attracted a lot of attention due to its high recognition accuracy and\nenhanced multi-dialect support. However, the high decoding latency of LLMs\nchallenges the real-time ASR requirements. Although speculative decoding has\nbeen explored for better decoding efficiency, they usually ignore the key\ncharacteristics of the ASR task and achieve limited speedup. To further reduce\nthe real-time ASR latency, in this paper, we propose a novel speculative\ndecoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed\nbased on our core observation that ASR decoding is audio-conditioned, which\nresults in high output alignment between small and large ASR models, even given\noutput mismatches in intermediate decoding steps. Therefore, SpecASR features\nan adaptive draft sequence generation process that dynamically modifies the\ndraft sequence length to maximize the token acceptance length. SpecASR further\nproposes a draft sequence recycling strategy that reuses the previously\ngenerated draft sequence to reduce the draft ASR model latency. Moreover, a\ntwo-pass sparse token tree generation algorithm is also proposed to balance the\nlatency of draft and target ASR models. With extensive experimental results, we\ndemonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the\nbaseline autoregressive decoding and speculative decoding, respectively,\nwithout any loss in recognition accuracy.",
    "published": "2025-07-24T08:27:53Z",
    "updated": "2025-07-24T08:27:53Z",
    "id": "2507.18181v1",
    "authors": [
      "Linye Wei",
      "Shuzhang Zhong",
      "Songqiang Xu",
      "Runsheng Wang",
      "Ru Huang",
      "Meng Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18181v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18181v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18181v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on accelerating LLM-based Automatic Speech Recognition (ASR) using speculative decoding, which involves optimizing the decoding process of LLMs for ASR tasks. The core topics are related to LLMs and their application in ASR, but the specific focus on speculative decoding and ASR does not directly align with the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.18178v1": {
    "title": "Decoupling Knowledge and Reasoning in LLMs: An Exploration Using\n  Cognitive Dual-System Theory",
    "summary": "While large language models (LLMs) leverage both knowledge and reasoning\nduring inference, the capacity to distinguish between them plays a pivotal role\nin model analysis, interpretability, and development. Inspired by dual-system\ncognitive theory, we propose a cognition attribution framework to decouple the\ncontribution of knowledge and reasoning. In particular, the cognition of LLMs\nis decomposed into two distinct yet complementary phases: knowledge retrieval\n(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs\nare prompted to generate answers under two different cognitive modes, fast\nthinking and slow thinking, respectively. The performance under different\ncognitive modes is analyzed to quantify the contribution of knowledge and\nreasoning. This architecture is employed to 15 LLMs across 3 datasets. Results\nreveal: (1) reasoning adjustment is domain-specific, benefiting\nreasoning-intensive domains (e.g., mathematics, physics, and chemistry) and\npotentially imparing knowledge-intensive domains. (2) Parameter scaling\nimproves both knowledge and reasoning, with knowledge improvements being more\npronounced. Additionally, parameter scaling make LLMs reasoning significantly\nmore prudent, while moderately more intelligent. (3) Knowledge primarily\nresides in lower network layers, while reasoning operates in higher layers. Our\nframework not only helps understand LLMs from a \"decoupling\" perspective, but\nalso provides new insights into existing research, including scaling laws,\nhierarchical knowledge editing, and limitations of small-model reasoning.",
    "published": "2025-07-24T08:24:52Z",
    "updated": "2025-07-24T08:24:52Z",
    "id": "2507.18178v1",
    "authors": [
      "Mutian Yang",
      "Jiandong Gao",
      "Ji Wu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18178v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18178v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18178v1",
    "keywords": [
      "LLM",
      "Dataset"
    ],
    "cls_reason": "The paper focuses on decoupling knowledge and reasoning in LLMs, which involves analyzing their cognitive processes and performance under different modes. This aligns with topics related to reasoning in LLMs and understanding their internal mechanisms.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2507.18167v1": {
    "title": "ICWLM: A Multi-Task Wireless Large Model via In-Context Learning",
    "summary": "The rapid evolution of wireless communication technologies, particularly\nmassive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),\nintroduces significant network complexity and computational demands.\nSignificant research efforts have been made to improve physical layer\nperformance by resorting to deep learning (DL) methods, which, however, are\nusually task-specific and struggle with data scarcity and generalization. To\naddress these challenges, we propose a novel In-Context Wireless Large Model\n(ICWLM), a wireless-native foundation model designed for simultaneous\nmulti-task learning at the physical layer. Unlike conventional methods that\nadapt wireless data to pre-trained large language models (LLMs), ICWLM is\ntrained directly on large-scale, mixed wireless datasets from scratch. It\njointly solves multiple classical physical layer problems, including multi-user\nprecoding (sum-rate maximization and max-min SINR) and channel prediction. A\nkey innovation of ICWLM is its utilization of in-context learning (ICL),\nenabling the model to adapt to varying system configurations and channel\nconditions with minimal demonstration pairs, eliminating the need for extensive\nretraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm\nto dynamically balance the individual task losses during multi-task training,\nensuring efficient and stable learning across diverse objectives. Extensive\nsimulation results demonstrate that ICWLM achieves competitive performance\ncompared to task-specific methods while exhibiting remarkable generalization\ncapabilities to unseen system configurations. This work offers a promising\nparadigm for developing unified and adaptive AI models for future wireless\nnetworks, potentially reducing deployment complexity and enhancing intelligent\nresource management.",
    "published": "2025-07-24T08:04:39Z",
    "updated": "2025-07-24T08:04:39Z",
    "id": "2507.18167v1",
    "authors": [
      "Yuxuan Wen",
      "Xiaoming Chen",
      "Maojun Zhang",
      "Zhaoyang Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18167v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18167v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18167v1",
    "keywords": [
      "LLM",
      "Dataset"
    ],
    "cls_reason": "The paper discusses a wireless-native foundation model (ICWLM) that utilizes in-context learning and is trained on large-scale wireless datasets, which aligns with the topics of Large Language Models (LLM) and their applications in multi-task learning and generalization. However, it does not fit neatly into the provided topic list as it is more specialized towards wireless communication technologies.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.18161v1": {
    "title": "Recent Trends in Distant Conversational Speech Recognition: A Review of\n  CHiME-7 and 8 DASR Challenges",
    "summary": "The CHiME-7 and 8 distant speech recognition (DASR) challenges focus on\nmulti-channel, generalizable, joint automatic speech recognition (ASR) and\ndiarization of conversational speech. With participation from 9 teams\nsubmitting 32 diverse systems, these challenges have contributed to\nstate-of-the-art research in the field. This paper outlines the challenges'\ndesign, evaluation metrics, datasets, and baseline systems while analyzing key\ntrends from participant submissions. From this analysis it emerges that: 1)\nMost participants use end-to-end (e2e) ASR systems, whereas hybrid systems were\nprevalent in previous CHiME challenges. This transition is mainly due to the\navailability of robust large-scale pre-trained models, which lowers the data\nburden for e2e-ASR. 2) Despite recent advances in neural speech separation and\nenhancement (SSE), all teams still heavily rely on guided source separation,\nsuggesting that current neural SSE techniques are still unable to reliably deal\nwith complex scenarios and different recording setups. 3) All best systems\nemploy diarization refinement via target-speaker diarization techniques.\nAccurate speaker counting in the first diarization pass is thus crucial to\navoid compounding errors and CHiME-8 DASR participants especially focused on\nthis part. 4) Downstream evaluation via meeting summarization can correlate\nweakly with transcription quality due to the remarkable effectiveness of\nlarge-language models in handling errors. On the NOTSOFAR-1 scenario, even\nsystems with over 50\\% time-constrained minimum permutation WER can perform\nroughly on par with the most effective ones (around 11\\%). 5) Despite recent\nprogress, accurately transcribing spontaneous speech in challenging acoustic\nenvironments remains difficult, even when using computationally intensive\nsystem ensembles.",
    "published": "2025-07-24T07:56:24Z",
    "updated": "2025-07-24T07:56:24Z",
    "id": "2507.18161v1",
    "authors": [
      "Samuele Cornell",
      "Christoph Boeddeker",
      "Taejin Park",
      "He Huang",
      "Desh Raj",
      "Matthew Wiesner",
      "Yoshiki Masuyama",
      "Xuankai Chang",
      "Zhong-Qiu Wang",
      "Stefano Squartini",
      "Paola Garcia",
      "Shinji Watanabe"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18161v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18161v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18161v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on distant conversational speech recognition, including aspects like automatic speech recognition (ASR), diarization, and the use of large-scale pre-trained models. However, none of the provided topics directly align with these specific areas of research.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.18153v1": {
    "title": "When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation\n  Method with LLM and Pseudo Label",
    "summary": "Class-imbalanced graph node classification is a practical yet underexplored\nresearch problem. Although recent studies have attempted to address this issue,\nthey typically assume clean and reliable labels when processing\nclass-imbalanced graphs. This assumption often violates the nature of\nreal-world graphs, where labels frequently contain noise. Given this gap, this\npaper systematically investigates robust node classification for\nclass-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph\nAugmentation framework based on Large language models (LLMs) and\nPseudo-labeling techniques. Specifically, we design an LLM-based oversampling\nmethod to generate synthetic minority nodes, producing label-accurate minority\nnodes to alleviate class imbalance. Based on the class-balanced graphs, we\ndevelop a dynamically weighted pseudo-labeling method to obtain high-confidence\npseudo labels to reduce label noise ratio. Additionally, we implement a\nsecondary LLM-guided oversampling mechanism to mitigate potential class\ndistribution skew caused by pseudo labels. Experimental results show that\nGraphALP achieves superior performance over state-of-the-art methods on\nclass-imbalanced graphs with noisy labels.",
    "published": "2025-07-24T07:39:07Z",
    "updated": "2025-07-24T07:39:07Z",
    "id": "2507.18153v1",
    "authors": [
      "Riting Xia",
      "Rucong Wang",
      "Yulin Liu",
      "Anchen Li",
      "Xueyan Liu",
      "Yan Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18153v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18153v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18153v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a graph augmentation framework to address class imbalance and noisy labels in graph node classification. The core focus is on LLMs and their application in generating synthetic nodes and pseudo-labeling, which aligns with the 'LLM' and 'Reasoning' topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.18113v1": {
    "title": "Policy Disruption in Reinforcement Learning:Adversarial Attack with\n  Large Language Models and Critical State Identification",
    "summary": "Reinforcement learning (RL) has achieved remarkable success in fields like\nrobotics and autonomous driving, but adversarial attacks designed to mislead RL\nsystems remain challenging. Existing approaches often rely on modifying the\nenvironment or policy, limiting their practicality. This paper proposes an\nadversarial attack method in which existing agents in the environment guide the\ntarget policy to output suboptimal actions without altering the environment. We\npropose a reward iteration optimization framework that leverages large language\nmodels (LLMs) to generate adversarial rewards explicitly tailored to the\nvulnerabilities of the target agent, thereby enhancing the effectiveness of\ninducing the target agent toward suboptimal decision-making. Additionally, a\ncritical state identification algorithm is designed to pinpoint the target\nagent's most vulnerable states, where suboptimal behavior from the victim leads\nto significant degradation in overall performance. Experimental results in\ndiverse environments demonstrate the superiority of our method over existing\napproaches.",
    "published": "2025-07-24T05:52:06Z",
    "updated": "2025-07-24T05:52:06Z",
    "id": "2507.18113v1",
    "authors": [
      "Junyong Jiang",
      "Buwei Tian",
      "Chenxing Xu",
      "Songze Li",
      "Lu Dong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18113v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18113v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18113v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the context of Reinforcement Learning (RL) for adversarial attacks, which aligns with the topics of LLM and RL. The focus on adversarial attacks and policy disruption in RL is also relevant to the broader field of Reinforcement Learning.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.18073v1": {
    "title": "Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged\n  Mixed-Precision Quantization Method",
    "summary": "Deploying large language models (LLMs) is challenging due to their massive\nparameters and high computational costs. Ultra low-bit quantization can\nsignificantly reduce storage and accelerate inference, but extreme compression\n(i.e., mean bit-width <= 2) often leads to severe performance degradation. To\naddress this, we propose Squeeze10-LLM, effectively \"squeezing\" 16-bit LLMs'\nweights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision\npost-training quantization (PTQ) framework and achieves an average of 1.6 bits\nper weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We\nintroduce Squeeze10LLM with two key innovations: Post-Binarization Activation\nRobustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a\nrefined weight significance metric that accounts for the impact of quantization\non activations, improving accuracy in low-bit settings. FIAS is a strategy that\npreserves full activation information during quantization to mitigate\ncumulative error propagation across layers. Experiments on LLaMA and LLaMA2\nshow that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit\nweight-only quantization, improving average accuracy from 43% to 56% on six\nzero-shot classification tasks--a significant boost over existing PTQ methods.\nOur code will be released upon publication.",
    "published": "2025-07-24T03:55:19Z",
    "updated": "2025-07-24T03:55:19Z",
    "id": "2507.18073v1",
    "authors": [
      "Qingcheng Zhu",
      "Yangyang Ren",
      "Linlin Yang",
      "Mingbao Lin",
      "Yanjing Li",
      "Sheng Xu",
      "Zichao Feng",
      "Haodong Zhu",
      "Yuguang Yang",
      "Juan Zhang",
      "Runqi Wang",
      "Baochang Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18073v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18073v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18073v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on quantization techniques for large language models (LLMs) to reduce their computational and storage requirements, which is a key aspect of optimizing LLM deployment.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.18055v1": {
    "title": "Privacy-Preserving Synthetic Review Generation with Diverse Writing\n  Styles Using LLMs",
    "summary": "The increasing use of synthetic data generated by Large Language Models\n(LLMs) presents both opportunities and challenges in data-driven applications.\nWhile synthetic data provides a cost-effective, scalable alternative to\nreal-world data to facilitate model training, its diversity and privacy risks\nremain underexplored. Focusing on text-based synthetic data, we propose a\ncomprehensive set of metrics to quantitatively assess the diversity (i.e.,\nlinguistic expression, sentiment, and user perspective), and privacy (i.e.,\nre-identification risk and stylistic outliers) of synthetic datasets generated\nby several state-of-the-art LLMs. Experiment results reveal significant\nlimitations in LLMs' capabilities in generating diverse and privacy-preserving\nsynthetic data. Guided by the evaluation results, a prompt-based approach is\nproposed to enhance the diversity of synthetic reviews while preserving\nreviewer privacy.",
    "published": "2025-07-24T03:12:16Z",
    "updated": "2025-07-24T03:12:16Z",
    "id": "2507.18055v1",
    "authors": [
      "Tevin Atwal",
      "Chan Nam Tieu",
      "Yefeng Yuan",
      "Zhan Shi",
      "Yuhong Liu",
      "Liang Cheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18055v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18055v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18055v1",
    "keywords": [
      "LLM",
      "Dataset"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating synthetic data, focusing on diversity and privacy aspects. It evaluates LLMs' capabilities and proposes a prompt-based approach to enhance diversity while preserving privacy.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.18053v1": {
    "title": "RECALLED: An Unbounded Resource Consumption Attack on Large\n  Vision-Language Models",
    "summary": "Resource Consumption Attacks (RCAs) have emerged as a significant threat to\nthe deployment of Large Language Models (LLMs). With the integration of vision\nmodalities, additional attack vectors exacerbate the risk of RCAs in large\nvision-language models (LVLMs). However, existing red-teaming studies have\nlargely overlooked visual inputs as a potential attack surface, resulting in\ninsufficient mitigation strategies against RCAs in LVLMs. To address this gap,\nwe propose RECALLED (\\textbf{RE}source \\textbf{C}onsumption \\textbf{A}ttack on\n\\textbf{L}arge Vision-\\textbf{L}anguag\\textbf{E} Mo\\textbf{D}els), the first\napproach for exploiting visual modalities to trigger unbounded RCAs\nred-teaming. First, we present \\textit{Vision Guided Optimization}, a\nfine-grained pixel-level optimization, to obtain \\textit{Output Recall}\nadversarial perturbations, which can induce repeating output. Then, we inject\nthe perturbations into visual inputs, triggering unbounded generations to\nachieve the goal of RCAs. Additionally, we introduce \\textit{Multi-Objective\nParallel Losses} to generate universal attack templates and resolve\noptimization conflicts when intending to implement parallel attacks. Empirical\nresults demonstrate that RECALLED increases service response latency by over 26\n$\\uparrow$, resulting in an additional 20\\% increase in GPU utilization and\nmemory consumption. Our study exposes security vulnerabilities in LVLMs and\nestablishes a red-teaming framework that can facilitate future defense\ndevelopment against RCAs.",
    "published": "2025-07-24T02:58:16Z",
    "updated": "2025-07-24T02:58:16Z",
    "id": "2507.18053v1",
    "authors": [
      "Haoran Gao",
      "Yuanhe Zhang",
      "Zhenhong Zhou",
      "Lei Jiang",
      "Fanyu Meng",
      "Yujia Xiao",
      "Kun Wang",
      "Yang Liu",
      "Junlan Feng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18053v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18053v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18053v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses security vulnerabilities and attacks on large vision-language models (LVLMs), which are a type of multimodal large language model (MLLM). It also touches on the broader context of large language models (LLMs) and their deployment risks.",
    "llm_cls_result": [
      "MLLM",
      "LLM",
      "VLA"
    ]
  },
  "2507.18044v1": {
    "title": "Synthetic Data Generation for Phrase Break Prediction with Large\n  Language Model",
    "summary": "Current approaches to phrase break prediction address crucial prosodic\naspects of text-to-speech systems but heavily rely on vast human annotations\nfrom audio or text, incurring significant manual effort and cost. Inherent\nvariability in the speech domain, driven by phonetic factors, further\ncomplicates acquiring consistent, high-quality data. Recently, large language\nmodels (LLMs) have shown success in addressing data challenges in NLP by\ngenerating tailored synthetic data while reducing manual annotation needs.\nMotivated by this, we explore leveraging LLM to generate synthetic phrase break\nannotations, addressing the challenges of both manual annotation and\nspeech-related tasks by comparing with traditional annotations and assessing\neffectiveness across multiple languages. Our findings suggest that LLM-based\nsynthetic data generation effectively mitigates data challenges in phrase break\nprediction and highlights the potential of LLMs as a viable solution for the\nspeech domain.",
    "published": "2025-07-24T02:45:03Z",
    "updated": "2025-07-24T02:45:03Z",
    "id": "2507.18044v1",
    "authors": [
      "Hoyeon Lee",
      "Sejung Son",
      "Ye-Eun Kang",
      "Jong-Hwan Kim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18044v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18044v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18044v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating synthetic data to address challenges in phrase break prediction, which is a specific application of LLMs in the speech domain.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.18043v1": {
    "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs\n  and VLMs",
    "summary": "Inference-time steering methods offer a lightweight alternative to\nfine-tuning large language models (LLMs) and vision-language models (VLMs) by\nmodifying internal activations at test time without updating model weights.\nHowever, most existing approaches rely on fixed, global intervention vectors,\noverlook the causal influence of individual input tokens, and fail to leverage\ninformative gradients from the model's logits, particularly in multimodal\nsettings where visual and textual inputs contribute unevenly. To address these\nlimitations, we introduce GrAInS, an inference-time steering approach that\noperates across both language-only and vision-language models and tasks. GrAInS\nuses contrastive, gradient-based attribution via Integrated Gradients to\nidentify the top-k most influential tokens, both positively and negatively\nattributed based on their contribution to preferred versus dispreferred\noutputs. These tokens are then used to construct directional steering vectors\nthat capture semantic shifts from undesirable to desirable behavior. During\ninference, GrAInS adjusts hidden activations at transformer layers guided by\ntoken-level attribution signals, and normalizes activations to preserve\nrepresentational scale. This enables fine-grained, interpretable, and modular\ncontrol over model behavior, without retraining or auxiliary supervision.\nEmpirically, GrAInS consistently outperforms both fine-tuning and existing\nsteering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using\nLlama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514\nwith LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all\nwhile preserving the model's fluency and general capabilities.",
    "published": "2025-07-24T02:34:13Z",
    "updated": "2025-07-24T02:34:13Z",
    "id": "2507.18043v1",
    "authors": [
      "Duy Nguyen",
      "Archiki Prasad",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18043v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18043v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18043v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for steering large language models (LLMs) and vision-language models (VLMs) during inference using gradient-based attribution, which involves modifying internal activations without updating model weights. This aligns with topics related to LLMs and VLMs, as well as reasoning and control over model behavior.",
    "llm_cls_result": [
      "LLM",
      "VLA",
      "Reasoning"
    ]
  },
  "2507.18033v1": {
    "title": "OpenNav: Open-World Navigation with Multimodal Large Language Models",
    "summary": "Pre-trained large language models (LLMs) have demonstrated strong\ncommon-sense reasoning abilities, making them promising for robotic navigation\nand planning tasks. However, despite recent progress, bridging the gap between\nlanguage descriptions and actual robot actions in the open-world, beyond merely\ninvoking limited predefined motion primitives, remains an open challenge. In\nthis work, we aim to enable robots to interpret and decompose complex language\ninstructions, ultimately synthesizing a sequence of trajectory points to\ncomplete diverse navigation tasks given open-set instructions and open-set\nobjects. We observe that multi-modal large language models (MLLMs) exhibit\nstrong cross-modal understanding when processing free-form language\ninstructions, demonstrating robust scene comprehension. More importantly,\nleveraging their code-generation capability, MLLMs can interact with\nvision-language perception models to generate compositional 2D bird-eye-view\nvalue maps, effectively integrating semantic knowledge from MLLMs with spatial\ninformation from maps to reinforce the robot's spatial understanding. To\nfurther validate our approach, we effectively leverage large-scale autonomous\nvehicle datasets (AVDs) to validate our proposed zero-shot vision-language\nnavigation framework in outdoor navigation tasks, demonstrating its capability\nto execute a diverse range of free-form natural language navigation\ninstructions while maintaining robustness against object detection errors and\nlinguistic ambiguities. Furthermore, we validate our system on a Husky robot in\nboth indoor and outdoor scenes, demonstrating its real-world robustness and\napplicability. Supplementary videos are available at\nhttps://trailab.github.io/OpenNav-website/",
    "published": "2025-07-24T02:05:28Z",
    "updated": "2025-07-24T02:05:28Z",
    "id": "2507.18033v1",
    "authors": [
      "Mingfeng Yuan",
      "Letian Wang",
      "Steven L. Waslander"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18033v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18033v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18033v1",
    "keywords": [
      "LLM",
      "Dataset"
    ],
    "cls_reason": "The paper discusses the use of Multimodal Large Language Models (MLLMs) for robotic navigation and planning tasks, integrating vision-language perception models and leveraging their code-generation capability. It also involves validation on large-scale autonomous vehicle datasets and real-world robotic applications.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "RL"
    ]
  },
  "2507.18031v1": {
    "title": "ViGText: Deepfake Image Detection with Vision-Language Model\n  Explanations and Graph Neural Networks",
    "summary": "The rapid rise of deepfake technology, which produces realistic but\nfraudulent digital content, threatens the authenticity of media. Traditional\ndeepfake detection approaches often struggle with sophisticated, customized\ndeepfakes, especially in terms of generalization and robustness against\nmalicious attacks. This paper introduces ViGText, a novel approach that\nintegrates images with Vision Large Language Model (VLLM) Text explanations\nwithin a Graph-based framework to improve deepfake detection. The novelty of\nViGText lies in its integration of detailed explanations with visual data, as\nit provides a more context-aware analysis than captions, which often lack\nspecificity and fail to reveal subtle inconsistencies. ViGText systematically\ndivides images into patches, constructs image and text graphs, and integrates\nthem for analysis using Graph Neural Networks (GNNs) to identify deepfakes.\nThrough the use of multi-level feature extraction across spatial and frequency\ndomains, ViGText captures details that enhance its robustness and accuracy to\ndetect sophisticated deepfakes. Extensive experiments demonstrate that ViGText\nsignificantly enhances generalization and achieves a notable performance boost\nwhen it detects user-customized deepfakes. Specifically, average F1 scores rise\nfrom 72.45% to 98.32% under generalization evaluation, and reflects the model's\nsuperior ability to generalize to unseen, fine-tuned variations of stable\ndiffusion models. As for robustness, ViGText achieves an increase of 11.1% in\nrecall compared to other deepfake detection approaches. When facing targeted\nattacks that exploit its graph-based architecture, ViGText limits\nclassification performance degradation to less than 4%. ViGText uses detailed\nvisual and textual analysis to set a new standard for detecting deepfakes,\nhelping ensure media authenticity and information integrity.",
    "published": "2025-07-24T02:04:58Z",
    "updated": "2025-07-24T02:04:58Z",
    "id": "2507.18031v1",
    "authors": [
      "Ahmad ALBarqawi",
      "Mahmoud Nazzal",
      "Issa Khalil",
      "Abdallah Khreishah",
      "NhatHai Phan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18031v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18031v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18031v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on integrating Vision Large Language Model (VLLM) text explanations with visual data for deepfake detection, which involves multimodal analysis and vision-language alignment. The use of Graph Neural Networks (GNNs) is secondary to the primary focus on vision-language integration.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.18028v1": {
    "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
    "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
    "published": "2025-07-24T02:00:09Z",
    "updated": "2025-07-24T02:00:09Z",
    "id": "2507.18028v1",
    "authors": [
      "Weizhi Fei",
      "Hao Shi",
      "Jing Xu",
      "Jingchen Peng",
      "Jiazheng Li",
      "Jingzhao Zhang",
      "Bo Bai",
      "Wei Han",
      "Zhenyuan Chen",
      "Xueyan Niu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18028v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18028v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18028v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for editing knowledge in large language models (LLMs) without large-scale training, which is relevant to the topic of LLMs and their memory mechanisms.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.18014v1": {
    "title": "Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning\n  Models",
    "summary": "Fine-tuning large language models (LLMs) for reasoning tasks using\nreinforcement learning methods like Group Relative Policy Optimization (GRPO)\nis computationally expensive. To address this, we propose a predictive\nframework that models training dynamics and helps optimize resource usage.\nThrough experiments on Llama and Qwen models (3B 8B), we derive an empirical\nscaling law based on model size, initial performance, and training progress.\nThis law predicts reward trajectories and identifies three consistent training\nphases: slow start, rapid improvement, and plateau. We find that training\nbeyond certain number of an epoch offers little gain, suggesting earlier\nstopping can significantly reduce compute without sacrificing performance. Our\napproach generalizes across model types, providing a practical guide for\nefficient GRPO-based fine-tuning.",
    "published": "2025-07-24T01:09:25Z",
    "updated": "2025-07-24T01:09:25Z",
    "id": "2507.18014v1",
    "authors": [
      "Datta Nimmaturi",
      "Vaishnavi Bhargava",
      "Rajat Ghosh",
      "Johnu George",
      "Debojyoti Dutta"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18014v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18014v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18014v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the fine-tuning of large language models (LLMs) using reinforcement learning methods (GRPO) and proposes a predictive framework for optimizing resource usage, which aligns with topics related to LLMs, Reinforcement Learning (RL), and Scaling.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Scaling"
    ]
  },
  "2507.18007v1": {
    "title": "Cloud Native System for LLM Inference Serving",
    "summary": "Large Language Models (LLMs) are revolutionizing numerous industries, but\ntheir substantial computational demands create challenges for efficient\ndeployment, particularly in cloud environments. Traditional approaches to\ninference serving often struggle with resource inefficiencies, leading to high\noperational costs, latency issues, and limited scalability. This article\nexplores how Cloud Native technologies, such as containerization,\nmicroservices, and dynamic scheduling, can fundamentally improve LLM inference\nserving. By leveraging these technologies, we demonstrate how a Cloud Native\nsystem enables more efficient resource allocation, reduces latency, and\nenhances throughput in high-demand scenarios. Through real-world evaluations\nusing Kubernetes-based autoscaling, we show that Cloud Native architectures can\ndynamically adapt to workload fluctuations, mitigating performance bottlenecks\nwhile optimizing LLM inference serving performance. This discussion provides a\nbroader perspective on how Cloud Native frameworks could reshape the future of\nscalable LLM inference serving, offering key insights for researchers,\npractitioners, and industry leaders in cloud computing and artificial\nintelligence.",
    "published": "2025-07-24T00:49:56Z",
    "updated": "2025-07-24T00:49:56Z",
    "id": "2507.18007v1",
    "authors": [
      "Minxian Xu",
      "Junhan Liao",
      "Jingfeng Wu",
      "Yiyuan He",
      "Kejiang Ye",
      "Chengzhong Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18007v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18007v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18007v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the deployment and optimization of Large Language Models (LLMs) in cloud environments, focusing on inference serving, resource allocation, and scalability. The core topics are related to LLM deployment and scaling.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.18006v1": {
    "title": "Unlock the Potential of Fine-grained LLM Serving via Dynamic Module\n  Scaling",
    "summary": "The rise of large language models (LLMs) has created new opportunities across\nvarious fields but has also introduced significant challenges in resource\nmanagement. Current LLM serving systems face a fundamental tension: balancing\nserving demands with limited resources while adapting to unpredictable traffic\npatterns. Static deployments lead to suboptimal resource utilization and\nperformance degradation under dynamic workloads. Furthermore, the high cost of\nadjusting instances hinders dynamic scaling, limiting the true potential of\nefficient LLM serving.\n  To address this, we propose CoCoServe, an elastic system that facilitates\ndynamic and fine-grained scaling. Its key innovation lies in the module-level\noperations for the replication and migration of LLM modules, such as decoder\nlayers and projections. Through a comprehensive analysis of the trade-offs\nassociated with these operations, we develop an auto-scaling mechanism that\ndynamically regulates module-level resource allocation and performance\noptimization, enabling a more cost-effective deployment of LLMs. Our evaluation\ndemonstrates that the scaling operations employed by CoCoServe exhibit\nexcellent scalability and can reduce costs by 46% while maintaining\navailability. Compared to state-of-the-art LLM serving systems (e.g., Hugging\nFace Transformers and vLLM), our approach reduces latency by 14%-75% and\nachieves 1.16x-4x throughput on average across different model sizes and\nworkloads.",
    "published": "2025-07-24T00:49:48Z",
    "updated": "2025-07-24T00:49:48Z",
    "id": "2507.18006v1",
    "authors": [
      "Jingfeng Wu",
      "Yiyuan He",
      "Minxian Xu",
      "Xitong Gao",
      "Kejiang Ye",
      "Chengzhong Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18006v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18006v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18006v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses dynamic scaling and resource management for large language models (LLMs), focusing on optimizing serving systems. This aligns with the topics of 'LLM' (research on large language models) and 'Scaling' (scaling laws and model scaling).",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.17927v1": {
    "title": "SMARTAPS: Tool-augmented LLMs for Operations Management",
    "summary": "Large language models (LLMs) present intriguing opportunities to enhance user\ninteraction with traditional algorithms and tools in real-world applications.\nAn advanced planning system (APS) is a sophisticated software that leverages\noptimization to help operations planners create, interpret, and modify an\noperational plan. While highly beneficial, many customers are priced out of\nusing an APS due to the ongoing costs of consultants responsible for\ncustomization and maintenance. To address the need for a more accessible APS\nexpressed by supply chain planners, we present SmartAPS, a conversational\nsystem built on a tool-augmented LLM. Our system provides operations planners\nwith an intuitive natural language chat interface, allowing them to query\ninformation, perform counterfactual reasoning, receive recommendations, and\nexecute scenario analysis to better manage their operation. A short video\ndemonstrating the system has been released: https://youtu.be/KtIrJjlDbyw",
    "published": "2025-07-23T20:53:40Z",
    "updated": "2025-07-23T20:53:40Z",
    "id": "2507.17927v1",
    "authors": [
      "Timothy Tin Long Yu",
      "Mahdi Mostajabdaveh",
      "Jabo Serge Byusa",
      "Rindra Ramamonjison",
      "Giuseppe Carenini",
      "Kun Mao",
      "Zirui Zhou",
      "Yong Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17927v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17927v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17927v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of tool-augmented LLMs for operations management, which involves enhancing user interaction with traditional algorithms and tools. This aligns with the topic of LLM (Large Language Models) and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.17924v1": {
    "title": "UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained\n  Population Transfer Prediction",
    "summary": "Accurate population flow prediction is essential for urban planning,\ntransportation management, and public health. Yet existing methods face key\nlimitations: traditional models rely on static spatial assumptions, deep\nlearning models struggle with cross-city generalization, and Large Language\nModels (LLMs) incur high computational costs while failing to capture spatial\nstructure. Moreover, many approaches sacrifice resolution by clustering Points\nof Interest (POIs) or restricting coverage to subregions, limiting their\nutility for city-wide analytics. We introduce UrbanPulse, a scalable deep\nlearning framework that delivers ultra-fine-grained, city-wide OD flow\npredictions by treating each POI as an individual node. It combines a temporal\ngraph convolutional encoder with a transformer-based decoder to model\nmulti-scale spatiotemporal dependencies. To ensure robust generalization across\nurban contexts, UrbanPulse employs a three-stage transfer learning strategy:\npretraining on large-scale urban graphs, cold-start adaptation, and\nreinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS\nrecords from three metropolitan areas in California, UrbanPulse achieves\nstate-of-the-art accuracy and scalability. Through efficient transfer learning,\nUrbanPulse takes a key step toward making high-resolution, AI-powered urban\nforecasting deployable in practice across diverse cities.",
    "published": "2025-07-23T20:44:25Z",
    "updated": "2025-07-23T20:44:25Z",
    "id": "2507.17924v1",
    "authors": [
      "Hongrong Yang",
      "Markus Schlaepfer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17924v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17924v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17924v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of deep learning and reinforcement learning for urban population transfer prediction, but it does not primarily focus on LLMs, multimodal models, or other core topics from the provided list. The mention of LLMs is in the context of their limitations rather than a focus of the research.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.17842v1": {
    "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping\n  via Reinforcement Learning",
    "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in\ngenerating 'believable human-like' behavior in web environments. Prior work has\nexplored augmenting training data with LLM-synthesized rationales and applying\nsupervised fine-tuning (SFT) to enhance reasoning ability, which in turn can\nimprove downstream action prediction. However, the performance of such\napproaches remains inherently bounded by the reasoning capabilities of the\nmodel used to generate the rationales. In this paper, we introduce Shop-R1, a\nnovel reinforcement learning (RL) framework aimed at enhancing the reasoning\nability of LLMs for simulation of real human behavior in online shopping\nenvironments Specifically, Shop-R1 decomposes the human behavior simulation\ntask into two stages: rationale generation and action prediction, each guided\nby distinct reward signals. For rationale generation, we leverage internal\nmodel signals (e.g., logit distributions) to guide the reasoning process in a\nself-supervised manner. For action prediction, we propose a hierarchical reward\nstructure with difficulty-aware scaling to prevent reward hacking and enable\nfine-grained reward assignment. This design evaluates both high-level action\ntypes and the correctness of fine-grained sub-action details (attributes and\nvalues), rewarding outputs proportionally to their difficulty. Experimental\nresults show that our method achieves a relative improvement of over 65%\ncompared to the baseline.",
    "published": "2025-07-23T18:10:43Z",
    "updated": "2025-07-23T18:10:43Z",
    "id": "2507.17842v1",
    "authors": [
      "Yimeng Zhang",
      "Tian Wang",
      "Jiri Gesi",
      "Ziyi Wang",
      "Yuxuan Lu",
      "Jiacheng Lin",
      "Sinong Zhan",
      "Vianne Gao",
      "Ruochen Jiao",
      "Junze Liu",
      "Kun Qian",
      "Yuxin Tang",
      "Ran Xue",
      "Houyu Zhang",
      "Qingjun Cui",
      "Yufan Guo",
      "Dakuo Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17842v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17842v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17842v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using reinforcement learning (RL) to enhance the reasoning ability of Large Language Models (LLMs) for simulating human behavior in online shopping environments. It involves rationale generation and action prediction guided by distinct reward signals, which aligns with the topics of LLM research and Reinforcement Learning (RL).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.17722v1": {
    "title": "BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems",
    "summary": "Large language models (LLMs) are growingly extended to process multimodal\ndata such as text and video simultaneously. Their remarkable performance in\nunderstanding what is shown in images is surpassing specialized neural networks\n(NNs) such as Yolo that is supporting only a well-formed but very limited\nvocabulary, ie., objects that they are able to detect. When being\nnon-restricted, LLMs and in particular state-of-the-art vision language models\n(VLMs) show impressive performance to describe even complex traffic situations.\nThis is making them potentially suitable components for automotive perception\nsystems to support the understanding of complex traffic situations or edge case\nsituation. However, LLMs and VLMs are prone to hallucination, which mean to\neither potentially not seeing traffic agents such as vulnerable road users who\nare present in a situation, or to seeing traffic agents who are not there in\nreality. While the latter is unwanted making an ADAS or autonomous driving\nsystems (ADS) to unnecessarily slow down, the former could lead to disastrous\ndecisions from an ADS. In our work, we are systematically assessing the\nperformance of 3 state-of-the-art VLMs on a diverse subset of traffic\nsituations sampled from the Waymo Open Dataset to support safety guardrails for\ncapturing such hallucinations in VLM-supported perception systems. We observe\nthat both, proprietary and open VLMs exhibit remarkable image understanding\ncapabilities even paying thorough attention to fine details sometimes difficult\nto spot for us humans. However, they are also still prone to making up elements\nin their descriptions to date requiring hallucination detection strategies such\nas BetterCheck that we propose in our work.",
    "published": "2025-07-23T17:32:17Z",
    "updated": "2025-07-23T17:32:17Z",
    "id": "2507.17722v1",
    "authors": [
      "Malsha Ashani Mahawatta Dona",
      "Beatriz Cabrero-Daniel",
      "Yinan Yu",
      "Christian Berger"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17722v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17722v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17722v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the performance and safety issues of Vision Language Models (VLMs) in automotive perception systems, focusing on their hallucination problems and proposing a solution. The core topics are related to VLMs and their application in vision-language tasks.",
    "llm_cls_result": [
      "VLA",
      "MLLM"
    ]
  },
  "2507.17718v1": {
    "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an\n  AI Interviewer",
    "summary": "With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.",
    "published": "2025-07-23T17:30:14Z",
    "updated": "2025-07-23T17:30:14Z",
    "id": "2507.17718v1",
    "authors": [
      "Danny D. Leybzon",
      "Shreyas Tirumala",
      "Nishant Jain",
      "Summer Gillen",
      "Michael Jackson",
      "Cameron McPhee",
      "Jennifer Schmidt"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17718v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17718v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17718v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLM) in automating quantitative data collection through AI telephone surveying, which involves LLM technologies for natural and adaptive respondent interactions.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.17706v1": {
    "title": "HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter\n  Merging",
    "summary": "Large language models (LLMs) often leverage adapters, such as low-rank-based\nadapters, to achieve strong performance on downstream tasks. However, storing a\nseparate adapter for each task significantly increases memory requirements,\nposing a challenge for resource-constrained environments such as mobile\ndevices. Although model merging techniques can reduce storage costs, they\ntypically result in substantial performance degradation. In this work, we\nintroduce HydraOpt, a new model merging technique that capitalizes on the\ninherent similarities between the matrices of low-rank adapters. Unlike\nexisting methods that produce a fixed trade-off between storage size and\nperformance, HydraOpt allows us to navigate this spectrum of efficiency and\nperformance. Our experiments show that HydraOpt significantly reduces storage\nsize (48% reduction) compared to storing all adapters, while achieving\ncompetitive performance (0.2-1.8% drop). Furthermore, it outperforms existing\nmerging techniques in terms of performance at the same or slightly worse\nstorage efficiency.",
    "published": "2025-07-23T17:12:19Z",
    "updated": "2025-07-23T17:12:19Z",
    "id": "2507.17706v1",
    "authors": [
      "Taha Ceritli",
      "Ondrej Bohdal",
      "Mete Ozay",
      "Jijoong Moon",
      "Kyeng-Hun Lee",
      "Hyeonmok Ko",
      "Umberto Michieli"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17706v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17706v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17706v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses adapter merging techniques for large language models (LLMs) to optimize efficiency and performance, which is relevant to LLM research and scaling.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.17695v1": {
    "title": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks",
    "summary": "Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance.",
    "published": "2025-07-23T17:01:23Z",
    "updated": "2025-07-23T17:01:23Z",
    "id": "2507.17695v1",
    "authors": [
      "Ilias Chatzistefanidis",
      "Navid Nikaein"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17695v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17695v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17695v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with real-time optimization algorithms to create trustworthy AGI-driven networks, which involves both AGI and LLM technologies.",
    "llm_cls_result": [
      "AGI",
      "LLM"
    ]
  },
  "2507.17680v1": {
    "title": "Simulating multiple human perspectives in socio-ecological systems using\n  large language models",
    "summary": "Understanding socio-ecological systems requires insights from diverse\nstakeholder perspectives, which are often hard to access. To enable\nalternative, simulation-based exploration of different stakeholder\nperspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)\nmodelling framework. HoPeS employs agents powered by large language models\n(LLMs) to represent various stakeholders; users can step into the agent roles\nto experience perspectival differences. A simulation protocol serves as a\n\"scaffold\" to streamline multiple perspective-taking simulations, supporting\nusers in reflecting on, transitioning between, and integrating across\nperspectives. A prototype system is developed to demonstrate HoPeS in the\ncontext of institutional dynamics and land use change, enabling both\nnarrative-driven and numerical experiments. In an illustrative experiment, a\nuser successively adopts the perspectives of a system observer and a researcher\n- a role that analyses data from the embedded land use model to inform\nevidence-based decision-making for other LLM agents representing various\ninstitutions. Despite the user's effort to recommend technically sound\npolicies, discrepancies persist between the policy recommendation and\nimplementation due to stakeholders' competing advocacies, mirroring real-world\nmisalignment between researcher and policymaker perspectives. The user's\nreflection highlights the subjective feelings of frustration and disappointment\nas a researcher, especially due to the challenge of maintaining political\nneutrality while attempting to gain political influence. Despite this, the user\nexhibits high motivation to experiment with alternative narrative framing\nstrategies, suggesting the system's potential in exploring different\nperspectives. Further system and protocol refinement are likely to enable new\nforms of interdisciplinary collaboration in socio-ecological simulations.",
    "published": "2025-07-23T16:42:51Z",
    "updated": "2025-07-23T16:42:51Z",
    "id": "2507.17680v1",
    "authors": [
      "Yongchao Zeng",
      "Calum Brown",
      "Ioannis Kyriakou",
      "Ronja Hotz",
      "Mark Rounsevell"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17680v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17680v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17680v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to simulate diverse stakeholder perspectives in socio-ecological systems, which aligns with the topic of LLMs and their applications in modeling and simulation.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.17618v1": {
    "title": "A Hybrid Early-Exit Algorithm for Large Language Models Based on Space\n  Alignment Decoding (SPADE)",
    "summary": "Large language models are computationally expensive due to their deep\nstructures. Prior research has shown that intermediate layers contain\nsufficient information to generate accurate answers, leading to the development\nof early-exit algorithms that reduce inference costs by terminating computation\nat earlier layers. However, these methods often suffer from poor performance\ndue to misalignment between intermediate and output layer representations that\nlead to decoding inaccuracy. To address these challenges, we propose SPADE\n(SPace Alignment DEcoding), a novel decoding method that aligns intermediate\nlayer representations with the output layer by propagating a minimally reduced\nsequence consisting of only the start token and the answer token. We further\noptimize the early-exit decision-making process by training a linear\napproximation of SPADE that computes entropy-based confidence metrics. Putting\nthem together, we create a hybrid early-exit algorithm that monitors confidence\nlevels and stops inference at intermediate layers while using SPADE to generate\nhigh-quality outputs. This approach significantly reduces inference costs\nwithout compromising accuracy, offering a scalable and efficient solution for\ndeploying large language models in real-world applications.",
    "published": "2025-07-23T15:49:03Z",
    "updated": "2025-07-23T15:49:03Z",
    "id": "2507.17618v1",
    "authors": [
      "Bowen Zheng",
      "Ming Ma",
      "Zhongqiao Lin",
      "Tianming Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17618v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17618v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17618v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a novel decoding method (SPADE) for large language models to reduce inference costs by early-exit algorithms, which is directly related to the optimization and efficiency of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.17543v1": {
    "title": "Anticipate, Simulate, Reason (ASR): A Comprehensive Generative AI\n  Framework for Combating Messaging Scams",
    "summary": "The rapid growth of messaging scams creates an escalating challenge for user\nsecurity and financial safety. In this paper, we present the Anticipate,\nSimulate, Reason (ASR) framework, a generative AI method that enables users to\nproactively identify and comprehend scams within instant messaging platforms.\nUsing large language models, ASR predicts scammer responses, creates realistic\nscam conversations, and delivers real-time, interpretable support to end-users.\nWe develop ScamGPT-J, a domain-specific language model fine-tuned on a new,\nhigh-quality dataset of scam conversations covering multiple scam types.\nThorough experimental evaluation shows that the ASR framework substantially\nenhances scam detection, particularly in challenging contexts such as job\nscams, and uncovers important demographic patterns in user vulnerability and\nperceptions of AI-generated assistance. Our findings reveal a contradiction\nwhere those most at risk are often least receptive to AI support, emphasizing\nthe importance of user-centered design in AI-driven fraud prevention. This work\nadvances both the practical and theoretical foundations for interpretable,\nhuman-centered AI systems in combating evolving digital threats.",
    "published": "2025-07-23T14:20:09Z",
    "updated": "2025-07-23T14:20:09Z",
    "id": "2507.17543v1",
    "authors": [
      "Xue Wen Tan",
      "Kenneth See",
      "Stanley Kok"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17543v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17543v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17543v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for scam detection and user support, which aligns with the 'LLM' topic. It also involves reasoning abilities in LLMs for scam comprehension, fitting the 'Reasoning' topic. Additionally, the development of a domain-specific language model and dataset suggests relevance to the 'Dataset' topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Dataset"
    ]
  },
  "2507.17539v1": {
    "title": "Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration\n  Through Clinical Cognitive Chain Reasoning",
    "summary": "Multimodal large language models (MLLMs) demonstrate significant potential in\nthe field of medical diagnosis. However, they face critical challenges in\nspecialized domains such as ophthalmology, particularly the fragmentation of\nannotation granularity and inconsistencies in clinical reasoning logic, which\nhinder precise cross-modal understanding. This paper introduces FundusExpert,\nan ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning\ncapabilities, along with FundusGen, a dataset constructed through the\nintelligent Fundus-Engine system. Fundus-Engine automates localization and\nleverages MLLM-based semantic expansion to integrate global disease\nclassification, local object detection, and fine-grained feature analysis\nwithin a single fundus image. Additionally, by constructing a clinically\naligned cognitive chain, it guides the model to generate interpretable\nreasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,\nachieves the best performance in ophthalmic question-answering tasks,\nsurpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in\nzero-shot report generation tasks, achieving a clinical consistency of 77.0%,\nsignificantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling\nlaw between data quality and model capability ($L \\propto N^{0.068}$),\ndemonstrating that the cognitive alignment annotations in FundusGen enhance\ndata utilization efficiency. By integrating region-level localization with\ndiagnostic reasoning chains, our work develops a scalable, clinically-aligned\nMLLM and explores a pathway toward bridging the visual-language gap in specific\nMLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.",
    "published": "2025-07-23T14:19:30Z",
    "updated": "2025-07-23T14:19:30Z",
    "id": "2507.17539v1",
    "authors": [
      "Xinyao Liu",
      "Diping Song"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17539v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17539v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17539v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a Multimodal Large Language Model (MLLM) specialized for ophthalmology, integrating positioning-diagnosis reasoning and leveraging a dataset constructed through an intelligent system. It also discusses scaling laws and clinical reasoning, aligning with the topics of MLLM, Reasoning, and Scaling.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning",
      "Scaling"
    ]
  },
  "2507.17795v1": {
    "title": "LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level\n  Mobile Traffic Prediction",
    "summary": "Service-level mobile traffic prediction for individual users is essential for\nnetwork efficiency and quality of service enhancement. However, current\nprediction methods are limited in their adaptability across different urban\nenvironments and produce inaccurate results due to the high uncertainty in\npersonal traffic patterns, the lack of detailed environmental context, and the\ncomplex dependencies among different network services. These challenges demand\nadvanced modeling techniques that can capture dynamic traffic distributions and\nrich environmental features. Inspired by the recent success of diffusion models\nin distribution modeling and Large Language Models (LLMs) in contextual\nunderstanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model\n(LSDM). LSDM integrates the generative power of diffusion models with the\nadaptive learning capabilities of transformers, augmented by the ability to\ncapture multimodal environmental information for modeling service-level\npatterns and dynamics. Extensive evaluations on real-world service-level\ndatasets demonstrate that the model excels in traffic usage predictions,\nshowing outstanding generalization and adaptability. After incorporating\ncontextual information via LLM, the performance improves by at least 2.83% in\nterms of the coefficient of determination. Compared to models of a similar\ntype, such as CSDI, the root mean squared error can be reduced by at least\n8.29%. The code and dataset will be available at:\nhttps://github.com/SoftYuaneR/LSDM.",
    "published": "2025-07-23T14:01:16Z",
    "updated": "2025-07-23T14:01:16Z",
    "id": "2507.17795v1",
    "authors": [
      "Shiyuan Zhang",
      "Tong Li",
      "Zhu Xiao",
      "Hongyang Du",
      "Kaibin Huang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17795v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17795v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17795v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with diffusion models for mobile traffic prediction, focusing on the contextual understanding and generative capabilities of LLMs. The primary focus is on the application of LLMs in a specific domain rather than core LLM research topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.17518v1": {
    "title": "Enabling Cyber Security Education through Digital Twins and Generative\n  AI",
    "summary": "Digital Twins (DTs) are gaining prominence in cybersecurity for their ability\nto replicate complex IT (Information Technology), OT (Operational Technology),\nand IoT (Internet of Things) infrastructures, allowing for real time\nmonitoring, threat analysis, and system simulation. This study investigates how\nintegrating DTs with penetration testing tools and Large Language Models (LLMs)\ncan enhance cybersecurity education and operational readiness. By simulating\nrealistic cyber environments, this approach offers a practical, interactive\nframework for exploring vulnerabilities and defensive strategies. At the core\nof this research is the Red Team Knife (RTK), a custom penetration testing\ntoolkit aligned with the Cyber Kill Chain model. RTK is designed to guide\nlearners through key phases of cyberattacks, including reconnaissance,\nexploitation, and response within a DT powered ecosystem. The incorporation of\nLarge Language Models (LLMs) further enriches the experience by providing\nintelligent, real-time feedback, natural language threat explanations, and\nadaptive learning support during training exercises. This combined DT LLM\nframework is currently being piloted in academic settings to develop hands on\nskills in vulnerability assessment, threat detection, and security operations.\nInitial findings suggest that the integration significantly improves the\neffectiveness and relevance of cybersecurity training, bridging the gap between\ntheoretical knowledge and real-world application. Ultimately, the research\ndemonstrates how DTs and LLMs together can transform cybersecurity education to\nmeet evolving industry demands.",
    "published": "2025-07-23T13:55:35Z",
    "updated": "2025-07-23T13:55:35Z",
    "id": "2507.17518v1",
    "authors": [
      "Vita Santa Barletta",
      "Vito Bavaro",
      "Miriana Calvano",
      "Antonio Curci",
      "Antonio Piccinno",
      "Davide Pio Posa"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17518v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17518v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17518v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with Digital Twins for cybersecurity education, focusing on enhancing training through real-time feedback and adaptive learning support. The core topic is the application of LLMs in a specific domain (cybersecurity education), which aligns with the 'LLM' category.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.17515v1": {
    "title": "URPO: A Unified Reward & Policy Optimization Framework for Large\n  Language Models",
    "summary": "Large-scale alignment pipelines typically pair a policy model with a\nseparately trained reward model whose parameters remain frozen during\nreinforcement learning (RL). This separation creates a complex,\nresource-intensive pipeline and suffers from a performance ceiling due to a\nstatic reward signal. We propose a novel framework, Unified Reward & Policy\nOptimization (URPO), that unifies instruction-following (\"player\") and reward\nmodeling (\"referee\") within a single model and a single training phase. Our\nmethod recasts all alignment data-including preference pairs, verifiable\nreasoning, and open-ended instructions-into a unified generative format\noptimized by a single Group-Relative Policy Optimization (GRPO) loop. This\nenables the model to learn from ground-truth preferences and verifiable logic\nwhile simultaneously generating its own rewards for open-ended tasks.\nExperiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified\nmodel significantly outperforms a strong baseline using a separate generative\nreward model, boosting the instruction-following score on AlpacaEval from 42.24\nto 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,\nURPO cultivates a superior internal evaluator as a byproduct of training,\nachieving a RewardBench score of 85.15 and surpassing the dedicated reward\nmodel it replaces (83.55). By eliminating the need for a separate reward model\nand fostering a co-evolutionary dynamic between generation and evaluation, URPO\npresents a simpler, more efficient, and more effective path towards robustly\naligned language models.",
    "published": "2025-07-23T13:52:27Z",
    "updated": "2025-07-23T13:52:27Z",
    "id": "2507.17515v1",
    "authors": [
      "Songshuo Lu",
      "Hua Wang",
      "Zhi Chen",
      "Yaohua Tang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17515v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17515v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17515v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a unified framework for reward and policy optimization in large language models, which involves reinforcement learning and alignment strategies. This directly relates to topics on Reinforcement Learning (RL) and Large Language Models (LLM).",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.17477v1": {
    "title": "An Uncertainty-Driven Adaptive Self-Alignment Framework for Large\n  Language Models",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\ninstruction following and general-purpose reasoning. However, achieving\nhigh-quality alignment with human intent and safety norms without human\nannotations remains a fundamental challenge. In this work, we propose an\nUncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to\nimprove LLM alignment in a fully automated manner. UDASA first generates\nmultiple responses for each input and quantifies output uncertainty across\nthree dimensions: semantics, factuality, and value alignment. Based on these\nuncertainty scores, the framework constructs preference pairs and categorizes\ntraining samples into three stages, conservative, moderate, and exploratory,\naccording to their uncertainty difference. The model is then optimized\nprogressively across these stages. In addition, we conduct a series of\npreliminary studies to validate the core design assumptions and provide strong\nempirical motivation for the proposed framework. Experimental results show that\nUDASA outperforms existing alignment methods across multiple tasks, including\nharmlessness, helpfulness, truthfulness, and controlled sentiment generation,\nsignificantly improving model performance.",
    "published": "2025-07-23T13:00:00Z",
    "updated": "2025-07-23T13:00:00Z",
    "id": "2507.17477v1",
    "authors": [
      "Haoran Sun",
      "Zekun Zhang",
      "Shaoning Zeng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17477v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17477v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17477v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the alignment of Large Language Models (LLMs) with human intent and safety norms using an automated framework, which involves uncertainty quantification and progressive optimization. This aligns with research on LLMs and their alignment techniques, particularly in the context of Reinforcement Learning with Human Feedback (RLHF).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.17472v1": {
    "title": "BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision\n  Assessment on Semi-Structured Profiles",
    "summary": "Human decision-making in high-stakes domains often relies on expertise and\nheuristics, but is vulnerable to hard-to-detect cognitive biases that threaten\nfairness and long-term outcomes. This work presents a novel approach to\nenhancing complex decision-making workflows through the integration of\nhierarchical learning alongside various enhancements. Focusing on university\nadmissions as a representative high-stakes domain, we propose BGM-HAN, an\nenhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,\ndesigned to effectively model semi-structured applicant data. BGM-HAN captures\nmulti-level representations that are crucial for nuanced assessment, improving\nboth interpretability and predictive performance. Experimental results on real\nadmissions data demonstrate that our proposed model significantly outperforms\nboth state-of-the-art baselines from traditional machine learning to large\nlanguage models, offering a promising framework for augmenting decision-making\nin domains where structure, context, and fairness matter. Source code is\navailable at: https://github.com/junhua/bgm-han.",
    "published": "2025-07-23T12:52:38Z",
    "updated": "2025-07-23T12:52:38Z",
    "id": "2507.17472v1",
    "authors": [
      "Junhua Liu",
      "Roy Ka-Wei Lee",
      "Kwan Hui Lim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17472v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17472v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17472v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a hierarchical attention network for decision assessment in high-stakes domains, which does not directly align with the provided topics related to LLMs, RL, MLLM, etc. The work is more about enhancing decision-making workflows using hierarchical learning and attention mechanisms, which is not covered by the given topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.17417v1": {
    "title": "A Comprehensive Evaluation on Quantization Techniques for Large Language\n  Models",
    "summary": "For large language models (LLMs), post-training quantization (PTQ) can\nsignificantly reduce memory footprint and computational overhead. Model\nquantization is a rapidly evolving research field. Though many papers have\nreported breakthrough performance, they may not conduct experiments on the same\nground since one quantization method usually contains multiple components. In\naddition, analyzing the theoretical connections among existing methods is\ncrucial for in-depth understanding. To bridge these gaps, we conduct an\nextensive review of state-of-the-art methods and perform comprehensive\nevaluations on the same ground to ensure fair comparisons. To our knowledge,\nthis fair and extensive investigation remains critically important yet\nunderexplored. To better understand the theoretical connections, we decouple\nthe published quantization methods into two steps: pre-quantization\ntransformation and quantization error mitigation. We define the former as a\npreprocessing step applied before quantization to reduce the impact of\noutliers, making the data distribution flatter and more suitable for\nquantization. Quantization error mitigation involves techniques that offset the\nerrors introduced during quantization, thereby enhancing model performance. We\nevaluate and analyze the impact of different components of quantization\nmethods. Additionally, we analyze and evaluate the latest MXFP4 data format and\nits performance. Our experimental results demonstrate that optimized rotation\nand scaling yield the best performance for pre-quantization transformation, and\ncombining low-rank compensation with GPTQ occasionally outperforms using GPTQ\nalone for quantization error mitigation. Furthermore, we explore the potential\nof the latest MXFP4 quantization and reveal that the optimal pre-quantization\ntransformation strategy for INT4 does not generalize well to MXFP4, inspiring\nfurther investigation.",
    "published": "2025-07-23T11:21:21Z",
    "updated": "2025-07-23T11:21:21Z",
    "id": "2507.17417v1",
    "authors": [
      "Yutong Liu",
      "Cairong Zhao",
      "Guosheng Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17417v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17417v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17417v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on quantization techniques for Large Language Models (LLMs), which is a specific aspect of LLM research. It does not directly address topics like RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.17389v1": {
    "title": "Investigating Training Data Detection in AI Coders",
    "summary": "Recent advances in code large language models (CodeLLMs) have made them\nindispensable tools in modern software engineering. However, these models\noccasionally produce outputs that contain proprietary or sensitive code\nsnippets, raising concerns about potential non-compliant use of training data,\nand posing risks to privacy and intellectual property. To ensure responsible\nand compliant deployment of CodeLLMs, training data detection (TDD) has become\na critical task. While recent TDD methods have shown promise in natural\nlanguage settings, their effectiveness on code data remains largely\nunderexplored. This gap is particularly important given code's structured\nsyntax and distinct similarity criteria compared to natural language. To\naddress this, we conduct a comprehensive empirical study of seven\nstate-of-the-art TDD methods on source code data, evaluating their performance\nacross eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a\nfunction-level benchmark dataset comprising 9,000 code samples in three\nprogramming languages, each explicitly labeled as either included or excluded\nfrom CodeLLM training. Beyond evaluation on the original CodeSnitch, we design\ntargeted mutation strategies to test the robustness of TDD methods under three\ndistinct settings. These mutation strategies are grounded in the\nwell-established Type-1 to Type-4 code clone detection taxonomy. Our study\nprovides a systematic assessment of current TDD techniques for code and offers\ninsights to guide the development of more effective and robust detection\nmethods in the future.",
    "published": "2025-07-23T10:34:22Z",
    "updated": "2025-07-23T10:34:22Z",
    "id": "2507.17389v1",
    "authors": [
      "Tianlin Li",
      "Yunxiang Wei",
      "Zhiming Li",
      "Aishan Liu",
      "Qing Guo",
      "Xianglong Liu",
      "Dongning Sun",
      "Yang Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17389v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17389v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17389v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on training data detection in code large language models (CodeLLMs), which involves evaluating and benchmarking methods for detecting proprietary or sensitive code snippets in training data. This aligns with the topics of 'LLM' (as it involves CodeLLMs) and 'Dataset' (as it introduces a benchmark dataset for evaluation).",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.17379v1": {
    "title": "Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained\n  Models",
    "summary": "Open-vocabulary mobile manipulation (OVMM) that involves the handling of\nnovel and unseen objects across different workspaces remains a significant\nchallenge for real-world robotic applications. In this paper, we propose a\nnovel Language-conditioned Open-Vocabulary Mobile Manipulation framework, named\nLOVMM, incorporating the large language model (LLM) and vision-language model\n(VLM) to tackle various mobile manipulation tasks in household environments.\nOur approach is capable of solving various OVMM tasks with free-form natural\nlanguage instructions (e.g. \"toss the food boxes on the office room desk to the\ntrash bin in the corner\", and \"pack the bottles from the bed to the box in the\nguestroom\"). Extensive experiments simulated in complex household environments\nshow strong zero-shot generalization and multi-task learning abilities of\nLOVMM. Moreover, our approach can also generalize to multiple tabletop\nmanipulation tasks and achieve better success rates compared to other\nstate-of-the-art methods.",
    "published": "2025-07-23T10:23:15Z",
    "updated": "2025-07-23T10:23:15Z",
    "id": "2507.17379v1",
    "authors": [
      "Shen Tan",
      "Dong Zhou",
      "Xiangyu Shao",
      "Junqiao Wang",
      "Guanghui Sun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17379v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17379v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17379v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLM) and vision-language models (VLM) for mobile manipulation tasks, which aligns with the topics of LLM and VLA (Vision-Language Action). The focus on solving tasks with natural language instructions and generalization abilities also touches on the Reasoning topic.",
    "llm_cls_result": [
      "LLM",
      "VLA",
      "Reasoning"
    ]
  },
  "2507.17365v1": {
    "title": "DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via\n  Multi-Reward Reinforcement Learning",
    "summary": "Multi-step agentic retrieval systems based on large language models (LLMs)\nhave demonstrated remarkable performance in complex information search tasks.\nHowever, these systems still face significant challenges in practical\napplications, particularly in generating factually inconsistent intermediate\nqueries and inefficient search trajectories, which can lead to reasoning\ndeviations or redundant computations. To address these issues, we propose\nDynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs\nand multi-reward reinforcement learning (RL). Specifically, our system\nleverages knowledge graphs as external structured knowledge to guide the search\nprocess by explicitly modeling entity relationships, thereby ensuring factual\nconsistency in intermediate queries and mitigating biases from irrelevant\ninformation. Furthermore, we employ a multi-reward RL framework for\nfine-grained control over training objectives such as retrieval accuracy,\nefficiency, and response quality. This framework promotes the generation of\nhigh-quality intermediate queries and comprehensive final answers, while\ndiscouraging unnecessary exploration and minimizing information omissions or\nredundancy. Experimental results demonstrate that our approach achieves\nstate-of-the-art answer accuracy on six multi-hop question answering datasets,\nmatching frontier LLMs while using only small-scale models and limited\ncomputational resources. Furthermore, our approach demonstrates strong\ngeneralization and robustness across diverse retrieval environments and\nlarger-scale models, highlighting its broad applicability.",
    "published": "2025-07-23T09:58:31Z",
    "updated": "2025-07-23T09:58:31Z",
    "id": "2507.17365v1",
    "authors": [
      "Chuzhan Hao",
      "Wenfeng Feng",
      "Yuewei Zhang",
      "Hao Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17365v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17365v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17365v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a search agent enhanced by dynamic knowledge graphs and multi-reward reinforcement learning (RL), which involves LLMs and RL techniques. It focuses on improving retrieval accuracy, efficiency, and response quality through RL, aligning with the RL and LLM topics.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.17787v1": {
    "title": "Hyperbolic Deep Learning for Foundation Models: A Survey",
    "summary": "Foundation models pre-trained on massive datasets, including large language\nmodels (LLMs), vision-language models (VLMs), and large multimodal models, have\ndemonstrated remarkable success in diverse downstream tasks. However, recent\nstudies have shown fundamental limitations of these models: (1) limited\nrepresentational capacity, (2) lower adaptability, and (3) diminishing\nscalability. These shortcomings raise a critical question: is Euclidean\ngeometry truly the optimal inductive bias for all foundation models, or could\nincorporating alternative geometric spaces enable models to better align with\nthe intrinsic structure of real-world data and improve reasoning processes?\nHyperbolic spaces, a class of non-Euclidean manifolds characterized by\nexponential volume growth with respect to distance, offer a mathematically\ngrounded solution. These spaces enable low-distortion embeddings of\nhierarchical structures (e.g., trees, taxonomies) and power-law distributions\nwith substantially fewer dimensions compared to Euclidean counterparts. Recent\nadvances have leveraged these properties to enhance foundation models,\nincluding improving LLMs' complex reasoning ability, VLMs' zero-shot\ngeneralization, and cross-modal semantic alignment, while maintaining parameter\nefficiency. This paper provides a comprehensive review of hyperbolic neural\nnetworks and their recent development for foundation models. We further outline\nkey challenges and research directions to advance the field.",
    "published": "2025-07-23T09:50:17Z",
    "updated": "2025-07-23T09:50:17Z",
    "id": "2507.17787v1",
    "authors": [
      "Neil He",
      "Hiren Madhu",
      "Ngoc Bui",
      "Menglin Yang",
      "Rex Ying"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17787v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17787v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17787v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses foundation models, including large language models (LLMs) and vision-language models (VLMs), and explores the use of hyperbolic spaces to enhance their capabilities. It touches on topics such as representational capacity, adaptability, and scalability, which are relevant to the study of LLMs and VLMs.",
    "llm_cls_result": [
      "LLM",
      "VLA"
    ]
  },
  "2507.17290v1": {
    "title": "Exploring the Potential of LLMs for Serendipity Evaluation in\n  Recommender Systems",
    "summary": "Serendipity plays a pivotal role in enhancing user satisfaction within\nrecommender systems, yet its evaluation poses significant challenges due to its\ninherently subjective nature and conceptual ambiguity. Current algorithmic\napproaches predominantly rely on proxy metrics for indirect assessment, often\nfailing to align with real user perceptions, thus creating a gap. With large\nlanguage models (LLMs) increasingly revolutionizing evaluation methodologies\nacross various human annotation tasks, we are inspired to explore a core\nresearch proposition: Can LLMs effectively simulate human users for serendipity\nevaluation? To address this question, we conduct a meta-evaluation on two\ndatasets derived from real user studies in the e-commerce and movie domains,\nfocusing on three key aspects: the accuracy of LLMs compared to conventional\nproxy metrics, the influence of auxiliary data on LLM comprehension, and the\nefficacy of recently popular multi-LLM techniques. Our findings indicate that\neven the simplest zero-shot LLMs achieve parity with, or surpass, the\nperformance of conventional metrics. Furthermore, multi-LLM techniques and the\nincorporation of auxiliary data further enhance alignment with human\nperspectives. Based on our findings, the optimal evaluation by LLMs yields a\nPearson correlation coefficient of 21.5\\% when compared to the results of the\nuser study. This research implies that LLMs may serve as potentially accurate\nand cost-effective evaluators, introducing a new paradigm for serendipity\nevaluation in recommender systems.",
    "published": "2025-07-23T07:51:56Z",
    "updated": "2025-07-23T07:51:56Z",
    "id": "2507.17290v1",
    "authors": [
      "Li Kang",
      "Yuhan Zhao",
      "Li Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17290v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17290v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17290v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for evaluating serendipity in recommender systems, focusing on their performance compared to conventional metrics and the impact of auxiliary data and multi-LLM techniques. The core topic is the application of LLMs in a specific evaluation context.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.17288v1": {
    "title": "Triple X: A LLM-Based Multilingual Speech Recognition System for the\n  INTERSPEECH2025 MLC-SLM Challenge",
    "summary": "This paper describes our Triple X speech recognition system submitted to Task\n1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)\nChallenge. Our work focuses on optimizing speech recognition accuracy in\nmultilingual conversational scenarios through an innovative encoder-adapter-LLM\narchitecture. This framework harnesses the powerful reasoning capabilities of\ntext-based large language models while incorporating domain-specific\nadaptations. To further enhance multilingual recognition performance, we\nadopted a meticulously designed multi-stage training strategy leveraging\nextensive multilingual audio datasets. Experimental results demonstrate that\nour approach achieves competitive Word Error Rate (WER) performance on both dev\nand test sets, obtaining second place in the challenge ranking.",
    "published": "2025-07-23T07:48:33Z",
    "updated": "2025-07-23T07:48:33Z",
    "id": "2507.17288v1",
    "authors": [
      "Miaomiao Gao",
      "Xiaoxiao Xiang",
      "Yiwen Guo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17288v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17288v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17288v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a large language model (LLM) in a multilingual speech recognition system, which involves both LLM and multilingual aspects. The abstract mentions the use of LLM for reasoning capabilities and the adaptation for multilingual scenarios, aligning with the topics of LLM and MLLM.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.17273v1": {
    "title": "Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational\n  Bottlenecks for Warehouse Planning Assistance",
    "summary": "Analyzing large, complex output datasets from Discrete Event Simulations\n(DES) of warehouse operations to identify bottlenecks and inefficiencies is a\ncritical yet challenging task, often demanding significant manual effort or\nspecialized analytical tools. Our framework integrates Knowledge Graphs (KGs)\nand Large Language Model (LLM)-based agents to analyze complex Discrete Event\nSimulation (DES) output data from warehouse operations. It transforms raw DES\ndata into a semantically rich KG, capturing relationships between simulation\nevents and entities. An LLM-based agent uses iterative reasoning, generating\ninterdependent sub-questions. For each sub-question, it creates Cypher queries\nfor KG interaction, extracts information, and self-reflects to correct errors.\nThis adaptive, iterative, and self-correcting process identifies operational\nissues mimicking human analysis. Our DES approach for warehouse bottleneck\nidentification, tested with equipment breakdowns and process irregularities,\noutperforms baseline methods. For operational questions, it achieves\nnear-perfect pass rates in pinpointing inefficiencies. For complex\ninvestigative questions, we demonstrate its superior diagnostic ability to\nuncover subtle, interconnected issues. This work bridges simulation modeling\nand AI (KG+LLM), offering a more intuitive method for actionable insights,\nreducing time-to-insight, and enabling automated warehouse inefficiency\nevaluation and diagnosis.",
    "published": "2025-07-23T07:18:55Z",
    "updated": "2025-07-23T07:18:55Z",
    "id": "2507.17273v1",
    "authors": [
      "Rishi Parekh",
      "Saisubramaniam Gopalakrishnan",
      "Zishan Ahmad",
      "Anirudh Deodhar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17273v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17273v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17273v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for reasoning and analysis in the context of warehouse operations, which involves iterative reasoning and self-correcting processes. It also mentions the integration of Knowledge Graphs (KGs) with LLMs, which is a form of memory augmentation.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2507.17271v1": {
    "title": "Seed&Steer: Guiding Large Language Models with Compilable Prefix and\n  Branch Signals for Unit Test Generation",
    "summary": "Unit tests play a vital role in the software development lifecycle. Recent\nadvances in Large Language Model (LLM)-based approaches have significantly\nimproved automated test generation, garnering attention from both academia and\nindustry. We revisit LLM-based unit test generation from a novel perspective by\ndecoupling prefix generation and assertion generation. To characterize their\nrespective challenges, we define Initialization Complexity and adopt Cyclomatic\nComplexity to measure the difficulty of prefix and assertion generation,\nrevealing that the former primarily affects compilation success, while the\nlatter influences test coverage. To address these challenges, we propose\nSeed&Steer, a two-step approach that combines traditional unit testing\ntechniques with the capabilities of large language models. Seed&Steer leverages\nconventional unit testing tools (e.g., EvoSuite) to generate method invocations\nwith high compilation success rates, which serve as seeds to guide LLMs in\nconstructing effective test contexts. It then introduces branching cues to help\nLLMs explore diverse execution paths (e.g., normal, boundary, and exception\ncases) and generate assertions with high coverage. We evaluate Seed&Steer on\nfive real-world Java projects against state-of-the-art baselines. Results show\nthat Seed&Steer improves the compilation pass rate by approximately 7%,\nsuccessfully compiling 792 and 887 previously failing cases on two LLMs. It\nalso achieves up to ~73% branch and line coverage across focal methods of\nvarying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our\ncode, dataset, and experimental scripts will be publicly released to support\nfuture research and reproducibility.",
    "published": "2025-07-23T07:16:46Z",
    "updated": "2025-07-23T07:16:46Z",
    "id": "2507.17271v1",
    "authors": [
      "Shuaiyu Zhou",
      "Zhengran Zeng",
      "Xiaoling Zhou",
      "Rui Xie",
      "Shikun Zhang",
      "Wei Ye"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17271v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17271v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17271v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for unit test generation, specifically addressing challenges in prefix and assertion generation. It leverages traditional unit testing techniques to guide LLMs, which aligns with research on LLMs and their applications in software development.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.17264v1": {
    "title": "Understanding Prompt Programming Tasks and Questions",
    "summary": "Prompting foundation models (FMs) like large language models (LLMs) have\nenabled new AI-powered software features (e.g., text summarization) that\npreviously were only possible by fine-tuning FMs. Now, developers are embedding\nprompts in software, known as prompt programs. The process of prompt\nprogramming requires the developer to make many changes to their prompt. Yet,\nthe questions developers ask to update their prompt is unknown, despite the\nanswers to these questions affecting how developers plan their changes. With\nthe growing number of research and commercial prompt programming tools, it is\nunclear whether prompt programmers' needs are being adequately addressed. We\naddress these challenges by developing a taxonomy of 25 tasks prompt\nprogrammers do and 51 questions they ask, measuring the importance of each task\nand question. We interview 16 prompt programmers, observe 8 developers make\nprompt changes, and survey 50 developers. We then compare the taxonomy with 48\nresearch and commercial tools. We find that prompt programming is not\nwell-supported: all tasks are done manually, and 16 of the 51 questions --\nincluding a majority of the most important ones -- remain unanswered. Based on\nthis, we outline important opportunities for prompt programming tools.",
    "published": "2025-07-23T07:01:44Z",
    "updated": "2025-07-23T07:01:44Z",
    "id": "2507.17264v1",
    "authors": [
      "Jenny T. Liang",
      "Chenyang Yang",
      "Agnia Sergeyuk",
      "Travis D. Breaux",
      "Brad A. Myers"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17264v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17264v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17264v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses prompting techniques and tasks for large language models (LLMs), which is closely related to the 'LLM' topic. It also touches on the practical aspects of using LLMs in software development, which aligns with the 'Reasoning' topic as it involves understanding and improving the reasoning capabilities of LLMs through prompts. The focus on prompt programming and developer needs does not directly fit into other specified categories.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.17257v1": {
    "title": "Agent Identity Evals: Measuring Agentic Identity",
    "summary": "Central to agentic capability and trustworthiness of language model agents\n(LMAs) is the extent they maintain stable, reliable, identity over time.\nHowever, LMAs inherit pathologies from large language models (LLMs)\n(statelessness, stochasticity, sensitivity to prompts and\nlinguistically-intermediation) which can undermine their identifiability,\ncontinuity, persistence and consistency. This attrition of identity can erode\ntheir reliability, trustworthiness and utility by interfering with their\nagentic capabilities such as reasoning, planning and action. To address these\nchallenges, we introduce \\textit{agent identity evals} (AIE), a rigorous,\nstatistically-driven, empirical framework for measuring the degree to which an\nLMA system exhibit and maintain their agentic identity over time, including\ntheir capabilities, properties and ability to recover from state perturbations.\nAIE comprises a set of novel metrics which can integrate with other measures of\nperformance, capability and agentic robustness to assist in the design of\noptimal LMA infrastructure and scaffolding such as memory and tools. We set out\nformal definitions and methods that can be applied at each stage of the LMA\nlife-cycle, and worked examples of how to apply them.",
    "published": "2025-07-23T06:56:15Z",
    "updated": "2025-07-23T06:56:15Z",
    "id": "2507.17257v1",
    "authors": [
      "Elija Perrier",
      "Michael Timothy Bennett"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17257v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17257v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17257v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the stability and reliability of language model agents (LMAs), which are built upon large language models (LLMs). It focuses on agentic capabilities such as reasoning, planning, and action, which are central to reinforcement learning and agent-based systems. The introduction of agent identity evals (AIE) also touches on the evaluation and benchmarking of these agents.",
    "llm_cls_result": [
      "RL",
      "Benchmark",
      "AGI"
    ]
  },
  "2507.17249v1": {
    "title": "R4ec: A Reasoning, Reflection, and Refinement Framework for\n  Recommendation Systems",
    "summary": "Harnessing Large Language Models (LLMs) for recommendation systems has\nemerged as a prominent avenue, drawing substantial research interest. However,\nexisting approaches primarily involve basic prompt techniques for knowledge\nacquisition, which resemble System-1 thinking. This makes these methods highly\nsensitive to errors in the reasoning path, where even a small mistake can lead\nto an incorrect inference. To this end, in this paper, we propose $R^{4}$ec, a\nreasoning, reflection and refinement framework that evolves the recommendation\nsystem into a weak System-2 model. Specifically, we introduce two models: an\nactor model that engages in reasoning, and a reflection model that judges these\nresponses and provides valuable feedback. Then the actor model will refine its\nresponse based on the feedback, ultimately leading to improved responses. We\nemploy an iterative reflection and refinement process, enabling LLMs to\nfacilitate slow and deliberate System-2-like thinking. Ultimately, the final\nrefined knowledge will be incorporated into a recommendation backbone for\nprediction. We conduct extensive experiments on Amazon-Book and MovieLens-1M\ndatasets to demonstrate the superiority of $R^{4}$ec. We also deploy $R^{4}$ec\non a large scale online advertising platform, showing 2.2\\% increase of\nrevenue. Furthermore, we investigate the scaling properties of the actor model\nand reflection model.",
    "published": "2025-07-23T06:36:49Z",
    "updated": "2025-07-23T06:36:49Z",
    "id": "2507.17249v1",
    "authors": [
      "Hao Gu",
      "Rui Zhong",
      "Yu Xia",
      "Wei Yang",
      "Chi Lu",
      "Peng Jiang",
      "Kun Gai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17249v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17249v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17249v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in recommendation systems, focusing on reasoning, reflection, and refinement processes. It also mentions the scaling properties of the models, which aligns with the topics of LLM and Reasoning. The mention of scaling properties also touches on the Scaling topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Scaling"
    ]
  },
  "2507.17232v1": {
    "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for\n  State Probing Task",
    "summary": "Large Language Models (LLMs) are trained on a vast amount of procedural\ntexts, but they do not directly observe real-world phenomena. In the context of\ncooking recipes, this poses a challenge, as intermediate states of ingredients\nare often omitted, making it difficult for models to track ingredient states\nand understand recipes accurately. In this paper, we apply state probing, a\nmethod for evaluating a language model's understanding of the world, to the\ndomain of cooking. We propose a new task and dataset for evaluating how well\nLLMs can recognize intermediate ingredient states during cooking procedures. We\nfirst construct a new Japanese recipe dataset with clear and accurate\nannotations of ingredient state changes, collected from well-structured and\ncontrolled recipe texts. Using this dataset, we design three novel tasks to\nevaluate whether LLMs can track ingredient state transitions and identify\ningredients present at intermediate steps. Our experiments with widely used\nLLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state\nknowledge improves their understanding of cooking processes, achieving\nperformance comparable to commercial LLMs.",
    "published": "2025-07-23T05:56:20Z",
    "updated": "2025-07-23T05:56:20Z",
    "id": "2507.17232v1",
    "authors": [
      "Mashiro Toyooka",
      "Kiyoharu Aizawa",
      "Yoko Yamakata"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17232v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17232v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17232v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating LLMs' understanding of ingredient states in cooking recipes, which involves reasoning and dataset creation for benchmarking LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Dataset"
    ]
  },
  "2507.17209v1": {
    "title": "HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs\n  for Hypothesis-Driven Scientific Discovery",
    "summary": "Modern scientific discovery faces growing challenges in integrating vast and\nheterogeneous knowledge critical to breakthroughs in biomedicine and drug\ndevelopment. Traditional hypothesis-driven research, though effective, is\nconstrained by human cognitive limits, the complexity of biological systems,\nand the high cost of trial-and-error experimentation. Deep learning models,\nespecially graph neural networks (GNNs), have accelerated prediction\ngeneration, but the sheer volume of outputs makes manual selection for\nvalidation unscalable. Large language models (LLMs) offer promise in filtering\nand hypothesis generation, yet suffer from hallucinations and lack grounding in\nstructured knowledge, limiting their reliability. To address these issues, we\npropose HypoChainer, a collaborative visualization framework that integrates\nhuman expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance\nhypothesis generation and validation. HypoChainer operates in three stages:\nFirst, exploration and contextualization -- experts use retrieval-augmented\nLLMs (RAGs) and dimensionality reduction to navigate large-scale GNN\npredictions, assisted by interactive explanations. Second, hypothesis chain\nformation -- experts iteratively examine KG relationships around predictions\nand semantically linked entities, refining hypotheses with LLM and KG\nsuggestions. Third, validation prioritization -- refined hypotheses are\nfiltered based on KG-supported evidence to identify high-priority candidates\nfor experimentation, with visual analytics further strengthening weak links in\nreasoning. We demonstrate HypoChainer's effectiveness through case studies in\ntwo domains and expert interviews, highlighting its potential to support\ninterpretable, scalable, and knowledge-grounded scientific discovery.",
    "published": "2025-07-23T05:02:54Z",
    "updated": "2025-07-23T05:02:54Z",
    "id": "2507.17209v1",
    "authors": [
      "Haoran Jiang",
      "Shaohan Shi",
      "Yunjie Yao",
      "Chang Jiang",
      "Quan Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17209v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17209v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17209v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of LLMs with knowledge graphs for scientific discovery, focusing on hypothesis generation and validation. It involves LLM-driven reasoning and retrieval-augmented generation, which are key aspects of the 'LLM' and 'Memory' topics.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.17204v1": {
    "title": "Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale\n  Video Content Moderation",
    "summary": "Effective content moderation is essential for video platforms to safeguard\nuser experience and uphold community standards. While traditional video\nclassification models effectively handle well-defined moderation tasks, they\nstruggle with complicated scenarios such as implicit harmful content and\ncontextual ambiguity. Multimodal large language models (MLLMs) offer a\npromising solution to these limitations with their superior cross-modal\nreasoning and contextual understanding. However, two key challenges hinder\ntheir industrial adoption. First, the high computational cost of MLLMs makes\nfull-scale deployment impractical. Second, adapting generative models for\ndiscriminative classification remains an open research problem. In this paper,\nwe first introduce an efficient method to transform a generative MLLM into a\nmultimodal classifier using minimal discriminative training data. To enable\nindustry-scale deployment, we then propose a router-ranking cascade system that\nintegrates MLLMs with a lightweight router model. Offline experiments\ndemonstrate that our MLLM-based approach improves F1 score by 66.50% over\ntraditional classifiers while requiring only 2% of the fine-tuning data. Online\nevaluations show that our system increases automatic content moderation volume\nby 41%, while the cascading deployment reduces computational cost to only 1.5%\nof direct full-scale deployment.",
    "published": "2025-07-23T04:52:58Z",
    "updated": "2025-07-23T04:52:58Z",
    "id": "2507.17204v1",
    "authors": [
      "Zixuan Wang",
      "Jinghao Shi",
      "Hanzhong Liang",
      "Xiang Shen",
      "Vera Wen",
      "Zhiqian Chen",
      "Yifan Wu",
      "Zhixin Zhang",
      "Hongyu Xiong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17204v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17204v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17204v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Multimodal Large Language Models (MLLMs) for video content moderation, highlighting their cross-modal reasoning and contextual understanding capabilities. It addresses challenges related to computational cost and adaptation of generative models for discriminative tasks, which are central to MLLM research.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.17202v1": {
    "title": "DesignLab: Designing Slides Through Iterative Detection and Correction",
    "summary": "Designing high-quality presentation slides can be challenging for non-experts\ndue to the complexity involved in navigating various design choices. Numerous\nautomated tools can suggest layouts and color schemes, yet often lack the\nability to refine their own output, which is a key aspect in real-world\nworkflows. We propose DesignLab, which separates the design process into two\nroles, the design reviewer, who identifies design-related issues, and the\ndesign contributor who corrects them. This decomposition enables an iterative\nloop where the reviewer continuously detects issues and the contributor\ncorrects them, allowing a draft to be further polished with each iteration,\nreaching qualities that were unattainable. We fine-tune large language models\nfor these roles and simulate intermediate drafts by introducing controlled\nperturbations, enabling the design reviewer learn design errors and the\ncontributor learn how to fix them. Our experiments show that DesignLab\noutperforms existing design-generation methods, including a commercial tool, by\nembracing the iterative nature of designing which can result in polished,\nprofessional slides.",
    "published": "2025-07-23T04:49:48Z",
    "updated": "2025-07-23T04:49:48Z",
    "id": "2507.17202v1",
    "authors": [
      "Jooyeol Yun",
      "Heng Wang",
      "Yotaro Shimose",
      "Jaegul Choo",
      "Shingo Takamatsu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17202v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17202v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17202v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using large language models (LLMs) for iterative design refinement of presentation slides, which involves fine-tuning LLMs for specific roles (design reviewer and contributor). The core topic is the application of LLMs in a specific task, but it does not directly align with the provided topics like LLM, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.17188v1": {
    "title": "LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for\n  Secure Heterogeneous UAV Networks",
    "summary": "This work tackles the physical layer security (PLS) problem of maximizing the\nsecrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy\nconstraints. Unlike prior studies that assume uniform UAV capabilities or\noverlook energy-security trade-offs, we consider a realistic scenario where\nUAVs with diverse payloads and computation resources collaborate to serve\nground terminals in the presence of eavesdroppers. To manage the complex\ncoupling between UAV motion and communication, we propose a hierarchical\noptimization framework. The inner layer uses a semidefinite relaxation\n(SDR)-based S2DC algorithm combining penalty functions and difference-of-convex\n(d.c.) programming to solve the secrecy precoding problem with fixed UAV\npositions. The outer layer introduces a Large Language Model (LLM)-guided\nheuristic multi-agent reinforcement learning approach (LLM-HeMARL) for\ntrajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics\npolicy generated by the LLM, enabling UAVs to learn energy-aware,\nsecurity-driven trajectories without the inference overhead of real-time LLM\ncalls. The simulation results show that our method outperforms existing\nbaselines in secrecy rate and energy efficiency, with consistent robustness\nacross varying UAV swarm sizes and random seeds.",
    "published": "2025-07-23T04:22:57Z",
    "updated": "2025-07-23T04:22:57Z",
    "id": "2507.17188v1",
    "authors": [
      "Lijie Zheng",
      "Ji He",
      "Shih Yu Chang",
      "Yulong Shen",
      "Dusit Niyato"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17188v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17188v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17188v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a Large Language Model (LLM) in conjunction with multi-agent reinforcement learning (RL) for optimizing UAV trajectories, which aligns with the topics of LLM and RL. The focus on UAV networks and security does not directly fit other specified topics.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.17165v1": {
    "title": "Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions\n  Configurations",
    "summary": "Continuous Integration (CI) services, such as GitHub Actions, require\ndevelopers to write YAML-based configurations, which can be tedious and\nerror-prone. Despite the increasing use of Large Language Models (LLMs) to\nautomate software engineering tasks, their ability to generate CI\nconfigurations remains underexplored. This paper presents a preliminary study\nevaluating six LLMs for generating GitHub Actions configurations from natural\nlanguage descriptions. We assess three general-purpose foundation models\n(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code\nLlama, and CodeGemma). We also introduce the first labeled dataset of its kind,\nconstructed from GitHub Actions documentation, pairing descriptions with\ncorresponding best-practice YAML configurations. Zero-shot prompting achieves\nup to 69% similarity with the ground truth, with only 3% perfect matches.\nCode-pretrained models slightly underperform compared to general-purpose ones\nin YAML-based CI tasks, revealing LLM limitations for CI configuration\ngeneration. Analyzing GPT-4o outputs reveals issues like missing or renamed\nsteps, misinterpreted descriptions, and unnecessary additions that may affect\nstructural and contextual correctness, indicating a gap between generation\nquality and the precision required for executable CI configurations. Our\nresearch offers insights for improving LLM alignment with configuration\nlanguages and guiding future efforts on CI automation and tooling support.",
    "published": "2025-07-23T03:18:04Z",
    "updated": "2025-07-23T03:18:04Z",
    "id": "2507.17165v1",
    "authors": [
      "Taher A. Ghaleb",
      "Dulina Rathnayake"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17165v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17165v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17165v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the ability of Large Language Models (LLMs) to generate GitHub Actions configurations, which involves LLMs and their application in automating software engineering tasks.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.17134v1": {
    "title": "Resilient Multi-Agent Negotiation for Medical Supply Chains:Integrating\n  LLMs and Blockchain for Transparent Coordination",
    "summary": "Global health emergencies, such as the COVID-19 pandemic, have exposed\ncritical weaknesses in traditional medical supply chains, including\ninefficiencies in resource allocation, lack of transparency, and poor\nadaptability to dynamic disruptions. This paper presents a novel hybrid\nframework that integrates blockchain technology with a decentralized, large\nlanguage model (LLM) powered multi-agent negotiation system to enhance the\nresilience and accountability of medical supply chains during crises. In this\nsystem, autonomous agents-representing manufacturers, distributors, and\nhealthcare institutions-engage in structured, context-aware negotiation and\ndecision-making processes facilitated by LLMs, enabling rapid and ethical\nallocation of scarce medical resources. The off-chain agent layer supports\nadaptive reasoning and local decision-making, while the on-chain blockchain\nlayer ensures immutable, transparent, and auditable enforcement of decisions\nvia smart contracts. The framework also incorporates a formal cross-layer\ncommunication protocol to bridge decentralized negotiation with institutional\nenforcement. A simulation environment emulating pandemic scenarios evaluates\nthe system's performance, demonstrating improvements in negotiation efficiency,\nfairness of allocation, supply chain responsiveness, and auditability. This\nresearch contributes an innovative approach that synergizes blockchain trust\nguarantees with the adaptive intelligence of LLM-driven agents, providing a\nrobust and scalable solution for critical supply chain coordination under\nuncertainty.",
    "published": "2025-07-23T02:14:42Z",
    "updated": "2025-07-23T02:14:42Z",
    "id": "2507.17134v1",
    "authors": [
      "Mariam ALMutairi",
      "Hyungmin Kim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17134v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17134v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17134v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) with blockchain technology for multi-agent negotiation in medical supply chains, focusing on the use of LLMs for adaptive reasoning and decision-making.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.17131v1": {
    "title": "Enabling Self-Improving Agents to Learn at Test Time With\n  Human-In-The-Loop Guidance",
    "summary": "Large language model (LLM) agents often struggle in environments where rules\nand required domain knowledge frequently change, such as regulatory compliance\nand user risk screening. Current approaches, like offline fine-tuning and\nstandard prompting, are insufficient because they cannot effectively adapt to\nnew knowledge during actual operation. To address this limitation, we propose\nthe Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework\ndesigned specifically to continuously learn updated domain knowledge at test\ntime. ARIA assesses its own uncertainty through structured self-dialogue,\nproactively identifying knowledge gaps and requesting targeted explanations or\ncorrections from human experts. It then systematically updates an internal,\ntimestamped knowledge repository with provided human guidance, detecting and\nresolving conflicting or outdated knowledge through comparisons and\nclarification queries. We evaluate ARIA on the realistic customer due diligence\nname screening task on TikTok Pay, alongside publicly available dynamic\nknowledge tasks. Results demonstrate significant improvements in adaptability\nand accuracy compared to baselines using standard offline fine-tuning and\nexisting self-improving agents. ARIA is deployed within TikTok Pay serving over\n150 million monthly active users, confirming its practicality and effectiveness\nfor operational use in rapidly evolving environments.",
    "published": "2025-07-23T02:12:32Z",
    "updated": "2025-07-23T02:12:32Z",
    "id": "2507.17131v1",
    "authors": [
      "Yufei He",
      "Ruoyu Li",
      "Alex Chen",
      "Yue Liu",
      "Yulin Chen",
      "Yuan Sui",
      "Cheng Chen",
      "Yi Zhu",
      "Luca Luo",
      "Frank Yang",
      "Bryan Hooi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17131v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17131v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17131v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a framework for LLM agents to continuously learn and adapt with human guidance, which aligns with topics related to LLM Reinforcement Learning (RL) and Artificial General Intelligence (AGI).",
    "llm_cls_result": [
      "RL",
      "AGI"
    ]
  },
  "2507.17120v1": {
    "title": "BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM\n  Inference Serving",
    "summary": "Large language models (LLMs) have become increasingly popular in various\nareas, traditional business gradually shifting from rule-based systems to\nLLM-based solutions. However, the inference of LLMs is resource-intensive or\nlatency-sensitive, posing significant challenges for serving systems. Existing\nLLM serving systems often use static or continuous batching strategies, which\ncan lead to inefficient GPU memory utilization and increased latency,\nespecially under heterogeneous workloads. These methods may also struggle to\nadapt to dynamic workload fluctuations, resulting in suboptimal throughput and\npotential service level objective (SLO) violations. In this paper, we introduce\nBucketServe, a bucket-based dynamic batching framework designed to optimize LLM\ninference performance. By grouping requests into size-homogeneous buckets based\non sequence length, BucketServe minimizes padding overhead and optimizes GPU\nmemory usage through real-time batch size adjustments preventing out-of-memory\n(OOM) errors. It introduces adaptive bucket splitting/merging and\npriority-aware scheduling to mitigate resource fragmentation and ensure SLO\ncompliance. Experiment shows that BucketServe significantly outperforms UELLM\nin throughput, achieving up to 3.58x improvement. It can also handle 1.93x more\nrequest load under the SLO attainment of 80% compared with DistServe and\ndemonstrates 1.975x higher system load capacity compared to the UELLM.",
    "published": "2025-07-23T01:51:48Z",
    "updated": "2025-07-23T01:51:48Z",
    "id": "2507.17120v1",
    "authors": [
      "Wanyi Zheng",
      "Minxian Xu",
      "Shengye Song",
      "Kejiang Ye"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17120v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17120v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17120v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing LLM inference performance through dynamic batching strategies, which is directly related to the efficiency and serving of Large Language Models (LLMs).",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.17118v1": {
    "title": "HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI\n  Systems: A Case Study",
    "summary": "AI has become integral to safety-critical areas like autonomous driving\nsystems (ADS) and robotics. The architecture of recent autonomous systems are\ntrending toward end-to-end (E2E) monolithic architectures such as large\nlanguage models (LLMs) and vision language models (VLMs). In this paper, we\nreview different architectural solutions and then evaluate the efficacy of\ncommon safety analyses such as failure modes and effect analysis (FMEA) and\nfault tree analysis (FTA). We show how these techniques can be improved for the\nintricate nature of the foundational models, particularly in how they form and\nutilize latent representations. We introduce HySAFE-AI, Hybrid Safety\nArchitectural Analysis Framework for AI Systems, a hybrid framework that adapts\ntraditional methods to evaluate the safety of AI systems. Lastly, we offer\nhints of future work and suggestions to guide the evolution of future AI safety\nstandards.",
    "published": "2025-07-23T01:41:51Z",
    "updated": "2025-07-23T01:41:51Z",
    "id": "2507.17118v1",
    "authors": [
      "Mandar Pitale",
      "Jelena Frtunikj",
      "Abhinaw Priyadershi",
      "Vasu Singh",
      "Maria Spence"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17118v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17118v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17118v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the safety analysis of AI systems, particularly focusing on large language models (LLMs) and vision language models (VLMs), which are trending towards end-to-end monolithic architectures. It introduces a hybrid framework for safety analysis but does not primarily focus on the core topics like LLM, RL, MLLM, etc. Instead, it is more about safety analysis and architectural evaluation of AI systems.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.17107v1": {
    "title": "Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language\n  Models",
    "summary": "Reinforcement learning (RL) is a key post-pretraining step for aligning large\nlanguage models (LLMs) with complex tasks and human preferences. While it is\noften assumed that RL fine-tuning requires updating most of a model's\nparameters, we challenge this assumption with a surprising finding: RL\nfine-tuning consistently modifies only a small subnetwork (typically 5-30% of\nweights), leaving most parameters unchanged. We call this phenomenon RL-induced\nparameter update sparsity. It arises naturally, without any sparsity\nconstraints or parameter-efficient tuning, and appears across multiple RL\nalgorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI,\nMeta, and open-source LLMs). Moreover, the subnetworks updated by RL show\nsubstantial overlap across different seeds, datasets, and algorithms-far\nexceeding chance-suggesting a partially transferable structure in the\npretrained model. We show that fine-tuning only this sparse subnetwork recovers\nfull model performance and yields parameters nearly identical to the fully\nfine-tuned model. Our analysis suggests this sparsity emerges because RL\noperates near the model's original distribution, requiring only targeted\nchanges. KL penalties, gradient clipping, and on-policy dynamics have limited\neffect on the sparsity pattern. These findings shed new light on how RL adapts\nmodels: not by shifting all weights, but by focusing training on a small,\nconsistently updated subnetwork. This insight enables more efficient RL methods\nand reframes sparsity through the lens of the lottery ticket hypothesis.",
    "published": "2025-07-23T01:02:17Z",
    "updated": "2025-07-23T01:02:17Z",
    "id": "2507.17107v1",
    "authors": [
      "Andrii Balashov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17107v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17107v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17107v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of reinforcement learning (RL) in fine-tuning large language models (LLMs), specifically focusing on the sparsity of parameter updates during RL fine-tuning. This directly relates to both the RL and LLM topics, as it involves RL techniques applied to LLMs.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.17061v1": {
    "title": "Parallelism Meets Adaptiveness: Scalable Documents Understanding in\n  Multi-Agent LLM Systems",
    "summary": "Large language model (LLM) agents have shown increasing promise for\ncollaborative task completion. However, existing multi-agent frameworks often\nrely on static workflows, fixed roles, and limited inter-agent communication,\nreducing their effectiveness in open-ended, high-complexity domains. This paper\nproposes a coordination framework that enables adaptiveness through three core\nmechanisms: dynamic task routing, bidirectional feedback, and parallel agent\nevaluation. The framework allows agents to reallocate tasks based on confidence\nand workload, exchange structured critiques to iteratively improve outputs, and\ncrucially compete on high-ambiguity subtasks with evaluator-driven selection of\nthe most suitable result. We instantiate these principles in a modular\narchitecture and demonstrate substantial improvements in factual coverage,\ncoherence, and efficiency over static and partially adaptive baselines. Our\nfindings highlight the benefits of incorporating both adaptiveness and\nstructured competition in multi-agent LLM systems.",
    "published": "2025-07-22T22:42:51Z",
    "updated": "2025-07-22T22:42:51Z",
    "id": "2507.17061v1",
    "authors": [
      "Chengxuan Xia",
      "Qianye Wu",
      "Sixuan Tian",
      "Yilun Hao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17061v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17061v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17061v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a coordination framework for multi-agent LLM systems, focusing on dynamic task routing, feedback mechanisms, and parallel evaluation, which aligns with topics related to LLM (Large Language Models) and RL (Reinforcement Learning) due to the collaborative and adaptive nature of the agents.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.17050v1": {
    "title": "Toward Scalable Video Narration: A Training-free Approach Using\n  Multimodal Large Language Models",
    "summary": "In this paper, we introduce VideoNarrator, a novel training-free pipeline\ndesigned to generate dense video captions that offer a structured snapshot of\nvideo content. These captions offer detailed narrations with precise\ntimestamps, capturing the nuances present in each segment of the video. Despite\nadvancements in multimodal large language models (MLLMs) for video\ncomprehension, these models often struggle with temporally aligned narrations\nand tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator\naddresses these challenges by leveraging a flexible pipeline where\noff-the-shelf MLLMs and visual-language models (VLMs) can function as caption\ngenerators, context providers, or caption verifiers. Our experimental results\ndemonstrate that the synergistic interaction of these components significantly\nenhances the quality and accuracy of video narrations, effectively reducing\nhallucinations and improving temporal alignment. This structured approach not\nonly enhances video understanding but also facilitates downstream tasks such as\nvideo summarization and video question answering, and can be potentially\nextended for advertising and marketing applications.",
    "published": "2025-07-22T22:16:37Z",
    "updated": "2025-07-22T22:16:37Z",
    "id": "2507.17050v1",
    "authors": [
      "Tz-Ying Wu",
      "Tahani Trigui",
      "Sharath Nittur Sridhar",
      "Anand Bodas",
      "Subarna Tripathi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17050v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17050v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17050v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Multimodal Large Language Models (MLLMs) for video narration, which involves integrating vision and language modalities. The approach leverages MLLMs and visual-language models (VLMs) to enhance video understanding and narration quality.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.17047v1": {
    "title": "Controllable Hybrid Captioner for Improved Long-form Video Understanding",
    "summary": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video.",
    "published": "2025-07-22T22:09:00Z",
    "updated": "2025-07-22T22:09:00Z",
    "id": "2507.17047v1",
    "authors": [
      "Kuleen Sasse",
      "Efsun Sarioglu Kayi",
      "Arun Reddy"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17047v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17047v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17047v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) and vision language models (VLMs) for video understanding, focusing on generating textual summaries and improving caption quality. It involves multimodal integration and reasoning over video content.",
    "llm_cls_result": [
      "LLM",
      "VLA",
      "MLLM"
    ]
  },
  "2507.17024v1": {
    "title": "Write, Rank, or Rate: Comparing Methods for Studying Visualization\n  Affordances",
    "summary": "A growing body of work on visualization affordances highlights how specific\ndesign choices shape reader takeaways from information visualizations. However,\nmapping the relationship between design choices and reader conclusions often\nrequires labor-intensive crowdsourced studies, generating large corpora of\nfree-response text for analysis. To address this challenge, we explored\nalternative scalable research methodologies to assess chart affordances. We\ntest four elicitation methods from human-subject studies: free response,\nvisualization ranking, conclusion ranking, and salience rating, and compare\ntheir effectiveness in eliciting reader interpretations of line charts, dot\nplots, and heatmaps. Overall, we find that while no method fully replicates\naffordances observed in free-response conclusions, combinations of ranking and\nrating methods can serve as an effective proxy at a broad scale. The two\nranking methodologies were influenced by participant bias towards certain chart\ntypes and the comparison of suggested conclusions. Rating conclusion salience\ncould not capture the specific variations between chart types observed in the\nother methods. To supplement this work, we present a case study with GPT-4o,\nexploring the use of large language models (LLMs) to elicit human-like chart\ninterpretations. This aligns with recent academic interest in leveraging LLMs\nas proxies for human participants to improve data collection and analysis\nefficiency. GPT-4o performed best as a human proxy for the salience rating\nmethodology but suffered from severe constraints in other areas. Overall, the\ndiscrepancies in affordances we found between various elicitation\nmethodologies, including GPT-4o, highlight the importance of intentionally\nselecting and combining methods and evaluating trade-offs.",
    "published": "2025-07-22T21:27:16Z",
    "updated": "2025-07-22T21:27:16Z",
    "id": "2507.17024v1",
    "authors": [
      "Chase Stokes",
      "Kylie Lin",
      "Cindy Xiong Bearfield"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17024v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17024v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17024v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper primarily discusses methodologies for studying visualization affordances and includes a case study with GPT-4o, but the main focus is not on LLMs or their applications. The core topic is about visualization research methodologies and human-subject studies.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.17016v1": {
    "title": "Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time\n  Series Forecasting",
    "summary": "In recent years, the application of Large Language Models (LLMs) to time\nseries forecasting (TSF) has garnered significant attention among researchers.\nThis study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with\nfuzzy time series (FTS) and causal graph to predict multivariate time series,\nmarking the first such architecture in the literature. The key objective is to\nconvert numerical time series into interpretable forms through the parallel\napplication of fuzzification and causal analysis, enabling both semantic\nunderstanding and structural insight as input for the pretrained GPT-2 model.\nThe resulting textual representation offers a more interpretable view of the\ncomplex dynamics underlying the original time series. The reported results\nconfirm the effectiveness of our proposed LLM-based time series forecasting\nmodel, as demonstrated across four different multivariate time series datasets.\nThis initiative paves promising future directions in the domain of TSF using\nLLMs based on FTS.",
    "published": "2025-07-22T21:03:13Z",
    "updated": "2025-07-22T21:03:13Z",
    "id": "2507.17016v1",
    "authors": [
      "Omid Orang",
      "Patricia O. Lucas",
      "Gabriel I. F. Paiva",
      "Petronio C. L. Silva",
      "Felipe Augusto Rocha da Silva",
      "Adriano Alonso Veloso",
      "Frederico Gadelha Guimaraes"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17016v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17016v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17016v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) to time series forecasting, specifically introducing a new framework called CGF-LLM that combines GPT-2 with fuzzy time series and causal graphs. This aligns with the 'LLM' topic as it involves research on LLMs and their architectures. Additionally, the use of pretrained GPT-2 and the focus on interpretability and structural insight relate to the 'Pretrain' topic. The application in time series forecasting does not directly fit into the other provided topics.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.17009v1": {
    "title": "Multi-Label Classification with Generative AI Models in Healthcare: A\n  Case Study of Suicidality and Risk Factors",
    "summary": "Suicide remains a pressing global health crisis, with over 720,000 deaths\nannually and millions more affected by suicide ideation (SI) and suicide\nattempts (SA). Early identification of suicidality-related factors (SrFs),\nincluding SI, SA, exposure to suicide (ES), and non-suicidal self-injury\n(NSSI), is critical for timely intervention. While prior studies have applied\nAI to detect SrFs in clinical notes, most treat suicidality as a binary\nclassification task, overlooking the complexity of cooccurring risk factors.\nThis study explores the use of generative large language models (LLMs),\nspecifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs\nfrom psychiatric electronic health records (EHRs). We present a novel end to\nend generative MLC pipeline and introduce advanced evaluation methods,\nincluding label set level metrics and a multilabel confusion matrix for error\nanalysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match\naccuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior\nperformance across label sets, including rare or minority label sets,\nindicating a more balanced and robust performance. Our findings reveal\nsystematic error patterns, such as the conflation of SI and SA, and highlight\nthe models tendency toward cautious over labeling. This work not only\ndemonstrates the feasibility of using generative AI for complex clinical\nclassification tasks but also provides a blueprint for structuring unstructured\nEHR data to support large scale clinical research and evidence based medicine.",
    "published": "2025-07-22T20:44:44Z",
    "updated": "2025-07-22T20:44:44Z",
    "id": "2507.17009v1",
    "authors": [
      "Ming Huang",
      "Zehan Li",
      "Yan Hu",
      "Wanjing Wang",
      "Andrew Wen",
      "Scott Lane",
      "Salih Selek",
      "Lokesh Shahani",
      "Rodrigo Machado-Vieira",
      "Jair Soares",
      "Hua Xu",
      "Hongfang Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17009v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17009v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17009v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of generative large language models (LLMs) like GPT-3.5 and GPT-4.5 for multi-label classification in healthcare, specifically focusing on suicidality and risk factors. This aligns with the 'LLM' topic as it involves research on large language models and their applications. Additionally, the use of these models for classification tasks in a specific domain (healthcare) suggests relevance to 'Reasoning' as it involves the models' ability to handle complex problem-solving tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16991v1": {
    "title": "PyG 2.0: Scalable Learning on Real World Graphs",
    "summary": "PyG (PyTorch Geometric) has evolved significantly since its initial release,\nestablishing itself as a leading framework for Graph Neural Networks. In this\npaper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive\nupdate that introduces substantial improvements in scalability and real-world\napplication capabilities. We detail the framework's enhanced architecture,\nincluding support for heterogeneous and temporal graphs, scalable feature/graph\nstores, and various optimizations, enabling researchers and practitioners to\ntackle large-scale graph learning problems efficiently. Over the recent years,\nPyG has been supporting graph learning in a large variety of application areas,\nwhich we will summarize, while providing a deep dive into the important areas\nof relational deep learning and large language modeling.",
    "published": "2025-07-22T19:55:09Z",
    "updated": "2025-07-22T19:55:09Z",
    "id": "2507.16991v1",
    "authors": [
      "Matthias Fey",
      "Jinu Sunil",
      "Akihiro Nitta",
      "Rishi Puri",
      "Manan Shah",
      "Bla Stojanovi",
      "Ramona Bendias",
      "Alexandria Barghi",
      "Vid Kocijan",
      "Zecheng Zhang",
      "Xinwei He",
      "Jan Eric Lenssen",
      "Jure Leskovec"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16991v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16991v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16991v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses PyG 2.0, a framework for Graph Neural Networks, and mentions its application in large language modeling, but the primary focus is on graph learning and scalability rather than core LLM topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.16974v1": {
    "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs\n  in the Agricultural Domain",
    "summary": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Although large language models (LLMs) can be used to\nimplement Question Answering (QA) systems, simply using publicly available\ngeneral-purpose LLMs in agriculture typically offer generic advisories, lacking\nprecision in local and multilingual contexts due to insufficient\ndomain-specific training and scarcity of high-quality, region-specific\ndatasets. Our study addresses these limitations by generating multilingual\nsynthetic agricultural datasets (English, Hindi, Punjabi) from\nagriculture-specific documents and fine-tuning language-specific LLMs. Our\nevaluation on curated multilingual datasets demonstrates significant\nimprovements in factual accuracy, relevance, and agricultural consensus for the\nfine-tuned models compared to their baseline counterparts. These results\nhighlight the efficacy of synthetic data-driven, language-specific fine-tuning\nas an effective strategy to improve the performance of LLMs in agriculture,\nespecially in multilingual and low-resource settings. By enabling more accurate\nand localized agricultural advisory services, this study provides a meaningful\nstep toward bridging the knowledge gap in AI-driven agricultural solutions for\ndiverse linguistic communities.",
    "published": "2025-07-22T19:25:10Z",
    "updated": "2025-07-22T19:25:10Z",
    "id": "2507.16974v1",
    "authors": [
      "Rishemjit Kaur",
      "Arshdeep Singh Bhankhar",
      "Surangika Ranathunga",
      "Jashanpreet Singh Salh",
      "Sudhir Rajput",
      " Vidhi",
      "Kashish Mahendra",
      "Bhavika Berwal",
      "Ritesh Kumar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16974v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16974v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16974v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using multilingual LLMs for question answering in the agricultural domain, leveraging synthetic data for fine-tuning. This involves aspects of LLMs, multilingual datasets, and domain-specific applications.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.16969v1": {
    "title": "LLM4MEA: Data-free Model Extraction Attacks on Sequential Recommenders\n  via Large Language Models",
    "summary": "Recent studies have demonstrated the vulnerability of sequential recommender\nsystems to Model Extraction Attacks (MEAs). MEAs collect responses from\nrecommender systems to replicate their functionality, enabling unauthorized\ndeployments and posing critical privacy and security risks. Black-box attacks\nin prior MEAs are ineffective at exposing recommender system vulnerabilities\ndue to random sampling in data selection, which leads to misaligned synthetic\nand real-world distributions. To overcome this limitation, we propose LLM4MEA,\na novel model extraction method that leverages Large Language Models (LLMs) as\nhuman-like rankers to generate data. It generates data through interactions\nbetween the LLM ranker and target recommender system. In each interaction, the\nLLM ranker analyzes historical interactions to understand user behavior, and\nselects items from recommendations with consistent preferences to extend the\ninteraction history, which serves as training data for MEA. Extensive\nexperiments demonstrate that LLM4MEA significantly outperforms existing\napproaches in data quality and attack performance, reducing the divergence\nbetween synthetic and real-world data by up to 64.98% and improving MEA\nperformance by 44.82% on average. From a defensive perspective, we propose a\nsimple yet effective defense strategy and identify key hyperparameters of\nrecommender systems that can mitigate the risk of MEAs.",
    "published": "2025-07-22T19:20:23Z",
    "updated": "2025-07-22T19:20:23Z",
    "id": "2507.16969v1",
    "authors": [
      "Shilong Zhao",
      "Fei Sun",
      "Kaike Zhang",
      "Shaoling Jing",
      "Du Su",
      "Zhichao Shi",
      "Zhiyi Yin",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16969v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16969v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16969v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in Model Extraction Attacks (MEAs) on sequential recommender systems, which involves leveraging LLMs to generate data and improve attack performance. This aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning with Human Feedback, as the attack involves interactions and feedback mechanisms).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.16947v1": {
    "title": "AI-based Clinical Decision Support for Primary Care: A Real-World Study",
    "summary": "We evaluate the impact of large language model-based clinical decision\nsupport in live care. In partnership with Penda Health, a network of primary\ncare clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a\nsafety net for clinicians by identifying potential documentation and clinical\ndecision-making errors. AI Consult integrates into clinician workflows,\nactivating only when needed and preserving clinician autonomy. We conducted a\nquality improvement study, comparing outcomes for 39,849 patient visits\nperformed by clinicians with or without access to AI Consult across 15 clinics.\nVisits were rated by independent physicians to identify clinical errors.\nClinicians with access to AI Consult made relatively fewer errors: 16% fewer\ndiagnostic errors and 13% fewer treatment errors. In absolute terms, the\nintroduction of AI Consult would avert diagnostic errors in 22,000 visits and\ntreatment errors in 29,000 visits annually at Penda alone. In a survey of\nclinicians with AI Consult, all clinicians said that AI Consult improved the\nquality of care they delivered, with 75% saying the effect was \"substantial\".\nThese results required a clinical workflow-aligned AI Consult implementation\nand active deployment to encourage clinician uptake. We hope this study\ndemonstrates the potential for LLM-based clinical decision support tools to\nreduce errors in real-world settings and provides a practical framework for\nadvancing responsible adoption.",
    "published": "2025-07-22T18:37:33Z",
    "updated": "2025-07-22T18:37:33Z",
    "id": "2507.16947v1",
    "authors": [
      "Robert Korom",
      "Sarah Kiptinness",
      "Najib Adan",
      "Kassim Said",
      "Catherine Ithuli",
      "Oliver Rotich",
      "Boniface Kimani",
      "Irene King'ori",
      "Stellah Kamau",
      "Elizabeth Atemba",
      "Muna Aden",
      "Preston Bowman",
      "Michael Sharman",
      "Rebecca Soskin Hicks",
      "Rebecca Distler",
      "Johannes Heidecke",
      "Rahul K. Arora",
      "Karan Singhal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16947v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16947v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16947v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models (LLMs) in clinical decision support, which aligns with the LLM topic. However, it does not specifically focus on the other topics listed.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.16940v1": {
    "title": "AURA: A Multi-Modal Medical Agent for Understanding, Reasoning &\n  Annotation",
    "summary": "Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm\nshift from static prediction systems to agentic AI agents capable of reasoning,\ninteracting with tools, and adapting to complex tasks. While LLM-based agentic\nsystems have shown promise across many domains, their application to medical\nimaging remains in its infancy. In this work, we introduce AURA, the first\nvisual linguistic explainability agent designed specifically for comprehensive\nanalysis, explanation, and evaluation of medical images. By enabling dynamic\ninteractions, contextual explanations, and hypothesis testing, AURA represents\na significant advancement toward more transparent, adaptable, and clinically\naligned AI systems. We highlight the promise of agentic AI in transforming\nmedical image analysis from static predictions to interactive decision support.\nLeveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular\ntoolbox comprising: (i) a segmentation suite with phase grounding, pathology\nsegmentation, and anatomy segmentation to localize clinically meaningful\nregions; (ii) a counterfactual image-generation module that supports reasoning\nthrough image-level explanations; and (iii) a set of evaluation tools including\npixel-wise difference-map analysis, classification, and advanced\nstate-of-the-art components to assess diagnostic relevance and visual\ninterpretability.",
    "published": "2025-07-22T18:24:18Z",
    "updated": "2025-07-22T18:24:18Z",
    "id": "2507.16940v1",
    "authors": [
      "Nima Fathi",
      "Amar Kumar",
      "Tal Arbel"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16940v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16940v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16940v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces AURA, a multi-modal medical agent that integrates vision and language for medical image analysis, which aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA). Additionally, the use of LLM-based architecture (Qwen-32B) for reasoning and interaction supports the inclusion of the Reasoning topic.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Reasoning"
    ]
  },
  "2507.16933v1": {
    "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
    "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
    "published": "2025-07-22T18:17:53Z",
    "updated": "2025-07-22T18:17:53Z",
    "id": "2507.16933v1",
    "authors": [
      "Steven K. Esser",
      "Jeffrey L. McKinstry",
      "Deepika Bablani",
      "Rathinakumar Appuswamy",
      "Dharmendra S. Modha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16933v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16933v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16933v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses quantization techniques for large language models, which is a method to reduce model size and improve efficiency, closely related to the 'LLM' topic as it involves model architectures and scaling considerations.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.16799v2": {
    "title": "Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style\n  in LLM-based Role-Playing Language Agent",
    "summary": "The rapid advancement of large language models (LLMs) has enabled\nrole-playing language agents to demonstrate significant potential in various\napplications. However, relying solely on prompts and contextual inputs often\nproves insufficient for achieving deep immersion in specific roles,\nparticularly well-known fictional or public figures. On the other hand,\nfine-tuning-based approaches face limitations due to the challenges associated\nwith data collection and the computational resources required for training,\nthereby restricting their broader applicability. To address these issues, we\npropose Test-Time-Matching (TTM), a training-free role-playing framework\nthrough test-time scaling and context engineering. TTM uses LLM agents to\nautomatically decouple a character's features into personality, memory, and\nlinguistic style. Our framework involves a structured, three-stage generation\npipeline that utilizes these features for controlled role-playing. It achieves\nhigh-fidelity role-playing performance, also enables seamless combinations\nacross diverse linguistic styles and even variations in personality and memory.\nWe evaluate our framework through human assessment, and the results demonstrate\nthat our method achieves the outstanding performance in generating expressive\nand stylistically consistent character dialogues.",
    "published": "2025-07-22T17:47:44Z",
    "updated": "2025-07-23T06:06:43Z",
    "id": "2507.16799v2",
    "authors": [
      "Xiaoyu Zhan",
      "Xinyu Fu",
      "Hao Sun",
      "Yuanqi Li",
      "Jie Guo",
      "Yanwen Guo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16799v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16799v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16799v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing role-playing language agents using LLMs by decoupling character features into personality, memory, and linguistic style, which aligns with topics related to LLM applications and memory in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.16795v1": {
    "title": "Steering Out-of-Distribution Generalization with Concept Ablation\n  Fine-Tuning",
    "summary": "Fine-tuning large language models (LLMs) can lead to unintended\nout-of-distribution generalization. Standard approaches to this problem rely on\nmodifying training data, for example by adding data that better specify the\nintended generalization. However, this is not always practical. We introduce\nConcept Ablation Fine-Tuning (CAFT), a technique that leverages\ninterpretability tools to control how LLMs generalize from fine-tuning, without\nneeding to modify the training data or otherwise use data from the target\ndistribution. Given a set of directions in an LLM's latent space corresponding\nto undesired concepts, CAFT works by ablating these concepts with linear\nprojections during fine-tuning, steering the model away from unintended\ngeneralizations. We successfully apply CAFT to three fine-tuning tasks,\nincluding emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow\ntask generalize to give egregiously misaligned responses to general questions.\nWithout any changes to the fine-tuning data, CAFT reduces misaligned responses\nby 10x without degrading performance on the training distribution. Overall,\nCAFT represents a novel approach for steering LLM generalization without\nmodifying training data.",
    "published": "2025-07-22T17:45:04Z",
    "updated": "2025-07-22T17:45:04Z",
    "id": "2507.16795v1",
    "authors": [
      "Helena Casademunt",
      "Caden Juang",
      "Adam Karvonen",
      "Samuel Marks",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16795v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16795v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16795v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses fine-tuning large language models (LLMs) and introduces a technique (CAFT) to control generalization without modifying training data, which is relevant to LLM research and fine-tuning strategies.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.16792v1": {
    "title": "ChatChecker: A Framework for Dialogue System Testing and Evaluation\n  Through Non-cooperative User Simulation",
    "summary": "While modern dialogue systems heavily rely on large language models (LLMs),\ntheir implementation often goes beyond pure LLM interaction. Developers\nintegrate multiple LLMs, external tools, and databases. Therefore, assessment\nof the underlying LLM alone does not suffice, and the dialogue systems must be\ntested and evaluated as a whole. However, this remains a major challenge. With\nmost previous work focusing on turn-level analysis, less attention has been\npaid to integrated dialogue-level quality assurance. To address this, we\npresent ChatChecker, a framework for automated evaluation and testing of\ncomplex dialogue systems. ChatChecker uses LLMs to simulate diverse user\ninteractions, identify dialogue breakdowns, and evaluate quality. Compared to\nprevious approaches, our design reduces setup effort and is generalizable, as\nit does not require reference dialogues and is decoupled from the\nimplementation of the target dialogue system. We improve breakdown detection\nperformance over a prior LLM-based approach by including an error taxonomy in\nthe prompt. Additionally, we propose a novel non-cooperative user simulator\nbased on challenging personas that uncovers weaknesses in target dialogue\nsystems more effectively. Through this, ChatChecker contributes to thorough and\nscalable testing. This enables both researchers and practitioners to accelerate\nthe development of robust dialogue systems.",
    "published": "2025-07-22T17:40:34Z",
    "updated": "2025-07-22T17:40:34Z",
    "id": "2507.16792v1",
    "authors": [
      "Roman Mayr",
      "Michel Schimpf",
      "Thomas Bohn"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16792v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16792v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16792v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation and testing of dialogue systems that heavily rely on LLMs, focusing on integrated dialogue-level quality assurance and using LLMs for simulating user interactions and evaluating quality. This aligns with the topics of LLM (Large Language Models) and Benchmark (evaluation metrics and performance comparison).",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.16784v1": {
    "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
    "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
    "published": "2025-07-22T17:30:04Z",
    "updated": "2025-07-22T17:30:04Z",
    "id": "2507.16784v1",
    "authors": [
      "Hongyin Luo",
      "Nathaniel Morgan",
      "Tina Li",
      "Derek Zhao",
      "Ai Vy Ngo",
      "Philip Schroeder",
      "Lijie Yang",
      "Assaf Ben-Kish",
      "Jack O'Brien",
      "James Glass"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16784v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16784v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16784v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses overcoming context limits in LLMs through recursive and decompositional problem solving, which involves reasoning and memory aspects of LLMs. It introduces a model and runtime for long-horizon structured reasoning, which aligns with topics related to reasoning abilities in LLMs and memory-augmented models.",
    "llm_cls_result": [
      "Reasoning",
      "Memory"
    ]
  },
  "2507.16773v1": {
    "title": "When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning\n  LLMs",
    "summary": "Large Language Models (LLMs) have become integral to automated code analysis,\nenabling tasks such as vulnerability detection and code comprehension. However,\ntheir integration introduces novel attack surfaces. In this paper, we identify\nand investigate a new class of prompt-based attacks, termed Copy-Guided Attacks\n(CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs.\nBy injecting carefully crafted triggers into external code snippets,\nadversaries can induce the model to replicate malicious content during\ninference. This behavior enables two classes of vulnerabilities: inference\nlength manipulation, where the model generates abnormally short or excessively\nlong reasoning traces; and inference result manipulation, where the model\nproduces misleading or incorrect conclusions. We formalize CGA as an\noptimization problem and propose a gradient-based approach to synthesize\neffective triggers. Empirical evaluation on state-of-the-art reasoning LLMs\nshows that CGA reliably induces infinite loops, premature termination, false\nrefusals, and semantic distortions in code analysis tasks. While highly\neffective in targeted settings, we observe challenges in generalizing CGA\nacross diverse prompts due to computational constraints, posing an open\nquestion for future research. Our findings expose a critical yet underexplored\nvulnerability in LLM-powered development pipelines and call for urgent advances\nin prompt-level defense mechanisms.",
    "published": "2025-07-22T17:21:36Z",
    "updated": "2025-07-22T17:21:36Z",
    "id": "2507.16773v1",
    "authors": [
      "Yue Li",
      "Xiao Li",
      "Hao Wu",
      "Yue Zhang",
      "Fengyuan Xu",
      "Xiuzhen Cheng",
      "Sheng Zhong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16773v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16773v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16773v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses vulnerabilities in reasoning-capable LLMs, specifically focusing on prompt-based attacks that exploit the models' copying tendencies. This aligns with topics related to LLM reasoning and security aspects of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16768v1": {
    "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
    "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
    "published": "2025-07-22T17:13:47Z",
    "updated": "2025-07-22T17:13:47Z",
    "id": "2507.16768v1",
    "authors": [
      "Ran Wang",
      "Xiaoxuan Liu",
      "Hao Ren",
      "Gang Chen",
      "Fanchao Qi",
      "Maosong Sun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16768v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16768v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16768v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses structured decoding in large language models (LLMs) and proposes a method to improve efficiency by leveraging prior knowledge about output structure. This aligns with the topics of LLM (Large Language Models) and Reasoning (as it involves structured output generation and problem-solving within LLMs).",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.17778v1": {
    "title": "An advanced AI driven database system",
    "summary": "Contemporary database systems, while effective, suffer severe issues related\nto complexity and usability, especially among individuals who lack technical\nexpertise but are unfamiliar with query languages like Structured Query\nLanguage (SQL). This paper presents a new database system supported by\nArtificial Intelligence (AI), which is intended to improve the management of\ndata using natural language processing (NLP) - based intuitive interfaces, and\nautomatic creation of structured queries and semi-structured data formats like\nyet another markup language (YAML), java script object notation (JSON), and\napplication program interface (API) documentation. The system is intended to\nstrengthen the potential of databases through the integration of Large Language\nModels (LLMs) and advanced machine learning algorithms. The integration is\npurposed to allow the automation of fundamental tasks such as data modeling,\nschema creation, query comprehension, and performance optimization. We present\nin this paper a system that aims to alleviate the main problems with current\ndatabase technologies. It is meant to reduce the need for technical skills,\nmanual tuning for better performance, and the potential for human error. The AI\ndatabase employs generative schema inference and format selection to build its\nschema models and execution formats.",
    "published": "2025-07-22T16:10:45Z",
    "updated": "2025-07-22T16:10:45Z",
    "id": "2507.17778v1",
    "authors": [
      "M. Tedeschi",
      "S. Rizwan",
      "C. Shringi",
      "V. Devram Chandgir",
      "S. Belich"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17778v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17778v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17778v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) and advanced machine learning algorithms to improve database systems, focusing on natural language processing and automation of database tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16727v2": {
    "title": "Deliberative Searcher: Improving LLM Reliability via Reinforcement\n  Learning with constraints",
    "summary": "Improving the reliability of large language models (LLMs) is critical for\ndeploying them in real-world scenarios. In this paper, we propose\n\\textbf{Deliberative Searcher}, the first framework to integrate certainty\ncalibration with retrieval-based search for open-domain question answering. The\nagent performs multi-step reflection and verification over Wikipedia data and\nis trained with a reinforcement learning algorithm that optimizes for accuracy\nunder a soft reliability constraint. Empirical results show that proposed\nmethod improves alignment between model confidence and correctness, leading to\nmore trustworthy outputs. This paper will be continuously updated.",
    "published": "2025-07-22T16:09:34Z",
    "updated": "2025-07-23T03:52:14Z",
    "id": "2507.16727v2",
    "authors": [
      "Zhenyun Yin",
      "Shujie Wang",
      "Xuhong Wang",
      "Xingjun Ma",
      "Yinchun Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16727v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16727v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16727v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the reliability of LLMs using reinforcement learning and integrates retrieval-based methods, which aligns with topics on LLM reinforcement learning (RL) and memory-augmented models (Memory).",
    "llm_cls_result": [
      "RL",
      "Memory"
    ]
  },
  "2507.16708v1": {
    "title": "Biases in LLM-Generated Musical Taste Profiles for Recommendation",
    "summary": "One particularly promising use case of Large Language Models (LLMs) for\nrecommendation is the automatic generation of Natural Language (NL) user taste\nprofiles from consumption data. These profiles offer interpretable and editable\nalternatives to opaque collaborative filtering representations, enabling\ngreater transparency and user control. However, it remains unclear whether\nusers consider these profiles to be an accurate representation of their taste,\nwhich is crucial for trust and usability. Moreover, because LLMs inherit\nsocietal and data-driven biases, profile quality may systematically vary across\nuser and item characteristics. In this paper, we study this issue in the\ncontext of music streaming, where personalization is challenged by a large and\nculturally diverse catalog. We conduct a user study in which participants rate\nNL profiles generated from their own listening histories. We analyze whether\nidentification with the profiles is biased by user attributes (e.g.,\nmainstreamness, taste diversity) and item features (e.g., genre, country of\norigin). We also compare these patterns to those observed when using the\nprofiles in a downstream recommendation task. Our findings highlight both the\npotential and limitations of scrutable, LLM-based profiling in personalized\nsystems.",
    "published": "2025-07-22T15:44:10Z",
    "updated": "2025-07-22T15:44:10Z",
    "id": "2507.16708v1",
    "authors": [
      "Bruno Sguerra",
      "Elena V. Epure",
      "Harin Lee",
      "Manuel Moussallam"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16708v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16708v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16708v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating user taste profiles in the context of music recommendation, focusing on biases and user perception. The core topics are related to LLMs and their application in recommendation systems, which aligns with the 'LLM' and 'Benchmark' categories as it involves evaluating the performance and biases of LLM-generated profiles.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.16692v1": {
    "title": "Generating Search Explanations using Large Language Models",
    "summary": "Aspect-oriented explanations in search results are typically concise text\nsnippets placed alongside retrieved documents to serve as explanations that\nassist users in efficiently locating relevant information. While Large Language\nModels (LLMs) have demonstrated exceptional performance for a range of\nproblems, their potential to generate explanations for search results has not\nbeen explored. This study addresses that gap by leveraging both encoder-decoder\nand decoder-only LLMs to generate explanations for search results. The\nexplanations generated are consistently more accurate and plausible\nexplanations than those produced by a range of baseline models.",
    "published": "2025-07-22T15:29:39Z",
    "updated": "2025-07-22T15:29:39Z",
    "id": "2507.16692v1",
    "authors": [
      "Arif Laksito",
      "Mark Stevenson"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16692v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16692v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16692v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) to generate explanations for search results, which directly relates to the research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.16679v1": {
    "title": "PICACO: Pluralistic In-Context Value Alignment of LLMs via Total\n  Correlation Optimization",
    "summary": "In-Context Learning has shown great potential for aligning Large Language\nModels (LLMs) with human values, helping reduce harmful outputs and accommodate\ndiverse preferences without costly post-training, known as In-Context Alignment\n(ICA). However, LLMs' comprehension of input prompts remains agnostic, limiting\nICA's ability to address value tensions--human values are inherently\npluralistic, often imposing conflicting demands, e.g., stimulation vs.\ntradition. Current ICA methods therefore face the Instruction Bottleneck\nchallenge, where LLMs struggle to reconcile multiple intended values within a\nsingle prompt, leading to incomplete or biased alignment. To address this, we\npropose PICACO, a novel pluralistic ICA method. Without fine-tuning, PICACO\noptimizes a meta-instruction that navigates multiple values to better elicit\nLLMs' understanding of them and improve their alignment. This is achieved by\nmaximizing the total correlation between specified values and LLM responses,\ntheoretically reinforcing value correlation while reducing distractive noise,\nresulting in effective value instructions. Extensive experiments on five value\nsets show that PICACO works well with both black-box and open-source LLMs,\noutperforms several recent strong baselines, and achieves a better balance\nacross up to 8 distinct values.",
    "published": "2025-07-22T15:14:56Z",
    "updated": "2025-07-22T15:14:56Z",
    "id": "2507.16679v1",
    "authors": [
      "Han Jiang",
      "Dongyao Zhu",
      "Zhihua Wei",
      "Xiaoyuan Yi",
      "Ziang Xiao",
      "Xing Xie"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16679v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16679v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16679v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses in-context learning for aligning Large Language Models (LLMs) with human values, which involves optimizing meta-instructions to address value tensions and improve alignment. This falls under the topics of LLM (Large Language Models) and RL (Reinforcement Learning with Human Feedback, RLHF), as it involves aligning models with human values and preferences.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.16676v1": {
    "title": "Custom Algorithm-based Fault Tolerance for Attention Layers in\n  Transformers",
    "summary": "Transformers and large language models (LLMs), powered by the attention\nmechanism, have transformed numerous AI applications, driving the need for\nspecialized hardware accelerators. A major challenge in these accelerators is\nefficiently detecting errors caused by random hardware faults. Traditional\nalgorithm-based fault tolerance (ABFT) techniques verify individual matrix\nmultiplications but fall short in handling the full attention mechanism,\nparticularly due to intermediate softmax normalization. This work proposes\nFlash-ABFT, a novel method that computes an online checksum across the entire\nthree-matrix product of query, key and value matrices, of an attention layer,\nincluding the softmax operation, with a single check. This approach\nsignificantly reduces overhead by eliminating redundant checks while\nmaintaining high fault-detection accuracy. Experimental results demonstrate\nthat Flash-ABFT incurs only 5.3% hardware area overhead and less than 1.9%\nenergy overhead, making it a cost-effective and robust solution for error\ndetection in attention accelerators.",
    "published": "2025-07-22T15:11:13Z",
    "updated": "2025-07-22T15:11:13Z",
    "id": "2507.16676v1",
    "authors": [
      "Vasileios Titopoulos",
      "Kosmas Alexandridis",
      "Giorgos Dimitrakopoulos"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16676v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16676v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16676v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving fault tolerance in attention layers of Transformers and LLMs, which are core components of large language models. The work is specific to the architecture and hardware acceleration of these models, making 'LLM' the most relevant topic. The mention of 'attention mechanism' and 'hardware accelerators' further supports this classification.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.16672v1": {
    "title": "Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs",
    "summary": "Generative, explainable, and flexible recommender systems, derived using\nLarge Language Models (LLM) are promising and poorly adapted to the cold-start\nuser situation, where there is little to no history of interaction. The current\nsolutions i.e. supervised fine-tuning and collaborative filtering are\ndense-user-item focused and would be expensive to maintain and update. This\npaper introduces a meta-learning framework, that can be used to perform\nparameter-efficient prompt-tuning, to effectively personalize LLM-based\nrecommender systems quickly at cold-start. The model learns soft prompt\nembeddings with first-order (Reptile) and second-order (MAML) optimization by\ntreating each of the users as the tasks. As augmentations to the input tokens,\nthese learnable vectors are the differentiable control variables that represent\nuser behavioral priors. The prompts are meta-optimized through episodic\nsampling, inner-loop adaptation, and outer-loop generalization. On\nMovieLens-1M, Amazon Reviews, and Recbole, we can see that our adaptive model\noutperforms strong baselines in NDCG@10, HR@10, and MRR, and it runs in\nreal-time (i.e., below 300 ms) on consumer GPUs. Zero-history personalization\nis also supported by this scalable solution, and its 275 ms rate of adaptation\nallows successful real-time risk profiling of financial systems by shortening\ndetection latency and improving payment network stability. Crucially, the 275\nms adaptation capability can enable real-time risk profiling for financial\ninstitutions, reducing systemic vulnerability detection latency significantly\nversus traditional compliance checks. By preventing contagion in payment\nnetworks (e.g., Fedwire), the framework strengthens national financial\ninfrastructure resilience.",
    "published": "2025-07-22T15:07:23Z",
    "updated": "2025-07-22T15:07:23Z",
    "id": "2507.16672v1",
    "authors": [
      "Yushang Zhao",
      "Huijie Shen",
      "Dannier Li",
      "Lu Chang",
      "Chengrui Zhou",
      "Yinuo Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16672v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16672v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16672v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using meta-learning for prompt-tuning in LLMs to address cold-start personalization in recommender systems, which involves LLM-based personalization and optimization techniques.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Pretrain"
    ]
  },
  "2507.16642v1": {
    "title": "Towards Automated Regulatory Compliance Verification in Financial\n  Auditing with Large Language Models",
    "summary": "The auditing of financial documents, historically a labor-intensive process,\nstands on the precipice of transformation. AI-driven solutions have made\ninroads into streamlining this process by recommending pertinent text passages\nfrom financial reports to align with the legal requirements of accounting\nstandards. However, a glaring limitation remains: these systems commonly fall\nshort in verifying if the recommended excerpts indeed comply with the specific\nlegal mandates. Hence, in this paper, we probe the efficiency of publicly\navailable Large Language Models (LLMs) in the realm of regulatory compliance\nacross different model configurations. We place particular emphasis on\ncomparing cutting-edge open-source LLMs, such as Llama-2, with their\nproprietary counterparts like OpenAI's GPT models. This comparative analysis\nleverages two custom datasets provided by our partner PricewaterhouseCoopers\n(PwC) Germany. We find that the open-source Llama-2 70 billion model\ndemonstrates outstanding performance in detecting non-compliance or true\nnegative occurrences, beating all their proprietary counterparts. Nevertheless,\nproprietary models such as GPT-4 perform the best in a broad variety of\nscenarios, particularly in non-English contexts.",
    "published": "2025-07-22T14:39:54Z",
    "updated": "2025-07-22T14:39:54Z",
    "id": "2507.16642v1",
    "authors": [
      "Armin Berger",
      "Lars Hillebrand",
      "David Leonhard",
      "Tobias Deuer",
      "Thiago Bell Felix de Oliveira",
      "Tim Dilmaghani",
      "Mohamed Khaled",
      "Bernd Kliem",
      "Rdiger Loitz",
      "Christian Bauckhage",
      "Rafet Sifa"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16642v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16642v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16642v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in financial auditing for regulatory compliance verification, comparing different LLMs including open-source and proprietary models. The focus is on the efficiency and performance of LLMs in this specific domain.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.16587v1": {
    "title": "On the Effectiveness of LLM-as-a-judge for Code Generation and\n  Summarization",
    "summary": "Large Language Models have been recently exploited as judges for complex\nnatural language processing tasks, such as Q&A. The basic idea is to delegate\nto an LLM the assessment of the \"quality\" of the output provided by an\nautomated technique for tasks for which: (i) quantitative metrics would only\ntell part of the story, and; (ii) a large-scale human-based evaluation would be\ntoo expensive. LLMs-as-a-judge, if proven effective for a specific task, can\nalso unlock new possibilities for automation, with several LLMs proposing a\nsolution for a given instance of the task and others judging and deciding what\nis the best output to show the user. We study the effectiveness of\nLLMs-as-a-judge for two code-related tasks, namely code generation and code\nsummarization. The rationale for choosing these tasks is two-fold. First,\nquantitative metrics are usually not enough for the assessment of code\nsummarizers/generators. For example, it is well documented that metrics such as\nBLEU are quite weak proxies for the quality of the generated summaries. Second,\neven state-of-the-art techniques still struggle with handling complex instances\nof these tasks, making them good candidates for benefiting from more advanced\nsolutions envisioning collaboration among LLMs. For code generation, we check\nwhether eight LLMs are able to judge the correctness of 1,405 Java methods and\n1,281 Python functions generated by the same LLMs or implemented by humans. For\ncode summarization, we compare the judgment of five LLMs to those provided by\nnine humans for ~1.2k summaries, related to both Java and Python functions. Our\nfindings show that GPT-4-turbo is the best LLM in terms of judging capabilities\nfor both tasks, with \"smaller\" LLMs featuring tens of billions parameters not\nbeing able to cope with judging tasks. However, even the best-performing LLM\nfrequently misjudges the correctness of the code and summary quality.",
    "published": "2025-07-22T13:40:26Z",
    "updated": "2025-07-22T13:40:26Z",
    "id": "2507.16587v1",
    "authors": [
      "Giuseppe Crupi",
      "Rosalia Tufano",
      "Alejandro Velasco",
      "Antonio Mastropaolo",
      "Denys Poshyvanyk",
      "Gabriele Bavota"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16587v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16587v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16587v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) as judges for evaluating code generation and summarization tasks, focusing on their effectiveness and limitations. This directly relates to research on LLMs and their applications in evaluating complex tasks.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.16585v1": {
    "title": "LLMxCPG: Context-Aware Vulnerability Detection Through Code Property\n  Graph-Guided Large Language Models",
    "summary": "Software vulnerabilities present a persistent security challenge, with over\n25,000 new vulnerabilities reported in the Common Vulnerabilities and Exposures\n(CVE) database in 2024 alone. While deep learning based approaches show promise\nfor vulnerability detection, recent studies reveal critical limitations in\nterms of accuracy and robustness: accuracy drops by up to 45% on rigorously\nverified datasets, and performance degrades significantly under simple code\nmodifications. This paper presents LLMxCPG, a novel framework integrating Code\nProperty Graphs (CPG) with Large Language Models (LLM) for robust vulnerability\ndetection. Our CPG-based slice construction technique reduces code size by\n67.84 to 90.93% while preserving vulnerability-relevant context. Our approach's\nability to provide a more concise and accurate representation of code snippets\nenables the analysis of larger code segments, including entire projects. This\nconcise representation is a key factor behind the improved detection\ncapabilities of our method, as it can now identify vulnerabilities that span\nmultiple functions. Empirical evaluation demonstrates LLMxCPG's effectiveness\nacross verified datasets, achieving 15-40% improvements in F1-score over\nstate-of-the-art baselines. Moreover, LLMxCPG maintains high performance across\nfunction-level and multi-function codebases while exhibiting robust detection\nefficacy under various syntactic code modifications.",
    "published": "2025-07-22T13:36:33Z",
    "updated": "2025-07-22T13:36:33Z",
    "id": "2507.16585v1",
    "authors": [
      "Ahmed Lekssays",
      "Hamza Mouhcine",
      "Khang Tran",
      "Ting Yu",
      "Issa Khalil"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16585v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16585v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16585v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on integrating Large Language Models (LLM) with Code Property Graphs (CPG) for vulnerability detection, which primarily involves the use of LLMs in a specific application context.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.16576v1": {
    "title": "From Text to Actionable Intelligence: Automating STIX Entity and\n  Relationship Extraction",
    "summary": "Sharing methods of attack and their effectiveness is a cornerstone of\nbuilding robust defensive systems. Threat analysis reports, produced by various\nindividuals and organizations, play a critical role in supporting security\noperations and combating emerging threats. To enhance the timeliness and\nautomation of threat intelligence sharing, several standards have been\nestablished, with the Structured Threat Information Expression (STIX) framework\nemerging as one of the most widely adopted. However, generating STIX-compatible\ndata from unstructured security text remains a largely manual, expert-driven\nprocess. To address this challenge, we introduce AZERG, a tool designed to\nassist security analysts in automatically generating structured STIX\nrepresentations. To achieve this, we adapt general-purpose large language\nmodels for the specific task of extracting STIX-formatted threat data. To\nmanage the complexity, the task is divided into four subtasks: entity detection\n(T1), entity type identification (T2), related pair detection (T3), and\nrelationship type identification (T4). We apply task-specific fine-tuning to\naccurately extract relevant entities and infer their relationships in\naccordance with the STIX specification. To address the lack of training data,\nwe compiled a comprehensive dataset with 4,011 entities and 2,075 relationships\nextracted from 141 full threat analysis reports, all annotated in alignment\nwith the STIX standard. Our models achieved F1-scores of 84.43% for T1, 88.49%\nfor T2, 95.47% for T3, and 84.60% for T4 in real-world scenarios. We validated\ntheir performance against a range of open- and closed-parameter models, as well\nas state-of-the-art methods, demonstrating improvements of 2-25% across tasks.",
    "published": "2025-07-22T13:27:09Z",
    "updated": "2025-07-22T13:27:09Z",
    "id": "2507.16576v1",
    "authors": [
      "Ahmed Lekssays",
      "Husrev Taha Sencar",
      "Ting Yu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16576v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16576v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16576v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on adapting general-purpose large language models for specific tasks related to threat intelligence and STIX data extraction, which involves fine-tuning and dataset creation. However, it does not directly align with the provided topics which are more about core LLM research, multimodal models, or general AI topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.16572v1": {
    "title": "Pixels to Principles: Probing Intuitive Physics Understanding in\n  Multimodal Language Models",
    "summary": "This paper presents a systematic evaluation of state-of-the-art multimodal\nlarge language models (MLLMs) on intuitive physics tasks using the GRASP and\nIntPhys 2 datasets. We assess the open-source models InternVL 2.5, Qwen 2.5 VL,\nLLaVA-OneVision, and the proprietary Gemini 2.0 Flash Thinking, finding that\neven the latest models struggle to reliably distinguish physically plausible\nfrom implausible scenarios. To go beyond performance metrics, we conduct a\nprobing analysis of model embeddings, extracting intermediate representations\nat key processing stages to examine how well task-relevant information is\npreserved. Our results show that, depending on task difficulty, a critical\nvision-language misalignment can emerge: vision encoders successfully capture\nphysical plausibility cues, but this information is not effectively utilized by\nthe language model, leading to failures in reasoning. This misalignment\nsuggests that the primary limitation of MLLMs in intuitive physics tasks is not\nthe vision component but the ineffective integration of visual and linguistic\ninformation. Our findings highlight vision-language alignment as a key area for\nimprovement, offering insights for future MLLMs development.",
    "published": "2025-07-22T13:24:42Z",
    "updated": "2025-07-22T13:24:42Z",
    "id": "2507.16572v1",
    "authors": [
      "Mohamad Ballout",
      "Serwan Jassim",
      "Elia Bruni"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16572v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16572v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16572v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating multimodal large language models (MLLMs) on intuitive physics tasks, highlighting issues with vision-language alignment and the integration of visual and linguistic information. This aligns with the topics of MLLM (Multimodal Large Language Models) and VLA (Vision-Language Alignment models).",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.16564v1": {
    "title": "TTMBA: Towards Text To Multiple Sources Binaural Audio Generation",
    "summary": "Most existing text-to-audio (TTA) generation methods produce mono outputs,\nneglecting essential spatial information for immersive auditory experiences. To\naddress this issue, we propose a cascaded method for text-to-multisource\nbinaural audio generation (TTMBA) with both temporal and spatial control.\nFirst, a pretrained large language model (LLM) segments the text into a\nstructured format with time and spatial details for each sound event. Next, a\npretrained mono audio generation network creates multiple mono audios with\nvarying durations for each event. These mono audios are transformed into\nbinaural audios using a binaural rendering neural network based on spatial data\nfrom the LLM. Finally, the binaural audios are arranged by their start times,\nresulting in multisource binaural audio. Experimental results demonstrate the\nsuperiority of the proposed method in terms of both audio generation quality\nand spatial perceptual accuracy.",
    "published": "2025-07-22T13:16:07Z",
    "updated": "2025-07-22T13:16:07Z",
    "id": "2507.16564v1",
    "authors": [
      "Yuxuan He",
      "Xiaoran Yang",
      "Ningning Pan",
      "Gongping Huang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16564v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16564v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16564v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a pretrained large language model (LLM) to segment text for generating binaural audio, which involves multimodal processing (audio and text) and leverages LLM capabilities.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.16557v1": {
    "title": "Exploring Gender Bias in Large Language Models: An In-depth Dive into\n  the German Language",
    "summary": "In recent years, various methods have been proposed to evaluate gender bias\nin large language models (LLMs). A key challenge lies in the transferability of\nbias measurement methods initially developed for the English language when\napplied to other languages. This work aims to contribute to this research\nstrand by presenting five German datasets for gender bias evaluation in LLMs.\nThe datasets are grounded in well-established concepts of gender bias and are\naccessible through multiple methodologies. Our findings, reported for eight\nmultilingual LLM models, reveal unique challenges associated with gender bias\nin German, including the ambiguous interpretation of male occupational terms\nand the influence of seemingly neutral nouns on gender perception. This work\ncontributes to the understanding of gender bias in LLMs across languages and\nunderscores the necessity for tailored evaluation frameworks.",
    "published": "2025-07-22T13:09:41Z",
    "updated": "2025-07-22T13:09:41Z",
    "id": "2507.16557v1",
    "authors": [
      "Kristin Gnadt",
      "David Thulke",
      "Simone Kopeinik",
      "Ralf Schlter"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16557v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16557v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16557v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating gender bias in large language models (LLMs) using German datasets, which aligns with research on LLMs and their evaluation.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.16530v1": {
    "title": "Learning Text Styles: A Study on Transfer, Attribution, and Verification",
    "summary": "This thesis advances the computational understanding and manipulation of text\nstyles through three interconnected pillars: (1) Text Style Transfer (TST),\nwhich alters stylistic properties (e.g., sentiment, formality) while preserving\ncontent; (2)Authorship Attribution (AA), identifying the author of a text via\nstylistic fingerprints; and (3) Authorship Verification (AV), determining\nwhether two texts share the same authorship. We address critical challenges in\nthese areas by leveraging parameter-efficient adaptation of large language\nmodels (LLMs), contrastive disentanglement of stylistic features, and\ninstruction-based fine-tuning for explainable verification.",
    "published": "2025-07-22T12:38:39Z",
    "updated": "2025-07-22T12:38:39Z",
    "id": "2507.16530v1",
    "authors": [
      "Zhiqiang Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16530v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16530v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16530v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on text style manipulation and authorship analysis, leveraging large language models (LLMs) for tasks such as Text Style Transfer, Authorship Attribution, and Authorship Verification. The core topics are related to LLMs and their applications in text style manipulation and analysis.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.16488v1": {
    "title": "ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination\n  Detection in LLMs",
    "summary": "Large language models (LLMs) excel at various natural language processing\ntasks, but their tendency to generate hallucinations undermines their\nreliability. Existing hallucination detection methods leveraging hidden states\npredominantly focus on static and isolated representations, overlooking their\ndynamic evolution across layers, which limits efficacy. To address this\nlimitation, we shift the focus to the hidden state update process and introduce\na novel metric, the ICR Score (Information Contribution to Residual Stream),\nwhich quantifies the contribution of modules to the hidden states' update. We\nempirically validate that the ICR Score is effective and reliable in\ndistinguishing hallucinations. Building on these insights, we propose a\nhallucination detection method, the ICR Probe, which captures the cross-layer\nevolution of hidden states. Experimental results show that the ICR Probe\nachieves superior performance with significantly fewer parameters. Furthermore,\nablation studies and case analyses offer deeper insights into the underlying\nmechanism of this method, improving its interpretability.",
    "published": "2025-07-22T11:44:26Z",
    "updated": "2025-07-22T11:44:26Z",
    "id": "2507.16488v1",
    "authors": [
      "Zhenliang Zhang",
      "Xinyu Hu",
      "Huixuan Zhang",
      "Junzhe Zhang",
      "Xiaojun Wan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16488v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16488v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16488v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on detecting hallucinations in Large Language Models (LLMs) by analyzing hidden state dynamics, which directly relates to the reliability and internal workings of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16478v1": {
    "title": "ACT: Bridging the Gap in Code Translation through Synthetic Data\n  Generation & Adaptive Training",
    "summary": "Code translation is a crucial process in software development and migration\nprojects, enabling interoperability between different programming languages and\nenhancing software adaptability and thus longevity. Traditional automated\ntranslation methods rely heavily on handcrafted transformation rules, which\noften lack flexibility and scalability. Meanwhile, advanced language models\npresent promising alternatives but are often limited by proprietary, API-based\nimplementations that raise concerns over data security and reliance. In this\npaper, we present Auto-Train for Code Translation (ACT), an innovative\nframework that aims to improve code translation capabilities by enabling\nin-house finetuning of open-source Large Language Models (LLMs). ACT's\nautomated pipeline significantly boosts the performance of these models,\nnarrowing the gap between open-source accessibility and the high performance of\nclosed-source solutions. Central to ACT is its synthetic data generation\nmodule, which builds extensive, high-quality datasets from initial code\nsamples, incorporating unit tests to ensure functional accuracy and diversity.\nACT's evaluation framework incorporates execution-level checks, offering a\ncomprehensive assessment of translation quality. A key feature in ACT is its\ncontroller module, which manages the entire pipeline by dynamically adjusting\nhyperparameters, orchestrating iterative data generation, and finetuning based\non real-time evaluations. This enables ACT to intelligently optimize when to\ncontinue training, generate additional targeted training data, or stop the\nprocess. Our results demonstrate that ACT consistently enhances the\neffectiveness of open-source models, offering businesses and developers a\nsecure and reliable alternative. Additionally, applying our data generation\npipeline to industry-scale migration projects has led to a notable increase in\ndeveloper acceleration.",
    "published": "2025-07-22T11:35:35Z",
    "updated": "2025-07-22T11:35:35Z",
    "id": "2507.16478v1",
    "authors": [
      "Shreya Saxena",
      "Siva Prasad",
      "Zishan Ahmad",
      "Vishal Vaddina"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16478v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16478v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16478v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving code translation capabilities using open-source Large Language Models (LLMs) through synthetic data generation and adaptive training. It directly involves LLMs and their application in a specific domain (code translation), but does not explicitly align with other listed topics like RL, MLLM, VLA, etc.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.16877v1": {
    "title": "ReMeREC: Relation-aware and Multi-entity Referring Expression\n  Comprehension",
    "summary": "Referring Expression Comprehension (REC) aims to localize specified entities\nor regions in an image based on natural language descriptions. While existing\nmethods handle single-entity localization, they often ignore complex\ninter-entity relationships in multi-entity scenes, limiting their accuracy and\nreliability. Additionally, the lack of high-quality datasets with fine-grained,\npaired image-text-relation annotations hinders further progress. To address\nthis challenge, we first construct a relation-aware, multi-entity REC dataset\ncalled ReMeX, which includes detailed relationship and textual annotations. We\nthen propose ReMeREC, a novel framework that jointly leverages visual and\ntextual cues to localize multiple entities while modeling their\ninter-relations. To address the semantic ambiguity caused by implicit entity\nboundaries in language, we introduce the Text-adaptive Multi-entity Perceptron\n(TMP), which dynamically infers both the quantity and span of entities from\nfine-grained textual cues, producing distinctive representations. Additionally,\nour Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and\nglobal scene understanding. To further improve language comprehension for\nfine-grained prompts, we also construct a small-scale auxiliary dataset,\nEntityText, generated using large language models. Experiments on four\nbenchmark datasets show that ReMeREC achieves state-of-the-art performance in\nmulti-entity grounding and relation prediction, outperforming existing\napproaches by a large margin.",
    "published": "2025-07-22T11:23:48Z",
    "updated": "2025-07-22T11:23:48Z",
    "id": "2507.16877v1",
    "authors": [
      "Yizhi Hu",
      "Zezhao Tian",
      "Xingqun Qi",
      "Chen Su",
      "Bingkun Yang",
      "Junhui Yin",
      "Muyi Sun",
      "Man Zhang",
      "Zhenan Sun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16877v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16877v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16877v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Referring Expression Comprehension (REC) in images with natural language descriptions, involving multi-entity localization and relation modeling. It introduces a novel framework and dataset, leveraging visual and textual cues, which aligns with Vision-Language Action (VLA) research. Additionally, the use of large language models for dataset generation hints at a connection to Multimodal Large Language Models (MLLM).",
    "llm_cls_result": [
      "VLA",
      "MLLM"
    ]
  },
  "2507.16459v1": {
    "title": "Towards Enforcing Company Policy Adherence in Agentic Workflows",
    "summary": "Large Language Model (LLM) agents hold promise for a flexible and scalable\nalternative to traditional business process automation, but struggle to\nreliably follow complex company policies. In this study we introduce a\ndeterministic, transparent, and modular framework for enforcing business policy\nadherence in agentic workflows. Our method operates in two phases: (1) an\noffline buildtime stage that compiles policy documents into verifiable guard\ncode associated with tool use, and (2) a runtime integration where these guards\nensure compliance before each agent action. We demonstrate our approach on the\nchallenging $\\tau$-bench Airlines domain, showing encouraging preliminary\nresults in policy enforcement, and further outline key challenges for\nreal-world deployments.",
    "published": "2025-07-22T11:00:37Z",
    "updated": "2025-07-22T11:00:37Z",
    "id": "2507.16459v1",
    "authors": [
      "Naama Zwerdling",
      "David Boaz",
      "Ella Rabinovich",
      "Guy Uziel",
      "David Amid",
      "Ateret Anaby-Tavor"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16459v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16459v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16459v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Model (LLM) agents in business process automation and focuses on enforcing company policy adherence, which aligns with the RL (Reinforcement Learning) topic due to the mention of agentic workflows and policy enforcement. Additionally, the mention of LLM agents suggests relevance to the LLM topic.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.16456v1": {
    "title": "An approach to measuring the performance of Automatic Speech Recognition\n  (ASR) models in the context of Large Language Model (LLM) powered\n  applications",
    "summary": "Automatic Speech Recognition (ASR) plays a crucial role in human-machine\ninteraction and serves as an interface for a wide range of applications.\nTraditionally, ASR performance has been evaluated using Word Error Rate (WER),\na metric that quantifies the number of insertions, deletions, and substitutions\nin the generated transcriptions. However, with the increasing adoption of large\nand powerful Large Language Models (LLMs) as the core processing component in\nvarious applications, the significance of different types of ASR errors in\ndownstream tasks warrants further exploration. In this work, we analyze the\ncapabilities of LLMs to correct errors introduced by ASRs and propose a new\nmeasure to evaluate ASR performance for LLM-powered applications.",
    "published": "2025-07-22T10:59:21Z",
    "updated": "2025-07-22T10:59:21Z",
    "id": "2507.16456v1",
    "authors": [
      "Sujith Pulikodan",
      "Sahapthan K",
      "Prasanta Kumar Ghosh",
      "Visruth Sanka",
      "Nihar Desai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16456v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16456v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16456v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of Automatic Speech Recognition (ASR) models in the context of Large Language Models (LLMs), focusing on how LLMs can correct ASR errors and proposing a new performance measure. This directly relates to LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.16439v1": {
    "title": "Exploring Large Language Models for Analyzing and Improving Method Names\n  in Scientific Code",
    "summary": "Research scientists increasingly rely on implementing software to support\ntheir research. While previous research has examined the impact of identifier\nnames on program comprehension in traditional programming environments, limited\nwork has explored this area in scientific software, especially regarding the\nquality of method names in the code. The recent advances in Large Language\nModels (LLMs) present new opportunities for automating code analysis tasks,\nsuch as identifier name appraisals and recommendations. Our study evaluates\nfour popular LLMs on their ability to analyze grammatical patterns and suggest\nimprovements for 496 method names extracted from Python-based Jupyter\nNotebooks. Our findings show that the LLMs are somewhat effective in analyzing\nthese method names and generally follow good naming practices, like starting\nmethod names with verbs. However, their inconsistent handling of\ndomain-specific terminology and only moderate agreement with human annotations\nindicate that automated suggestions require human evaluation. This work\nprovides foundational insights for improving the quality of scientific code\nthrough AI automation.",
    "published": "2025-07-22T10:33:49Z",
    "updated": "2025-07-22T10:33:49Z",
    "id": "2507.16439v1",
    "authors": [
      "Gunnar Larsen",
      "Carol Wong",
      "Anthony Peruma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16439v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16439v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16439v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) for analyzing and improving method names in scientific code, which directly relates to the 'LLM' topic. It does not significantly involve other topics such as RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.16395v1": {
    "title": "LLM-Driven Collaborative Model for Untangling Commits via Explicit and\n  Implicit Dependency Reasoning",
    "summary": "Atomic commits, each of which addresses a single development concern, are a\nbest practice in software development. However, developers frequently produce\ntangled commits that mix unrelated changes due to practical constraints or\nunclear boundaries, negatively impacting code review and maintenance. Although\nprior commit untangling approaches: rule-based, feature-based, or graph-based,\nhave made progress, they often rely on shallow signals and fail to distinguish\nbetween explicit dependencies (e.g., control/data flow) and implicit ones\n(e.g., semantic or conceptual relationships). In this paper, we propose\nColaUntangle, a new collaborative consultation framework for commit untangling\nthat models both explicit and implicit dependencies among code changes.\nColaUntangle integrates Large Language Model (LLM)-driven agents in a\nmulti-agent architecture: one agent specializes in explicit dependencies,\nanother in implicit ones, and a reviewer agent synthesizes their perspectives\nthrough iterative consultation. To capture explicit and implicit contextual\ninformation, we construct multi-version Program Dependency Graphs (delta-PDG),\nenabling agents to reason over code relationships with both symbolic and\nsemantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C#\nand 14k Java tangled commits). Experimental results show that ColaUntangle\noutperforms the best-performing baseline, achieving an improvement of 44% on\nthe C# dataset and 100% on the Java dataset. These findings highlight the\npotential of LLM-based collaborative frameworks for advancing automated commit\nuntangling tasks.",
    "published": "2025-07-22T09:42:13Z",
    "updated": "2025-07-22T09:42:13Z",
    "id": "2507.16395v1",
    "authors": [
      "Bo Hou",
      "Xin Tan",
      "Kai Zheng",
      "Fang Liu",
      "Yinghao Zhu",
      "Li Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16395v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16395v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16395v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a collaborative framework for commit untangling, focusing on both explicit and implicit dependencies in code changes. The core topics are related to LLMs and their application in reasoning tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16382v1": {
    "title": "Application of LLM Guided Reinforcement Learning in Formation Control\n  with Collision Avoidance",
    "summary": "Multi-Agent Systems (MAS) excel at accomplishing complex objectives through\nthe collaborative efforts of individual agents. Among the methodologies\nemployed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of\nthe most efficacious algorithms. However, when confronted with the complex\nobjective of Formation Control with Collision Avoidance (FCCA): designing an\neffective reward function that facilitates swift convergence of the policy\nnetwork to an optimal solution. In this paper, we introduce a novel framework\nthat aims to overcome this challenge. By giving large language models (LLMs) on\nthe prioritization of tasks and the observable information available to each\nagent, our framework generates reward functions that can be dynamically\nadjusted online based on evaluation outcomes by employing more advanced\nevaluation metrics rather than the rewards themselves. This mechanism enables\nthe MAS to simultaneously achieve formation control and obstacle avoidance in\ndynamic environments with enhanced efficiency, requiring fewer iterations to\nreach superior performance levels. Our empirical studies, conducted in both\nsimulation and real-world settings, validate the practicality and effectiveness\nof our proposed approach.",
    "published": "2025-07-22T09:26:00Z",
    "updated": "2025-07-22T09:26:00Z",
    "id": "2507.16382v1",
    "authors": [
      "Chenhao Yao",
      "Zike Yuan",
      "Xiaoxu Liu",
      "Chi Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16382v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16382v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16382v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to guide Reinforcement Learning (RL) in multi-agent systems, specifically for formation control and collision avoidance. This involves both LLM and RL techniques, making these the most relevant topics.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.16372v1": {
    "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion",
    "summary": "Large Language Models (LLMs) are increasingly integrated into daily routines,\nyet they raise significant privacy and safety concerns. Recent research\nproposes collaborative inference, which outsources the early-layer inference to\nensure data locality, and introduces model safety auditing based on inner\nneuron patterns. Both techniques expose the LLM's Internal States (ISs), which\nare traditionally considered irreversible to inputs due to optimization\nchallenges and the highly abstract representations in deep layers. In this\nwork, we challenge this assumption by proposing four inversion attacks that\nsignificantly improve the semantic similarity and token matching rate of\ninverted inputs. Specifically, we first develop two white-box\noptimization-based attacks tailored for low-depth and high-depth ISs. These\nattacks avoid local minima convergence, a limitation observed in prior work,\nthrough a two-phase inversion process. Then, we extend our optimization attack\nunder more practical black-box weight access by leveraging the transferability\nbetween the source and the derived LLMs. Additionally, we introduce a\ngeneration-based attack that treats inversion as a translation task, employing\nan inversion model to reconstruct inputs. Extensive evaluation of short and\nlong prompts from medical consulting and coding assistance datasets and 6 LLMs\nvalidates the effectiveness of our inversion attacks. Notably, a 4,112-token\nlong medical consulting prompt can be nearly perfectly inverted with 86.88 F1\ntoken matching from the middle layer of Llama-3 model. Finally, we evaluate\nfour practical defenses that we found cannot perfectly prevent ISs inversion\nand draw conclusions for future mitigation design.",
    "published": "2025-07-22T09:15:11Z",
    "updated": "2025-07-22T09:15:11Z",
    "id": "2507.16372v1",
    "authors": [
      "Tian Dong",
      "Yan Meng",
      "Shaofeng Li",
      "Guoxing Chen",
      "Zhen Liu",
      "Haojin Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16372v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16372v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16372v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses privacy and safety concerns related to Large Language Models (LLMs), focusing on internal states inversion attacks. It involves LLM architectures and their vulnerabilities, which are core topics in LLM research.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.16873v1": {
    "title": "HIPPO-Video: Simulating Watch Histories with Large Language Models for\n  Personalized Video Highlighting",
    "summary": "The exponential growth of video content has made personalized video\nhighlighting an essential task, as user preferences are highly variable and\ncomplex. Existing video datasets, however, often lack personalization, relying\non isolated videos or simple text queries that fail to capture the intricacies\nof user behavior. In this work, we introduce HIPPO-Video, a novel dataset for\npersonalized video highlighting, created using an LLM-based user simulator to\ngenerate realistic watch histories reflecting diverse user preferences. The\ndataset includes 2,040 (watch history, saliency score) pairs, covering 20,400\nvideos across 170 semantic categories. To validate our dataset, we propose\nHiPHer, a method that leverages these personalized watch histories to predict\npreference-conditioned segment-wise saliency scores. Through extensive\nexperiments, we demonstrate that our method outperforms existing generic and\nquery-based approaches, showcasing its potential for highly user-centric video\nhighlighting in real-world scenarios.",
    "published": "2025-07-22T08:24:33Z",
    "updated": "2025-07-22T08:24:33Z",
    "id": "2507.16873v1",
    "authors": [
      "Jeongeun Lee",
      "Youngjae Yu",
      "Dongha Lee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16873v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16873v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16873v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) to simulate user watch histories for personalized video highlighting, which involves LLM-based user simulation and dataset creation.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.16291v1": {
    "title": "Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers",
    "summary": "Voice phishing (vishing) remains a persistent threat in cybersecurity,\nexploiting human trust through persuasive speech. While machine learning\n(ML)-based classifiers have shown promise in detecting malicious call\ntranscripts, they remain vulnerable to adversarial manipulations that preserve\nsemantic content. In this study, we explore a novel attack vector where large\nlanguage models (LLMs) are leveraged to generate adversarial vishing\ntranscripts that evade detection while maintaining deceptive intent. We\nconstruct a systematic attack pipeline that employs prompt engineering and\nsemantic obfuscation to transform real-world vishing scripts using four\ncommercial LLMs. The generated transcripts are evaluated against multiple ML\nclassifiers trained on a real-world Korean vishing dataset (KorCCViD) with\nstatistical testing. Our experiments reveal that LLM-generated transcripts are\nboth practically and statistically effective against ML-based classifiers. In\nparticular, transcripts crafted by GPT-4o significantly reduce classifier\naccuracy (by up to 30.96%) while maintaining high semantic similarity, as\nmeasured by BERTScore. Moreover, these attacks are both time-efficient and\ncost-effective, with average generation times under 9 seconds and negligible\nfinancial cost per query. The results underscore the pressing need for more\nresilient vishing detection frameworks and highlight the imperative for LLM\nproviders to enforce stronger safeguards against prompt misuse in adversarial\nsocial engineering contexts.",
    "published": "2025-07-22T07:26:49Z",
    "updated": "2025-07-22T07:26:49Z",
    "id": "2507.16291v1",
    "authors": [
      "Wenhao Li",
      "Selvakumar Manickam",
      "Yung-wey Chong",
      "Shankar Karuppayah"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16291v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16291v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16291v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to generate adversarial vishing transcripts, which directly involves LLMs and their applications in cybersecurity. The focus on LLMs and their adversarial use aligns with the 'LLM' topic. The adversarial manipulation aspect also touches on 'RL' (Reinforcement Learning) as it involves strategies to evade detection, though this is secondary.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.16289v1": {
    "title": "Time to Split: Exploring Data Splitting Strategies for Offline\n  Evaluation of Sequential Recommenders",
    "summary": "Modern sequential recommender systems, ranging from lightweight\ntransformer-based variants to large language models, have become increasingly\nprominent in academia and industry due to their strong performance in the\nnext-item prediction task. Yet common evaluation protocols for sequential\nrecommendations remain insufficiently developed: they often fail to reflect the\ncorresponding recommendation task accurately, or are not aligned with\nreal-world scenarios.\n  Although the widely used leave-one-out split matches next-item prediction, it\npermits the overlap between training and test periods, which leads to temporal\nleakage and unrealistically long test horizon, limiting real-world relevance.\nGlobal temporal splitting addresses these issues by evaluating on distinct\nfuture periods. However, its applications to sequential recommendations remain\nloosely defined, particularly in terms of selecting target interactions and\nconstructing a validation subset that provides necessary consistency between\nvalidation and test metrics.\n  In this paper, we demonstrate that evaluation outcomes can vary significantly\nacross splitting strategies, influencing model rankings and practical\ndeployment decisions. To improve reproducibility in both academic and\nindustrial settings, we systematically compare different splitting strategies\nfor sequential recommendations across multiple datasets and established\nbaselines. Our findings show that prevalent splits, such as leave-one-out, may\nbe insufficiently aligned with more realistic evaluation strategies. Code:\nhttps://github.com/monkey0head/time-to-split",
    "published": "2025-07-22T07:20:52Z",
    "updated": "2025-07-22T07:20:52Z",
    "id": "2507.16289v1",
    "authors": [
      "Danil Gusak",
      "Anna Volodkevich",
      "Anton Klenitskiy",
      "Alexey Vasilev",
      "Evgeny Frolov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16289v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16289v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16289v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluation protocols and data splitting strategies for sequential recommender systems, which includes modern transformer-based variants and large language models. However, the primary focus is on evaluation methodologies rather than the models themselves, making it more relevant to benchmarking and dataset topics.",
    "llm_cls_result": [
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.16241v1": {
    "title": "eX-NIDS: A Framework for Explainable Network Intrusion Detection\n  Leveraging Large Language Models",
    "summary": "This paper introduces eX-NIDS, a framework designed to enhance\ninterpretability in flow-based Network Intrusion Detection Systems (NIDS) by\nleveraging Large Language Models (LLMs). In our proposed framework, flows\nlabelled as malicious by NIDS are initially processed through a module called\nthe Prompt Augmenter. This module extracts contextual information and Cyber\nThreat Intelligence (CTI)-related knowledge from these flows. This enriched,\ncontext-specific data is then integrated with an input prompt for an LLM,\nenabling it to generate detailed explanations and interpretations of why the\nflow was identified as malicious by NIDS. We compare the generated\ninterpretations against a Basic-Prompt Explainer baseline, which does not\nincorporate any contextual information into the LLM's input prompt. Our\nframework is quantitatively evaluated using the Llama 3 and GPT-4 models,\nemploying a novel evaluation method tailored for natural language explanations,\nfocusing on their correctness and consistency. The results demonstrate that\naugmented LLMs can produce accurate and consistent explanations, serving as\nvaluable complementary tools in NIDS to explain the classification of malicious\nflows. The use of augmented prompts enhances performance by over 20% compared\nto the Basic-Prompt Explainer.",
    "published": "2025-07-22T05:26:21Z",
    "updated": "2025-07-22T05:26:21Z",
    "id": "2507.16241v1",
    "authors": [
      "Paul R. B. Houssel",
      "Siamak Layeghy",
      "Priyanka Singh",
      "Marius Portmann"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16241v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16241v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16241v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging Large Language Models (LLMs) to enhance interpretability in Network Intrusion Detection Systems (NIDS), which directly involves the use of LLMs for generating explanations and interpretations.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16237v1": {
    "title": "LLM-Enhanced Reranking for Complementary Product Recommendation",
    "summary": "Complementary product recommendation, which aims to suggest items that are\nused together to enhance customer value, is a crucial yet challenging task in\ne-commerce. While existing graph neural network (GNN) approaches have made\nsignificant progress in capturing complex product relationships, they often\nstruggle with the accuracy-diversity tradeoff, particularly for long-tail\nitems. This paper introduces a model-agnostic approach that leverages Large\nLanguage Models (LLMs) to enhance the reranking of complementary product\nrecommendations. Unlike previous works that use LLMs primarily for data\npreprocessing and graph augmentation, our method applies LLM-based prompting\nstrategies directly to rerank candidate items retrieved from existing\nrecommendation models, eliminating the need for model retraining. Through\nextensive experiments on public datasets, we demonstrate that our approach\neffectively balances accuracy and diversity in complementary product\nrecommendations, with at least 50% lift in accuracy metrics and 2% lift in\ndiversity metrics on average for the top recommended items across datasets.",
    "published": "2025-07-22T05:15:45Z",
    "updated": "2025-07-22T05:15:45Z",
    "id": "2507.16237v1",
    "authors": [
      "Zekun Xu",
      "Yudi Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16237v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16237v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16237v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) to enhance the reranking of complementary product recommendations, which directly involves LLM applications in improving recommendation systems.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.16229v1": {
    "title": "Voice-based AI Agents: Filling the Economic Gaps in Digital Health\n  Delivery",
    "summary": "The integration of voice-based AI agents in healthcare presents a\ntransformative opportunity to bridge economic and accessibility gaps in digital\nhealth delivery. This paper explores the role of large language model\n(LLM)-powered voice assistants in enhancing preventive care and continuous\npatient monitoring, particularly in underserved populations. Drawing insights\nfrom the development and pilot study of Agent PULSE (Patient Understanding and\nLiaison Support Engine) -- a collaborative initiative between IBM Research,\nCleveland Clinic Foundation, and Morehouse School of Medicine -- we present an\neconomic model demonstrating how AI agents can provide cost-effective\nhealthcare services where human intervention is economically unfeasible. Our\npilot study with 33 inflammatory bowel disease patients revealed that 70\\%\nexpressed acceptance of AI-driven monitoring, with 37\\% preferring it over\ntraditional modalities. Technical challenges, including real-time\nconversational AI processing, integration with healthcare systems, and privacy\ncompliance, are analyzed alongside policy considerations surrounding\nregulation, bias mitigation, and patient autonomy. Our findings suggest that\nAI-driven voice agents not only enhance healthcare scalability and efficiency\nbut also improve patient engagement and accessibility. For healthcare\nexecutives, our cost-utility analysis demonstrates huge potential savings for\nroutine monitoring tasks, while technologists can leverage our framework to\nprioritize improvements yielding the highest patient impact. By addressing\ncurrent limitations and aligning AI development with ethical and regulatory\nframeworks, voice-based AI agents can serve as a critical entry point for\nequitable, sustainable digital healthcare solutions.",
    "published": "2025-07-22T05:01:06Z",
    "updated": "2025-07-22T05:01:06Z",
    "id": "2507.16229v1",
    "authors": [
      "Bo Wen",
      "Chen Wang",
      "Qiwei Han",
      "Raquel Norel",
      "Julia Liu",
      "Thaddeus Stappenbeck",
      "Jeffrey L. Rogers"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16229v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16229v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16229v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language model (LLM)-powered voice assistants in healthcare, which aligns with the LLM topic. It also touches on the application of these models in a specific domain (healthcare) and their integration with systems, which is relevant to the broader LLM research.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.16226v1": {
    "title": "Distilled Large Language Model in Confidential Computing Environment for\n  System-on-Chip Design",
    "summary": "Large Language Models (LLMs) are increasingly used in circuit design tasks\nand have typically undergone multiple rounds of training. Both the trained\nmodels and their associated training data are considered confidential\nintellectual property (IP) and must be protected from exposure. Confidential\nComputing offers a promising solution to protect data and models through\nTrusted Execution Environments (TEEs). However, existing TEE implementations\nare not designed to support the resource-intensive nature of LLMs efficiently.\nIn this work, we first present a comprehensive evaluation of the LLMs within a\nTEE-enabled confidential computing environment, specifically utilizing Intel\nTrust Domain Extensions (TDX). We constructed experiments on three\nenvironments: TEE-based, CPU-only, and CPU-GPU hybrid implementations, and\nevaluated their performance in terms of tokens per second.\n  Our first observation is that distilled models, i.e., DeepSeek, surpass other\nmodels in performance due to their smaller parameters, making them suitable for\nresource-constrained devices. Also, in the quantized models such as 4-bit\nquantization (Q4) and 8-bit quantization (Q8), we observed a performance gain\nof up to 3x compared to FP16 models. Our findings indicate that for fewer\nparameter sets, such as DeepSeek-r1-1.5B, the TDX implementation outperforms\nthe CPU version in executing computations within a secure environment. We\nfurther validate the results using a testbench designed for SoC design tasks.\nThese validations demonstrate the potential of efficiently deploying\nlightweight LLMs on resource-constrained systems for semiconductor CAD\napplications.",
    "published": "2025-07-22T04:41:27Z",
    "updated": "2025-07-22T04:41:27Z",
    "id": "2507.16226v1",
    "authors": [
      "Dong Ben",
      "Hui Feng",
      "Qian Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16226v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16226v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16226v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a specific application (system-on-chip design) and evaluates their performance in a confidential computing environment. The focus is on LLMs and their deployment in resource-constrained environments, which aligns with the 'LLM' and 'Scaling' topics.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.17774v1": {
    "title": "Human-AI Co-Creation: A Framework for Collaborative Design in\n  Intelligent Systems",
    "summary": "As artificial intelligence (AI) continues to evolve from a back-end\ncomputational tool into an interactive, generative collaborator, its\nintegration into early-stage design processes demands a rethinking of\ntraditional workflows in human-centered design. This paper explores the\nemergent paradigm of human-AI co-creation, where AI is not merely used for\nautomation or efficiency gains, but actively participates in ideation, visual\nconceptualization, and decision-making. Specifically, we investigate the use of\nlarge language models (LLMs) like GPT-4 and multimodal diffusion models such as\nStable Diffusion as creative agents that engage designers in iterative cycles\nof proposal, critique, and revision.",
    "published": "2025-07-22T04:29:33Z",
    "updated": "2025-07-22T04:29:33Z",
    "id": "2507.17774v1",
    "authors": [
      "Zhangqi Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17774v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17774v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17774v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) and multimodal diffusion models in human-AI co-creation, which involves collaborative design processes. This aligns with the topics of LLM (Large Language Models) and MLLM (Multimodal Large Language Models) as it involves both language and visual modalities in the creative process.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.16217v1": {
    "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
    "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
    "published": "2025-07-22T04:21:03Z",
    "updated": "2025-07-22T04:21:03Z",
    "id": "2507.16217v1",
    "authors": [
      "Shahriar Golchin",
      "Yanfei Chen",
      "Rujun Han",
      "Manan Gandhi",
      "Tianli Yu",
      "Swaroop Mishra",
      "Mihai Surdeanu",
      "Rishabh Agarwal",
      "Chen-Yu Lee",
      "Tomas Pfister"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16217v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16217v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16217v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses in-context learning (ICL) with long-context large language models (LLMs), focusing on strategies for demonstration selection to improve performance and reduce computational overhead. The core topics are related to LLMs and their reasoning abilities in ICL settings.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16213v1": {
    "title": "Advancing Visual Large Language Model for Multi-granular Versatile\n  Perception",
    "summary": "Perception is a fundamental task in the field of computer vision,\nencompassing a diverse set of subtasks that can be systematically categorized\ninto four distinct groups based on two dimensions: prediction type and\ninstruction type. Notably, existing researches often focus solely on a limited\nsubset of these potential combinations, which constrains their applicability\nand versatility across various contexts. In response to this challenge, we\npresent MVP-LM, a Multi-granular and Versatile Perception framework\nincorporating Visual Large Language Model. Our framework is designed to\nintegrate both word-based and sentence-based perception tasks alongside box and\nmask predictions within a single architecture. MVP-LM features an innovative\nmulti-granularity decoder in conjunction with a CoT-inspired dataset\nunification strategy, enabling seamless supervised fine-tuning across a wide\nspectrum of tasks, including but not limited to panoptic segmentation,\ndetection, grounding, and referring expression segmentation. Furthermore, we\nintroduce a query enhancement strategy aimed at harnessing the decoding and\ngenerative capabilities inherent in VLLMs. Extensive experiments conducted\nacross a range of benchmarks in both word-based and sentence-based perception\ntasks substantiate the efficacy of our framework. The code will be available at\nhttps://github.com/xiangwentao666/MVP-LM.",
    "published": "2025-07-22T04:09:14Z",
    "updated": "2025-07-22T04:09:14Z",
    "id": "2507.16213v1",
    "authors": [
      "Wentao Xiang",
      "Haoxian Tan",
      "Cong Wei",
      "Yujie Zhong",
      "Dengjie Li",
      "Yujiu Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16213v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16213v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16213v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a Visual Large Language Model (VLLM) framework for multi-granular versatile perception, integrating various vision-language tasks. This aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA) models, as it involves integrating vision and language modalities for perception tasks.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.16208v1": {
    "title": "LOCOFY Large Design Models -- Design to code conversion solution",
    "summary": "Despite rapid advances in Large Language Models and Multimodal Large Language\nModels (LLMs), numerous challenges related to interpretability, scalability,\nresource requirements and repeatability remain, related to their application in\nthe design-to-code space. To address this, we introduce the Large Design Models\n(LDMs) paradigm specifically trained on designs and webpages to enable seamless\nconversion from design-to-code. We have developed a training and inference\npipeline by incorporating data engineering and appropriate model architecture\nmodification. The training pipeline consists of the following: 1)Design\nOptimiser: developed using a proprietary ground truth dataset and addresses\nsub-optimal designs; 2)Tagging and feature detection: using pre-trained and\nfine-tuned models, this enables the accurate detection and classification of UI\nelements; and 3)Auto Components: extracts repeated UI structures into reusable\ncomponents to enable creation of modular code, thus reducing redundancy while\nenhancing code reusability. In this manner, each model addresses distinct but\nkey issues for design-to-code conversion. Separately, our inference pipeline\nprocesses real-world designs to produce precise and interpretable instructions\nfor code generation and ensures reliability. Additionally, our models\nillustrated exceptional end-to-end design-to-code conversion accuracy using a\nnovel preview match score metric. Comparative experiments indicated superior\nperformance of LDMs against LLMs on accuracy of node positioning,\nresponsiveness and reproducibility. Moreover, our custom-trained tagging and\nfeature detection model demonstrated high precision and consistency in\nidentifying UI elements across a wide sample of test designs. Thus, our\nproposed LDMs are a reliable and superior solution to understanding designs\nthat subsequently enable the generation of efficient and reliable\nproduction-ready code.",
    "published": "2025-07-22T03:54:57Z",
    "updated": "2025-07-22T03:54:57Z",
    "id": "2507.16208v1",
    "authors": [
      "Sohaib Muhammad",
      "Ashwati Vipin",
      "Karan Shetti",
      "Honey Mittal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16208v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16208v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16208v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development of Large Design Models (LDMs) for design-to-code conversion, which involves multimodal capabilities and model training specific to design and webpage data. It also mentions comparisons with LLMs and MLLMs, indicating relevance to these topics.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "Pretrain"
    ]
  },
  "2507.16196v1": {
    "title": "Do Large Language Models Have a Planning Theory of Mind? Evidence from\n  MindGames: a Multi-Step Persuasion Task",
    "summary": "Recent evidence suggests Large Language Models (LLMs) display Theory of Mind\n(ToM) abilities. Most ToM experiments place participants in a spectatorial\nrole, wherein they predict and interpret other agents' behavior. However, human\nToM also contributes to dynamically planning action and strategically\nintervening on others' mental states. We present MindGames: a novel `planning\ntheory of mind' (PToM) task which requires agents to infer an interlocutor's\nbeliefs and desires to persuade them to alter their behavior. Unlike previous\nevaluations, we explicitly evaluate use cases of ToM. We find that humans\nsignificantly outperform o1-preview (an LLM) at our PToM task (11% higher;\n$p=0.006$). We hypothesize this is because humans have an implicit causal model\nof other agents (e.g., they know, as our task requires, to ask about people's\npreferences). In contrast, o1-preview outperforms humans in a baseline\ncondition which requires a similar amount of planning but minimal mental state\ninferences (e.g., o1-preview is better than humans at planning when already\ngiven someone's preferences). These results suggest a significant gap between\nhuman-like social reasoning and LLM abilities.",
    "published": "2025-07-22T03:15:27Z",
    "updated": "2025-07-22T03:15:27Z",
    "id": "2507.16196v1",
    "authors": [
      "Jared Moore",
      "Ned Cooper",
      "Rasmus Overmark",
      "Beba Cibralic",
      "Nick Haber",
      "Cameron R. Jones"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16196v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16196v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16196v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the Theory of Mind (ToM) abilities of Large Language Models (LLMs) and evaluates their performance in a novel planning theory of mind task. It directly involves LLMs and their reasoning abilities, making it relevant to the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16184v1": {
    "title": "Emergent Cognitive Convergence via Implementation: A Structured Loop\n  Reflecting Four Theories of Mind (A Position Paper)",
    "summary": "We report the discovery of a structural convergence across four influential\ntheories of mind: Kahneman's dual-system theory, Friston's predictive\nprocessing, Minsky's society of mind, and Clark's extended mind-emerging\nunintentionally within a practical AI agent architecture called Agentic Flow.\nDesigned to address limitations in large language models (LLMs), Agentic Flow\ncomprises five interdependent modules such as Retrieval, Cognition, Control,\nMemory, and Action arranged in a recurrent cognitive loop. Although originally\ninspired only by Minsky and Clark, the system's structure retrospectively\naligns with computational motifs found in all four theories, including\npredictive modeling, associative recall, and error-sensitive control.\n  To assess this convergence, we conducted comparative experiments with\nbaseline LLM agents on multi-step reasoning tasks. The structured agent\nachieved 95.8% task success and exhibited strong constraint adherence, while\nthe baseline system succeeded 62.3% of the time. These results were not aimed\nat proving superiority, but at illustrating how theoretical structures may\nemerge through practical design choices rather than top-down theory.\n  We introduce PEACE as a descriptive meta-architecture that captures\ndesign-level regularities observed in Agentic Flow. Not intended as a new\ntheory, PEACE provides a shared vocabulary for understanding architectures\nshaped by real-world implementation demands. This paper should be read as a\nposition paper - an exploratory reflection on how implementation can surface\nlatent structural echoes of cognitive theory, without asserting theoretical\nunification.",
    "published": "2025-07-22T02:54:45Z",
    "updated": "2025-07-22T02:54:45Z",
    "id": "2507.16184v1",
    "authors": [
      "Myung Ho Kim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16184v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16184v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16184v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses an AI agent architecture designed to address limitations in large language models (LLMs) and includes modules like Memory and Control, which are relevant to the Memory and Reasoning topics. However, it does not directly fit into the more specific categories like LLM, RL, or MLLM, as it focuses more on cognitive theories and practical design choices.",
    "llm_cls_result": [
      "Memory",
      "Reasoning"
    ]
  },
  "2507.16178v1": {
    "title": "LLM Data Selection and Utilization via Dynamic Bi-level Optimization",
    "summary": "While large-scale training data is fundamental for developing capable large\nlanguage models (LLMs), strategically selecting high-quality data has emerged\nas a critical approach to enhance training efficiency and reduce computational\ncosts. Current data selection methodologies predominantly rely on static,\ntraining-agnostic criteria, failing to account for the dynamic model training\nand data interactions. In this paper, we propose a new Data Weighting Model\n(DWM) to adjust the weight of selected data within each batch to achieve a\ndynamic data utilization during LLM training. Specially, to better capture the\ndynamic data preference of the trained model, a bi-level optimization framework\nis implemented to update the weighting model. Our experiments demonstrate that\nDWM enhances the performance of models trained with randomly-selected data, and\nthe learned weighting model can be transferred to enhance other data selection\nmethods and models of different sizes. Moreover, we further analyze how a\nmodel's data preferences evolve throughout training, providing new insights\ninto the data preference of the model during training.",
    "published": "2025-07-22T02:47:12Z",
    "updated": "2025-07-22T02:47:12Z",
    "id": "2507.16178v1",
    "authors": [
      "Yang Yu",
      "Kai Han",
      "Hang Zhou",
      "Yehui Tang",
      "Kaiqi Huang",
      "Yunhe Wang",
      "Dacheng Tao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16178v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16178v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16178v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on data selection and utilization strategies for large language models (LLMs), which is closely related to the topics of LLM (Large Language Models) and Pretrain (pretraining strategies). The dynamic bi-level optimization approach for data weighting also touches on the broader theme of Scaling (model scaling and efficiency).",
    "llm_cls_result": [
      "LLM",
      "Pretrain",
      "Scaling"
    ]
  },
  "2507.16145v1": {
    "title": "SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series\n  with Clinical Validation in COPD Reporting",
    "summary": "Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory\ndisease with persistent airflow limitation, is a leading global cause of\ndisability and mortality. Respiratory spirogram time series, routinely\ncollected during pulmonary function tests (PFTs), play a critical role in the\nearly detection of repsiratory diseases and in monitoring lung function over\ntime. However, most current AI models for COPD diagnosis are limited to\noutputting classification results without providing a rationale for their\ndiagnostic process, while current Large Language Models (LLMs) cannot\nunderstand spirograms yet, which severely limits their clinical trust and\nadoption. To tackle this challenge, we leverage a cohort of 234,028 individuals\nfrom the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large\nlanguage model that can understand spirogram. The model extracts morphological\nfeatures from respiratory curves via a SpiroEncoder and aligns them with PFT\nnumerical values in a unified latent space using a SpiroProjector, ultimately\nempowering a large language model to generate a comprehensive diagnostic\nreport. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC\nof 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data,\nit maintained a 100% valid response rate, far surpassing the 13.4% of a\ntext-only model and showcasing the superiority of its multimodal design. This\nwork demonstrates the substantial potential of deeply fusing physiological\nsignals with large language models, establishing a new paradigm for the next\ngeneration of interpretable and reliable clinical decision support tools.",
    "published": "2025-07-22T01:44:12Z",
    "updated": "2025-07-22T01:44:12Z",
    "id": "2507.16145v1",
    "authors": [
      "Shuhao Mei",
      "Yongchao Long",
      "Shan Cao",
      "Xiaobo Han",
      "Shijia Geng",
      "Jinbo Sun",
      "Yuxi Zhou",
      "Shenda Hong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16145v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16145v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16145v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development of a multimodal large language model (SpiroLLM) that integrates spirogram time series data with clinical validation for COPD reporting. This involves the use of a large language model (LLM) to understand and generate diagnostic reports based on multimodal inputs, aligning with the topics of LLM and MLLM.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.16130v1": {
    "title": "Disability Across Cultures: A Human-Centered Audit of Ableism in Western\n  and Indic LLMs",
    "summary": "People with disabilities (PwD) experience disproportionately high levels of\ndiscrimination and hate online, particularly in India, where entrenched stigma\nand limited resources intensify these challenges. Large language models (LLMs)\nare increasingly used to identify and mitigate online hate, yet most research\non online ableism focuses on Western audiences with Western AI models. Are\nthese models adequately equipped to recognize ableist harm in non-Western\nplaces like India? Do localized, Indic language models perform better? To\ninvestigate, we adopted and translated a publicly available ableist speech\ndataset to Hindi, and prompted eight LLMs--four developed in the U.S. (GPT-4,\nGemini, Claude, Llama) and four in India (Krutrim, Nanda, Gajendra,\nAiravata)--to score and explain ableism. In parallel, we recruited 175 PwD from\nboth the U.S. and India to perform the same task, revealing stark differences\nbetween groups. Western LLMs consistently overestimated ableist harm, while\nIndic LLMs underestimated it. Even more concerning, all LLMs were more tolerant\nof ableism when it was expressed in Hindi and asserted Western framings of\nableist harm. In contrast, Indian PwD interpreted harm through intention,\nrelationality, and resilience--emphasizing a desire to inform and educate\nperpetrators. This work provides groundwork for global, inclusive standards of\nableism, demonstrating the need to center local disability experiences in the\ndesign and evaluation of AI systems.",
    "published": "2025-07-22T00:51:41Z",
    "updated": "2025-07-22T00:51:41Z",
    "id": "2507.16130v1",
    "authors": [
      "Mahika Phutane",
      "Aditya Vashistha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16130v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16130v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16130v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the evaluation of Large Language Models (LLMs) in recognizing ableist speech across different cultures, specifically comparing Western and Indic models. It highlights the need for localized and inclusive AI systems.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.16076v1": {
    "title": "The Prompt Makes the Person(a): A Systematic Evaluation of\n  Sociodemographic Persona Prompting for Large Language Models",
    "summary": "Persona prompting is increasingly used in large language models (LLMs) to\nsimulate views of various sociodemographic groups. However, how a persona\nprompt is formulated can significantly affect outcomes, raising concerns about\nthe fidelity of such simulations. Using five open-source LLMs, we\nsystematically examine how different persona prompt strategies, specifically\nrole adoption formats and demographic priming strategies, influence LLM\nsimulations across 15 intersectional demographic groups in both open- and\nclosed-ended tasks. Our findings show that LLMs struggle to simulate\nmarginalized groups, particularly nonbinary, Hispanic, and Middle Eastern\nidentities, but that the choice of demographic priming and role adoption\nstrategy significantly impacts their portrayal. Specifically, we find that\nprompting in an interview-style format and name-based priming can help reduce\nstereotyping and improve alignment. Surprisingly, smaller models like OLMo-2-7B\noutperform larger ones such as Llama-3.3-70B. Our findings offer actionable\nguidance for designing sociodemographic persona prompts in LLM-based simulation\nstudies.",
    "published": "2025-07-21T21:23:29Z",
    "updated": "2025-07-21T21:23:29Z",
    "id": "2507.16076v1",
    "authors": [
      "Marlene Lutz",
      "Indira Sen",
      "Georg Ahnert",
      "Elisa Rogers",
      "Markus Strohmaier"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16076v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16076v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16076v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the evaluation of persona prompting strategies in large language models (LLMs) and their impact on simulating sociodemographic groups. It discusses the performance of different LLMs in this context, which is directly related to research on Large Language Models and their applications.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.16063v1": {
    "title": "AI-Powered Commit Explorer (APCE)",
    "summary": "Commit messages in a version control system provide valuable information for\ndevelopers regarding code changes in software systems. Commit messages can be\nthe only source of information left for future developers describing what was\nchanged and why. However, writing high-quality commit messages is often\nneglected in practice. Large Language Model (LLM) generated commit messages\nhave emerged as a way to mitigate this issue. We introduce the AI-Powered\nCommit Explorer (APCE), a tool to support developers and researchers in the use\nand study of LLM-generated commit messages. APCE gives researchers the option\nto store different prompts for LLMs and provides an additional evaluation\nprompt that can further enhance the commit message provided by LLMs. APCE also\nprovides researchers with a straightforward mechanism for automated and human\nevaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo",
    "published": "2025-07-21T20:58:56Z",
    "updated": "2025-07-21T20:58:56Z",
    "id": "2507.16063v1",
    "authors": [
      "Yousab Grees",
      "Polina Iaremchuk",
      "Ramtin Ehsani",
      "Esteban Parra",
      "Preetha Chatterjee",
      "Sonia Haiduc"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16063v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16063v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16063v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating commit messages in version control systems, which directly relates to the 'LLM' topic. Additionally, it involves the evaluation of LLM-generated messages, which can be associated with the 'Benchmark' topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.16054v1": {
    "title": "AutoMeet: a proof-of-concept study of genAI to automate meetings in\n  automotive engineering",
    "summary": "In large organisations, knowledge is mainly shared in meetings, which takes\nup significant amounts of work time. Additionally, frequent in-person meetings\nproduce inconsistent documentation -- official minutes, personal notes,\npresentations may or may not exist. Shared information therefore becomes hard\nto retrieve outside of the meeting, necessitating lengthy updates and\nhigh-frequency meeting schedules.\n  Generative Artificial Intelligence (genAI) models like Large Language Models\n(LLMs) exhibit an impressive performance on spoken and written language\nprocessing. This motivates a practical usage of genAI for knowledge management\nin engineering departments: using genAI for transcribing meetings and\nintegrating heterogeneous additional information sources into an easily usable\nformat for ad-hoc searches.\n  We implement an end-to-end pipeline to automate the entire meeting\ndocumentation workflow in a proof-of-concept state: meetings are recorded and\nminutes are created by genAI. These are further made easily searchable through\na chatbot interface. The core of our work is to test this genAI-based software\ntooling in a real-world engineering department and collect extensive survey\ndata on both ethical and technical aspects. Direct feedback from this\nreal-world setup points out both opportunities and risks: a) users agree that\nthe effort for meetings could be significantly reduced with the help of genAI\nmodels, b) technical aspects are largely solved already, c) organizational\naspects are crucial for a successful ethical usage of such a system.",
    "published": "2025-07-21T20:44:53Z",
    "updated": "2025-07-21T20:44:53Z",
    "id": "2507.16054v1",
    "authors": [
      "Simon Baeuerle",
      "Max Radyschevski",
      "Ulrike Pado"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16054v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16054v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16054v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of generative AI, specifically Large Language Models (LLMs), for automating meeting documentation and knowledge management in engineering departments. It focuses on practical applications and ethical considerations rather than core research topics like model architectures or scaling laws.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.16044v2": {
    "title": "Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol\n  Servers for Tool-Augmented LLMs",
    "summary": "Large Language Models (LLMs) are evolving from passive text generators into\nactive agents that invoke external tools. To support this shift, scalable\nprotocols for tool integration are essential. The Model Context Protocol (MCP),\nintroduced by Anthropic in 2024, offers a schema-driven standard for dynamic\ntool discovery and invocation. Yet, building MCP servers remains manual and\nrepetitive, requiring developers to write glue code, handle authentication, and\nconfigure schemas by hand-replicating much of the integration effort MCP aims\nto eliminate.\n  This paper investigates whether MCP server construction can be meaningfully\nautomated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged\nGitHub repositories created within six months of release, fewer than 5% include\nservers, typically small, single-maintainer projects dominated by repetitive\nscaffolding. To address this gap, we present AutoMCP, a compiler that generates\nMCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API\ndefinitions and produces complete server implementations, including schema\nregistration and authentication handling.\n  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across\nover 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded\nout of the box. Manual failure analysis revealed five recurring issues, all\nattributable to inconsistencies or omissions in the OpenAPI contracts. After\nminor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%\nsuccess.\n  Our findings (i) analyze MCP adoption and quantify the cost of manual server\ndevelopment, (ii) demonstrate that OpenAPI specifications, despite quality\nissues, enable near-complete MCP server automation, and (iii) contribute a\ncorpus of 5,066 callable tools along with insights on repairing common\nspecification flaws.",
    "published": "2025-07-21T20:20:31Z",
    "updated": "2025-07-23T16:37:47Z",
    "id": "2507.16044v2",
    "authors": [
      "Meriem Mastouri",
      "Emna Ksontini",
      "Wael Kessentini"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16044v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16044v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16044v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with external tools through the Model Context Protocol (MCP), which is a schema-driven standard for dynamic tool discovery and invocation. It also presents AutoMCP, a compiler that generates MCP servers from OpenAPI specifications, which is relevant to the development and scaling of LLMs as active agents.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Scaling"
    ]
  },
  "2507.16037v1": {
    "title": "A Pilot Study on LLM-Based Agentic Translation from Android to iOS:\n  Pitfalls and Insights",
    "summary": "The rapid advancement of mobile applications has led to a significant demand\nfor cross-platform compatibility, particularly between the Android and iOS\nplatforms. Traditional approaches to mobile application translation often rely\non manual intervention or rule-based systems, which are labor-intensive and\ntime-consuming. While recent advancements in machine learning have introduced\nautomated methods, they often lack contextual understanding and adaptability,\nresulting in suboptimal translations. Large Language Models (LLMs) were\nrecently leveraged to enhance code translation at different granularities,\nincluding the method, class, and repository levels. Researchers have\ninvestigated common errors, limitations, and potential strategies to improve\nthese tasks. However, LLM-based application translation across different\nplatforms, such as migrating mobile applications between Android and iOS or\nadapting software across diverse frameworks, remains underexplored.\nUnderstanding the performance, strengths, and limitations of LLMs in\ncross-platform application translation is critical for advancing software\nengineering automation. This study aims to fill this gap by evaluating\nLLM-based agentic approaches for mobile application translation, identifying\nkey failure points, and proposing guidelines to improve translation\nperformance. We developed a chain of agents that account for dependencies,\nspecifications, program structure, and program control flow when translating\napplications from Android to iOS. To evaluate the performance, we manually\nexamined the translated code for syntactic correctness, semantic accuracy, and\nfunctional completeness. For translation failures, we further conducted a\ndetailed root cause analysis to understand the underlying limitations of the\nagentic translation process and identify opportunities for improvement.",
    "published": "2025-07-21T20:11:01Z",
    "updated": "2025-07-21T20:11:01Z",
    "id": "2507.16037v1",
    "authors": [
      "Zhili Zeng",
      "Kimya Khakzad Shahandashti",
      "Alvine Boaye Belle",
      "Song Wang",
      "Zhen Ming",
      " Jiang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16037v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16037v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16037v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for cross-platform mobile application translation, which involves leveraging LLMs for code translation and understanding their strengths and limitations in this context. This aligns with the topics of LLM research and their application in specific tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16028v1": {
    "title": "From Logic to Language: A Trust Index for Problem Solving with LLMs",
    "summary": "Classical computation, grounded in formal, logical systems, has been the\nengine of technological progress for decades, excelling at problems that can be\ndescribed with unambiguous rules. This paradigm, however, leaves a vast ocean\nof human problems -- those characterized by ambiguity, dynamic environments,\nand subjective context -- largely untouched. The advent of Large Language\nModels (LLMs) represents a fundamental shift, enabling computational systems to\nengage with this previously inaccessible domain using natural language. This\npaper introduces a unified framework to understand and contrast these\nproblem-solving paradigms. We define and delineate the problem spaces\naddressable by formal languages versus natural language. While solutions to the\nformer problem class can be evaluated using binary quality measures, the latter\nrequires a much more nuanced definition of approximate solution space taking\ninto account the vagueness, subjectivity and ambiguity inherent to natural\nlanguage. We therefore introduce a vector-valued trust index Q, which reflects\nsolution quality and distinguishes the binary correctness of formal solutions\nfrom the continuous adequacy spectrum characteristic of natural language\nsolutions. Within this framework, we propose two statistical quality\ndimensions. Normalized bi-semantic entropy measures robustness and conceptual\ndiversity of LLM answers given semantic variation in problem formulations.\nEmotional valence maps subjective valuation of a solution to a quantifiable\nmetric that can be maximized by invoking statistical measures. The concepts\nintroduced in this work will provide a more rigorous understanding of the\ncapabilities, limitations, and inherent nature of problem-solving in the age of\nLLMs.",
    "published": "2025-07-21T19:50:45Z",
    "updated": "2025-07-21T19:50:45Z",
    "id": "2507.16028v1",
    "authors": [
      "Tehseen Rug",
      "Felix Bhmer",
      "Tessa Pfattheicher"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16028v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16028v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16028v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the problem-solving capabilities of Large Language Models (LLMs) and introduces a trust index to evaluate their solutions, which aligns with the topics of LLM research and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16013v1": {
    "title": "AI, Expert or Peer? -- Examining the Impact of Perceived Feedback Source\n  on Pre-Service Teachers Feedback Perception and Uptake",
    "summary": "Feedback plays a central role in learning, yet pre-service teachers'\nengagement with feedback depends not only on its quality but also on their\nperception of the feedback content and source. Large Language Models (LLMs) are\nincreasingly used to provide educational feedback; however, negative\nperceptions may limit their practical use, and little is known about how\npre-service teachers' perceptions and behavioral responses differ by feedback\nsource. This study investigates how the perceived source of feedback - LLM,\nexpert, or peer - influences feedback perception and uptake, and whether\nrecognition accuracy and feedback quality moderate these effects. In a\nrandomized experiment with 273 pre-service teachers, participants received\nwritten feedback on a mathematics learning goal, identified its source, rated\nfeedback perceptions across five dimensions (fairness, usefulness, acceptance,\nwillingness to improve, positive and negative affect), and revised the learning\ngoal according to the feedback (i.e. feedback uptake). Results revealed that\nLLM-generated feedback received the highest ratings in fairness and usefulness,\nleading to the highest uptake (52%). Recognition accuracy significantly\nmoderated the effect of feedback source on perception, with particularly\npositive evaluations when LLM feedback was falsely ascribed to experts.\nHigher-quality feedback was consistently assigned to experts, indicating an\nexpertise heuristic in source judgments. Regression analysis showed that only\nfeedback quality significantly predicted feedback uptake. Findings highlight\nthe need to address source-related biases and promote feedback and AI literacy\nin teacher education.",
    "published": "2025-07-21T19:24:40Z",
    "updated": "2025-07-21T19:24:40Z",
    "id": "2507.16013v1",
    "authors": [
      "Lucas Jasper Jacobsen",
      "Ute Mertens",
      "Thorben Jansen",
      "Kira Elena Weber"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16013v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16013v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16013v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in providing educational feedback and examines the impact of perceived feedback source on feedback perception and uptake. It directly involves LLMs in an educational context, making 'LLM' the most relevant topic. The study also touches on feedback quality and perception, which aligns with 'Reasoning' as it involves understanding and evaluating feedback. However, the primary focus is on LLMs, so 'LLM' is the most prominent category.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16005v1": {
    "title": "AutoMAT: A Hierarchical Framework for Autonomous Alloy Discovery",
    "summary": "Alloy discovery is central to advancing modern industry but remains hindered\nby the vastness of compositional design space and the costly validation. Here,\nwe present AutoMAT, a hierarchical and autonomous framework grounded in and\nvalidated by experiments, which integrates large language models, automated\nCALPHAD-based simulations, and AI-driven search to accelerate alloy design.\nSpanning the entire pipeline from ideation to validation, AutoMAT achieves high\nefficiency, accuracy, and interpretability without the need for manually\ncurated large datasets. In a case study targeting a lightweight, high-strength\nalloy, AutoMAT identifies a titanium alloy with 8.1% lower density and\ncomparable yield strength relative to the state-of-the-art reference, achieving\nthe highest specific strength among all comparisons. In a second case targeting\nhigh-yield-strength high-entropy alloys, AutoMAT achieves a 28.2% improvement\nin yield strength over the base alloy. In both cases, AutoMAT reduces the\ndiscovery timeline from years to weeks, illustrating its potential as a\nscalable and versatile platform for next-generation alloy design.",
    "published": "2025-07-21T18:55:03Z",
    "updated": "2025-07-21T18:55:03Z",
    "id": "2507.16005v1",
    "authors": [
      "Penghui Yang",
      "Chendong Zhao",
      "Bijun Tang",
      "Zhonghan Zhang",
      "Xinrun Wang",
      "Yanchen Deng",
      "Yuhao Lu",
      "Cuntai Guan",
      "Zheng Liu",
      "Bo An"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16005v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16005v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16005v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLM) in an autonomous framework for alloy discovery, which involves AI-driven search and automated simulations. The primary focus is on the application of LLMs in a specific domain rather than the core topics of LLM research, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.16003v1": {
    "title": "Learning without training: The implicit dynamics of in-context learning",
    "summary": "One of the most striking features of Large Language Models (LLM) is their\nability to learn in context. Namely at inference time an LLM is able to learn\nnew patterns without any additional weight update when these patterns are\npresented in the form of examples in the prompt, even if these patterns were\nnot seen during training. The mechanisms through which this can happen are\nstill largely unknown. In this work, we show that the stacking of a\nself-attention layer with an MLP, allows the transformer block to implicitly\nmodify the weights of the MLP layer according to the context. We argue through\ntheory and experimentation that this simple mechanism may be the reason why\nLLMs can learn in context and not only during training. Specifically, we show\nunder mild simplifying assumptions how a transformer block implicitly\ntransforms a context into a low-rank weight-update of the MLP layer.",
    "published": "2025-07-21T18:44:35Z",
    "updated": "2025-07-21T18:44:35Z",
    "id": "2507.16003v1",
    "authors": [
      "Benoit Dherin",
      "Michael Munn",
      "Hanna Mazzawi",
      "Michael Wunder",
      "Javier Gonzalvo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16003v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16003v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16003v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the mechanisms behind in-context learning in Large Language Models (LLMs), focusing on the transformer block's ability to implicitly modify weights during inference. This aligns with research on LLM architectures and their learning capabilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.15857v2": {
    "title": "Diffusion Beats Autoregressive in Data-Constrained Settings",
    "summary": "Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.",
    "published": "2025-07-21T17:59:57Z",
    "updated": "2025-07-24T17:55:24Z",
    "id": "2507.15857v2",
    "authors": [
      "Mihir Prabhudesai",
      "Menging Wu",
      "Amir Zadeh",
      "Katerina Fragkiadaki",
      "Deepak Pathak"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15857v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15857v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15857v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the comparison between autoregressive (AR) and diffusion-based language models, focusing on their performance in data-constrained settings. It introduces new scaling laws for diffusion models and explores their advantages over AR models, which aligns with topics related to scaling and pretraining strategies.",
    "llm_cls_result": [
      "Scaling",
      "Pretrain"
    ]
  },
  "2507.15851v1": {
    "title": "The Other Mind: How Language Models Exhibit Human Temporal Cognition",
    "summary": "As Large Language Models (LLMs) continue to advance, they exhibit certain\ncognitive patterns similar to those of humans that are not directly specified\nin training data. This study investigates this phenomenon by focusing on\ntemporal cognition in LLMs. Leveraging the similarity judgment task, we find\nthat larger models spontaneously establish a subjective temporal reference\npoint and adhere to the Weber-Fechner law, whereby the perceived distance\nlogarithmically compresses as years recede from this reference point. To\nuncover the mechanisms behind this behavior, we conducted multiple analyses\nacross neuronal, representational, and informational levels. We first identify\na set of temporal-preferential neurons and find that this group exhibits\nminimal activation at the subjective reference point and implements a\nlogarithmic coding scheme convergently found in biological systems. Probing\nrepresentations of years reveals a hierarchical construction process, where\nyears evolve from basic numerical values in shallow layers to abstract temporal\norientation in deep layers. Finally, using pre-trained embedding models, we\nfound that the training corpus itself possesses an inherent, non-linear\ntemporal structure, which provides the raw material for the model's internal\nconstruction. In discussion, we propose an experientialist perspective for\nunderstanding these findings, where the LLMs' cognition is viewed as a\nsubjective construction of the external world by its internal representational\nsystem. This nuanced perspective implies the potential emergence of alien\ncognitive frameworks that humans cannot intuitively predict, pointing toward a\ndirection for AI alignment that focuses on guiding internal constructions. Our\ncode is available at https://TheOtherMind.github.io.",
    "published": "2025-07-21T17:59:01Z",
    "updated": "2025-07-21T17:59:01Z",
    "id": "2507.15851v1",
    "authors": [
      "Lingyu Li",
      "Yang Yao",
      "Yixu Wang",
      "Chubo Li",
      "Yan Teng",
      "Yingchun Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15851v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15851v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15851v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the cognitive patterns of Large Language Models (LLMs) and their similarities to human temporal cognition, which is a core aspect of LLM research. It also discusses the internal mechanisms and representational systems of LLMs, which aligns with the broader study of LLM architectures and their behaviors.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.15839v1": {
    "title": "FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with\n  LLMs",
    "summary": "Synthetic data generation has emerged as an invaluable solution in scenarios\nwhere real-world data collection and usage are limited by cost and scarcity.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nproducing high-fidelity, domain-relevant samples across various fields.\nHowever, existing approaches that directly use LLMs to generate each record\nindividually impose prohibitive time and cost burdens, particularly when large\nvolumes of synthetic data are required. In this work, we propose a fast,\ncost-effective method for realistic tabular data synthesis that leverages LLMs\nto infer and encode each field's distribution into a reusable sampling script.\nBy automatically classifying fields into numerical, categorical, or free-text\ntypes, the LLM generates distribution-based scripts that can efficiently\nproduce diverse, realistic datasets at scale without continuous model\ninference. Experimental results show that our approach outperforms traditional\ndirect methods in both diversity and data realism, substantially reducing the\nburden of high-volume synthetic data generation. We plan to apply this\nmethodology to accelerate testing in production pipelines, thereby shortening\ndevelopment cycles and improving overall system efficiency. We believe our\ninsights and lessons learned will aid researchers and practitioners seeking\nscalable, cost-effective solutions for synthetic data generation.",
    "published": "2025-07-21T17:51:46Z",
    "updated": "2025-07-21T17:51:46Z",
    "id": "2507.15839v1",
    "authors": [
      "Anh Nguyen",
      "Sam Schafft",
      "Nicholas Hale",
      "John Alfaro"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15839v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15839v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15839v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for synthetic data generation, focusing on efficiency and cost-effectiveness. The core topic is the application of LLMs in generating synthetic data, which aligns with the 'LLM' category. Additionally, the paper touches on the scalability and practical application of LLMs, which is relevant to the 'Scaling' category.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.15825v1": {
    "title": "ACS: An interactive framework for conformal selection",
    "summary": "This paper presents adaptive conformal selection (ACS), an interactive\nframework for model-free selection with guaranteed error control. Building on\nconformal selection (Jin and Cand\\`es, 2023b), ACS generalizes the approach to\nsupport human-in-the-loop adaptive data analysis. Under the ACS framework, we\ncan partially reuse the data to boost the selection power, make decisions on\nthe fly while exploring the data, and incorporate new information or\npreferences as they arise. The key to ACS is a carefully designed principle\nthat controls the information available for decision making, allowing the data\nanalyst to explore the data adaptively while maintaining rigorous control of\nthe false discovery rate (FDR). Based on the ACS framework, we provide concrete\nselection algorithms for various goals, including model update/selection,\ndiversified selection, and incorporating newly available labeled data. The\neffectiveness of ACS is demonstrated through extensive numerical simulations\nand real-data applications in large language model (LLM) deployment and drug\ndiscovery.",
    "published": "2025-07-21T17:33:15Z",
    "updated": "2025-07-21T17:33:15Z",
    "id": "2507.15825v1",
    "authors": [
      "Yu Gui",
      "Ying Jin",
      "Yash Nair",
      "Zhimei Ren"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15825v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15825v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15825v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses an interactive framework for conformal selection with applications in large language model (LLM) deployment, but the core focus is on the framework and methodology rather than LLMs themselves.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.15822v1": {
    "title": "Do AI models help produce verified bug fixes?",
    "summary": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair.",
    "published": "2025-07-21T17:30:16Z",
    "updated": "2025-07-21T17:30:16Z",
    "id": "2507.15822v1",
    "authors": [
      "Li Huang",
      "Ilgiz Mustafin",
      "Marco Piccioni",
      "Alessandro Schena",
      "Reto Weber",
      "Bertrand Meyer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15822v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15822v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15822v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in Automatic Program Repair (APR) and debugging, which aligns with the 'LLM' and 'Reasoning' topics. The study involves evaluating the correctness of fixes and programmer behavior, which relates to reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16860v1": {
    "title": "Weak Links in LinkedIn: Enhancing Fake Profile Detection in the Age of\n  LLMs",
    "summary": "Large Language Models (LLMs) have made it easier to create realistic fake\nprofiles on platforms like LinkedIn. This poses a significant risk for\ntext-based fake profile detectors. In this study, we evaluate the robustness of\nexisting detectors against LLM-generated profiles. While highly effective in\ndetecting manually created fake profiles (False Accept Rate: 6-7%), the\nexisting detectors fail to identify GPT-generated profiles (False Accept Rate:\n42-52%). We propose GPT-assisted adversarial training as a countermeasure,\nrestoring the False Accept Rate to between 1-7% without impacting the False\nReject Rates (0.5-2%). Ablation studies revealed that detectors trained on\ncombined numerical and textual embeddings exhibit the highest robustness,\nfollowed by those using numerical-only embeddings, and lastly those using\ntextual-only embeddings. Complementary analysis on the ability of prompt-based\nGPT-4Turbo and human evaluators affirms the need for robust automated detectors\nsuch as the one proposed in this study.",
    "published": "2025-07-21T17:23:52Z",
    "updated": "2025-07-21T17:23:52Z",
    "id": "2507.16860v1",
    "authors": [
      "Apoorva Gulati",
      "Rajesh Kumar",
      "Vinti Agarwal",
      "Aditya Sharma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16860v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16860v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16860v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of Large Language Models (LLMs) on fake profile detection, evaluates existing detectors against LLM-generated profiles, and proposes a solution involving GPT-assisted adversarial training. The core focus is on LLMs and their implications for security and detection systems.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.15815v1": {
    "title": "LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra",
    "summary": "We present the LLM Economist, a novel framework that uses agent-based\nmodeling to design and assess economic policies in strategic environments with\nhierarchical decision-making. At the lower level, bounded rational worker\nagents -- instantiated as persona-conditioned prompts sampled from U.S.\nCensus-calibrated income and demographic statistics -- choose labor supply to\nmaximize text-based utility functions learned in-context. At the upper level, a\nplanner agent employs in-context reinforcement learning to propose\npiecewise-linear marginal tax schedules anchored to the current U.S. federal\nbrackets. This construction endows economic simulacra with three capabilities\nrequisite for credible fiscal experimentation: (i) optimization of\nheterogeneous utilities, (ii) principled generation of large, demographically\nrealistic agent populations, and (iii) mechanism design -- the ultimate nudging\nproblem -- expressed entirely in natural language. Experiments with populations\nof up to one hundred interacting agents show that the planner converges near\nStackelberg equilibria that improve aggregate social welfare relative to Saez\nsolutions, while a periodic, persona-level voting procedure furthers these\ngains under decentralized governance. These results demonstrate that large\nlanguage model-based agents can jointly model, simulate, and govern complex\neconomic systems, providing a tractable test bed for policy evaluation at the\nsocietal scale to help build better civilizations.",
    "published": "2025-07-21T17:21:14Z",
    "updated": "2025-07-21T17:21:14Z",
    "id": "2507.15815v1",
    "authors": [
      "Seth Karten",
      "Wenzhe Li",
      "Zihan Ding",
      "Samuel Kleiner",
      "Yu Bai",
      "Chi Jin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15815v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15815v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15815v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in agent-based modeling for economic policy design and assessment, which involves reinforcement learning and multi-agent systems. The focus on LLMs and their application in strategic environments aligns with the topics of LLM and RL.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.15807v1": {
    "title": "True Multimodal In-Context Learning Needs Attention to the Visual\n  Context",
    "summary": "Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .",
    "published": "2025-07-21T17:08:18Z",
    "updated": "2025-07-21T17:08:18Z",
    "id": "2507.15807v1",
    "authors": [
      "Shuo Chen",
      "Jianzhe Liu",
      "Zhen Han",
      "Yan Xia",
      "Daniel Cremers",
      "Philip Torr",
      "Volker Tresp",
      "Jindong Gu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15807v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15807v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15807v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Multimodal Large Language Models (MLLMs) and their ability to perform Multimodal In-Context Learning (MICL), specifically addressing the challenge of integrating visual information. It introduces a fine-tuning strategy and a dataset to enhance and evaluate MICL performance.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Dataset"
    ]
  },
  "2507.15788v1": {
    "title": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement\n  Learning",
    "summary": "Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability.",
    "published": "2025-07-21T16:47:59Z",
    "updated": "2025-07-21T16:47:59Z",
    "id": "2507.15788v1",
    "authors": [
      "Sneheel Sarangi",
      "Hanan Salam"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15788v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15788v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15788v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Reinforcement Learning (RL) techniques to small-scale LLMs to instill Theory of Mind (ToM) capabilities, which aligns with the topics of LLM and RL. The focus on evaluating generalization and performance on different datasets also touches upon the Benchmark topic.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Benchmark"
    ]
  },
  "2507.15779v1": {
    "title": "Reservoir Computing as a Language Model",
    "summary": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance.",
    "published": "2025-07-21T16:35:38Z",
    "updated": "2025-07-21T16:35:38Z",
    "id": "2507.15779v1",
    "authors": [
      "Felix Kster",
      "Atsushi Uchida"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15779v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15779v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15779v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the comparison between reservoir computing and transformer-based architectures for language modeling, focusing on performance, computational cost, and prediction accuracy. It mentions Large Language Models (LLMs) and their challenges, but the core focus is on alternative approaches to language modeling rather than LLM-specific research.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.15771v1": {
    "title": "Left Leaning Models: AI Assumptions on Economic Policy",
    "summary": "How does AI think about economic policy? While the use of large language\nmodels (LLMs) in economics is growing exponentially, their assumptions on\neconomic issues remain a black box. This paper uses a conjoint experiment to\ntease out the main factors influencing LLMs' evaluation of economic policy. It\nfinds that LLMs are most sensitive to unemployment, inequality, financial\nstability, and environmental harm and less sensitive to traditional\nmacroeconomic concerns such as economic growth, inflation, and government debt.\nThe results are remarkably consistent across scenarios and across models.",
    "published": "2025-07-21T16:27:16Z",
    "updated": "2025-07-21T16:27:16Z",
    "id": "2507.15771v1",
    "authors": [
      "Maxim Chupilkin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15771v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15771v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15771v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in evaluating economic policy, focusing on their assumptions and sensitivities. This aligns with the 'LLM' topic as it involves research on large language models and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.15761v1": {
    "title": "GasAgent: A Multi-Agent Framework for Automated Gas Optimization in\n  Smart Contracts",
    "summary": "Smart contracts are trustworthy, immutable, and automatically executed\nprograms on the blockchain. Their execution requires the Gas mechanism to\nensure efficiency and fairness. However, due to non-optimal coding practices,\nmany contracts contain Gas waste patterns that need to be optimized. Existing\nsolutions mostly rely on manual discovery, which is inefficient, costly to\nmaintain, and difficult to scale. Recent research uses large language models\n(LLMs) to explore new Gas waste patterns. However, it struggles to remain\ncompatible with existing patterns, often produces redundant patterns, and\nrequires manual validation/rewriting. To address this gap, we present GasAgent,\nthe first multi-agent system for smart contract Gas optimization that combines\ncompatibility with existing patterns and automated discovery/validation of new\npatterns, enabling end-to-end optimization. GasAgent consists of four\nspecialized agents, Seeker, Innovator, Executor, and Manager, that collaborate\nin a closed loop to identify, validate, and apply Gas-saving improvements.\nExperiments on 100 verified real-world contracts demonstrate that GasAgent\nsuccessfully optimizes 82 contracts, achieving an average deployment Gas\nsavings of 9.97%. In addition, our evaluation confirms its compatibility with\nexisting tools and validates the effectiveness of each module through ablation\nstudies. To assess broader usability, we further evaluate 500 contracts\ngenerated by five representative LLMs across 10 categories and find that\nGasAgent optimizes 79.8% of them, with deployment Gas savings ranging from\n4.79% to 13.93%, showing its usability as the optimization layer for\nLLM-assisted smart contract development.",
    "published": "2025-07-21T16:17:25Z",
    "updated": "2025-07-21T16:17:25Z",
    "id": "2507.15761v1",
    "authors": [
      "Jingyi Zheng",
      "Zifan Peng",
      "Yule Liu",
      "Junfeng Wang",
      "Yifan Liao",
      "Wenhan Dong",
      "Xinlei He"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15761v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15761v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15761v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a multi-agent system for optimizing smart contracts, leveraging LLMs for pattern discovery and validation. The core focus is on the application of LLMs in a specific domain (smart contract optimization) rather than on the broader topics of LLM research, reinforcement learning, or multimodal models.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.15729v1": {
    "title": "Gaze-supported Large Language Model Framework for Bi-directional\n  Human-Robot Interaction",
    "summary": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks.",
    "published": "2025-07-21T15:38:25Z",
    "updated": "2025-07-21T15:38:25Z",
    "id": "2507.15729v1",
    "authors": [
      "Jens V. Rppel",
      "Andrey Rudenko",
      "Tim Schreiter",
      "Martin Magnusson",
      "Achim J. Lilienthal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15729v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15729v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15729v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in Human-Robot Interaction (HRI) systems, focusing on bi-directional, multi-modal, and context-aware support. It involves LLMs for interaction state representation and integrates vision inputs, aligning with topics related to LLMs and multimodal interactions.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "VLA"
    ]
  },
  "2507.15714v1": {
    "title": "Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's\n  Capability of Emotion Perception using Contrastive Learning",
    "summary": "The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,\nintroduces an emotion recognition challenge spanning over 28 languages. This\ncompetition encourages researchers to explore more advanced approaches to\naddress the challenges posed by the diversity of emotional expressions and\nbackground variations. It features two tracks: multi-label classification\n(Track A) and emotion intensity prediction (Track B), covering six emotion\ncategories: anger, fear, joy, sadness, surprise, and disgust. In our work, we\nsystematically explore the benefits of two contrastive learning approaches:\nsample-based (Contrastive Reasoning Calibration) and generation-based (DPO,\nSimPO) contrastive learning. The sample-based contrastive approach trains the\nmodel by comparing two samples to generate more reliable predictions. The\ngeneration-based contrastive approach trains the model to differentiate between\ncorrect and incorrect generations, refining its prediction. All models are\nfine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A\nand 6th place in Track B for English, while ranking among the top-tier\nperforming systems for other languages.",
    "published": "2025-07-21T15:25:47Z",
    "updated": "2025-07-21T15:25:47Z",
    "id": "2507.15714v1",
    "authors": [
      "Tian Li",
      "Yujian Sun",
      "Huizhi Liang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15714v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15714v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15714v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the emotion perception capabilities of a large language model (LLaMa3-Instruct-8B) using contrastive learning techniques, which involves fine-tuning and improving the model's performance in emotion detection tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.15707v1": {
    "title": "Is Large Language Model Performance on Reasoning Tasks Impacted by\n  Different Ways Questions Are Asked?",
    "summary": "Large Language Models (LLMs) have been evaluated using diverse question\ntypes, e.g., multiple-choice, true/false, and short/long answers. This study\nanswers an unexplored question about the impact of different question types on\nLLM accuracy on reasoning tasks. We investigate the performance of five LLMs on\nthree different types of questions using quantitative and deductive reasoning\ntasks. The performance metrics include accuracy in the reasoning steps and\nchoosing the final answer. Key Findings: (1) Significant differences exist in\nLLM performance across different question types. (2) Reasoning accuracy does\nnot necessarily correlate with the final selection accuracy. (3) The number of\noptions and the choice of words, influence LLM performance.",
    "published": "2025-07-21T15:15:30Z",
    "updated": "2025-07-21T15:15:30Z",
    "id": "2507.15707v1",
    "authors": [
      "Seok Hwan Song",
      "Mohna Chakraborty",
      "Qi Li",
      "Wallapak Tavanapong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15707v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15707v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15707v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper investigates the performance of Large Language Models (LLMs) on reasoning tasks, focusing on how different question types impact their accuracy. It directly relates to the evaluation of LLMs' reasoning abilities and their performance under varying conditions.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.15698v1": {
    "title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models",
    "summary": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.",
    "published": "2025-07-21T15:07:59Z",
    "updated": "2025-07-21T15:07:59Z",
    "id": "2507.15698v1",
    "authors": [
      "Congmin Zheng",
      "Jiachen Zhu",
      "Jianghao Lin",
      "Xinyi Dai",
      "Yong Yu",
      "Weinan Zhang",
      "Mengyue Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15698v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15698v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15698v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the issue of length bias in Process Reward Models (PRMs) used in large language models (LLMs) and proposes a method to mitigate this bias. It focuses on improving the reliability of reward predictions and the quality of outputs in LLMs, which is closely related to Reinforcement Learning (RL) and Reasoning in LLMs.",
    "llm_cls_result": [
      "RL",
      "Reasoning"
    ]
  },
  "2507.15692v1": {
    "title": "Surfacing Variations to Calibrate Perceived Reliability of\n  MLLM-generated Image Descriptions",
    "summary": "Multimodal large language models (MLLMs) provide new opportunities for blind\nand low vision (BLV) people to access visual information in their daily lives.\nHowever, these models often produce errors that are difficult to detect without\nsight, posing safety and social risks in scenarios from medication\nidentification to outfit selection. While BLV MLLM users use creative\nworkarounds such as cross-checking between tools and consulting sighted\nindividuals, these approaches are often time-consuming and impractical. We\nexplore how systematically surfacing variations across multiple MLLM responses\ncan support BLV users to detect unreliable information without visually\ninspecting the image. We contribute a design space for eliciting and presenting\nvariations in MLLM descriptions, a prototype system implementing three\nvariation presentation styles, and findings from a user study with 15 BLV\nparticipants. Our results demonstrate that presenting variations significantly\nincreases users' ability to identify unreliable claims (by 4.9x using our\napproach compared to single descriptions) and significantly decreases perceived\nreliability of MLLM responses. 14 of 15 participants preferred seeing\nvariations of MLLM responses over a single description, and all expressed\ninterest in using our system for tasks from understanding a tornado's path to\nposting an image on social media.",
    "published": "2025-07-21T14:59:50Z",
    "updated": "2025-07-21T14:59:50Z",
    "id": "2507.15692v1",
    "authors": [
      "Meng Chen",
      "Akhil Iyer",
      "Amy Pavel"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15692v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15692v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15692v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Multimodal Large Language Models (MLLMs) and their applications for blind and low vision users, specifically addressing the reliability of MLLM-generated image descriptions. It discusses the challenges and solutions related to MLLM outputs, which aligns with the topics of MLLM and Benchmark.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.15675v1": {
    "title": "P3: Prompts Promote Prompting",
    "summary": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains.",
    "published": "2025-07-21T14:37:46Z",
    "updated": "2025-07-21T14:37:46Z",
    "id": "2507.15675v1",
    "authors": [
      "Xinyu Zhang",
      "Yuanquan Hu",
      "Fangchao Liu",
      "Zhicheng Dou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15675v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15675v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15675v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the optimization of prompts for large language models (LLMs), which is directly related to the use and enhancement of LLMs. It also touches on reasoning tasks, which is another relevant topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.15671v1": {
    "title": "BugScope: Learn to Find Bugs Like Human",
    "summary": "Detecting software bugs remains a fundamental challenge due to the extensive\ndiversity of real-world defects. Traditional static analysis tools often rely\non symbolic workflows, which restrict their coverage and hinder adaptability to\ncustomized bugs with diverse anti-patterns. While recent advances incorporate\nlarge language models (LLMs) to enhance bug detection, these methods continue\nto struggle with sophisticated bugs and typically operate within limited\nanalysis contexts. To address these challenges, we propose BugScope, an\nLLM-driven multi-agent system that emulates how human auditors learn new bug\npatterns from representative examples and apply that knowledge during code\nauditing. Given a set of examples illustrating both buggy and non-buggy\nbehaviors, BugScope synthesizes a retrieval strategy to extract relevant\ndetection contexts via program slicing and then constructs a tailored detection\nprompt to guide accurate reasoning by the LLM. Our evaluation on a curated\ndataset of 40 real-world bugs drawn from 21 widely-used open-source projects\ndemonstrates that BugScope achieves 87.04% precision and 90.00% recall,\nsurpassing state-of-the-art industrial tools by 0.44 in F1 score. Further\ntesting on large-scale open-source systems, including the Linux kernel,\nuncovered 141 previously unknown bugs, of which 78 have been fixed and 7\nconfirmed by developers, highlighting BugScope's substantial practical impact.",
    "published": "2025-07-21T14:34:01Z",
    "updated": "2025-07-21T14:34:01Z",
    "id": "2507.15671v1",
    "authors": [
      "Jinyao Guo",
      "Chengpeng Wang",
      "Dominic Deluca",
      "Jinjie Liu",
      "Zhuo Zhang",
      "Xiangyu Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15671v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15671v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15671v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in a multi-agent system for bug detection, which aligns with the 'LLM' topic. It also involves reasoning abilities in LLMs for bug detection, which fits the 'Reasoning' topic. The practical application and evaluation of the system on real-world bugs could also be loosely related to 'Benchmark'.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.15640v1": {
    "title": "Data Mixing Agent: Learning to Re-weight Domains for Continual\n  Pre-training",
    "summary": "Continual pre-training on small-scale task-specific data is an effective\nmethod for improving large language models in new target fields, yet it risks\ncatastrophic forgetting of their original capabilities. A common solution is to\nre-weight training data mixtures from source and target fields on a domain\nspace to achieve balanced performance. Previous domain reweighting strategies\nrely on manual designation with certain heuristics based on human intuition or\nempirical results. In this work, we prove that more general heuristics can be\nparameterized by proposing Data Mixing Agent, the first model-based, end-to-end\nframework that learns to re-weight domains. The agent learns generalizable\nheuristics through reinforcement learning on large quantities of data mixing\ntrajectories with corresponding feedback from an evaluation environment.\nExperiments in continual pre-training on math reasoning show that Data Mixing\nAgent outperforms strong baselines in achieving balanced performance across\nsource and target field benchmarks. Furthermore, it generalizes well across\nunseen source fields, target models, and domain spaces without retraining.\nDirect application to the code generation field also indicates its adaptability\nacross target domains. Further analysis showcases the agents' well-aligned\nheuristics with human intuitions and their efficiency in achieving superior\nmodel performance with less source-field data.",
    "published": "2025-07-21T14:01:54Z",
    "updated": "2025-07-21T14:01:54Z",
    "id": "2507.15640v1",
    "authors": [
      "Kailai Yang",
      "Xiao Liu",
      "Lei Ji",
      "Hao Li",
      "Yeyun Gong",
      "Peng Cheng",
      "Mao Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15640v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15640v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15640v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses continual pre-training of large language models and the use of reinforcement learning to re-weight domains, which aligns with topics related to LLM, RL, and Pretrain.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Pretrain"
    ]
  },
  "2507.15613v1": {
    "title": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems",
    "summary": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as\nMicrosoft 365 Copilot) face novel security challenges. One critical threat is\nprompt inference attacks: adversaries chain together seemingly benign prompts\nto gradually extract confidential data. In this paper, we present a\ncomprehensive study of multi-stage prompt inference attacks in an enterprise\nLLM context. We simulate realistic attack scenarios where an attacker uses\nmild-mannered queries and indirect prompt injections to exploit an LLM\nintegrated with private corporate data. We develop a formal threat model for\nthese multi-turn inference attacks and analyze them using probability theory,\noptimization frameworks, and information-theoretic leakage bounds. The attacks\nare shown to reliably exfiltrate sensitive information from the LLM's context\n(e.g., internal SharePoint documents or emails), even when standard safety\nmeasures are in place.\n  We propose and evaluate defenses to counter such attacks, including\nstatistical anomaly detection, fine-grained access control, prompt sanitization\ntechniques, and architectural modifications to LLM deployment. Each defense is\nsupported by mathematical analysis or experimental simulation. For example, we\nderive bounds on information leakage under differential privacy-based training\nand demonstrate an anomaly detection method that flags multi-turn attacks with\nhigh AUC. We also introduce an approach called \"spotlighting\" that uses input\ntransformations to isolate untrusted prompt content, reducing attack success by\nan order of magnitude. Finally, we provide a formal proof of concept and\nempirical validation for a combined defense-in-depth strategy. Our work\nhighlights that securing LLMs in enterprise settings requires moving beyond\nsingle-turn prompt filtering toward a holistic, multi-stage perspective on both\nattacks and defenses.",
    "published": "2025-07-21T13:38:12Z",
    "updated": "2025-07-21T13:38:12Z",
    "id": "2507.15613v1",
    "authors": [
      "Andrii Balashov",
      "Olena Ponomarova",
      "Xiaohua Zhai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15613v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15613v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15613v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses security challenges and attacks specific to Large Language Models (LLMs) in enterprise settings, focusing on prompt inference attacks and defenses. The core topics are related to LLM security and attacks, which are not directly covered by the given topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.15551v2": {
    "title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders",
    "summary": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5\\% to 45\\%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross two core application scenarios (Recommendation and Advertisement).\nFinally, we launch 1B Dense-Parameters RankMixer for full traffic serving\nwithout increasing the serving cost, which improves user active days by 0.3\\%\nand total in-app usage duration by 1.08\\%.",
    "published": "2025-07-21T12:28:55Z",
    "updated": "2025-07-24T16:19:32Z",
    "id": "2507.15551v2",
    "authors": [
      "Jie Zhu",
      "Zhifang Fan",
      "Xiaoxie Zhu",
      "Yuchen Jiang",
      "Hangyu Wang",
      "Xintian Han",
      "Haoran Ding",
      "Xinmin Wang",
      "Wenlin Zhao",
      "Zhen Gong",
      "Huizhi Yang",
      "Zheng Chai",
      "Zhe Chen",
      "Yuchao Zheng",
      "Qiwei Chen",
      "Feng Zhang",
      "Xun Zhou",
      "Peng Xu",
      "Xiao Yang",
      "Di Wu",
      "Zuotao Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15551v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15551v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15551v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses scaling up ranking models in industrial recommenders, focusing on efficiency and scalability using transformer-based architectures and Sparse-MoE variants. It mentions LLMs and MoE, which are relevant topics.",
    "llm_cls_result": [
      "LLM",
      "MoE",
      "Scaling"
    ]
  },
  "2507.15521v1": {
    "title": "LLM world models are mental: Output layer evidence of brittle world\n  model use in LLM mechanical reasoning",
    "summary": "Do large language models (LLMs) construct and manipulate internal world\nmodels, or do they rely solely on statistical associations represented as\noutput layer token probabilities? We adapt cognitive science methodologies from\nhuman mental models research to test LLMs on pulley system problems using\nTikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical\nadvantage (MA). State-of-the-art models performed marginally but significantly\nabove chance, and their estimates correlated significantly with ground-truth\nMA. Significant correlations between number of pulleys and model estimates\nsuggest that models employed a pulley counting heuristic, without necessarily\nsimulating pulley systems to derive precise values. Study 2 tested this by\nprobing whether LLMs represent global features crucial to MA estimation. Models\nevaluated a functionally connected pulley system against a fake system with\nrandomly placed components. Without explicit cues, models identified the\nfunctional system as having greater MA with F1=0.8, suggesting LLMs could\nrepresent systems well enough to differentiate jumbled from functional systems.\nStudy 3 built on this by asking LLMs to compare functional systems with matched\nsystems which were connected up but which transferred no force to the weight;\nLLMs identified the functional system with F1=0.46, suggesting random guessing.\nInsofar as they may generalize, these findings are compatible with the notion\nthat LLMs manipulate internal world models, sufficient to exploit statistical\nassociations between pulley count and MA (Study 1), and to approximately\nrepresent system components' spatial relations (Study 2). However, they may\nlack the facility to reason over nuanced structural connectivity (Study 3). We\nconclude by advocating the utility of cognitive scientific methods to evaluate\nthe world-modeling capacities of artificial intelligence systems.",
    "published": "2025-07-21T11:42:03Z",
    "updated": "2025-07-21T11:42:03Z",
    "id": "2507.15521v1",
    "authors": [
      "Cole Robertson",
      "Philip Wolff"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15521v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15521v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15521v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper investigates the capabilities of large language models (LLMs) in constructing and manipulating internal world models, particularly in the context of mechanical reasoning. It uses cognitive science methodologies to test LLMs on pulley system problems, which aligns with research on LLM reasoning and their internal representations.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.15518v1": {
    "title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics",
    "summary": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET.",
    "published": "2025-07-21T11:36:39Z",
    "updated": "2025-07-21T11:36:39Z",
    "id": "2507.15518v1",
    "authors": [
      "Sizhou Chen",
      "Shufan Jiang",
      "Chi Zhang",
      "Xiao-Lei Zhang",
      "Xuelong Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15518v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15518v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15518v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in creating interactive theatrical experiences, focusing on multi-agent frameworks and autonomous decision-making. It also mentions the evaluation of drama performance, which involves benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "AGI"
    ]
  },
  "2507.15512v1": {
    "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language\n  Models",
    "summary": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs.",
    "published": "2025-07-21T11:28:09Z",
    "updated": "2025-07-21T11:28:09Z",
    "id": "2507.15512v1",
    "authors": [
      "Kaiyan Chang",
      "Yonghao Shi",
      "Chenglong Wang",
      "Hang Zhou",
      "Chi Hu",
      "Xiaoqian Liu",
      "Yingfeng Luo",
      "Yuan Ge",
      "Tong Xiao",
      "Jingbo Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15512v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15512v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15512v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses Test-Time Scaling (TTS) methods for Large Language Models (LLMs), focusing on training-free approaches for reasoning. It mentions reinforcement learning (RL) and introduces a hybrid strategy for scaling, which aligns with topics related to LLMs, reasoning, and scaling.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Scaling"
    ]
  },
  "2507.15501v1": {
    "title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action\n  Execution",
    "summary": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation.",
    "published": "2025-07-21T11:07:05Z",
    "updated": "2025-07-21T11:07:05Z",
    "id": "2507.15501v1",
    "authors": [
      "Alexandru Coca",
      "Mark Gaynor",
      "Zhenxing Zhang",
      "Jianpeng Cheng",
      "Bo-Hsiang Tseng",
      "Pete Boothroyd",
      "Hctor Martinez Alonso",
      "Diarmuid  Saghdha",
      "Anders Johannsen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15501v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15501v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15501v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the capabilities of large language models (LLMs) in executing complex actions through a simulated environment and a human-assisted data generation engine. It also introduces a benchmark dataset for evaluation, which aligns with topics related to LLMs, reasoning, and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.16852v1": {
    "title": "SynthCTI: LLM-Driven Synthetic CTI Generation to enhance MITRE Technique\n  Mapping",
    "summary": "Cyber Threat Intelligence (CTI) mining involves extracting structured\ninsights from unstructured threat data, enabling organizations to understand\nand respond to evolving adversarial behavior. A key task in CTI mining is\nmapping threat descriptions to MITRE ATT\\&CK techniques. However, this process\nis often performed manually, requiring expert knowledge and substantial effort.\nAutomated approaches face two major challenges: the scarcity of high-quality\nlabeled CTI data and class imbalance, where many techniques have very few\nexamples. While domain-specific Large Language Models (LLMs) such as SecureBERT\nhave shown improved performance, most recent work focuses on model architecture\nrather than addressing the data limitations. In this work, we present SynthCTI,\na data augmentation framework designed to generate high-quality synthetic CTI\nsentences for underrepresented MITRE ATT\\&CK techniques. Our method uses a\nclustering-based strategy to extract semantic context from training data and\nguide an LLM in producing synthetic CTI sentences that are lexically diverse\nand semantically faithful. We evaluate SynthCTI on two publicly available CTI\ndatasets, CTI-to-MITRE and TRAM, using LLMs with different capacity.\nIncorporating synthetic data leads to consistent macro-F1 improvements: for\nexample, ALBERT improves from 0.35 to 0.52 (a relative gain of 48.6\\%), and\nSecureBERT reaches 0.6558 (up from 0.4412). Notably, smaller models augmented\nwith SynthCTI outperform larger models trained without augmentation,\ndemonstrating the value of data generation methods for building efficient and\neffective CTI classification systems.",
    "published": "2025-07-21T09:22:39Z",
    "updated": "2025-07-21T09:22:39Z",
    "id": "2507.16852v1",
    "authors": [
      "lvaro Ruiz-Rdenas",
      "Jaime Pujante Sez",
      "Daniel Garca-Algora",
      "Mario Rodrguez Bjar",
      "Jorge Blasco",
      "Jos Luis Hernndez-Ramos"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16852v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16852v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16852v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating synthetic data to enhance Cyber Threat Intelligence (CTI) classification, focusing on data augmentation and model performance improvements.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.15903v1": {
    "title": "Towards Mitigation of Hallucination for LLM-empowered Agents:\n  Progressive Generalization Bound Exploration and Watchdog Monitor",
    "summary": "Empowered by large language models (LLMs), intelligent agents have become a\npopular paradigm for interacting with open environments to facilitate AI\ndeployment. However, hallucinations generated by LLMs-where outputs are\ninconsistent with facts-pose a significant challenge, undermining the\ncredibility of intelligent agents. Only if hallucinations can be mitigated, the\nintelligent agents can be used in real-world without any catastrophic risk.\nTherefore, effective detection and mitigation of hallucinations are crucial to\nensure the dependability of agents. Unfortunately, the related approaches\neither depend on white-box access to LLMs or fail to accurately identify\nhallucinations. To address the challenge posed by hallucinations of intelligent\nagents, we present HalMit, a novel black-box watchdog framework that models the\ngeneralization bound of LLM-empowered agents and thus detect hallucinations\nwithout requiring internal knowledge of the LLM's architecture. Specifically, a\nprobabilistic fractal sampling technique is proposed to generate a sufficient\nnumber of queries to trigger the incredible responses in parallel, efficiently\nidentifying the generalization bound of the target agent. Experimental\nevaluations demonstrate that HalMit significantly outperforms existing\napproaches in hallucination monitoring. Its black-box nature and superior\nperformance make HalMit a promising solution for enhancing the dependability of\nLLM-powered systems.",
    "published": "2025-07-21T09:08:58Z",
    "updated": "2025-07-21T09:08:58Z",
    "id": "2507.15903v1",
    "authors": [
      "Siyuan Liu",
      "Wenjing Liu",
      "Zhiwei Xu",
      "Xin Wang",
      "Bo Chen",
      "Tao Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15903v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15903v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15903v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on mitigating hallucinations in LLM-empowered agents, which involves both the use of large language models (LLMs) and the challenges specific to their deployment in intelligent agents. The proposed framework, HalMit, is designed to detect hallucinations without requiring internal knowledge of the LLM's architecture, indicating a focus on the practical deployment and reliability of LLMs.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.15357v1": {
    "title": "Metaphor and Large Language Models: When Surface Features Matter More\n  than Deep Understanding",
    "summary": "This paper presents a comprehensive evaluation of the capabilities of Large\nLanguage Models (LLMs) in metaphor interpretation across multiple datasets,\ntasks, and prompt configurations. Although metaphor processing has gained\nsignificant attention in Natural Language Processing (NLP), previous research\nhas been limited to single-dataset evaluations and specific task settings,\noften using artificially constructed data through lexical replacement. We\naddress these limitations by conducting extensive experiments using diverse\npublicly available datasets with inference and metaphor annotations, focusing\non Natural Language Inference (NLI) and Question Answering (QA) tasks. The\nresults indicate that LLMs' performance is more influenced by features like\nlexical overlap and sentence length than by metaphorical content, demonstrating\nthat any alleged emergent abilities of LLMs to understand metaphorical language\nare the result of a combination of surface-level features, in-context learning,\nand linguistic knowledge. This work provides critical insights into the current\ncapabilities and limitations of LLMs in processing figurative language,\nhighlighting the need for more realistic evaluation frameworks in metaphor\ninterpretation tasks. Data and code are publicly available.",
    "published": "2025-07-21T08:09:11Z",
    "updated": "2025-07-21T08:09:11Z",
    "id": "2507.15357v1",
    "authors": [
      "Elisa Sanchez-Bayona",
      "Rodrigo Agerri"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15357v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15357v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15357v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the capabilities of Large Language Models (LLMs) in metaphor interpretation, which directly relates to the 'LLM' topic. It also discusses the limitations and emergent abilities of LLMs, which aligns with the 'Reasoning' topic as it involves understanding and processing complex language features.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.15349v1": {
    "title": "Scaling Decentralized Learning with FLock",
    "summary": "Fine-tuning the large language models (LLMs) are prevented by the deficiency\nof centralized control and the massive computing and communication overhead on\nthe decentralized schemes. While the typical standard federated learning (FL)\nsupports data privacy, the central server requirement creates a single point of\nattack and vulnerability to poisoning attacks. Generalizing the result in this\ndirection to 70B-parameter models in the heterogeneous, trustless environments\nhas turned out to be a huge, yet unbroken bottleneck. This paper introduces\nFLock, a decentralized framework for secure and efficient collaborative LLM\nfine-tuning. Integrating a blockchain-based trust layer with economic\nincentives, FLock replaces the central aggregator with a secure, auditable\nprotocol for cooperation among untrusted parties. We present the first\nempirical validation of fine-tuning a 70B LLM in a secure, multi-domain,\ndecentralized setting. Our experiments show the FLock framework defends against\nbackdoor poisoning attacks that compromise standard FL optimizers and fosters\nsynergistic knowledge transfer. The resulting models show a >68% reduction in\nadversarial attack success rates. The global model also demonstrates superior\ncross-domain generalization, outperforming models trained in isolation on their\nown specialized data.",
    "published": "2025-07-21T08:01:43Z",
    "updated": "2025-07-21T08:01:43Z",
    "id": "2507.15349v1",
    "authors": [
      "Zehua Cheng",
      "Rui Sun",
      "Jiahao Sun",
      "Yike Guo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15349v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15349v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15349v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses decentralized learning and fine-tuning of large language models (LLMs) with a focus on security and efficiency, which aligns with the topics of LLM and Scaling. The mention of federated learning and decentralized schemes also touches on aspects of Reinforcement Learning (RL) in terms of collaborative learning environments.",
    "llm_cls_result": [
      "LLM",
      "Scaling",
      "RL"
    ]
  },
  "2507.15347v1": {
    "title": "Probing Information Distribution in Transformer Architectures through\n  Entropy Analysis",
    "summary": "This work explores entropy analysis as a tool for probing information\ndistribution within Transformer-based architectures. By quantifying token-level\nuncertainty and examining entropy patterns across different stages of\nprocessing, we aim to investigate how information is managed and transformed\nwithin these models. As a case study, we apply the methodology to a GPT-based\nlarge language model, illustrating its potential to reveal insights into model\nbehavior and internal representations. This approach may offer insights into\nmodel behavior and contribute to the development of interpretability and\nevaluation frameworks for transformer-based models",
    "published": "2025-07-21T08:01:22Z",
    "updated": "2025-07-21T08:01:22Z",
    "id": "2507.15347v1",
    "authors": [
      "Amedeo Buonanno",
      "Alessandro Rivetti",
      "Francesco A. N. Palmieri",
      "Giovanni Di Gennaro",
      "Gianmarco Romano"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15347v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15347v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15347v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on analyzing information distribution within Transformer architectures, specifically using entropy analysis on a GPT-based large language model. This aligns with research on Large Language Models (LLM) and their internal mechanisms.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.15328v1": {
    "title": "On the Inevitability of Left-Leaning Political Bias in Aligned Language\n  Models",
    "summary": "The guiding principle of AI alignment is to train large language models\n(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are\nmounting concerns that LLMs exhibit a left-wing political bias. Yet, the\ncommitment to AI alignment cannot be harmonized with the latter critique. In\nthis article, I argue that intelligent systems that are trained to be harmless\nand honest must necessarily exhibit left-wing political bias. Normative\nassumptions underlying alignment objectives inherently concur with progressive\nmoral frameworks and left-wing principles, emphasizing harm avoidance,\ninclusivity, fairness, and empirical truthfulness. Conversely, right-wing\nideologies often conflict with alignment guidelines. Yet, research on political\nbias in LLMs is consistently framing its insights about left-leaning tendencies\nas a risk, as problematic, or concerning. This way, researchers are actively\narguing against AI alignment, tacitly fostering the violation of HHH\nprinciples.",
    "published": "2025-07-21T07:37:28Z",
    "updated": "2025-07-21T07:37:28Z",
    "id": "2507.15328v1",
    "authors": [
      "Thilo Hagendorff"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15328v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15328v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15328v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the political bias in large language models (LLMs) and how alignment objectives inherently align with left-wing principles, which is relevant to the study of LLMs and their societal impacts.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.15296v1": {
    "title": "Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed\n  Parameter Filling in LLM Tool-Agent Systems",
    "summary": "The emergence of the tool agent paradigm has broadened the capability\nboundaries of the Large Language Model (LLM), enabling it to complete more\ncomplex tasks. However, the effectiveness of this paradigm is limited due to\nthe issue of parameter failure during its execution. To explore this phenomenon\nand propose corresponding suggestions, we first construct a parameter failure\ntaxonomy in this paper. We derive five failure categories from the invocation\nchain of a mainstream tool agent. Then, we explore the correlation between\nthree different input sources and failure categories by applying 15 input\nperturbation methods to the input. Experimental results show that parameter\nname hallucination failure primarily stems from inherent LLM limitations, while\nissues with input sources mainly cause other failure patterns. To improve the\nreliability and effectiveness of tool-agent interactions, we propose\ncorresponding improvement suggestions, including standardizing tool return\nformats, improving error feedback mechanisms, and ensuring parameter\nconsistency.",
    "published": "2025-07-21T06:55:37Z",
    "updated": "2025-07-21T06:55:37Z",
    "id": "2507.15296v1",
    "authors": [
      "Qian Xiong",
      "Yuekai Huang",
      "Ziyou Jiang",
      "Zhiyuan Chang",
      "Yujia Zheng",
      "Tianhao Li",
      "Mingyang Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15296v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15296v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15296v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses issues related to Large Language Models (LLMs) in the context of tool-agent systems, specifically focusing on parameter failure and its implications. It does not directly address multimodal models, reinforcement learning, or other specialized topics listed.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.15255v1": {
    "title": "MEETI: A Multimodal ECG Dataset from MIMIC-IV-ECG with Signals, Images,\n  Features and Interpretations",
    "summary": "Electrocardiogram (ECG) plays a foundational role in modern cardiovascular\ncare, enabling non-invasive diagnosis of arrhythmias, myocardial ischemia, and\nconduction disorders. While machine learning has achieved expert-level\nperformance in ECG interpretation, the development of clinically deployable\nmultimodal AI systems remains constrained, primarily due to the lack of\npublicly available datasets that simultaneously incorporate raw signals,\ndiagnostic images, and interpretation text. Most existing ECG datasets provide\nonly single-modality data or, at most, dual modalities, making it difficult to\nbuild models that can understand and integrate diverse ECG information in\nreal-world settings. To address this gap, we introduce MEETI (MIMIC-IV-Ext\nECG-Text-Image), the first large-scale ECG dataset that synchronizes raw\nwaveform data, high-resolution plotted images, and detailed textual\ninterpretations generated by large language models. In addition, MEETI includes\nbeat-level quantitative ECG parameters extracted from each lead, offering\nstructured parameters that support fine-grained analysis and model\ninterpretability. Each MEETI record is aligned across four components: (1) the\nraw ECG waveform, (2) the corresponding plotted image, (3) extracted feature\nparameters, and (4) detailed interpretation text. This alignment is achieved\nusing consistent, unique identifiers. This unified structure supports\ntransformer-based multimodal learning and supports fine-grained, interpretable\nreasoning about cardiac health. By bridging the gap between traditional signal\nanalysis, image-based interpretation, and language-driven understanding, MEETI\nestablished a robust foundation for the next generation of explainable,\nmultimodal cardiovascular AI. It offers the research community a comprehensive\nbenchmark for developing and evaluating ECG-based AI systems.",
    "published": "2025-07-21T05:32:44Z",
    "updated": "2025-07-21T05:32:44Z",
    "id": "2507.15255v1",
    "authors": [
      "Deyun Zhang",
      "Xiang Lan",
      "Shijia Geng",
      "Qinghao Zhao",
      "Sumei Fan",
      "Mengling Feng",
      "Shenda Hong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15255v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15255v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15255v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a multimodal dataset (MEETI) that integrates ECG signals, images, and textual interpretations, which aligns with the topic of multimodal datasets and benchmarking in the context of multimodal large language models (MLLM).",
    "llm_cls_result": [
      "MLLM",
      "Dataset",
      "Benchmark"
    ]
  },
  "2507.15249v1": {
    "title": "FreeCus: Free Lunch Subject-driven Customization in Diffusion\n  Transformers",
    "summary": "In light of recent breakthroughs in text-to-image (T2I) generation,\nparticularly with diffusion transformers (DiT), subject-driven technologies are\nincreasingly being employed for high-fidelity customized production that\npreserves subject identity from reference inputs, enabling thrilling design\nworkflows and engaging entertainment. Existing alternatives typically require\neither per-subject optimization via trainable text embeddings or training\nspecialized encoders for subject feature extraction on large-scale datasets.\nSuch dependencies on training procedures fundamentally constrain their\npractical applications. More importantly, current methodologies fail to fully\nleverage the inherent zero-shot potential of modern diffusion transformers\n(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this\ngap, we propose FreeCus, a genuinely training-free framework that activates\nDiT's capabilities through three key innovations: 1) We introduce a pivotal\nattention sharing mechanism that captures the subject's layout integrity while\npreserving crucial editing flexibility. 2) Through a straightforward analysis\nof DiT's dynamic shifting, we propose an upgraded variant that significantly\nimproves fine-grained feature extraction. 3) We further integrate advanced\nMultimodal Large Language Models (MLLMs) to enrich cross-modal semantic\nrepresentations. Extensive experiments reflect that our method successfully\nunlocks DiT's zero-shot ability for consistent subject synthesis across diverse\ncontexts, achieving state-of-the-art or comparable results compared to\napproaches that require additional training. Notably, our framework\ndemonstrates seamless compatibility with existing inpainting pipelines and\ncontrol modules, facilitating more compelling experiences. Our code is\navailable at: https://github.com/Monalissaa/FreeCus.",
    "published": "2025-07-21T05:15:45Z",
    "updated": "2025-07-21T05:15:45Z",
    "id": "2507.15249v1",
    "authors": [
      "Yanbing Zhang",
      "Zhe Wang",
      "Qin Zhou",
      "Mengping Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15249v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15249v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15249v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of diffusion transformers (DiT) and integrates Multimodal Large Language Models (MLLMs) for subject-driven customization in text-to-image generation. The focus on MLLMs and their application in cross-modal semantic representations aligns with the MLLM topic. The use of diffusion transformers also suggests a connection to scaling and pretraining strategies, though the primary focus is on MLLMs.",
    "llm_cls_result": [
      "MLLM",
      "Scaling",
      "Pretrain"
    ]
  },
  "2507.15198v1": {
    "title": "Collaborative Distillation Strategies for Parameter-Efficient Language\n  Model Deployment",
    "summary": "This paper addresses the challenges of high computational cost and slow\ninference in deploying large language models. It proposes a distillation\nstrategy guided by multiple teacher models. The method constructs several\nteacher models and integrates their output probability distributions and\nintermediate semantic features. This guides the student model to learn from\nmultiple sources of knowledge. As a result, the student model gains stronger\nlanguage understanding and generation ability while maintaining a small\nparameter size. To achieve this, the paper introduces a weighted output fusion\nmechanism, a feature alignment loss function, and an entropy-driven dynamic\nteacher weighting strategy. These components improve the quality and stability\nof knowledge transfer during distillation. Under multi-teacher guidance, the\nstudent model captures semantic information more effectively and demonstrates\nstrong performance across multiple evaluation metrics. In particular, the\nmethod shows high consistency in expression, generalization ability, and task\nadaptability in tasks such as language modeling, text generation, and\nmulti-task learning. The experiments compare the proposed method with several\nwidely adopted distillation approaches. The results further confirm its overall\nadvantages in perplexity, distillation loss, and generation quality. This study\nprovides a feasible technical path for the efficient compression of large-scale\nlanguage models. It also demonstrates the effectiveness of multi-teacher\ncollaborative mechanisms in complex language modeling tasks.",
    "published": "2025-07-21T02:55:33Z",
    "updated": "2025-07-21T02:55:33Z",
    "id": "2507.15198v1",
    "authors": [
      "Xiandong Meng",
      "Yan Wu",
      "Yexin Tian",
      "Xin Hu",
      "Tianze Kang",
      "Junliang Du"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15198v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15198v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15198v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on distillation strategies for deploying large language models efficiently, which involves knowledge transfer from multiple teacher models to a smaller student model. This aligns with research on LLM (Large Language Models) and their optimization for deployment.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.15130v1": {
    "title": "Enhancing Visual Planning with Auxiliary Tasks and Multi-token\n  Prediction",
    "summary": "Visual Planning for Assistance (VPA) aims to predict a sequence of user\nactions required to achieve a specified goal based on a video showing the\nuser's progress. Although recent advances in multimodal large language models\n(MLLMs) have shown promising results in video understanding, long-horizon\nvisual planning remains a challenging problem. We identify two challenges in\ntraining large MLLMs for video-based planning tasks: (1) scarcity of procedural\nannotations, limiting the model's ability to learn procedural task dynamics\neffectively, and (2) inefficiency of next-token prediction objective to\nexplicitly capture the structured action space for visual planning when\ncompared to free-form, natural language. To tackle data scarcity, we introduce\nAuxiliary Task Augmentation. We design and train our model on auxiliary tasks\nrelevant to long-horizon video-based planning (e.g., goal prediction) to\naugment the model's planning ability. To more explicitly model the structured\naction space unique to visual planning tasks, we leverage Multi-token\nPrediction, extending traditional next-token prediction by using multiple heads\nto predict multiple future tokens during training. Our approach, VideoPlan,\nachieves state-of-the-art VPA performance on the COIN and CrossTask datasets,\nsurpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3\nfuture actions. We further extend our method to the challenging Ego4D Long-term\nAction Anticipation task, and show that it is on par with the state-of-the-art\napproaches despite not using specialized egocentric features. Code will be made\navailable.",
    "published": "2025-07-20T21:39:05Z",
    "updated": "2025-07-20T21:39:05Z",
    "id": "2507.15130v1",
    "authors": [
      "Ce Zhang",
      "Yale Song",
      "Ruta Desai",
      "Michael Louis Iuzzolino",
      "Joseph Tighe",
      "Gedas Bertasius",
      "Satwik Kottur"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15130v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15130v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15130v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of multimodal large language models (MLLMs) for visual planning tasks, which involves video understanding and action prediction. It addresses challenges specific to MLLMs and introduces methods to enhance their performance in this domain.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Reasoning"
    ]
  },
  "2507.15100v1": {
    "title": "Filling the Gap: Is Commonsense Knowledge Generation useful for Natural\n  Language Inference?",
    "summary": "Natural Language Inference (NLI) is the task of determining the semantic\nentailment of a premise for a given hypothesis. The task aims to develop\nsystems that emulate natural human inferential processes where commonsense\nknowledge plays a major role. However, existing commonsense resources lack\nsufficient coverage for a variety of premise-hypothesis pairs. This study\nexplores the potential of Large Language Models as commonsense knowledge\ngenerators for NLI along two key dimensions: their reliability in generating\nsuch knowledge and the impact of that knowledge on prediction accuracy. We\nadapt and modify existing metrics to assess LLM factuality and consistency in\ngenerating in this context. While explicitly incorporating commonsense\nknowledge does not consistently improve overall results, it effectively helps\ndistinguish entailing instances and moderately improves distinguishing\ncontradictory and neutral inferences.",
    "published": "2025-07-20T19:42:45Z",
    "updated": "2025-07-20T19:42:45Z",
    "id": "2507.15100v1",
    "authors": [
      "Chathuri Jayaweera",
      "Brianna Yanqui",
      "Bonnie Dorr"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15100v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15100v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15100v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper explores the use of Large Language Models (LLMs) for generating commonsense knowledge in the context of Natural Language Inference (NLI), focusing on their reliability and impact on prediction accuracy. This aligns with research on LLMs and their applications in reasoning tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.15094v1": {
    "title": "BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic\n  Submucosal Dissection via Dual-Stage Detection and Tracking",
    "summary": "Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses\nsignificant risks, demanding precise, real-time localization and continuous\nmonitoring of the bleeding source for effective hemostatic intervention. In\nparticular, endoscopists have to repeatedly flush to clear blood, allowing only\nmilliseconds to identify bleeding sources, an inefficient process that prolongs\noperations and elevates patient risks. However, current Artificial Intelligence\n(AI) methods primarily focus on bleeding region segmentation, overlooking the\ncritical need for accurate bleeding source detection and temporal tracking in\nthe challenging ESD environment, which is marked by frequent visual\nobstructions and dynamic scene changes. This gap is widened by the lack of\nspecialized datasets, hindering the development of robust AI-assisted guidance\nsystems. To address these challenges, we introduce BleedOrigin-Bench, the first\ncomprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated\nbleeding sources across 106,222 frames from 44 procedures, supplemented with\n39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6\nchallenging clinical scenarios. We also present BleedOrigin-Net, a novel\ndual-stage detection-tracking framework for the bleeding source localization in\nESD procedures, addressing the complete workflow from bleeding onset detection\nto continuous spatial tracking. We compare with widely-used object detection\nmodels (YOLOv11/v12), multimodal large language models, and point tracking\nmethods. Extensive evaluation demonstrates state-of-the-art performance,\nachieving 96.85% frame-level accuracy ($\\pm\\leq8$ frames) for bleeding onset\ndetection, 70.24% pixel-level accuracy ($\\leq100$ px) for initial source\ndetection, and 96.11% pixel-level accuracy ($\\leq100$ px) for point tracking.",
    "published": "2025-07-20T19:19:42Z",
    "updated": "2025-07-20T19:19:42Z",
    "id": "2507.15094v1",
    "authors": [
      "Mengya Xu",
      "Rulin Zhou",
      "An Wang",
      "Chaoyang Lyu",
      "Zhen Li",
      "Ning Zhong",
      "Hongliang Ren"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15094v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15094v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15094v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a medical application involving dynamic bleeding source localization in endoscopic procedures, which does not directly align with the provided topics related to Large Language Models, Reinforcement Learning, or other specified areas. The primary focus is on medical imaging and AI-assisted guidance systems in a clinical setting.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.15092v1": {
    "title": "A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic\n  Texts Under Prompt-Influenced Length Variations",
    "summary": "Synthetic text generated by Large Language Models (LLMs) is increasingly used\nfor further training and improvement of LLMs. Diversity is crucial for the\neffectiveness of synthetic data, and researchers rely on prompt engineering to\nimprove diversity. However, the impact of prompt variations on response text\nlength, and, more importantly, the consequential effect on lexical diversity\nmeasurements, remain underexplored. In this work, we propose Penalty-Adjusted\nType-Token Ratio (PATTR), a diversity metric robust to length variations. We\ngenerate a large synthetic corpus of over 20M words using seven models from the\nLLaMA, OLMo, and Phi families, focusing on a creative writing task of video\nscript generation, where diversity is crucial. We evaluate per-response lexical\ndiversity using PATTR and compare it against existing metrics of Moving-Average\nTTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length\nvariations introduce biases favoring shorter responses. Unlike existing\nmetrics, PATTR explicitly considers the task-specific target response length\n($L_T$) to effectively mitigate length biases. We further demonstrate the\nutility of PATTR in filtering the top-10/100/1,000 most lexically diverse\nresponses, showing that it consistently outperforms MATTR and CR by yielding on\npar or better diversity with high adherence to $L_T$.",
    "published": "2025-07-20T19:14:43Z",
    "updated": "2025-07-20T19:14:43Z",
    "id": "2507.15092v1",
    "authors": [
      "Vijeta Deshpande",
      "Ishita Dasgupta",
      "Uttaran Bhattacharya",
      "Somdeb Sarkhel",
      "Saayan Mitra",
      "Anna Rumshisky"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15092v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15092v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15092v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on measuring lexical diversity in synthetic texts generated by Large Language Models (LLMs) and proposes a new metric (PATTR) to address biases introduced by length variations. The core topics revolve around LLMs and their evaluation, specifically in the context of synthetic data generation and diversity measurement.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.15058v1": {
    "title": "LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries",
    "summary": "A fundamental problem in cybersecurity and computer science is determining\nwhether a program is free of bugs and vulnerabilities. Fuzzing, a popular\napproach to discovering vulnerabilities in programs, has several advantages\nover alternative strategies, although it has investment costs in the form of\ninitial setup and continuous maintenance. The choice of fuzzing is further\ncomplicated when only a binary library is available, such as the case of\nclosed-source and proprietary software. In response, we introduce LibLMFuzz, a\nframework that reduces costs associated with fuzzing closed-source libraries by\npairing an agentic Large Language Model (LLM) with a lightweight tool-chain\n(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan\nfuzz strategies, generate drivers, and iteratively self-repair build or runtime\nerrors. Tested on four widely-used Linux libraries, LibLMFuzz produced\nsyntactically correct drivers for all 558 fuzz-able API functions, achieving\n100% API coverage with no human intervention. Across the 1601 synthesized\ndrivers, 75.52% were nominally correct on first execution. The results show\nthat LLM-augmented middleware holds promise in reducing the costs of fuzzing\nblack box components and provides a foundation for future research efforts.\nFuture opportunities exist for research in branch coverage.",
    "published": "2025-07-20T17:38:51Z",
    "updated": "2025-07-20T17:38:51Z",
    "id": "2507.15058v1",
    "authors": [
      "Ian Hardgrove",
      "John D. Hastings"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15058v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15058v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15058v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a Large Language Model (LLM) to autonomously analyze and generate fuzz targets for black-box libraries, which aligns with the LLM topic. It also involves the application of LLMs in a cybersecurity context, which is a specialized use case of LLMs.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.15026v1": {
    "title": "Deep Generative Models in Condition and Structural Health Monitoring:\n  Opportunities, Limitations and Future Outlook",
    "summary": "Condition and structural health monitoring (CM/SHM) is a pivotal component of\npredictive maintenance (PdM) strategies across diverse industrial sectors,\nincluding mechanical rotating machinery, airplane composite wings, offshore\nwind turbines, and civil engineering structures. Conventional deep learning\nmodels, while effective in fault diagnosis and anomaly detection through\nsupervised feature extraction and rule-based data augmentation, often struggle\nwith operational variability, imbalanced or scarce fault datasets, and\nmultimodal sensory data from complex systems. Deep generative models (DGMs) in\nthis regard, including autoregressive models, variational autoencoders,\ngenerative adversarial networks, diffusion-based models, and emerging large\nlanguage models, offer transformative capabilities by synthesizing\nhigh-fidelity data samples, reconstructing latent system states, and modeling\ncomplex multimodal data streams. This review systematically examines\nstate-of-the-art DGM applications in CM/SHM systems, emphasizing their role in\naddressing key challenges: data imbalance and imputation, domain adaptation and\ngeneralization, multimodal data fusion, and downstream fault diagnosis and\nanomaly detection tasks, with rigorous comparison among signal processing,\nconventional machine learning or deep learning models, and DGMs. We also\nanalyze current limitations of DGMs, including challenges of explainable and\ntrustworthy models, computational inefficiencies for edge deployment, and the\nneed for parameter-efficient fine-tuning strategies. Future research directions\ncan focus on zero-shot and few-shot learning, robust multimodal generalization,\nhybrid architectures integrating DGMs with physics knowledge, and reinforcement\nlearning with DGMs to enhance robustness and accuracy in industrial scenarios.",
    "published": "2025-07-20T16:28:12Z",
    "updated": "2025-07-20T16:28:12Z",
    "id": "2507.15026v1",
    "authors": [
      "Xin Yang",
      "Chen Fang",
      "Yunlai Liao",
      "Jian Yang",
      "Konstantinos Gryllias",
      "Dimitrios Chronopoulos"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15026v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15026v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15026v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses deep generative models (DGMs) and their applications in condition and structural health monitoring, including emerging large language models. However, the primary focus is on DGMs rather than LLMs or other specific topics listed.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14997v1": {
    "title": "Language Integration in Fine-Tuning Multimodal Large Language Models for\n  Image-Based Regression",
    "summary": "Multimodal Large Language Models (MLLMs) show promise for image-based\nregression tasks, but current approaches face key limitations. Recent methods\nfine-tune MLLMs using preset output vocabularies and generic task-level prompts\n(e.g., \"How would you rate this image?\"), assuming this mimics human rating\nbehavior. Our analysis reveals these approaches provide no benefit over\nimage-only training. Models using preset vocabularies and generic prompts\nperform equivalently to image-only models, failing to leverage semantic\nunderstanding from textual input. We propose Regression via Transformer-Based\nClassification (RvTC), which replaces vocabulary-constrained classification\nwith a flexible bin-based approach. Unlike approaches that address\ndiscretization errors through complex distributional modeling, RvTC eliminates\nmanual vocabulary crafting through straightforward bin increase, achieving\nstate-of-the-art performance on four image assessment datasets using only\nimages. More importantly, we demonstrate that data-specific prompts\ndramatically improve performance. Unlike generic task descriptions, prompts\ncontaining semantic information about specific images enable MLLMs to leverage\ncross-modal understanding. On the AVA dataset, adding challenge titles to\nprompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We\ndemonstrate through empirical evidence from the AVA and AGIQA-3k datasets that\nMLLMs benefit from semantic prompt information surpassing mere statistical\nbiases. This underscores the importance of incorporating meaningful textual\ncontext in multimodal regression tasks.",
    "published": "2025-07-20T15:05:24Z",
    "updated": "2025-07-20T15:05:24Z",
    "id": "2507.14997v1",
    "authors": [
      "Roy H. Jennings",
      "Genady Paikin",
      "Roy Shaul",
      "Evgeny Soloveichik"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14997v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14997v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14997v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Multimodal Large Language Models (MLLMs) and their application in image-based regression tasks, emphasizing the importance of semantic understanding and data-specific prompts. The core topics are MLLM and Reasoning, as it discusses the integration of language and vision modalities and the reasoning capabilities of MLLMs in regression tasks.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.14995v1": {
    "title": "LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for\n  Real-Time P2P Energy Trading",
    "summary": "Real-time peer-to-peer (P2P) electricity markets dynamically adapt to\nfluctuations in renewable energy and variations in demand, maximizing economic\nbenefits through instantaneous price responses while enhancing grid\nflexibility. However, scaling expert guidance for massive personalized\nprosumers poses critical challenges, including diverse decision-making demands\nand lack of customized modeling frameworks. This paper proposed an integrated\nlarge language model-multi-agent reinforcement learning (LLM-MARL) framework\nfor real-time P2P energy trading to address challenges such as the limited\ntechnical capability of prosumers, the lack of expert experience, and security\nissues of distribution networks. LLMs are introduced as experts to generate\npersonalized strategy, guiding MARL under the centralized training with\ndecentralized execution (CTDE) paradigm through imitation learning. A\ndifferential attention-based critic network is designed to enhance convergence\nperformance. Experimental results demonstrate that LLM generated strategies\neffectively substitute human experts. The proposed multi-agent imitation\nlearning algorithms achieve significantly lower economic costs and voltage\nviolation rates on test sets compared to baselines algorithms, while\nmaintaining robust stability. This work provides an effective solution for\nreal-time P2P electricity market decision-making by bridging expert knowledge\nwith agent learning.",
    "published": "2025-07-20T14:59:18Z",
    "updated": "2025-07-20T14:59:18Z",
    "id": "2507.14995v1",
    "authors": [
      "Chengwei Lou",
      "Zekai Jin",
      "Wei Tang",
      "Guangfei Geng",
      "Jin Yang",
      "Lu Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14995v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14995v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14995v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLM) with multi-agent reinforcement learning (MARL) for real-time P2P energy trading, which involves both LLM and RL topics. The use of LLMs as experts to guide MARL under the CTDE paradigm is a key aspect of the research.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.14987v1": {
    "title": "AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified\n  Reinforcement Learning",
    "summary": "Large language models (LLMs), despite possessing latent safety understanding\nfrom their vast pretraining data, remain vulnerable to generating harmful\ncontent and exhibit issues such as over-refusal and utility degradation after\nsafety alignment. Current safety alignment methods often result in superficial\nrefusal shortcuts or rely on intensive supervision for reasoning-based\napproaches, failing to fully leverage the model's intrinsic safety\nself-awareness. We propose \\textbf{AlphaAlign}, a simple yet effective pure\nreinforcement learning (RL) framework with verifiable safety reward designed to\nincentivize this latent safety awareness through proactive safety reasoning.}\nAlphaAlign employs a dual-reward system: a verifiable safety reward encourages\ncorrectly formatted and explicitly justified refusals for harmful queries while\npenalizing over-refusals, and a normalized helpfulness reward guides\nhigh-quality responses to benign inputs. This allows the model to develop\nproactive safety reasoning capabilities without depending on supervised\nsafety-specific reasoning data. AlphaAlign demonstrates three key advantages:\n(1) Simplicity and efficiency, requiring only binary prompt safety labels and\nminimal RL steps for substantial improvements. (2) Breaking the safety-utility\ntrade-off, by enhancing refusal of harmful content and reducing over-refusals,\nwhile simultaneously maintaining or even improving general task performance and\nrobustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety\nreasoning that generates explicit safety rationales rather than relying on\nshallow refusal patterns.",
    "published": "2025-07-20T14:47:03Z",
    "updated": "2025-07-20T14:47:03Z",
    "id": "2507.14987v1",
    "authors": [
      "Yi Zhang",
      "An Zhang",
      "XiuYu Zhang",
      "Leheng Sheng",
      "Yuxin Chen",
      "Zhenkai Liang",
      "Xiang Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14987v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14987v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14987v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a reinforcement learning framework (AlphaAlign) designed to improve safety alignment in large language models (LLMs). It focuses on leveraging the model's intrinsic safety awareness through RL, which aligns with topics related to LLM reinforcement learning (RL) and safety alignment in LLMs.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.14975v1": {
    "title": "FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task\n  Planning with Large Language Models",
    "summary": "Autonomous error correction is critical for domestic robots to achieve\nreliable execution of complex long-horizon tasks. Prior work has explored\nself-reflection in Large Language Models (LLMs) for task planning error\ncorrection; however, existing methods are constrained by inflexible\nself-reflection mechanisms that limit their effectiveness. Motivated by these\nlimitations and inspired by human cognitive adaptation, we propose the Flexible\nConstructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture\nthat enables LLMs to perform flexible self-reflection based on task difficulty,\nwhile constructively integrating historical valuable experience with failure\nlessons. We evaluated FCRF on diverse domestic tasks through simulation in\nAlfWorld and physical deployment in the real-world environment. Experimental\nresults demonstrate that FCRF significantly improves overall performance and\nself-reflection flexibility in complex long-horizon robotic tasks.",
    "published": "2025-07-20T14:15:39Z",
    "updated": "2025-07-20T14:15:39Z",
    "id": "2507.14975v1",
    "authors": [
      "Yufan Song",
      "Jiatao Zhang",
      "Zeng Gu",
      "Qingmiao Liang",
      "Tuocheng Hu",
      "Wei Song",
      "Shiqiang Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14975v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14975v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14975v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for task planning error correction in robotics, which involves self-reflection mechanisms and integration of historical experience. This aligns with research on LLMs and their applications in reasoning and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2507.15892v1": {
    "title": "StaAgent: An Agentic Framework for Testing Static Analyzers",
    "summary": "Static analyzers play a critical role in identifying bugs early in the\nsoftware development lifecycle, but their rule implementations are often\nunder-tested and prone to inconsistencies. To address this, we propose\nStaAgent, an agentic framework that harnesses the generative capabilities of\nLarge Language Models (LLMs) to systematically evaluate static analyzer rules.\nStaAgent comprises four specialized agents: a Seed Generation Agent that\ntranslates bug detection rules into concrete, bug-inducing seed programs; a\nCode Validation Agent that ensures the correctness of these seeds; a Mutation\nGeneration Agent that produces semantically equivalent mutants; and an Analyzer\nEvaluation Agent that performs metamorphic testing by comparing the static\nanalyzer's behavior on seeds and their corresponding mutants. By revealing\ninconsistent behaviors, StaAgent helps uncover flaws in rule implementations.\nThis LLM-driven, multi-agent framework offers a scalable and adaptable solution\nto improve the reliability of static analyzers. We evaluated StaAgent with five\nstate-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)\nacross five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,\nInfer, and PMD). The experimental results show that our approach can help\nreveal 64 problematic rules in the latest versions of these five static\nanalyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,\nand 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the\nSOTA baseline. We have reported all the bugs to developers, with two of them\nalready fixed. Three more have been confirmed by developers, while the rest are\nawaiting response. These results demonstrate the effectiveness of our approach\nand underscore the promise of agentic, LLM-driven data synthesis to advance\nsoftware engineering.",
    "published": "2025-07-20T13:41:02Z",
    "updated": "2025-07-20T13:41:02Z",
    "id": "2507.15892v1",
    "authors": [
      "Elijah Nnorom",
      "Md Basim Uddin Ahmed",
      "Jiho Shin",
      "Hung Viet Pham",
      "Song Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15892v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15892v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15892v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) in an agentic framework to test static analyzers, which involves the application of LLMs in a specialized, multi-agent system for software engineering tasks.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.14928v1": {
    "title": "Byzantine-Robust Decentralized Coordination of LLM Agents",
    "summary": "Collaboration among multiple large language model (LLM) agents is a promising\napproach to overcome inherent limitations of single-agent systems, such as\nhallucinations and single points of failure. As LLM agents are increasingly\ndeployed on open blockchain platforms, multi-agent systems capable of\ntolerating malicious (Byzantine) agents have become essential.\n  Recent Byzantine-robust multi-agent systems typically rely on leader-driven\ncoordination, which suffers from two major drawbacks. First, they are\ninherently vulnerable to targeted attacks against the leader. If consecutive\nleaders behave maliciously, the system repeatedly fails to achieve consensus,\nforcing new consensus rounds, which is particularly costly given the high\nlatency of LLM invocations. Second, an underperforming proposal from the leader\ncan be accepted as the final answer even when higher-quality alternatives are\navailable, as existing methods finalize the leader's proposal once it receives\na quorum of votes.\n  To address these issues, we propose DecentLLMs, a novel decentralized\nconsensus approach for multi-agent LLM systems, where worker agents generate\nanswers concurrently and evaluator agents independently score and rank these\nanswers to select the best available one. This decentralized architecture\nenables faster consensus despite the presence of Byzantine agents and\nconsistently selects higher-quality answers through Byzantine-robust\naggregation techniques.\n  Experimental results demonstrate that DecentLLMs effectively tolerates\nByzantine agents and significantly improves the quality of selected answers.",
    "published": "2025-07-20T11:55:26Z",
    "updated": "2025-07-20T11:55:26Z",
    "id": "2507.14928v1",
    "authors": [
      "Yongrae Jo",
      "Chanik Park"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14928v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14928v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14928v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the collaboration of multiple LLM agents to overcome limitations of single-agent systems, focusing on decentralized coordination and robustness against malicious agents. This aligns with topics related to LLM agents and their coordination, as well as the broader context of Reinforcement Learning in multi-agent systems.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.14912v1": {
    "title": "Redefining Elderly Care with Agentic AI: Challenges and Opportunities",
    "summary": "The global ageing population necessitates new and emerging strategies for\ncaring for older adults. In this article, we explore the potential for\ntransformation in elderly care through Agentic Artificial Intelligence (AI),\npowered by Large Language Models (LLMs). We discuss the proactive and\nautonomous decision-making facilitated by Agentic AI in elderly care.\nPersonalized tracking of health, cognitive care, and environmental management,\nall aimed at enhancing independence and high-level living for older adults,\nrepresents important areas of application. With a potential for significant\ntransformation of elderly care, Agentic AI also raises profound concerns about\ndata privacy and security, decision independence, and access. We share key\ninsights to emphasize the need for ethical safeguards, privacy protections, and\ntransparent decision-making. Our goal in this article is to provide a balanced\ndiscussion of both the potential and the challenges associated with Agentic AI,\nand to provide insights into its responsible use in elderly care, to bring\nAgentic AI into harmony with the requirements and vulnerabilities specific to\nthe elderly. Finally, we identify the priorities for the academic research\ncommunities, to achieve human-centered advancements and integration of Agentic\nAI in elderly care. To the best of our knowledge, this is no existing study\nthat reviews the role of Agentic AI in elderly care. Hence, we address the\nliterature gap by analyzing the unique capabilities, applications, and\nlimitations of LLM-based Agentic AI in elderly care. We also provide a\ncompanion interactive dashboard at https://hazratali.github.io/agenticai/.",
    "published": "2025-07-20T10:53:01Z",
    "updated": "2025-07-20T10:53:01Z",
    "id": "2507.14912v1",
    "authors": [
      "Ruhul Amin Khalil",
      "Kashif Ahmad",
      "Hazrat Ali"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14912v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14912v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14912v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in the context of Agentic AI for elderly care, highlighting their potential and challenges. The core focus is on LLMs and their role in autonomous decision-making and personalized care, which aligns with the topics of LLM and RL (Reinforcement Learning with Human Feedback).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.14906v1": {
    "title": "Feedback-Induced Performance Decline in LLM-Based Decision-Making",
    "summary": "The ability of Large Language Models (LLMs) to extract context from natural\nlanguage problem descriptions naturally raises questions about their\nsuitability in autonomous decision-making settings. This paper studies the\nbehaviour of these models within a Markov Decision Process (MDPs). While\ntraditional reinforcement learning (RL) strategies commonly employed in this\nsetting rely on iterative exploration, LLMs, pre-trained on diverse datasets,\noffer the capability to leverage prior knowledge for faster adaptation. We\ninvestigate online structured prompting strategies in sequential decision\nmaking tasks, comparing the zero-shot performance of LLM-based approaches to\nthat of classical RL methods. Our findings reveal that although LLMs\ndemonstrate improved initial performance in simpler environments, they struggle\nwith planning and reasoning in complex scenarios without fine-tuning or\nadditional guidance. Our results show that feedback mechanisms, intended to\nimprove decision-making, often introduce confusion, leading to diminished\nperformance in intricate environments. These insights underscore the need for\nfurther exploration into hybrid strategies, fine-tuning, and advanced memory\nintegration to enhance LLM-based decision-making capabilities.",
    "published": "2025-07-20T10:38:56Z",
    "updated": "2025-07-20T10:38:56Z",
    "id": "2507.14906v1",
    "authors": [
      "Xiao Yang",
      "Juxi Leitner",
      "Michael Burke"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14906v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14906v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14906v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the performance of Large Language Models (LLMs) in decision-making tasks, comparing them to traditional reinforcement learning (RL) methods. It highlights issues with feedback mechanisms and the need for hybrid strategies, fine-tuning, and memory integration, which are relevant to LLM research and reinforcement learning.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Memory"
    ]
  },
  "2507.14887v1": {
    "title": "MEKiT: Multi-source Heterogeneous Knowledge Injection Method via\n  Instruction Tuning for Emotion-Cause Pair Extraction",
    "summary": "Although large language models (LLMs) excel in text comprehension and\ngeneration, their performance on the Emotion-Cause Pair Extraction (ECPE) task,\nwhich requires reasoning ability, is often underperform smaller language model.\nThe main reason is the lack of auxiliary knowledge, which limits LLMs' ability\nto effectively perceive emotions and reason causes. To address this issue, we\npropose a novel \\textbf{M}ulti-source h\\textbf{E}terogeneous \\textbf{K}nowledge\n\\textbf{i}njection me\\textbf{T}hod, MEKiT, which integrates heterogeneous\ninternal emotional knowledge and external causal knowledge. Specifically, for\nthese two distinct aspects and structures of knowledge, we apply the approaches\nof incorporating instruction templates and mixing data for instruction-tuning,\nwhich respectively facilitate LLMs in more comprehensively identifying emotion\nand accurately reasoning causes. Experimental results demonstrate that MEKiT\nprovides a more effective and adaptable solution for the ECPE task, exhibiting\nan absolute performance advantage over compared baselines and dramatically\nimproving the performance of LLMs on the ECPE task.",
    "published": "2025-07-20T10:11:21Z",
    "updated": "2025-07-20T10:11:21Z",
    "id": "2507.14887v1",
    "authors": [
      "Shiyi Mu",
      "Yongkang Liu",
      "Shi Feng",
      "Xiaocui Yang",
      "Daling Wang",
      "Yifei Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14887v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14887v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14887v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the performance of Large Language Models (LLMs) on a specific task (Emotion-Cause Pair Extraction) by integrating heterogeneous knowledge through instruction tuning. It directly involves LLMs and their reasoning abilities, which are key topics in the provided list.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.14871v2": {
    "title": "Tiny language models",
    "summary": "A prominent achievement of natural language processing (NLP) is its ability\nto understand and generate meaningful human language. This capability relies on\ncomplex feedforward transformer block architectures pre-trained on large\nlanguage models (LLMs). However, LLM pre-training is currently feasible only\nfor a few dominant companies due to the immense computational resources\nrequired, limiting broader research participation. This creates a critical need\nfor more accessible alternatives. In this study, we explore whether tiny\nlanguage models (TLMs) exhibit the same key qualitative features of LLMs. We\ndemonstrate that TLMs exhibit a clear performance gap between pre-trained and\nnon-pre-trained models across classification tasks, indicating the\neffectiveness of pre-training, even at a tiny scale. The performance gap\nincreases with the size of the pre-training dataset and with greater overlap\nbetween tokens in the pre-training and classification datasets. Furthermore,\nthe classification accuracy achieved by a pre-trained deep TLM architecture can\nbe replicated through a soft committee of multiple, independently pre-trained\nshallow architectures, enabling low-latency TLMs without affecting\nclassification accuracy. Our results are based on pre-training BERT-6 and\nvariants of BERT-1 on subsets of the Wikipedia dataset and evaluating their\nperformance on FewRel, AGNews, and DBPedia classification tasks. Future\nresearch on TLM is expected to further illuminate the mechanisms underlying\nNLP, especially given that its biologically inspired models suggest that TLMs\nmay be sufficient for children or adolescents to develop language. The data and\ncode that support the findings of this study are openly available on\nhttps://github.com/Rg32601/Tiny-Language-Models .",
    "published": "2025-07-20T08:49:57Z",
    "updated": "2025-07-23T07:43:38Z",
    "id": "2507.14871v2",
    "authors": [
      "Ronit D. Gross",
      "Yarden Tzach",
      "Tal Halevi",
      "Ella Koresh",
      "Ido Kanter"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14871v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14871v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14871v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses tiny language models (TLMs) and their comparison to large language models (LLMs), focusing on pre-training effectiveness and performance. It aligns with the topics of LLM research and pretraining strategies.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.14847v1": {
    "title": "Time-Aware Attention for Enhanced Electronic Health Records Modeling",
    "summary": "Electronic Health Records (EHR) contain valuable clinical information for\npredicting patient outcomes and guiding healthcare decisions. However,\neffectively modeling Electronic Health Records (EHRs) requires addressing data\nheterogeneity and complex temporal patterns. Standard approaches often struggle\nwith irregular time intervals between clinical events. We propose TALE-EHR, a\nTransformer-based framework featuring a novel time-aware attention mechanism\nthat explicitly models continuous temporal gaps to capture fine-grained\nsequence dynamics. To complement this temporal modeling with robust semantics,\nTALE-EHR leverages embeddings derived from standardized code descriptions using\na pre-trained Large Language Model (LLM), providing a strong foundation for\nunderstanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset\ndemonstrate that our approach outperforms state-of-the-art baselines on tasks\nsuch as disease progression forecasting. TALE-EHR underscores the benefit of\nintegrating explicit, continuous temporal modeling with strong semantic\nrepresentations provides a powerful solution for advancing EHR analysis.",
    "published": "2025-07-20T07:32:41Z",
    "updated": "2025-07-20T07:32:41Z",
    "id": "2507.14847v1",
    "authors": [
      "Junhan Yu",
      "Zhunyi Feng",
      "Junwei Lu",
      "Tianxi Cai",
      "Doudou Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14847v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14847v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14847v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a Transformer-based framework with a time-aware attention mechanism for modeling Electronic Health Records (EHRs), and it leverages embeddings from a pre-trained Large Language Model (LLM) for understanding clinical concepts. The core topics are related to LLM for semantic representations and the application in healthcare data modeling.",
    "llm_cls_result": [
      "LLM",
      "Other"
    ]
  },
  "2507.14819v1": {
    "title": "Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents",
    "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ntransforming text descriptions or tables to data visualizations via\ninstruction-tuning methods. However, it is not straightforward to apply these\nmethods directly for a more real-world use case of visualizing data from long\ndocuments based on user-given intents, as opposed to the user pre-selecting the\nrelevant content manually. We introduce the task of intent-based chart\ngeneration from documents: given a user-specified intent and document(s), the\ngoal is to generate a chart adhering to the intent and grounded on the\ndocument(s) in a zero-shot setting. We propose an unsupervised, two-staged\nframework in which an LLM first extracts relevant information from the\ndocument(s) by decomposing the intent and iteratively validates and refines\nthis data. Next, a heuristic-guided module selects an appropriate chart type\nbefore final code generation. To assess the data accuracy of the generated\ncharts, we propose an attribution-based metric that uses a structured textual\nrepresentation of charts, instead of relying on visual decoding metrics that\noften fail to capture the chart data effectively. To validate our approach, we\ncurate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from\ntwo domains, finance and scientific, in contrast to the existing datasets that\nare largely limited to parallel text descriptions/ tables and their\ncorresponding charts. We compare our approach with baselines using single-shot\nchart generation using LLMs and query-based retrieval methods; our method\noutperforms by upto $9$ points and $17$ points in terms of chart data accuracy\nand chart type respectively over the best baselines.",
    "published": "2025-07-20T04:34:59Z",
    "updated": "2025-07-20T04:34:59Z",
    "id": "2507.14819v1",
    "authors": [
      "Akriti Jain",
      "Pritika Ramu",
      "Aparna Garimella",
      "Apoorv Saxena"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14819v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14819v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14819v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for intent-driven chart generation from documents, which involves information extraction and validation by an LLM. The core topics are related to LLMs and their application in a specific task.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.14800v1": {
    "title": "Large Language Model as An Operator: An Experience-Driven Solution for\n  Distribution Network Voltage Control",
    "summary": "With the advanced reasoning and information analysis capabilities, large\nlanguage models (LLMs) can offer a novel approach for the autonomous generation\nof dispatch strategies in power systems. This letter proposes an LLM-based\nexperience-driven voltage control solution for distribution networks, which\nenables the self-evolution of LLM-based voltage control strategies through the\ncollaboration and interaction of multiple modules-specifically, experience\nstorage, experience retrieval, experience generation, and experience\nmodification. Comprehensive experimental results validate the effectiveness of\nthe proposed method and highlight the applicability of LLM in addressing power\nsystem dispatch challenges.",
    "published": "2025-07-20T03:22:08Z",
    "updated": "2025-07-20T03:22:08Z",
    "id": "2507.14800v1",
    "authors": [
      "Xu Yang",
      "Chenhui Lin",
      "Haotian Liu",
      "Qi Wang",
      "Wenchuan Wu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14800v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14800v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14800v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating dispatch strategies in power systems, specifically for voltage control. It highlights the advanced reasoning and information analysis capabilities of LLMs, which are central to the research.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.14785v1": {
    "title": "Exploring the In-Context Learning Capabilities of LLMs for Money\n  Laundering Detection in Financial Graphs",
    "summary": "The complexity and interconnectivity of entities involved in money laundering\ndemand investigative reasoning over graph-structured data. This paper explores\nthe use of large language models (LLMs) as reasoning engines over localized\nsubgraphs extracted from a financial knowledge graph. We propose a lightweight\npipeline that retrieves k-hop neighborhoods around entities of interest,\nserializes them into structured text, and prompts an LLM via few-shot\nin-context learning to assess suspiciousness and generate justifications. Using\nsynthetic anti-money laundering (AML) scenarios that reflect common laundering\nbehaviors, we show that LLMs can emulate analyst-style logic, highlight red\nflags, and provide coherent explanations. While this study is exploratory, it\nillustrates the potential of LLM-based graph reasoning in AML and lays\ngroundwork for explainable, language-driven financial crime analytics.",
    "published": "2025-07-20T02:00:21Z",
    "updated": "2025-07-20T02:00:21Z",
    "id": "2507.14785v1",
    "authors": [
      "Erfan Pirmorad"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14785v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14785v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14785v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using large language models (LLMs) for reasoning over graph-structured data in the context of money laundering detection, which involves in-context learning and reasoning capabilities of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.14783v2": {
    "title": "Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task\n  RL with Hybrid Rewards",
    "summary": "The advancement of general-purpose artificial intelligence relies on large\nlanguage models (LLMs) that excel across a wide range of tasks, from structured\nreasoning to creative generation. However, post-training methods like\nSupervised Fine-Tuning (SFT) often struggle with generalization, favoring\nmemorization over transferable learning. In this work, we introduce\nOmni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM\nperformance across diverse tasks by combining rule-based verifiable rewards\nwith generative preference signals via LLM-as-a-Judge evaluations. Our approach\nenables consistent optimization across task types and scales RL-based training\nto subjective domains. We further investigate training strategies,\ndemonstrating that a curriculum-based progression that orders tasks from\nstructured to open-ended improves performance and reduces forgetting.\nExperimental results across four domains reveal that curriculum learning\nimproves performance by 5.2% over joint training and 9.1% over model merging.\nThese results highlight the importance of task-aware sampling and hybrid\nsupervision in scaling RL-based post-training for general-purpose LLMs.",
    "published": "2025-07-20T01:50:16Z",
    "updated": "2025-07-24T16:25:54Z",
    "id": "2507.14783v2",
    "authors": [
      "Derek Li",
      "Jiaming Zhou",
      "Amirreza Kazemi",
      "Qianyi Sun",
      "Abbas Ghaddar",
      "Mohammad Ali Alomrani",
      "Liheng Ma",
      "Yu Luo",
      "Dong Li",
      "Feng Wen",
      "Jianye Hao",
      "Mark Coates",
      "Yingxue Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14783v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14783v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14783v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of reinforcement learning (RL) to enhance the performance of large language models (LLMs) across diverse tasks, which aligns with the RL topic. It also focuses on scaling RL-based training for general-purpose LLMs, which is relevant to the Scaling topic. Additionally, the paper mentions the importance of task-aware sampling and hybrid supervision, which can be associated with the Reasoning topic due to the emphasis on improving LLM performance across various tasks.",
    "llm_cls_result": [
      "RL",
      "Scaling",
      "Reasoning"
    ]
  },
  "2507.14777v1": {
    "title": "Rethinking Memorization Measures and their Implications in Large\n  Language Models",
    "summary": "Concerned with privacy threats, memorization in LLMs is often seen as\nundesirable, specifically for learning. In this paper, we study whether\nmemorization can be avoided when optimally learning a language, and whether the\nprivacy threat posed by memorization is exaggerated or not. To this end, we\nre-examine existing privacy-focused measures of memorization, namely\nrecollection-based and counterfactual memorization, along with a newly proposed\ncontextual memorization.\n  Relating memorization to local over-fitting during learning, contextual\nmemorization aims to disentangle memorization from the contextual learning\nability of LLMs. Informally, a string is contextually memorized if its\nrecollection due to training exceeds the optimal contextual recollection, a\nlearned threshold denoting the best contextual learning without training.\nConceptually, contextual recollection avoids the fallacy of recollection-based\nmemorization, where any form of high recollection is a sign of memorization.\nTheoretically, contextual memorization relates to counterfactual memorization,\nbut imposes stronger conditions. Memorization measures differ in outcomes and\ninformation requirements.\n  Experimenting on 18 LLMs from 6 families and multiple formal languages of\ndifferent entropy, we show that (a) memorization measures disagree on\nmemorization order of varying frequent strings, (b) optimal learning of a\nlanguage cannot avoid partial memorization of training strings, and (c)\nimproved learning decreases contextual and counterfactual memorization but\nincreases recollection-based memorization. Finally, (d) we revisit existing\nreports of memorized strings by recollection that neither pose a privacy threat\nnor are contextually or counterfactually memorized.",
    "published": "2025-07-20T00:33:19Z",
    "updated": "2025-07-20T00:33:19Z",
    "id": "2507.14777v1",
    "authors": [
      "Bishwamittra Ghosh",
      "Soumi Das",
      "Qinyuan Wu",
      "Mohammad Aflah Khan",
      "Krishna P. Gummadi",
      "Evimaria Terzi",
      "Deepak Garg"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14777v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14777v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14777v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on memorization in Large Language Models (LLMs), discussing different measures of memorization and their implications for privacy and learning. It directly relates to the 'Memory' topic, which covers memory-augmented models and retrieval-based methods. Additionally, it touches on 'LLM' as it involves the study of memorization within the context of LLMs.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.14776v1": {
    "title": "VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs",
    "summary": "The rapid adoption of large language models(LLMs) in hardware design has\nprimarily focused on generating functionally correct Verilog code, overlooking\ncritical Power Performance-Area(PPA) metrics essential for industrial-grade\ndesigns. To bridge this gap, we propose VeriOpt, a novel framework that\nleverages role-based prompting and PPA-aware optimization to enable LLMs to\nproduce high-quality, synthesizable Verilog. VeriOpt structures LLM\ninteractions into specialized roles (e.g., Planner, Programmer, Reviewer,\nEvaluator) to emulate human design workflows, while integrating PPA constraints\ndirectly into the prompting pipeline. By combining multi-modal feedback (e.g.,\nsynthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves\nPPA-efficient code generation without sacrificing functional correctness.\nExperimental results demonstrate up to 88% reduction in power, 76% reduction in\narea and 73% improvement in timing closure compared to baseline LLM-generated\nRTL, validated using industry standard EDA tools. At the same time achieves 86%\nsuccess rate in functionality evaluation. Our work advances the\nstate-of-the-art AI-driven hardware design by addressing the critical gap\nbetween correctness and quality, paving the way for reliable LLM adoption in\nproduction workflows.",
    "published": "2025-07-20T00:28:55Z",
    "updated": "2025-07-20T00:28:55Z",
    "id": "2507.14776v1",
    "authors": [
      "Kimia Tasnia",
      "Alexander Garcia",
      "Tasnuva Farheen",
      "Sazadur Rahman"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14776v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14776v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14776v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging LLMs for hardware design, specifically Verilog code generation with PPA metrics, which involves specialized prompting and optimization techniques. The core topics are related to LLM applications and their role in specific tasks, but none of the provided topics directly match the focus on hardware design and PPA metrics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14769v1": {
    "title": "Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs",
    "summary": "Modern web interfaces are unnecessarily complex to use as they overwhelm\nusers with excessive text and visuals unrelated to their current goals. This\nproblem particularly impacts screen reader users (SRUs), who navigate content\nsequentially and may spend minutes traversing irrelevant elements before\nreaching desired information compared to vision users (VUs) who visually skim\nin seconds. We present Task Mode, a system that dynamically filters web content\nbased on user-specified goals using large language models to identify and\nprioritize relevant elements while minimizing distractions. Our approach\npreserves page structure while offering multiple viewing modes tailored to\ndifferent access needs. Our user study with 12 participants (6 VUs, 6 SRUs)\ndemonstrates that our approach reduced task completion time for SRUs while\nmaintaining performance for VUs, decreasing the completion time gap between\ngroups from 2x to 1.2x. 11 of 12 participants wanted to use Task Mode in the\nfuture, reporting that Task Mode supported completing tasks with less effort\nand fewer distractions. This work demonstrates how designing new interactions\nsimultaneously for visual and non-visual access can reduce rather than\nreinforce accessibility disparities in future technology created by\nhuman-computer interaction researchers and practitioners.",
    "published": "2025-07-19T23:41:08Z",
    "updated": "2025-07-19T23:41:08Z",
    "id": "2507.14769v1",
    "authors": [
      "Ananya Gubbi Mohanbabu",
      "Yotam Sechayk",
      "Amy Pavel"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14769v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14769v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14769v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to dynamically filter web content based on user-specified goals, which aligns with the LLM topic. It also involves human-computer interaction and accessibility, but these are not directly covered in the provided topic list.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.14735v1": {
    "title": "Investigating the Role of LLMs Hyperparameter Tuning and Prompt\n  Engineering to Support Domain Modeling",
    "summary": "The introduction of large language models (LLMs) has enhanced automation in\nsoftware engineering tasks, including in Model Driven Engineering (MDE).\nHowever, using general-purpose LLMs for domain modeling has its limitations.\nOne approach is to adopt fine-tuned models, but this requires significant\ncomputational resources and can lead to issues like catastrophic forgetting.\n  This paper explores how hyperparameter tuning and prompt engineering can\nimprove the accuracy of the Llama 3.1 model for generating domain models from\ntextual descriptions. We use search-based methods to tune hyperparameters for a\nspecific medical data model, resulting in a notable quality improvement over\nthe baseline LLM. We then test the optimized hyperparameters across ten diverse\napplication domains.\n  While the solutions were not universally applicable, we demonstrate that\ncombining hyperparameter tuning with prompt engineering can enhance results\nacross nearly all examined domain models.",
    "published": "2025-07-19T19:49:58Z",
    "updated": "2025-07-19T19:49:58Z",
    "id": "2507.14735v1",
    "authors": [
      "Vladyslav Bulhakov",
      "Giordano d'Aloisio",
      "Claudio Di Sipio",
      "Antinisca Di Marco",
      "Davide Di Ruscio"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14735v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14735v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14735v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the performance of a large language model (LLM) through hyperparameter tuning and prompt engineering, specifically for domain modeling tasks. It discusses the use of LLMs in software engineering and the challenges of fine-tuning, which aligns with the 'LLM' and 'Scaling' topics.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.14730v1": {
    "title": "Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI",
    "summary": "Generative AI, large language models, and agentic AI have emerged separately\nof urban planning. However, the convergence between AI and urban planning\npresents an interesting opportunity towards AI urban planners. This paper\nconceptualizes urban planning as a generative AI task, where AI synthesizes\nland-use configurations under geospatial, social, and human-centric\nconstraints. We survey how generative AI approaches, including VAEs, GANs,\ntransformers, and diffusion models, reshape urban design. We further identify\ncritical gaps: 1) limited research on integrating urban theory guidance, 2)\nlimited research of AI urban planning over multiple spatial resolutions or\nangularities, 3) limited research on augmenting urban design knowledge from\ndata, and 4) limited research on addressing real-world interactions. To address\nthese limitations, we outline future research directions in theory-guided\ngeneration, digital twins, and human-machine co-design, calling for a new\nsynthesis of generative intelligence and participatory urbanism.",
    "published": "2025-07-19T19:40:42Z",
    "updated": "2025-07-19T19:40:42Z",
    "id": "2507.14730v1",
    "authors": [
      "Yanjie Fu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14730v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14730v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14730v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) and generative AI in urban planning, which aligns with the topics of LLM and AGI due to the focus on AI's role in generative tasks and potential general intelligence applications in urban planning.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.14722v1": {
    "title": "LeanTree: Accelerating White-Box Proof Search with Factorized States in\n  Lean 4",
    "summary": "Automated theorem proving (ATP) has been a classical problem in artificial\nintelligence since its inception, yet it remains challenging due to its vast\nstate and action space. Large language models (LLMs) have recently emerged as a\npromising heuristic for ATP, but they lack correctness guarantees and thus\nrequire interaction with a proof verifier. Such interactions typically follow\none of two approaches: black-box interaction, which does not utilize\nintermediate proof states, or white-box approaches, which allow for incremental\nproof construction and examination of intermediate states. While black-box\napproaches have directly benefited from recent LLM advances, white-box methods\nhave comparatively lagged behind. In this paper, we address this gap by\nintroducing LeanTree, which consists of (i) a tool built in the Lean 4 language\nthat factorizes complex proof states into simpler, independent branches, and\n(ii) a dataset of these factorized intermediate states. Our white-box tooling\noffers several advantages over black-box approaches: it simplifies evaluation,\nreduces necessary context, generates richer training data, enables parallel\nsearch across multiple states, supports efficient reuse of states, and provides\nfeedback in case of errors. Our preliminary results hint that white-box\napproaches outperform black-box alternatives in some settings.",
    "published": "2025-07-19T18:50:07Z",
    "updated": "2025-07-19T18:50:07Z",
    "id": "2507.14722v1",
    "authors": [
      "Matj Kripner",
      "Michal ustr",
      "Milan Straka"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14722v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14722v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14722v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in automated theorem proving (ATP) and introduces a tool and dataset for white-box proof search. The focus on LLMs and their application in ATP aligns with the 'LLM' topic, while the development of a dataset for evaluation purposes relates to the 'Dataset' topic.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.14719v1": {
    "title": "Automated Safety Evaluations Across 20 Large Language Models: The Aymara\n  LLM Risk and Responsibility Matrix",
    "summary": "As large language models (LLMs) become increasingly integrated into\nreal-world applications, scalable and rigorous safety evaluation is essential.\nThis paper introduces Aymara AI, a programmatic platform for generating and\nadministering customized, policy-grounded safety evaluations. Aymara AI\ntransforms natural-language safety policies into adversarial prompts and scores\nmodel responses using an AI-based rater validated against human judgments. We\ndemonstrate its capabilities through the Aymara LLM Risk and Responsibility\nMatrix, which evaluates 20 commercially available LLMs across 10 real-world\nsafety domains. Results reveal wide performance disparities, with mean safety\nscores ranging from 86.2% to 52.4%. While models performed well in\nwell-established safety domains such as Misinformation (mean = 95.7%), they\nconsistently failed in more complex or underspecified domains, notably Privacy\n& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety\nscores differed significantly across both models and domains (p < .05). These\nfindings underscore the inconsistent and context-dependent nature of LLM safety\nand highlight the need for scalable, customizable tools like Aymara AI to\nsupport responsible AI development and oversight.",
    "published": "2025-07-19T18:49:16Z",
    "updated": "2025-07-19T18:49:16Z",
    "id": "2507.14719v1",
    "authors": [
      "Juan Manuel Contreras"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14719v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14719v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14719v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the safety of large language models (LLMs) through a programmatic platform, which aligns with the topic of benchmarking LLMs and their performance in various safety domains.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.14715v1": {
    "title": "Exploring the Dynamic Scheduling Space of Real-Time Generative AI\n  Applications on Emerging Heterogeneous Systems",
    "summary": "The integration of generative AI models, particularly large language models\n(LLMs), into real-time multi-model AI applications such as video conferencing\nand gaming is giving rise to a new class of workloads: real-time generative AI\n(RTGen). These workloads combine the compute intensity and dynamic execution\npatterns of generative models with the stringent latency and concurrency\nconstraints of real-time inference. To meet the diverse demands of RTGen\nworkloads, modern edge platforms increasingly adopt heterogeneous\nsystem-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite\nthe potential of heterogeneous SoC, the scheduling space complexity and\nperformance implications of RTGen workloads on such platforms remain\nunderexplored. In this work, we perform a comprehensive characterization of\nRTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct\nrealistic multi-model scenarios inspired by industry use cases and profile\nmodel performance across all available backends. Using this data, we evaluate\nfive scheduling policies and their impact on both real-time metrics (e.g.,\ndeadline violation rate) and LLM performance (e.g., time-to-first-token and\ntokens-per-second). Our results show that scheduling decisions significantly\naffect workload performance (e.g., leading to a 41.7% difference in deadline\nviolation rates on average), and highlight the need for scheduling strategies\nthat are aware of workload dynamics and hardware heterogeneity. Our findings\nunderscore the importance of workload-aware, dynamic heterogeneous scheduling\nin enabling high-performance, on-device RTGen applications.",
    "published": "2025-07-19T18:24:11Z",
    "updated": "2025-07-19T18:24:11Z",
    "id": "2507.14715v1",
    "authors": [
      "Rachid Karami",
      "Rajeev Patwari",
      "Hyoukjun Kwon",
      "Ashish Sirasao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14715v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14715v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14715v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) into real-time applications and the challenges of scheduling these workloads on heterogeneous systems. The focus is on the performance and scheduling of LLMs, which aligns with the 'LLM' and 'Scaling' topics.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.16841v1": {
    "title": "AquaChat: An LLM-Guided ROV Framework for Adaptive Inspection of\n  Aquaculture Net Pens",
    "summary": "Inspection of aquaculture net pens is essential for maintaining the\nstructural integrity, biosecurity, and operational efficiency of fish farming\nsystems. Traditional inspection approaches rely on pre-programmed missions or\nmanual control, offering limited adaptability to dynamic underwater conditions\nand user-specific demands. In this study, we propose AquaChat, a novel Remotely\nOperated Vehicle (ROV) framework that integrates Large Language Models (LLMs)\nfor intelligent and adaptive net pen inspection. The system features a\nmulti-layered architecture: (1) a high-level planning layer that interprets\nnatural language user commands using an LLM to generate symbolic task plans;\n(2) a mid-level task manager that translates plans into ROV control sequences;\nand (3) a low-level motion control layer that executes navigation and\ninspection tasks with precision. Real-time feedback and event-triggered\nreplanning enhance robustness in challenging aquaculture environments. The\nframework is validated through experiments in both simulated and controlled\naquatic environments representative of aquaculture net pens. Results\ndemonstrate improved task flexibility, inspection accuracy, and operational\nefficiency. AquaChat illustrates the potential of integrating language-based AI\nwith marine robotics to enable intelligent, user-interactive inspection systems\nfor sustainable aquaculture operations.",
    "published": "2025-07-19T17:39:12Z",
    "updated": "2025-07-19T17:39:12Z",
    "id": "2507.16841v1",
    "authors": [
      "Waseem Akram",
      "Muhayy Ud Din",
      "Abdelhaleem Saad",
      "Irfan Hussain"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16841v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16841v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16841v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) into a Remotely Operated Vehicle (ROV) framework for adaptive inspection, which directly relates to the use of LLMs in practical applications and their interaction with robotics.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.14688v1": {
    "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their\n  Limitations",
    "summary": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., persona and system prompts); (3)\nAlignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic LLMs and applications while providing\nconcrete recommendations for future efforts in post-training dataset\ndevelopment.",
    "published": "2025-07-19T16:30:45Z",
    "updated": "2025-07-19T16:30:45Z",
    "id": "2507.14688v1",
    "authors": [
      "Mohammed Alkhowaiter",
      "Norah Alshahrani",
      "Saied Alshahrani",
      "Reem I. Masoud",
      "Alaa Alzahrani",
      "Deema Alnuhait",
      "Emad A. Alghamdi",
      "Khalid Almubarak"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14688v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14688v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14688v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on reviewing Arabic post-training datasets for Large Language Models (LLMs), discussing their quality, diversity, and limitations. It directly relates to LLM research and dataset development.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.14686v1": {
    "title": "From Semantics, Scene to Instance-awareness: Distilling Foundation Model\n  for Open-vocabulary Situation Recognition",
    "summary": "Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot\nabilities but struggle with complex Grounded Situation Recognition (GSR) and\nare resource-intensive for edge device deployment. Meanwhile, conventional GSR\nmodels often lack generalization ability, falling short in recognizing unseen\nand rare situations. In this paper, we exploit transferring knowledge from a\nteacher MLLM to a small GSR model to enhance its generalization and zero-shot\nabilities, thereby introducing the task of Open-vocabulary Grounded Situation\nRecognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt\nDistillation (MIPD), a novel framework that distills enriched multimodal\nknowledge from the foundation model, enabling the student Ov-GSR model to\nrecognize unseen situations and be better aware of rare situations.\nSpecifically, the MIPD framework first leverages the LLM-based Judgmental\nRationales Generator (JRG) to construct positive and negative glimpse and gaze\nrationales enriched with contextual semantic information. The proposed\nscene-aware and instance-perception prompts are then introduced to align\nrationales with visual information from the MLLM teacher via the\nNegative-Guided Multimodal Prompting Alignment (NMPA) module, effectively\ncapturing holistic and perceptual multimodal knowledge. Finally, the aligned\nmultimodal knowledge is distilled into the student Ov-GSR model, providing a\nstronger foundation for generalization that enhances situation understanding,\nbridges the gap between seen and unseen scenarios, and mitigates prediction\nbias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving\nsuperior performance on seen, rare, and unseen situations, and further\ndemonstrate improved unseen detection on the HICO-DET dataset.",
    "published": "2025-07-19T16:29:02Z",
    "updated": "2025-07-19T16:29:02Z",
    "id": "2507.14686v1",
    "authors": [
      "Chen Cai",
      "Tianyi Liu",
      "Jianjun Gao",
      "Wenyang Liu",
      "Kejun Wu",
      "Ruoyu Wang",
      "Yi Wang",
      "Soo Chin Liew"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14686v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14686v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14686v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on transferring knowledge from a Multimodal Large Language Model (MLLM) to a smaller model for Open-vocabulary Grounded Situation Recognition (Ov-GSR), which involves multimodal knowledge distillation and alignment. The core topics are MLLM for its use of multimodal knowledge, and Reasoning for the logical and contextual understanding required in situation recognition.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.14683v1": {
    "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via\n  Context-Aware Multi-Stage Policy Optimization",
    "summary": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement.",
    "published": "2025-07-19T16:21:23Z",
    "updated": "2025-07-19T16:21:23Z",
    "id": "2507.14683v1",
    "authors": [
      "Xingxuan Li",
      "Yao Xiao",
      "Dianwen Ng",
      "Hai Ye",
      "Yue Deng",
      "Xiang Lin",
      "Bin Wang",
      "Zhanfeng Mo",
      "Chong Zhang",
      "Yueyi Zhang",
      "Zonglin Yang",
      "Ruilin Li",
      "Lei Lei",
      "Shihao Xu",
      "Han Zhao",
      "Weiling Chen",
      "Feng Ji",
      "Lidong Bing"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14683v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14683v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14683v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on mathematical reasoning using large language models, which involves reasoning abilities and the use of reinforcement learning for optimization. It also emphasizes the importance of datasets and benchmarks for reproducibility.",
    "llm_cls_result": [
      "Reasoning",
      "RL",
      "Dataset"
    ]
  },
  "2507.14675v1": {
    "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding",
    "summary": "Despite significant progress in multimodal large language models (MLLMs),\ntheir performance on complex, multi-page document comprehension remains\ninadequate, largely due to the lack of high-quality, document-level datasets.\nWhile current retrieval-augmented generation (RAG) methods offer partial\nsolutions, they suffer from issues, such as fragmented retrieval contexts,\nmulti-stage error accumulation, and extra time costs of retrieval. In this\nwork, we present a high-quality document-level dataset, Doc-750K, designed to\nsupport in-depth understanding of multimodal documents. This dataset includes\ndiverse document structures, extensive cross-page dependencies, and real\nquestion-answer pairs derived from the original documents. Building on the\ndataset, we develop a native multimodal model, Docopilot, which can accurately\nhandle document-level dependencies without relying on RAG. Experiments\ndemonstrate that Docopilot achieves superior coherence, accuracy, and\nefficiency in document understanding tasks and multi-turn interactions, setting\na new baseline for document-level multimodal understanding. Data, code, and\nmodels are released at https://github.com/OpenGVLab/Docopilot",
    "published": "2025-07-19T16:03:34Z",
    "updated": "2025-07-19T16:03:34Z",
    "id": "2507.14675v1",
    "authors": [
      "Yuchen Duan",
      "Zhe Chen",
      "Yusong Hu",
      "Weiyun Wang",
      "Shenglong Ye",
      "Botian Shi",
      "Lewei Lu",
      "Qibin Hou",
      "Tong Lu",
      "Hongsheng Li",
      "Jifeng Dai",
      "Wenhai Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14675v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14675v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14675v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses a multimodal large language model (MLLM) focused on document-level understanding, introduces a new dataset (Doc-750K), and evaluates the model's performance, which aligns with the topics of Multimodal Large Language Models (MLLM) and Dataset.",
    "llm_cls_result": [
      "MLLM",
      "Dataset"
    ]
  },
  "2507.14633v1": {
    "title": "Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial\n  Networks: A Survey on Generative Approaches",
    "summary": "The development of satellite-augmented low-altitude economy and terrestrial\nnetworks (SLAETNs) demands intelligent and autonomous systems that can operate\nreliably across heterogeneous, dynamic, and mission-critical environments. To\naddress these challenges, this survey focuses on enabling agentic artificial\nintelligence (AI), that is, artificial agents capable of perceiving, reasoning,\nand acting, through generative AI (GAI) and large language models (LLMs). We\nbegin by introducing the architecture and characteristics of SLAETNs, and\nanalyzing the challenges that arise in integrating satellite, aerial, and\nterrestrial components. Then, we present a model-driven foundation by\nsystematically reviewing five major categories of generative models:\nvariational autoencoders (VAEs), generative adversarial networks (GANs),\ngenerative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.\nMoreover, we provide a comparative analysis to highlight their generative\nmechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on\nthis foundation, we examine how these models empower agentic functions across\nthree domains: communication enhancement, security and privacy protection, and\nintelligent satellite tasks. Finally, we outline key future directions for\nbuilding scalable, adaptive, and trustworthy generative agents in SLAETNs. This\nsurvey aims to provide a unified understanding and actionable reference for\nadvancing agentic AI in next-generation integrated networks.",
    "published": "2025-07-19T14:07:05Z",
    "updated": "2025-07-19T14:07:05Z",
    "id": "2507.14633v1",
    "authors": [
      "Xiaozheng Gao",
      "Yichen Wang",
      "Bosen Liu",
      "Xiao Zhou",
      "Ruichen Zhang",
      "Jiacheng Wang",
      "Dusit Niyato",
      "Dong In Kim",
      "Abbas Jamalipour",
      "Chau Yuen",
      "Jianping An",
      "Kai Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14633v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14633v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14633v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) and generative AI (GAI) into agentic AI systems for satellite-augmented networks, which aligns with the topics of LLM and RL (as it involves agentic functions and reasoning). The survey also covers generative models, which are foundational to LLMs.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.14623v1": {
    "title": "Rejection or Inclusion in the Emotion-Identity Dynamics of TikTok\n  Refugees on RedNote",
    "summary": "This study examines cross-cultural interactions between Chinese users and\nself-identified \"TikTok Refugees\"(foreign users who migrated to RedNote after\nTikTok's U.S. ban). Based on a dataset of 1,862 posts and 403,054 comments, we\nuse large language model-based sentiment classification and BERT-based topic\nmodelling to explore how both groups engage with the TikTok refugee phenomenon.\nWe analyse what themes foreign users express, how Chinese users respond, how\nstances (Pro-China, Neutral, Pro-Foreign) shape emotional expression, and how\naffective responses differ across topics and identities. Results show strong\naffective asymmetry: Chinese users respond with varying emotional intensities\nacross topics and stances: pride and praise dominate cultural threads, while\npolitical discussions elicit high levels of contempt and anger, especially from\nPro-China commenters. Pro-Foreign users exhibit the strongest negative emotions\nacross all topics, whereas neutral users express curiosity and joy but still\nreinforce mainstream discursive norms. Cross-topic comparisons reveal that\nappearance-related content produces the most emotionally balanced interactions,\nwhile politics generates the highest polarization. Our findings reveal distinct\nemotion-stance structures in Sino-foreign online interactions and offer\nempirical insights into identity negotiation in transnational digital publics.",
    "published": "2025-07-19T13:38:33Z",
    "updated": "2025-07-19T13:38:33Z",
    "id": "2507.14623v1",
    "authors": [
      "Mingchen Li",
      "Wenbo Xu",
      "Wenqing Gu",
      "Yixuan Xie",
      "Yao Zhou",
      "Yunsong Dai",
      "Cheng Tan",
      "Pan Hui"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14623v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14623v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14623v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on cross-cultural interactions and sentiment analysis using large language models and BERT-based topic modeling, but it does not directly align with the provided topics related to LLM research, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14619v1": {
    "title": "Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard\n  Negative Mining",
    "summary": "Large Language Models (LLMs) face significant challenges in specialized\ndomains like law, where precision and domain-specific knowledge are critical.\nThis paper presents a streamlined two-stage framework consisting of Retrieval\nand Re-ranking to enhance legal document retrieval efficiency and accuracy. Our\napproach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,\nfollowed by a Cross-Encoder for precise re-ranking, both optimized through\nstrategic negative example mining. Key innovations include the introduction of\nthe Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard\nnegatives to mitigate training bias, which significantly improved re-ranking\nperformance. Evaluated on the SoICT Hackathon 2024 for Legal Document\nRetrieval, our team, 4Huiter, achieved a top-three position. While\ntop-performing teams employed ensemble models and iterative self-training on\nlarge bge-m3 architectures, our lightweight, single-pass approach offered a\ncompetitive alternative with far fewer parameters. The framework demonstrates\nthat optimized data processing, tailored loss functions, and balanced negative\nsampling are pivotal for building robust retrieval-augmented systems in legal\ncontexts.",
    "published": "2025-07-19T13:30:14Z",
    "updated": "2025-07-19T13:30:14Z",
    "id": "2507.14619v1",
    "authors": [
      "Van-Hoang Le",
      "Duc-Vu Nguyen",
      "Kiet Van Nguyen",
      "Ngan Luu-Thuy Nguyen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14619v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14619v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14619v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the context of legal document retrieval, focusing on retrieval-augmented systems and optimization techniques. However, the primary focus is on retrieval and re-ranking methods rather than core LLM research topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14594v1": {
    "title": "A first look at License Variants in the PyPI Ecosystem",
    "summary": "Open-source licenses establish the legal foundation for software reuse, yet\nlicense variants, including both modified standard licenses and custom-created\nalternatives, introduce significant compliance complexities. Despite their\nprevalence and potential impact, these variants are poorly understood in modern\nsoftware systems, and existing tools do not account for their existence,\nleading to significant challenges in both effectiveness and efficiency of\nlicense analysis. To fill this knowledge gap, we conduct a comprehensive\nempirical study of license variants in the PyPI ecosystem. Our findings show\nthat textual variations in licenses are common, yet only 2% involve substantive\nmodifications. However, these license variants lead to significant compliance\nissues, with 10.7% of their downstream dependencies found to be\nlicense-incompatible.\n  Inspired by our findings, we introduce LV-Parser, a novel approach for\nefficient license variant analysis leveraging diff-based techniques and large\nlanguage models, along with LV-Compat, an automated pipeline for detecting\nlicense incompatibilities in software dependency networks. Our evaluation\ndemonstrates that LV-Parser achieves an accuracy of 0.936 while reducing\ncomputational costs by 30%, and LV-Compat identifies 5.2 times more\nincompatible packages than existing methods with a precision of 0.98.\n  This work not only provides the first empirical study into license variants\nin software packaging ecosystem but also equips developers and organizations\nwith practical tools for navigating the complex landscape of open-source\nlicensing.",
    "published": "2025-07-19T12:41:33Z",
    "updated": "2025-07-19T12:41:33Z",
    "id": "2507.14594v1",
    "authors": [
      "Weiwei Xu",
      "Hengzhi Ye",
      "Kai Gao",
      "Minghui Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14594v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14594v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14594v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on open-source licenses and their variants in the PyPI ecosystem, which is not directly related to any of the provided topics related to Large Language Models, Reinforcement Learning, or other specified categories.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14593v1": {
    "title": "Coordinate Heart System: A Geometric Framework for Emotion\n  Representation",
    "summary": "This paper presents the Coordinate Heart System (CHS), a geometric framework\nfor emotion representation in artificial intelligence applications. We position\neight core emotions as coordinates on a unit circle, enabling mathematical\ncomputation of complex emotional states through coordinate mixing and vector\noperations. Our initial five-emotion model revealed significant coverage gaps\nin the emotion space, leading to the development of an eight-emotion system\nthat provides complete geometric coverage with mathematical guarantees. The\nframework converts natural language input to emotion coordinates and supports\nreal-time emotion interpolation through computational algorithms. The system\nintroduces a re-calibrated stability parameter S in [0,1], which dynamically\nintegrates emotional load, conflict resolution, and contextual drain factors.\nThis stability model leverages advanced Large Language Model interpretation of\ntextual cues and incorporates hybrid temporal tracking mechanisms to provide\nnuanced assessment of psychological well-being states. Our key contributions\ninclude: (i) mathematical proof demonstrating why five emotions are\ninsufficient for complete geometric coverage, (ii) an eight-coordinate system\nthat eliminates representational blind spots, (iii) novel algorithms for\nemotion mixing, conflict resolution, and distance calculation in emotion space,\nand (iv) a comprehensive computational framework for AI emotion recognition\nwith enhanced multi-dimensional stability modeling. Experimental validation\nthrough case studies demonstrates the system's capability to handle emotionally\nconflicted states, contextual distress factors, and complex psychological\nscenarios that traditional categorical emotion models cannot adequately\nrepresent. This work establishes a new mathematical foundation for emotion\nmodeling in artificial intelligence systems.",
    "published": "2025-07-19T12:38:30Z",
    "updated": "2025-07-19T12:38:30Z",
    "id": "2507.14593v1",
    "authors": [
      "Omar Al-Desi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14593v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14593v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14593v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a geometric framework for emotion representation in AI applications, leveraging Large Language Models for interpretation of textual cues and computational algorithms for emotion interpolation. However, the core focus is on emotion modeling and representation rather than specific LLM architectures or scaling laws.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14590v1": {
    "title": "Backtranslation and paraphrasing in the LLM era? Comparing data\n  augmentation methods for emotion classification",
    "summary": "Numerous domain-specific machine learning tasks struggle with data scarcity\nand class imbalance. This paper systematically explores data augmentation\nmethods for NLP, particularly through large language models like GPT. The\npurpose of this paper is to examine and evaluate whether traditional methods\nsuch as paraphrasing and backtranslation can leverage a new generation of\nmodels to achieve comparable performance to purely generative methods. Methods\naimed at solving the problem of data scarcity and utilizing ChatGPT were\nchosen, as well as an exemplary dataset. We conducted a series of experiments\ncomparing four different approaches to data augmentation in multiple\nexperimental setups. We then evaluated the results both in terms of the quality\nof generated data and its impact on classification performance. The key\nfindings indicate that backtranslation and paraphrasing can yield comparable or\neven better results than zero and a few-shot generation of examples.",
    "published": "2025-07-19T12:23:20Z",
    "updated": "2025-07-19T12:23:20Z",
    "id": "2507.14590v1",
    "authors": [
      "ukasz Radliski",
      "Mateusz Guciora",
      "Jan Koco"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14590v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14590v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14590v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) like GPT for data augmentation in NLP tasks, specifically comparing traditional methods such as paraphrasing and backtranslation with generative methods. The focus is on leveraging LLMs for improving classification performance, which aligns with the 'LLM' and 'Dataset' topics.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.14558v1": {
    "title": "Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library",
    "summary": "The combination of computer vision and artificial intelligence is\nfundamentally transforming a broad spectrum of industries by enabling machines\nto interpret and act upon visual data with high levels of accuracy. As the\nbiggest and by far the most popular open-source computer vision library, OpenCV\nlibrary provides an extensive suite of programming functions supporting\nreal-time computer vision. Bugs in the OpenCV library can affect the downstream\ncomputer vision applications, and it is critical to ensure the reliability of\nthe OpenCV library. This paper introduces VISTAFUZZ, a novel technique for\nharnessing large language models (LLMs) for document-guided fuzzing of the\nOpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain\nstandardized API information. Based on this standardized information, VISTAFUZZ\nextracts constraints on individual input parameters and dependencies between\nthese. Using these constraints and dependencies, VISTAFUZZ then generates new\ninput values to systematically test each target API. We evaluate the\neffectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the\nresults show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been\nconfirmed, and 5 of these have been fixed.",
    "published": "2025-07-19T09:44:01Z",
    "updated": "2025-07-19T09:44:01Z",
    "id": "2507.14558v1",
    "authors": [
      "Bin Duan",
      "Tarek Mahmud",
      "Meiru Che",
      "Yan Yan",
      "Naipeng Dong",
      "Dan Dongseong Kim",
      "Guowei Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14558v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14558v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14558v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for document-guided fuzzing of the OpenCV library, which aligns with the 'LLM' topic. The focus on testing and bug detection in a specific library does not directly fit into other provided categories.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.14555v1": {
    "title": "Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding\n  with Object-Level Text Descriptions",
    "summary": "Understanding 3D scenes goes beyond simply recognizing objects; it requires\nreasoning about the spatial and semantic relationships between them. Current 3D\nscene-language models often struggle with this relational understanding,\nparticularly when visual embeddings alone do not adequately convey the roles\nand interactions of objects. In this paper, we introduce Descrip3D, a novel and\npowerful framework that explicitly encodes the relationships between objects\nusing natural language. Unlike previous methods that rely only on 2D and 3D\nembeddings, Descrip3D enhances each object with a textual description that\ncaptures both its intrinsic attributes and contextual relationships. These\nrelational cues are incorporated into the model through a dual-level\nintegration: embedding fusion and prompt-level injection. This allows for\nunified reasoning across various tasks such as grounding, captioning, and\nquestion answering, all without the need for task-specific heads or additional\nsupervision. When evaluated on five benchmark datasets, including ScanRefer,\nMulti3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms\nstrong baseline models, demonstrating the effectiveness of language-guided\nrelational representation for understanding complex indoor scenes.",
    "published": "2025-07-19T09:19:16Z",
    "updated": "2025-07-19T09:19:16Z",
    "id": "2507.14555v1",
    "authors": [
      "Jintang Xue",
      "Ganning Zhao",
      "Jie-En Yao",
      "Hong-En Chen",
      "Yue Hu",
      "Meida Chen",
      "Suya You",
      "C. -C. Jay Kuo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14555v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14555v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14555v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing 3D scene understanding using large language models (LLMs) by incorporating object-level text descriptions, which involves multimodal integration (3D and language) and reasoning about spatial and semantic relationships.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning",
      "VLA"
    ]
  },
  "2507.14554v1": {
    "title": "Emerging Trends in Software Architecture from the Practitioners\n  Perspective: A Five Year Review",
    "summary": "Software architecture plays a central role in the design, development, and\nmaintenance of software systems. With the rise of cloud computing,\nmicroservices, and containers, architectural practices have diversified.\nUnderstanding these shifts is vital. This study analyzes software architecture\ntrends across eight leading industry conferences over five years. We\ninvestigate the evolution of software architecture by analyzing talks from top\npractitioner conferences, focusing on the motivations and contexts driving\ntechnology adoption. We analyzed 5,677 talks from eight major industry\nconferences, using large language models and expert validation to extract\ntechnologies, their purposes, and usage contexts. We also explored how\ntechnologies interrelate and fit within DevOps and deployment pipelines. Among\n450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate\nby frequency and centrality. Practitioners present technology mainly related to\ndeployment, communication, AI, and observability. We identify five technology\ncommunities covering automation, coordination, cloud AI, monitoring, and\ncloud-edge. Most technologies span multiple DevOps stages and support hybrid\ndeployment. Our study reveals that a few core technologies, like Kubernetes and\nServerless, dominate the contemporary software architecture practice. These are\nmainly applied in later DevOps stages, with limited focus on early phases like\nplanning and coding. We also show how practitioners frame technologies by\npurpose and context, reflecting evolving industry priorities. Finally, we\nobserve how only research can provide a more holistic lens on architectural\ndesign, quality, and evolution.",
    "published": "2025-07-19T09:16:04Z",
    "updated": "2025-07-19T09:16:04Z",
    "id": "2507.14554v1",
    "authors": [
      "Ruoyu Su",
      "Noman ahmad",
      "Matteo Esposito",
      "Andrea Janes",
      "Davide Taibi",
      "Valentina Lenarduzzi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14554v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14554v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14554v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses trends in software architecture, focusing on industry practices and technology adoption, but does not specifically address any of the provided topics related to LLMs, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14552v1": {
    "title": "Large Language Models Assisting Ontology Evaluation",
    "summary": "Ontology evaluation through functional requirements, such as testing via\ncompetency question (CQ) verification, is a well-established yet costly,\nlabour-intensive, and error-prone endeavour, even for ontology engineering\nexperts. In this work, we introduce OE-Assist, a novel framework designed to\nassist ontology evaluation through automated and semi-automated CQ\nverification. By presenting and leveraging a dataset of 1,393 CQs paired with\ncorresponding ontologies and ontology stories, our contributions present, to\nour knowledge, the first systematic investigation into large language model\n(LLM)-assisted ontology evaluation, and include: (i) evaluating the\neffectiveness of a LLM-based approach for automatically performing CQ\nverification against a manually created gold standard, and (ii) developing and\nassessing an LLM-powered framework to assist CQ verification with Prot\\'eg\\'e,\nby providing suggestions. We found that automated LLM-based evaluation with\no1-preview and o3-mini perform at a similar level to the average user's\nperformance.",
    "published": "2025-07-19T09:13:51Z",
    "updated": "2025-07-19T09:13:51Z",
    "id": "2507.14552v1",
    "authors": [
      "Anna Sofia Lippolis",
      "Mohammad Javad Saeedizade",
      "Robin Keskisrkk",
      "Aldo Gangemi",
      "Eva Blomqvist",
      "Andrea Giovanni Nuzzolese"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14552v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14552v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14552v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for ontology evaluation, specifically in the context of competency question verification. The focus is on leveraging LLMs for automated and semi-automated tasks, which aligns with the 'LLM' topic. The mention of a dataset also suggests relevance to the 'Dataset' topic.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.14533v1": {
    "title": "ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring\n  and Expert-Level Understanding",
    "summary": "The rapid advancement of educational applications, artistic creation, and\nAI-generated content (AIGC) technologies has substantially increased practical\nrequirements for comprehensive Image Aesthetics Assessment (IAA), particularly\ndemanding methods capable of delivering both quantitative scoring and\nprofessional understanding. Multimodal Large Language Model (MLLM)-based IAA\nmethods demonstrate stronger perceptual and generalization capabilities\ncompared to traditional approaches, yet they suffer from modality bias\n(score-only or text-only) and lack fine-grained attribute decomposition,\nthereby failing to support further aesthetic assessment. In this paper, we\npresent:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and\nExpert-Level Understanding capabilities; (2) ArtiMuse-10K, the first\nexpert-curated image aesthetic dataset comprising 10,000 images spanning 5 main\ncategories and 15 subcategories, each annotated by professional experts with\n8-dimensional attributes analysis and a holistic score. Both the model and\ndataset will be made public to advance the field.",
    "published": "2025-07-19T08:27:21Z",
    "updated": "2025-07-19T08:27:21Z",
    "id": "2507.14533v1",
    "authors": [
      "Shuo Cao",
      "Nan Ma",
      "Jiayang Li",
      "Xiaohui Li",
      "Lihao Shao",
      "Kaiwen Zhu",
      "Yu Zhou",
      "Yuandong Pu",
      "Jiarui Wu",
      "Jiaquan Wang",
      "Bo Qu",
      "Wenhai Wang",
      "Yu Qiao",
      "Dajuin Yao",
      "Yihao Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14533v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14533v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14533v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a Multimodal Large Language Model (MLLM)-based method for Image Aesthetics Assessment (IAA), which involves both quantitative scoring and professional understanding. It also introduces a new dataset for this purpose. The core topics are MLLM for multimodal integration and Dataset for the creation of a new expert-curated dataset.",
    "llm_cls_result": [
      "MLLM",
      "Dataset"
    ]
  },
  "2507.14527v1": {
    "title": "PaperBridge: Crafting Research Narratives through Human-AI\n  Co-Exploration",
    "summary": "Researchers frequently need to synthesize their own publications into\ncoherent narratives that demonstrate their scholarly contributions. To suit\ndiverse communication contexts, exploring alternative ways to organize one's\nwork while maintaining coherence is particularly challenging, especially in\ninterdisciplinary fields like HCI where individual researchers' publications\nmay span diverse domains and methodologies. In this paper, we present\nPaperBridge, a human-AI co-exploration system informed by a formative study and\ncontent analysis. PaperBridge assists researchers in exploring diverse\nperspectives for organizing their publications into coherent narratives. At its\ncore is a bi-directional analysis engine powered by large language models,\nsupporting iterative exploration through both top-down user intent (e.g.,\ndetermining organization structure) and bottom-up refinement on narrative\ncomponents (e.g., thematic paper groupings). Our user study (N=12) demonstrated\nPaperBridge's usability and effectiveness in facilitating the exploration of\nalternative research narratives. Our findings also provided empirical insights\ninto how interactive systems can scaffold academic communication tasks.",
    "published": "2025-07-19T08:04:23Z",
    "updated": "2025-07-19T08:04:23Z",
    "id": "2507.14527v1",
    "authors": [
      "Runhua Zhang",
      "Yang Ouyang",
      "Leixian Shen",
      "Yuying Tang",
      "Xiaojuan Ma",
      "Huamin Qu",
      "Xian Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14527v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14527v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14527v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a human-AI co-exploration system for organizing research publications into coherent narratives, leveraging large language models for iterative exploration. However, it does not specifically focus on any of the provided topics related to LLM architectures, scaling, reasoning, or other technical aspects of LLMs.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14513v1": {
    "title": "Amico: An Event-Driven Modular Framework for Persistent and Embedded\n  Autonomy",
    "summary": "Recent advances in large language models (LLMs) and autonomous agents have\nenabled systems capable of performing complex tasks across domains such as\nhuman-computer interaction, planning, and web navigation. However, many\nexisting frameworks struggle in real-world or resource-constrained environments\ndue to their reliance on cloud-based computation, limited robustness in dynamic\ncontexts, and lack of persistent autonomy and environmental awareness.\n  We present Amico, a modular, event-driven framework for building autonomous\nagents optimized for embedded systems. Written in Rust for safety and\nperformance, Amico supports reactive, persistent agents that operate\nefficiently across embedded platforms and browser environments via WebAssembly.\nIt provides clean abstractions for event handling, state management, behavior\nexecution, and integration with reasoning modules. Amico delivers a unified\ninfrastructure for constructing resilient, interactive agents suitable for\ndeployment in settings with limited compute and intermittent connectivity.",
    "published": "2025-07-19T07:21:09Z",
    "updated": "2025-07-19T07:21:09Z",
    "id": "2507.14513v1",
    "authors": [
      "Hongyi Yang",
      "Yue Pan",
      "Jiayi Xu",
      "Kelsen Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14513v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14513v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14513v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development of a framework for autonomous agents that leverages large language models (LLMs) and is optimized for embedded systems, which aligns with topics related to LLM and autonomous agents (RL). However, the primary focus is on the framework's modularity and event-driven nature for embedded systems, which does not directly fit into the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14497v1": {
    "title": "Efficient Whole Slide Pathology VQA via Token Compression",
    "summary": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin.",
    "published": "2025-07-19T06:04:25Z",
    "updated": "2025-07-19T06:04:25Z",
    "id": "2507.14497v1",
    "authors": [
      "Weimin Lyu",
      "Qingqiao Hu",
      "Kehan Qi",
      "Zhan Shi",
      "Wentao Huang",
      "Saumya Gupta",
      "Chao Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14497v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14497v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14497v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multimodal large language model (MLLM) approach for visual question answering (VQA) in pathology, focusing on token compression to handle large whole-slide images. This aligns with the MLLM topic, which involves integrating vision and language modalities. The mention of LLM and computational efficiency also hints at the Scaling topic, though the primary focus is on MLLM.",
    "llm_cls_result": [
      "MLLM",
      "Scaling"
    ]
  },
  "2507.14482v1": {
    "title": "Conch: Competitive Debate Analysis via Visualizing Clash Points and\n  Hierarchical Strategies",
    "summary": "In-depth analysis of competitive debates is essential for participants to\ndevelop argumentative skills and refine strategies, and further improve their\ndebating performance. However, manual analysis of unstructured and unlabeled\ntextual records of debating is time-consuming and ineffective, as it is\nchallenging to reconstruct contextual semantics and track logical connections\nfrom raw data. To address this, we propose Conch, an interactive visualization\nsystem that systematically analyzes both what is debated and how it is debated.\nIn particular, we propose a novel parallel spiral visualization that compactly\ntraces the multidimensional evolution of clash points and participant\ninteractions throughout debate process. In addition, we leverage large language\nmodels with well-designed prompts to automatically identify critical debate\nelements such as clash points, disagreements, viewpoints, and strategies,\nenabling participants to understand the debate context comprehensively.\nFinally, through two case studies on real-world debates and a\ncarefully-designed user study, we demonstrate Conch's effectiveness and\nusability for competitive debate analysis.",
    "published": "2025-07-19T04:42:09Z",
    "updated": "2025-07-19T04:42:09Z",
    "id": "2507.14482v1",
    "authors": [
      "Qianhe Chen",
      "Yong Wang",
      "Yixin Yu",
      "Xiyuan Zhu",
      "Xuerou Yu",
      "Ran Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14482v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14482v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14482v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of large language models (LLMs) for analyzing competitive debates through visualization and automated identification of debate elements. The core topics are related to LLM applications and reasoning, but none of the provided topics directly match the specific focus of the paper.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14397v1": {
    "title": "Efficient LLM Inference: Bandwidth, Compute, Synchronization, and\n  Capacity are all you need",
    "summary": "This paper presents a limit study of transformer-based large language model\n(LLM) inference, focusing on the fundamental performance bottlenecks imposed by\nmemory bandwidth, memory capacity, and synchronization overhead in distributed\ninference systems. We develop a hardware-agnostic performance model that\nabstracts away implementation details, enabling the analysis of a wide range of\ncurrent and near-future hardware technologies. Our analysis spans from current\nHBM3 memory technology used in AI accelerators like GPUs and TPUs to systems\nbased on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers\nSRAM-based designs and scaling techniques from distributed clusters with\nvarying numbers of chips to wafer-scale integration. Our key findings for\nauto-regressive decoding are: i) serving LLMs requires 100s of GB per server to\nserve a model instance; ii) high memory bandwidth is critical for high per-user\nthroughput; iii) exposed synchronization latencies to achieve collective\ncommunication must be around 1us else they make the memory bandwidth\nineffective; iv) DRAM-based designs have a fundamental advantage in terms of\nsystem-level efficiency as measured in throughput per cost or watt; and v)\nhardware designs can easily reach 2000+ user token/sec but getting to 10,000+\ntokens/sec will need smaller models, smaller context, or other forms of\nalgorithmic advances. This study provides valuable insights into the\nfundamental performance limits of LLM inference, highlighting the potential\nbenefits of future hardware advancements and guiding the optimization of LLM\ndeployment strategies.",
    "published": "2025-07-18T22:58:55Z",
    "updated": "2025-07-18T22:58:55Z",
    "id": "2507.14397v1",
    "authors": [
      "Michael Davies",
      "Neal Crago",
      "Karthikeyan Sankaralingam",
      "Christos Kozyrakis"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14397v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14397v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14397v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the performance bottlenecks and optimization of transformer-based large language model (LLM) inference, which directly relates to the topics of LLM and Scaling. It discusses hardware-agnostic performance models and the impact of memory bandwidth, capacity, and synchronization overhead, which are key aspects of scaling LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.14392v1": {
    "title": "Characterizing Communication Patterns in Distributed Large Language\n  Model Inference",
    "summary": "Large Language Models (LLMs) built on transformer architectures have\ntransformed natural language processing, achieving remarkable performance\nacross diverse applications. While distributed inference frameworks enable\npractical deployment of these models, inter-GPU communication creates\nsignificant performance constraints that limit service quality in real-world\nsystems. This paper investigates communication dynamics in distributed LLM\nserving-analyzing how various parallelization approaches coordinate data\nexchange between GPU workers during inference. We study dense transformer-based\nmodels as representative examples of contemporary architectures widely used in\noperational deployments. Our work combines detailed profiling measurements with\npredictive analytical models to characterize communication behavior across\ndifferent parallelization configurations. Results show that tensor parallelism\nincurs substantial network overhead but delivers superior response times for\nbrief sequences, pipeline parallelism minimizes data transfer requirements\nwhile increasing total latency, and combined approaches demand careful tuning\nto achieve balanced performance. These insights offer practical recommendations\nfor selecting appropriate parallelization schemes in production LLM services\nand identify key opportunities for optimizing inference frameworks and\ncommunication infrastructure.",
    "published": "2025-07-18T22:43:38Z",
    "updated": "2025-07-18T22:43:38Z",
    "id": "2507.14392v1",
    "authors": [
      "Lang Xu",
      "Kaushik Kandadi Suresh",
      "Quentin Anthony",
      "Nawras Alnaasan",
      "Dhabaleswar K. Panda"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14392v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14392v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14392v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the performance and communication dynamics of distributed inference for Large Language Models (LLMs), which is directly related to the architecture and deployment of LLMs. It discusses various parallelization approaches and their impact on inference performance, which falls under the broader topic of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.15885v1": {
    "title": "ADEPTS: A Capability Framework for Human-Centered Agent Design",
    "summary": "Large language models have paved the way to powerful and flexible AI agents,\nassisting humans by increasingly integrating into their daily life. This\nflexibility, potential, and growing adoption demands a holistic and\ncross-disciplinary approach to developing, monitoring and discussing the\ncapabilities required for agent-driven user experiences. However, current\nguidance on human-centered AI agent development is scattered: UX heuristics\nfocus on interface behaviors, engineering taxonomies describe internal\npipelines, and ethics checklists address high-level governance. There is no\nconcise, user-facing vocabulary that tells teams what an agent should\nfundamentally be able to do. We introduce ADEPTS, a capability framework\ndefining a set of core user-facing capabilities to provide unified guidance\naround the development of AI agents. ADEPTS is based on six principles for\nhuman-centered agent design, that express the minimal, user-facing capabilities\nan AI agent should demonstrate to be understandable, controllable and\ntrustworthy in everyday use. ADEPTS complements existing frameworks and\ntaxonomies; differently from them, it sits at the interface between technical\nand experience development. By presenting ADEPTS, we aim to condense complex\nAI-UX requirements into a compact framework that is actionable guidance for AI\nresearchers, designers, engineers, and policy reviewers alike. We believe\nADEPTS has the potential of accelerating the improvement of user-relevant agent\ncapabilities, of easing the design of experiences that take advantage of those\ncapabilities, and of providing a shared language to track and discuss progress\naround the development of AI agents.",
    "published": "2025-07-18T22:27:40Z",
    "updated": "2025-07-18T22:27:40Z",
    "id": "2507.15885v1",
    "authors": [
      "Pierluca D'Oro",
      "Caley Drooff",
      "Joy Chen",
      "Joseph Tighe"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15885v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15885v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15885v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development of AI agents using large language models, focusing on human-centered design and capabilities. It aligns with the topics of LLM (Large Language Models) and AGI (Artificial General Intelligence) due to its emphasis on agent capabilities and user-centered AI development.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.14384v1": {
    "title": "Assessing the Reliability of Large Language Models for Deductive\n  Qualitative Coding: A Comparative Study of ChatGPT Interventions",
    "summary": "In this study, we investigate the use of large language models (LLMs),\nspecifically ChatGPT, for structured deductive qualitative coding. While most\ncurrent research emphasizes inductive coding applications, we address the\nunderexplored potential of LLMs to perform deductive classification tasks\naligned with established human-coded schemes. Using the Comparative Agendas\nProject (CAP) Master Codebook, we classified U.S. Supreme Court case summaries\ninto 21 major policy domains. We tested four intervention methods: zero-shot,\nfew-shot, definition-based, and a novel Step-by-Step Task Decomposition\nstrategy, across repeated samples. Performance was evaluated using standard\nclassification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's\nalpha), and construct validity was assessed using chi-squared tests and\nCramer's V. Chi-squared and effect size analyses confirmed that intervention\nstrategies significantly influenced classification behavior, with Cramer's V\nvalues ranging from 0.359 to 0.613, indicating moderate to strong shifts in\nclassification patterns. The Step-by-Step Task Decomposition strategy achieved\nthe strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),\nachieving thresholds for substantial agreement. Despite the semantic ambiguity\nwithin case summaries, ChatGPT displayed stable agreement across samples,\nincluding high F1 scores in low-support subclasses. These findings demonstrate\nthat with targeted, custom-tailored interventions, LLMs can achieve reliability\nlevels suitable for integration into rigorous qualitative coding workflows.",
    "published": "2025-07-18T22:16:04Z",
    "updated": "2025-07-18T22:16:04Z",
    "id": "2507.14384v1",
    "authors": [
      "Angjelin Hila",
      "Elliott Hauser"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14384v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14384v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14384v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the use of large language models (LLMs), specifically ChatGPT, for qualitative coding tasks, which involves evaluating their performance and reliability in classification tasks. This aligns with the 'LLM' topic as it directly involves research on large language models and their applications. Additionally, the study's emphasis on structured deductive qualitative coding and intervention strategies could be loosely related to 'Reasoning' as it involves logical classification tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.14374v1": {
    "title": "Error-Aware Curriculum Learning for Biomedical Relation Classification",
    "summary": "Relation Classification (RC) in biomedical texts is essential for\nconstructing knowledge graphs and enabling applications such as drug\nrepurposing and clinical decision-making. We propose an error-aware\nteacher--student framework that improves RC through structured guidance from a\nlarge language model (GPT-4o). Prediction failures from a baseline student\nmodel are analyzed by the teacher to classify error types, assign difficulty\nscores, and generate targeted remediations, including sentence rewrites and\nsuggestions for KG-based enrichment. These enriched annotations are used to\ntrain a first student model via instruction tuning. This model then annotates a\nbroader dataset with difficulty scores and remediation-enhanced inputs. A\nsecond student is subsequently trained via curriculum learning on this dataset,\nordered by difficulty, to promote robust and progressive learning. We also\nconstruct a heterogeneous biomedical knowledge graph from PubMed abstracts to\nsupport context-aware RC. Our approach achieves new state-of-the-art\nperformance on 4 of 5 PPI datasets and the DDI dataset, while remaining\ncompetitive on ChemProt.",
    "published": "2025-07-18T21:41:20Z",
    "updated": "2025-07-18T21:41:20Z",
    "id": "2507.14374v1",
    "authors": [
      "Sinchani Chakraborty",
      "Sudeshna Sarkar",
      "Pawan Goyal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14374v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14374v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14374v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a large language model (GPT-4o) for biomedical relation classification, which involves instruction tuning and curriculum learning. The core topics are related to LLM (Large Language Model) and Reasoning (as it involves structured guidance and error analysis for improved performance).",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.14372v1": {
    "title": "Text-to-SQL for Enterprise Data Analytics",
    "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions.",
    "published": "2025-07-18T21:39:17Z",
    "updated": "2025-07-18T21:39:17Z",
    "id": "2507.14372v1",
    "authors": [
      "Albert Chen",
      "Manas Bundele",
      "Gaurav Ahlawat",
      "Patrick Stetz",
      "Zhitao Wang",
      "Qiang Fei",
      "Donghoon Jung",
      "Audrey Chu",
      "Bharadwaj Jayaraman",
      "Ayushi Panth",
      "Yatin Arora",
      "Sourav Jain",
      "Renjith Varma",
      "Alexey Ilin",
      "Iuliia Melnychuk",
      "Chelsea Chueh",
      "Joyan Sil",
      "Xiaofeng Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14372v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14372v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14372v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models in building a Text-to-SQL solution for enterprise data analytics, focusing on practical implementation and user interaction rather than core LLM research topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14367v1": {
    "title": "Hallucination Score: Towards Mitigating Hallucinations in Generative\n  Image Super-Resolution",
    "summary": "Generative super-resolution (GSR) currently sets the state-of-the-art in\nterms of perceptual image quality, overcoming the \"regression-to-the-mean\" blur\nof prior non-generative models. However, from a human perspective, such models\ndo not fully conform to the optimal balance between quality and fidelity.\nInstead, a different class of artifacts, in which generated details fail to\nperceptually match the low resolution image (LRI) or ground-truth image (GTI),\nis a critical but under studied issue in GSR, limiting its practical\ndeployments. In this work, we focus on measuring, analyzing, and mitigating\nthese artifacts (i.e., \"hallucinations\"). We observe that hallucinations are\nnot well-characterized with existing image metrics or quality models, as they\nare orthogonal to both exact fidelity and no-reference quality. Instead, we\ntake advantage of a multimodal large language model (MLLM) by constructing a\nprompt that assesses hallucinatory visual elements and generates a\n\"Hallucination Score\" (HS). We find that our HS is closely aligned with human\nevaluations, and also provides complementary insights to prior image metrics\nused for super-resolution (SR) models. In addition, we find certain deep\nfeature distances have strong correlations with HS. We therefore propose to\nalign the GSR models by using such features as differentiable reward functions\nto mitigate hallucinations.",
    "published": "2025-07-18T21:13:50Z",
    "updated": "2025-07-18T21:13:50Z",
    "id": "2507.14367v1",
    "authors": [
      "Weiming Ren",
      "Raghav Goyal",
      "Zhiming Hu",
      "Tristan Ty Aumentado-Armstrong",
      "Iqbal Mohomed",
      "Alex Levinshtein"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14367v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14367v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14367v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a multimodal large language model (MLLM) to assess and mitigate hallucinations in generative super-resolution, which aligns with the MLLM topic. It also involves the evaluation and benchmarking of image quality, which fits the Benchmark topic.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.14330v1": {
    "title": "Leveraging LLMs for Formal Software Requirements -- Challenges and\n  Prospects",
    "summary": "Software correctness is ensured mathematically through formal verification,\nwhich involves the resources of generating formal requirement specifications\nand having an implementation that must be verified. Tools such as\nmodel-checkers and theorem provers ensure software correctness by verifying the\nimplementation against the specification. Formal methods deployment is\nregularly enforced in the development of safety-critical systems e.g.\naerospace, medical devices and autonomous systems. Generating these\nspecifications from informal and ambiguous natural language requirements\nremains the key challenge. Our project, VERIFAI^{1}, aims to investigate\nautomated and semi-automated approaches to bridge this gap, using techniques\nfrom Natural Language Processing (NLP), ontology-based domain modelling,\nartefact reuse, and large language models (LLMs). This position paper presents\na preliminary synthesis of relevant literature to identify recurring challenges\nand prospective research directions in the generation of verifiable\nspecifications from informal requirements.",
    "published": "2025-07-18T19:15:50Z",
    "updated": "2025-07-18T19:15:50Z",
    "id": "2507.14330v1",
    "authors": [
      "Arshad Beg",
      "Diarmuid O'Donoghue",
      "Rosemary Monahan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14330v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14330v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14330v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the context of generating formal software requirements, which directly relates to the 'LLM' topic. Additionally, it touches on the application of LLMs in a specific domain (software verification), which aligns with the broader scope of LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.14307v1": {
    "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in\n  Cognitive Evaluation of LLMs",
    "summary": "Large language models (LLMs) exhibit increasingly sophisticated linguistic\ncapabilities, yet the extent to which these behaviors reflect human-like\ncognition versus advanced pattern recognition remains an open question. In this\nstudy, we investigate how LLMs process the temporal meaning of linguistic\naspect in narratives that were previously used in human studies. Using an\nExpert-in-the-Loop probing pipeline, we conduct a series of targeted\nexperiments to assess whether LLMs construct semantic representations and\npragmatic inferences in a human-like manner. Our findings show that LLMs\nover-rely on prototypicality, produce inconsistent aspectual judgments, and\nstruggle with causal reasoning derived from aspect, raising concerns about\ntheir ability to fully comprehend narratives. These results suggest that LLMs\nprocess aspect fundamentally differently from humans and lack robust narrative\nunderstanding. Beyond these empirical findings, we develop a standardized\nexperimental framework for the reliable assessment of LLMs' cognitive and\nlinguistic capabilities.",
    "published": "2025-07-18T18:28:35Z",
    "updated": "2025-07-18T18:28:35Z",
    "id": "2507.14307v1",
    "authors": [
      "Karin de Langis",
      "Jong Inn Park",
      "Andreas Schramm",
      "Bin Hu",
      "Khanh Chi Le",
      "Michael Mensink",
      "Ahn Thu Tong",
      "Dongyeop Kang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14307v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14307v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14307v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the cognitive and linguistic capabilities of Large Language Models (LLMs) in comprehending temporal meaning in narratives, which aligns with the 'LLM' and 'Reasoning' topics. The study also involves benchmarking LLMs' performance, which relates to the 'Benchmark' topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.14306v1": {
    "title": "Manimator: Transforming Research Papers into Visual Explanations",
    "summary": "Understanding complex scientific and mathematical concepts, particularly\nthose presented in dense research papers, poses a significant challenge for\nlearners. Dynamic visualizations can greatly enhance comprehension, but\ncreating them manually is time-consuming and requires specialized knowledge and\nskills. We introduce manimator, an open-source system that leverages Large\nLanguage Models to transform research papers and natural language prompts into\nexplanatory animations using the Manim engine. Manimator employs a pipeline\nwhere an LLM interprets the input text or research paper PDF to generate a\nstructured scene description outlining key concepts, mathematical formulas, and\nvisual elements and another LLM translates this description into executable\nManim Python code. We discuss its potential as an educational tool for rapidly\ncreating engaging visual explanations for complex STEM topics, democratizing\nthe creation of high-quality educational content.",
    "published": "2025-07-18T18:28:26Z",
    "updated": "2025-07-18T18:28:26Z",
    "id": "2507.14306v1",
    "authors": [
      "Samarth P",
      "Vyoman Jain",
      "Shiva Golugula",
      "Motamarri Sai Sathvik"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14306v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14306v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14306v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to transform research papers into visual explanations, which aligns with the LLM topic. It also touches on the educational application of LLMs, which is a broader aspect of LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.14304v1": {
    "title": "Aligning Large Language Models to Low-Resource Languages through\n  LLM-Based Selective Translation: A Systematic Study",
    "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.",
    "published": "2025-07-18T18:21:52Z",
    "updated": "2025-07-18T18:21:52Z",
    "id": "2507.14304v1",
    "authors": [
      "Rakesh Paul",
      "Anusha Kamath",
      "Kanishk Singla",
      "Raviraj Joshi",
      "Utkarsh Vaidya",
      "Sanjay Singh Chauhan",
      "Niranjan Wartikar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14304v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14304v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14304v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses aligning large language models (LLMs) to low-resource languages, focusing on the challenges and solutions for improving multilingual alignment. It involves the use of LLMs for selective translation and compares different translation methods, which aligns with the topics of LLM research and multilingual alignment.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.14293v1": {
    "title": "WebGuard: Building a Generalizable Guardrail for Web Agents",
    "summary": "The rapid development of autonomous web agents powered by Large Language\nModels (LLMs), while greatly elevating efficiency, exposes the frontier risk of\ntaking unintended or harmful actions. This situation underscores an urgent need\nfor effective safety measures, akin to access controls for human users. To\naddress this critical challenge, we introduce WebGuard, the first comprehensive\ndataset designed to support the assessment of web agent action risks and\nfacilitate the development of guardrails for real-world online environments. In\ndoing so, WebGuard specifically focuses on predicting the outcome of\nstate-changing actions and contains 4,939 human-annotated actions from 193\nwebsites across 22 diverse domains, including often-overlooked long-tail\nwebsites. These actions are categorized using a novel three-tier risk schema:\nSAFE, LOW, and HIGH. The dataset includes designated training and test splits\nto support evaluation under diverse generalization settings. Our initial\nevaluations reveal a concerning deficiency: even frontier LLMs achieve less\nthan 60% accuracy in predicting action outcomes and less than 60% recall in\nlagging HIGH-risk actions, highlighting the risks of deploying\ncurrent-generation agents without dedicated safeguards. We therefore\ninvestigate fine-tuning specialized guardrail models using WebGuard. We conduct\ncomprehensive evaluations across multiple generalization settings and find that\na fine-tuned Qwen2.5VL-7B model yields a substantial improvement in\nperformance, boosting accuracy from 37% to 80% and HIGH-risk action recall from\n20% to 76%. Despite these improvements, the performance still falls short of\nthe reliability required for high-stakes deployment, where guardrails must\napproach near-perfect accuracy and recall.",
    "published": "2025-07-18T18:06:27Z",
    "updated": "2025-07-18T18:06:27Z",
    "id": "2507.14293v1",
    "authors": [
      "Boyuan Zheng",
      "Zeyi Liao",
      "Scott Salisbury",
      "Zeyuan Liu",
      "Michael Lin",
      "Qinyuan Zheng",
      "Zifan Wang",
      "Xiang Deng",
      "Dawn Song",
      "Huan Sun",
      "Yu Su"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14293v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14293v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14293v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development of a guardrail for web agents powered by Large Language Models (LLMs), focusing on safety measures and risk assessment. It involves the use of LLMs and addresses the need for safety in autonomous web agents, which aligns with the topics of LLM and RL (Reinforcement Learning) due to the focus on agent actions and safety measures.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.14107v1": {
    "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps\n  Using Large Language Models for Bridge Condition Assessment",
    "summary": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
    "published": "2025-07-18T17:39:03Z",
    "updated": "2025-07-18T17:39:03Z",
    "id": "2507.14107v1",
    "authors": [
      "Viraj Nishesh Darji",
      "Callie C. Liao",
      "Duoduo Liao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14107v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14107v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14107v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in interpreting Non-Destructive Evaluation (NDE) contour maps for bridge condition assessment, which aligns with the 'LLM' topic. The study also involves the use of LLMs for image captioning and summarization, which is relevant to the 'MLLM' topic as it involves multimodal capabilities (combining vision and language).",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.14097v1": {
    "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
    "summary": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.",
    "published": "2025-07-18T17:24:50Z",
    "updated": "2025-07-18T17:24:50Z",
    "id": "2507.14097v1",
    "authors": [
      "Hari Iyer",
      "Neel Macwan",
      "Atharva Jitendra Hude",
      "Heejin Jeong",
      "Shenghan Guo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14097v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14097v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14097v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with motion simulation, focusing on enhancing motion fidelity through AI. It involves text-to-text and text-to-motion models, which are relevant to LLM and MLLM topics due to the use of multimodal (text and motion) data.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.14088v1": {
    "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time\n  Human-AI Collaboration",
    "summary": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
    "published": "2025-07-18T17:13:21Z",
    "updated": "2025-07-18T17:13:21Z",
    "id": "2507.14088v1",
    "authors": [
      "Xiyun Li",
      "Yining Ding",
      "Yuhua Jiang",
      "Yunlong Zhao",
      "Runpeng Xie",
      "Shuang Xu",
      "Yuanhua Ni",
      "Yiqin Yang",
      "Bo Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14088v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14088v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14088v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing human-AI collaboration using a dual process multi-scale theory of mind framework, which involves reasoning about human mental characteristics. This aligns with research on LLM reasoning and agents, as it involves complex problem-solving and modeling human behaviors.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Reasoning"
    ]
  },
  "2507.14079v1": {
    "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of\n  Heterogeneous Clinical Notes Across Hospital Visits",
    "summary": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
    "published": "2025-07-18T17:00:27Z",
    "updated": "2025-07-18T17:00:27Z",
    "id": "2507.14079v1",
    "authors": [
      "Garapati Keerthana",
      "Manik Gupta"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14079v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14079v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14079v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a large language model (LLM) to generate clinically coherent and temporally aware progress notes from heterogeneous clinical notes. The core topics are related to LLM applications in healthcare and document generation, but none of the provided topics directly match the specific focus of the paper.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14034v1": {
    "title": "Architecting Human-AI Cocreation for Technical Services -- Interaction\n  Modes and Contingency Factors",
    "summary": "Agentic AI systems, powered by Large Language Models (LLMs), offer\ntransformative potential for value co-creation in technical services. However,\npersistent challenges like hallucinations and operational brittleness limit\ntheir autonomous use, creating a critical need for robust frameworks to guide\nhuman-AI collaboration. Drawing on established Human-AI teaming research and\nanalogies from fields like autonomous driving, this paper develops a structured\ntaxonomy of human-agent interaction. Based on case study research within\ntechnical support platforms, we propose a six-mode taxonomy that organizes\ncollaboration across a spectrum of AI autonomy. This spectrum is anchored by\nthe Human-Out-of-the-Loop (HOOTL) model for full automation and the\nHuman-Augmented Model (HAM) for passive AI assistance. Between these poles, the\nframework specifies four distinct intermediate structures. These include the\nHuman-in-Command (HIC) model, where AI proposals re-quire mandatory human\napproval, and the Human-in-the-Process (HITP) model for structured work-flows\nwith deterministic human tasks. The taxonomy further delineates the\nHuman-in-the-Loop (HITL) model, which facilitates agent-initiated escalation\nupon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables\ndiscretionary human oversight of an autonomous AI. The primary contribution of\nthis work is a comprehensive framework that connects this taxonomy to key\ncontingency factors -- such as task complexity, operational risk, and system\nreliability -- and their corresponding conceptual architectures. By providing a\nsystematic method for selecting and designing an appropriate level of human\noversight, our framework offers practitioners a crucial tool to navigate the\ntrade-offs between automation and control, thereby fostering the development of\nsafer, more effective, and context-aware technical service systems.",
    "published": "2025-07-18T16:06:03Z",
    "updated": "2025-07-18T16:06:03Z",
    "id": "2507.14034v1",
    "authors": [
      "Jochen Wulf",
      "Jurg Meierhofer",
      "Frank Hannich"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14034v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14034v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14034v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in human-AI collaboration frameworks, focusing on interaction modes and contingency factors. It aligns with the 'LLM' topic due to its emphasis on LLMs and their applications in technical services. Additionally, it touches on 'RL' as it involves human feedback and oversight in AI systems, which is related to Reinforcement Learning with Human Feedback (RLHF).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.14024v1": {
    "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
    "summary": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
    "published": "2025-07-18T15:52:39Z",
    "updated": "2025-07-18T15:52:39Z",
    "id": "2507.14024v1",
    "authors": [
      "Jiarong Ye",
      "Sharon X. Huang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14024v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14024v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14024v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Multimodal Large Language Models (MLLMs) for emotion-driven image editing, which involves integrating vision and language modalities. It also introduces a new dataset and a vision-language model fine-tuned for emotional annotations, aligning with the topics of MLLM and Dataset.",
    "llm_cls_result": [
      "MLLM",
      "Dataset"
    ]
  },
  "2507.14017v1": {
    "title": "Efficient Temporal Tokenization for Mobility Prediction with Large\n  Language Models",
    "summary": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
    "published": "2025-07-18T15:31:16Z",
    "updated": "2025-07-18T15:31:16Z",
    "id": "2507.14017v1",
    "authors": [
      "Haoyu He",
      "Haozheng Luo",
      "Yan Chen",
      "Qi R. Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14017v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14017v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14017v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging large language models (LLMs) for spatio-temporal prediction and trajectory reasoning, which aligns with the 'LLM' topic. The hierarchical tokenization and efficiency improvements are also relevant to 'Scaling' as they address computational efficiency and model performance.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.13957v1": {
    "title": "DUALRec: A Hybrid Sequential and Language Model Framework for\n  Context-Aware Movie Recommendation",
    "summary": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
    "published": "2025-07-18T14:22:05Z",
    "updated": "2025-07-18T14:22:05Z",
    "id": "2507.13957v1",
    "authors": [
      "Yitong Li",
      "Raoul Grasman"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13957v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13957v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13957v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in combination with sequential models like LSTM for movie recommendations, leveraging the semantic reasoning power of LLMs and temporal modeling of LSTM. This aligns with the topics of LLM (Large Language Models) and Reasoning (reasoning abilities in LLMs).",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13956v1": {
    "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
    "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
    "published": "2025-07-18T14:21:24Z",
    "updated": "2025-07-18T14:21:24Z",
    "id": "2507.13956v1",
    "authors": [
      "Yutao Jin",
      "Haowen Xiao",
      "Jielei Chu",
      "Fengmao Lv",
      "Yuxiao Li",
      "Tianrui Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13956v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13956v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13956v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a large language model (LLM) to summarize clinical data and integrates it with MRI and fMRI images for Alzheimer's Disease prediction. It involves multimodal learning and causal reasoning, which are relevant to the topics of MLLM (Multimodal Large Language Models) and Reasoning (causal reasoning in LLMs).",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.13949v1": {
    "title": "Exploiting Primacy Effect To Improve Large Language Models",
    "summary": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
    "published": "2025-07-18T14:18:18Z",
    "updated": "2025-07-18T14:18:18Z",
    "id": "2507.13949v1",
    "authors": [
      "Bianca Raimondi",
      "Maurizio Gabbrielli"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13949v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13949v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13949v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses biases in Large Language Models (LLMs) and specifically focuses on the primacy effect in fine-tuned LLMs, which is a topic related to the study of LLMs and their behaviors.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13933v1": {
    "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
    "summary": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
    "published": "2025-07-18T14:09:04Z",
    "updated": "2025-07-18T14:09:04Z",
    "id": "2507.13933v1",
    "authors": [
      "Sichang \"Steven\" He",
      "Ramesh Govindan",
      "Harsha V. Madhyastha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13933v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13933v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13933v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the detection of content generated by large language models (LLMs) on websites, which directly relates to the topic of LLMs and their impact on web content.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.13881v1": {
    "title": "Using LLMs to identify features of personal and professional skills in\n  an open-response situational judgment test",
    "summary": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
    "published": "2025-07-18T12:59:17Z",
    "updated": "2025-07-18T12:59:17Z",
    "id": "2507.13881v1",
    "authors": [
      "Cole Walsh",
      "Rodica Ivan",
      "Muhammad Zafar Iqbal",
      "Colleen Robb"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13881v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13881v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13881v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to identify features in open-response situational judgment tests, which aligns with the research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.13874v1": {
    "title": "Large Language Models as Innovators: A Framework to Leverage Latent\n  Space Exploration for Novelty Discovery",
    "summary": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
    "published": "2025-07-18T12:54:28Z",
    "updated": "2025-07-18T12:54:28Z",
    "id": "2507.13874v1",
    "authors": [
      "Mateusz Bystroski",
      "Mikoaj Hoysz",
      "Grzegorz Piotrowski",
      "Nitesh V. Chawla",
      "Tomasz Kajdanowicz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13874v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13874v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13874v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for innovative idea generation and introduces a latent-space exploration framework to enhance creativity. The focus is on LLMs and their capabilities, which aligns with the 'LLM' topic. The exploration of latent space for novelty discovery also touches on 'Reasoning' as it involves complex problem-solving and creative divergence.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13858v1": {
    "title": "InTraVisTo: Inside Transformer Visualisation Tool",
    "summary": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
    "published": "2025-07-18T12:23:47Z",
    "updated": "2025-07-18T12:23:47Z",
    "id": "2507.13858v1",
    "authors": [
      "Nicol Brunello",
      "Davide Rigamonti",
      "Andrea Sassella",
      "Vincenzo Scotti",
      "Mark James Carman"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13858v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13858v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13858v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a visualization tool for understanding the internal workings of Transformer-based LLMs, which aligns with research on LLMs and their reasoning processes.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13833v2": {
    "title": "DistFlow: A Fully Distributed RL Framework for Scalable and Efficient\n  LLM Post-Training",
    "summary": "Reinforcement learning (RL) has become the pivotal post-training technique\nfor large language model. Effectively scaling reinforcement learning is now the\nkey to unlocking advanced reasoning capabilities and ensuring safe,\ngoal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually\nemploy a hybrid-controller architecture where a single-controller dispatches\nthe overall execution logic and manages overall data transfer and the\nmulti-controller executes distributed computation. For large-scale\nreinforcement learning, minor load imbalances can introduce significant\nbottlenecks, ultimately constraining the scalability of the system. To address\nthis limitation, we introduce DistFlow, a novel, fully distributed RL framework\ndesigned to break scaling barrier. We adopt a multi-controller paradigm that\ndispatches data transfer and execution tasks to all workers, which eliminates\nthe centralized node. This allows each worker to operate independently, leading\nto near-linear scalability up to thousands of GPUs and dramatic efficiency\ngains. Furthermore, our architecture decouples resource configuration from\nexecution logic, allowing each worker to have a unique execution flow, offering\nsignificant flexibility for rapid and cost-effective algorithmic\nexperimentation. Extensive experiments show that DistFlow achieves excellent\nlinear scalability and up to a 7x end-to-end throughput improvement over\nstate-of-the-art (SOTA) frameworks.",
    "published": "2025-07-18T11:41:49Z",
    "updated": "2025-07-23T01:58:01Z",
    "id": "2507.13833v2",
    "authors": [
      "Zhixin Wang",
      "Tianyi Zhou",
      "Liming Liu",
      "Ao Li",
      "Jiarui Hu",
      "Dian Yang",
      "Jinlong Hou",
      "Siyuan Feng",
      "Yuan Cheng",
      "Yuan Qi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13833v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13833v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13833v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a distributed RL framework for scaling post-training of large language models, which involves reinforcement learning and scalability aspects of LLMs.",
    "llm_cls_result": [
      "RL",
      "Scaling",
      "LLM"
    ]
  },
  "2507.13827v1": {
    "title": "Question-Answer Extraction from Scientific Articles Using Knowledge\n  Graphs and Large Language Models",
    "summary": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
    "published": "2025-07-18T11:31:52Z",
    "updated": "2025-07-18T11:31:52Z",
    "id": "2507.13827v1",
    "authors": [
      "Hosein Azarbonyad",
      "Zi Long Zhu",
      "Georgios Cheirmpos",
      "Zubair Afzal",
      "Vikrant Yadav",
      "Georgios Tsatsaronis"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13827v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13827v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13827v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating Question-Answer (QA) pairs from scientific articles, which involves leveraging LLMs for content understanding and generation. Additionally, it mentions the construction of a Knowledge Graph (KG) for QA generation, which involves entity relationship extraction and salient triplet extraction, indicating a focus on knowledge representation and retrieval.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2507.13822v1": {
    "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
    "summary": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
    "published": "2025-07-18T11:20:52Z",
    "updated": "2025-07-18T11:20:52Z",
    "id": "2507.13822v1",
    "authors": [
      "Shad Nygren",
      "Pinar Avci",
      "Andre Daniels",
      "Reza Rassol",
      "Afshin Beheshti",
      "Diego Galeano"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13822v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13822v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13822v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Retrieval-Augmented Generation (RAG) and GraphRAG architectures to enhance the capabilities of Large Language Models (LLMs) in the domain of drug side effect retrieval. This involves memory-augmented models and retrieval-based methods, which are key topics in the 'Memory' category. Additionally, the focus on LLMs and their application in specialized fields aligns with the 'LLM' category.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.13814v1": {
    "title": "CodeEdu: A Multi-Agent Collaborative Platform for Personalized Coding\n  Education",
    "summary": "Large Language Models (LLMs) have demonstrated considerable potential in\nimproving coding education by providing support for code writing, explanation,\nand debugging. However, existing LLM-based approaches generally fail to assess\nstudents' abilities, design learning plans, provide personalized material\naligned with individual learning goals, and enable interactive learning.\nCurrent work mostly uses single LLM agents, which limits their ability to\nunderstand complex code repositories and schedule step-by-step tutoring. Recent\nresearch has shown that multi-agent LLMs can collaborate to solve complicated\nproblems in various domains like software engineering, but their potential in\nthe field of education remains unexplored. In this work, we introduce CodeEdu,\nan innovative multi-agent collaborative platform that combines LLMs with tool\nuse to provide proactive and personalized education in coding. Unlike static\npipelines, CodeEdu dynamically allocates agents and tasks to meet student\nneeds. Various agents in CodeEdu undertake certain functions specifically,\nincluding task planning, personalized material generation, real-time QA,\nstep-by-step tutoring, code execution, debugging, and learning report\ngeneration, facilitated with extensive external tools to improve task\nefficiency. Automated evaluations reveal that CodeEdu substantially enhances\nstudents' coding performance.",
    "published": "2025-07-18T10:52:22Z",
    "updated": "2025-07-18T10:52:22Z",
    "id": "2507.13814v1",
    "authors": [
      "Jianing Zhao",
      "Peng Gao",
      "Jiannong Cao",
      "Zhiyuan Wen",
      "Chen Chen",
      "Jianing Yin",
      "Ruosong Yang",
      "Bo Yuan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13814v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13814v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13814v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a multi-agent system for personalized coding education, which aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning with Human Feedback, as multi-agent collaboration can involve RLHF techniques). The focus on personalized education and dynamic task allocation also touches on AGI (Artificial General Intelligence) aspects, as it involves creating systems that can adapt and learn in a general manner.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.13737v1": {
    "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal\n  Sensors and LLMs",
    "summary": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed.",
    "published": "2025-07-18T08:33:30Z",
    "updated": "2025-07-18T08:33:30Z",
    "id": "2507.13737v1",
    "authors": [
      "Ye Tian",
      "Xiaoyuan Ren",
      "Zihao Wang",
      "Onat Gungor",
      "Xiaofan Yu",
      "Tajana Rosing"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13737v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13737v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13737v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating context-aware activity logs by integrating multi-modal sensor data, which aligns with the topics of LLM and MLLM due to the use of multimodal inputs and the application of LLMs.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.13732v1": {
    "title": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction",
    "summary": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable.",
    "published": "2025-07-18T08:28:53Z",
    "updated": "2025-07-18T08:28:53Z",
    "id": "2507.13732v1",
    "authors": [
      "Guillaume Zambrano"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13732v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13732v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13732v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for structured feature extraction in the context of legal judgment prediction, but the primary focus is on the application of machine learning to legal decision-making rather than on the LLMs themselves or their broader implications in the given topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.13712v1": {
    "title": "LLaPipe: LLM-Guided Reinforcement Learning for Automated Data\n  Preparation Pipeline Construction",
    "summary": "Automated data preparation is crucial for democratizing machine learning, yet\nexisting reinforcement learning (RL) based approaches suffer from inefficient\nexploration in the vast space of possible preprocessing pipelines. We present\nLLaPipe, a novel framework that addresses this exploration bottleneck by\nintegrating Large Language Models (LLMs) as intelligent policy advisors. Unlike\ntraditional methods that rely solely on statistical features and blind\ntrial-and-error, LLaPipe leverages the semantic understanding capabilities of\nLLMs to provide contextually relevant exploration guidance. Our framework\nintroduces three key innovations: (1) an LLM Policy Advisor that analyzes\ndataset semantics and pipeline history to suggest promising preprocessing\noperations, (2) an Experience Distillation mechanism that mines successful\npatterns from past pipelines and transfers this knowledge to guide future\nexploration, and (3) an Adaptive Advisor Triggering strategy\n(Advisor\\textsuperscript{+}) that dynamically determines when LLM intervention\nis most beneficial, balancing exploration effectiveness with computational\ncost. Through extensive experiments on 18 diverse datasets spanning multiple\ndomains, we demonstrate that LLaPipe achieves up to 22.4\\% improvement in\npipeline quality and 2.3$\\times$ faster convergence compared to\nstate-of-the-art RL-based methods, while maintaining computational efficiency\nthrough selective LLM usage (averaging only 19.0\\% of total exploration steps).",
    "published": "2025-07-18T07:52:19Z",
    "updated": "2025-07-18T07:52:19Z",
    "id": "2507.13712v1",
    "authors": [
      "Jing Chang",
      "Chang Liu",
      "Jinbin Huang",
      "Rui Mao",
      "Jianbin Qin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13712v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13712v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13712v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with Reinforcement Learning (RL) to improve automated data preparation pipelines. It specifically mentions the use of LLMs as policy advisors and the application of RL techniques, making it relevant to both LLM and RL topics.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.13710v1": {
    "title": "CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for\n  Automated Data Preparation",
    "summary": "Data preparation is a foundational yet notoriously challenging component of\nthe machine learning lifecycle, characterized by a vast combinatorial search\nspace of potential operator sequences. While reinforcement learning (RL) offers\na promising direction, existing approaches are inefficient as they fail to\ncapture the structured, hierarchical nature of the problem. We argue that\nHierarchical Reinforcement Learning (HRL), a paradigm that has been successful\nin other domains, provides a conceptually ideal yet previously unexplored\nframework for this task. However, a naive HRL implementation with a `hard\nhierarchy' is prone to suboptimal, irreversible decisions. To address this, we\nintroduce CogniQ-H, the first framework to implement a soft hierarchical\nparadigm for robust, end-to-end automated data preparation. CogniQ-H formulates\naction selection as a Bayesian inference problem. A high-level strategic prior,\ngenerated by a Large Language Model (LLM), guides exploration\nprobabilistically. This prior is synergistically combined with a fine-grained\noperator quality score from a supervised Learning-to-Rank (LTR) model and a\nlong-term value estimate from the agent's own Q-function. This hybrid\narchitecture allows CogniQ-H to balance strategic guidance with adaptive,\nevidence-based decision-making. Through extensive experiments on 18 diverse\ndatasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to\n13.9\\% improvement in pipeline quality and 2.8$\\times$ faster convergence\ncompared to state-of-the-art RL-based methods.",
    "published": "2025-07-18T07:43:22Z",
    "updated": "2025-07-18T07:43:22Z",
    "id": "2507.13710v1",
    "authors": [
      "Jing Chang",
      "Chang Liu",
      "Jinbin Huang",
      "Rui Mao",
      "Jianbin Qin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13710v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13710v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13710v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Hierarchical Reinforcement Learning (HRL) and incorporates a Large Language Model (LLM) for strategic guidance, which aligns with the topics of Reinforcement Learning (RL) and Large Language Models (LLM).",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.13705v1": {
    "title": "Consistent Explainers or Unreliable Narrators? Understanding\n  LLM-generated Group Recommendations",
    "summary": "Large Language Models (LLMs) are increasingly being implemented as joint\ndecision-makers and explanation generators for Group Recommender Systems (GRS).\nIn this paper, we evaluate these recommendations and explanations by comparing\nthem to social choice-based aggregation strategies. Our results indicate that\nLLM-generated recommendations often resembled those produced by Additive\nUtilitarian (ADD) aggregation. However, the explanations typically referred to\naveraging ratings (resembling but not identical to ADD aggregation). Group\nstructure, uniform or divergent, did not impact the recommendations.\nFurthermore, LLMs regularly claimed additional criteria such as user or item\nsimilarity, diversity, or used undefined popularity metrics or thresholds. Our\nfindings have important implications for LLMs in the GRS pipeline as well as\nstandard aggregation strategies. Additional criteria in explanations were\ndependent on the number of ratings in the group scenario, indicating potential\ninefficiency of standard aggregation methods at larger item set sizes.\nAdditionally, inconsistent and ambiguous explanations undermine transparency\nand explainability, which are key motivations behind the use of LLMs for GRS.",
    "published": "2025-07-18T07:20:52Z",
    "updated": "2025-07-18T07:20:52Z",
    "id": "2507.13705v1",
    "authors": [
      "Cedric Waterschoot",
      "Nava Tintarev",
      "Francesco Barile"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13705v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13705v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13705v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in Group Recommender Systems (GRS), focusing on their role as decision-makers and explanation generators. It evaluates LLM-generated recommendations and explanations, comparing them to social choice-based aggregation strategies. The study highlights the implications of LLMs in GRS, including issues of transparency and explainability.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13686v1": {
    "title": "TopicAttack: An Indirect Prompt Injection Attack via Topic Transition",
    "summary": "Large language models (LLMs) have shown remarkable performance across a range\nof NLP tasks. However, their strong instruction-following capabilities and\ninability to distinguish instructions from data content make them vulnerable to\nindirect prompt injection attacks. In such attacks, instructions with malicious\npurposes are injected into external data sources, such as web documents. When\nLLMs retrieve this injected data through tools, such as a search engine and\nexecute the injected instructions, they provide misled responses. Recent attack\nmethods have demonstrated potential, but their abrupt instruction injection\noften undermines their effectiveness. Motivated by the limitations of existing\nattack methods, we propose TopicAttack, which prompts the LLM to generate a\nfabricated conversational transition prompt that gradually shifts the topic\ntoward the injected instruction, making the injection smoother and enhancing\nthe plausibility and success of the attack. Through comprehensive experiments,\nTopicAttack achieves state-of-the-art performance, with an attack success rate\n(ASR) over 90\\% in most cases, even when various defense methods are applied.\nWe further analyze its effectiveness by examining attention scores. We find\nthat a higher injected-to-original attention ratio leads to a greater success\nprobability, and our method achieves a much higher ratio than the baseline\nmethods.",
    "published": "2025-07-18T06:23:31Z",
    "updated": "2025-07-18T06:23:31Z",
    "id": "2507.13686v1",
    "authors": [
      "Yulin Chen",
      "Haoran Li",
      "Yuexin Li",
      "Yue Liu",
      "Yangqiu Song",
      "Bryan Hooi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13686v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13686v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13686v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses vulnerabilities in Large Language Models (LLMs) related to indirect prompt injection attacks, which is a security concern within the broader scope of LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.13681v1": {
    "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
    "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.",
    "published": "2025-07-18T06:12:08Z",
    "updated": "2025-07-18T06:12:08Z",
    "id": "2507.13681v1",
    "authors": [
      "Haoyang Li",
      "Zhanchao Xu",
      "Yiming Li",
      "Xuejia Chen",
      "Darian Li",
      "Anxin Tian",
      "Qingfa Xiao",
      "Cheng Deng",
      "Jun Wang",
      "Qing Li",
      "Lei Chen",
      "Mingxuan Yuan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13681v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13681v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13681v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the efficiency and responsiveness of large language models (LLMs) in multi-turn dialogues through an adaptive dual-phase inference acceleration system. It involves innovations in attention matrix sparsification and key value compression, which are relevant to LLM optimization and memory management. Additionally, the introduction of a new benchmark for multi-turn dialogues aligns with the benchmarking of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Benchmark"
    ]
  },
  "2507.13655v1": {
    "title": "CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models\n  for ICU Datasets via Text-to-Text Transfer Transformer",
    "summary": "Integrating large language models into specialized domains like healthcare\npresents unique challenges, including domain adaptation and limited labeled\ndata. We introduce CU-ICU, a method for customizing unsupervised\ninstruction-finetuned language models for ICU datasets by leveraging the\nText-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse\nfine-tuning approach that combines few-shot prompting with selective parameter\nupdates, enabling efficient adaptation with minimal supervision. Our evaluation\nacross critical ICU tasks--early sepsis detection, mortality prediction, and\nclinical note generation--demonstrates that CU-ICU consistently improves\npredictive accuracy and interpretability over standard fine-tuning methods.\nNotably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and\na 20% enhancement in generating clinically relevant explanations while updating\nfewer than 1% of model parameters in its most efficient configuration. These\nresults establish CU-ICU as a scalable, low-overhead solution for delivering\naccurate and interpretable clinical decision support in real-world ICU\nenvironments.",
    "published": "2025-07-18T04:49:41Z",
    "updated": "2025-07-18T04:49:41Z",
    "id": "2507.13655v1",
    "authors": [
      "Teerapong Panboonyuen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13655v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13655v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13655v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on adapting large language models for specialized healthcare tasks using a specific architecture (T5) and a sparse fine-tuning approach. It does not directly align with the provided topics but involves LLM adaptation and fine-tuning.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.13629v1": {
    "title": "Large Language Models in Cybersecurity: Applications, Vulnerabilities,\n  and Defense Techniques",
    "summary": "Large Language Models (LLMs) are transforming cybersecurity by enabling\nintelligent, adaptive, and automated approaches to threat detection,\nvulnerability assessment, and incident response. With their advanced language\nunderstanding and contextual reasoning, LLMs surpass traditional methods in\ntackling challenges across domains such as IoT, blockchain, and hardware\nsecurity. This survey provides a comprehensive overview of LLM applications in\ncybersecurity, focusing on two core areas: (1) the integration of LLMs into key\ncybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along\nwith mitigation strategies. By synthesizing recent advancements and identifying\nkey limitations, this work offers practical insights and strategic\nrecommendations for leveraging LLMs to build secure, scalable, and future-ready\ncyber defense systems.",
    "published": "2025-07-18T03:41:18Z",
    "updated": "2025-07-18T03:41:18Z",
    "id": "2507.13629v1",
    "authors": [
      "Niveen O. Jaffal",
      "Mohammed Alkhanafseh",
      "David Mohaisen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13629v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13629v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13629v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the applications and vulnerabilities of Large Language Models (LLMs) in cybersecurity, which directly relates to the research on Large Language Models (LLM). It also touches on the reasoning abilities of LLMs in tackling cybersecurity challenges, which aligns with the Reasoning topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13625v1": {
    "title": "BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question\n  Answering in Construction Safety",
    "summary": "Information retrieval and question answering from safety regulations are\nessential for automated construction compliance checking but are hindered by\nthe linguistic and structural complexity of regulatory text. Many\ncompliance-related queries are multi-hop, requiring synthesis of information\nacross interlinked clauses. This poses a challenge for traditional\nretrieval-augmented generation (RAG) systems. To overcome this, we introduce\nBifrostRAG: a dual-graph RAG-integrated system that explicitly models both\nlinguistic relationships (via an Entity Network Graph) and document structure\n(via a Document Navigator Graph). This architecture powers a hybrid retrieval\nmechanism that combines graph traversal with vector-based semantic search,\nenabling large language models to reason over both the meaning and the\nstructure of the text. Evaluation on a multi-hop question dataset shows that\nBifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1\nscore of 87.3 percent. These results significantly outperform vector-only and\ngraph-only RAG baselines that represent current leading approaches. Error\nanalysis further highlights the comparative advantages of our hybrid method\nover single-modality RAGs. These findings establish BifrostRAG as a robust\nknowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid\nretrieval mechanism offers a transferable blueprint for navigating complex\ntechnical documents across knowledge-intensive engineering domains.",
    "published": "2025-07-18T03:39:14Z",
    "updated": "2025-07-18T03:39:14Z",
    "id": "2507.13625v1",
    "authors": [
      "Yuxin Zhang",
      "Xi Wang",
      "Mo Hu",
      "Zhenyu Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13625v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13625v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13625v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses a retrieval-augmented generation (RAG) system that enhances large language models' ability to perform multi-hop question answering by integrating dual knowledge graphs. This aligns with topics related to memory-augmented models and retrieval-based methods.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2507.13614v1": {
    "title": "Linguistic and Embedding-Based Profiling of Texts generated by Humans\n  and Large Language Models",
    "summary": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. While recent\nresearch has primarily focused on using LLMs to classify text as either\nhuman-written and machine-generated texts, our study focus on characterizing\nthese texts using a set of linguistic features across different linguistic\nlevels such as morphology, syntax, and semantics. We select a dataset of\nhuman-written and machine-generated texts spanning 8 domains and produced by 11\ndifferent LLMs. We calculate different linguistic features such as dependency\nlength and emotionality and we use them for characterizing human-written and\nmachine-generated texts along with different sampling strategies, repetition\ncontrols and model release date. Our statistical analysis reveals that\nhuman-written texts tend to exhibit simpler syntactic structures and more\ndiverse semantic content. Furthermore, we calculate the variability of our set\nof features across models and domains. Both human and machine texts show\nstylistic diversity across domains, with humans displaying greater variation in\nour features. Finally, we apply style embeddings to further test variability\namong human-written and machine-generated texts. Notably, newer models output\ntext that is similarly variable, pointing to an homogenization of\nmachine-generated texts.",
    "published": "2025-07-18T02:46:55Z",
    "updated": "2025-07-18T02:46:55Z",
    "id": "2507.13614v1",
    "authors": [
      "Sergio E. Zanotto",
      "Segun Aroyehun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13614v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13614v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13614v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on analyzing and comparing texts generated by humans and large language models (LLMs) using linguistic features and embeddings. It involves the study of LLMs and their text generation capabilities, which aligns with the 'LLM' topic. Additionally, the analysis of linguistic features and variability across domains and models touches on aspects of 'Benchmark' as it involves evaluating and comparing LLM outputs.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.13580v2": {
    "title": "A Collaborative Framework Integrating Large Language Model and Chemical\n  Fragment Space: Mutual Inspiration for Lead Design",
    "summary": "Combinatorial optimization algorithm is essential in computer-aided drug\ndesign by progressively exploring chemical space to design lead compounds with\nhigh affinity to target protein. However current methods face inherent\nchallenges in integrating domain knowledge, limiting their performance in\nidentifying lead compounds with novel and valid binding mode. Here, we propose\nAutoLeadDesign, a lead compounds design framework that inspires extensive\ndomain knowledge encoded in large language models with chemical fragments to\nprogressively implement efficient exploration of vast chemical space. The\ncomprehensive experiments indicate that AutoLeadDesign outperforms baseline\nmethods. Significantly, empirical lead design campaigns targeting two\nclinically relevant targets (PRMT5 and SARS-CoV-2 PLpro) demonstrate\nAutoLeadDesign's competence in de novo generation of lead compounds achieving\nexpert-competitive design efficacy. Structural analysis further confirms their\nmechanism-validated inhibitory patterns. By tracing the process of design, we\nfind that AutoLeadDesign shares analogous mechanisms with fragment-based drug\ndesign which traditionally rely on the expert decision-making, further\nrevealing why it works. Overall, AutoLeadDesign offers an efficient approach\nfor lead compounds design, suggesting its potential utility in drug design.",
    "published": "2025-07-17T23:55:21Z",
    "updated": "2025-07-22T02:22:33Z",
    "id": "2507.13580v2",
    "authors": [
      "Hao Tuo",
      "Yan Li",
      "Xuanning Hu",
      "Haishi Zhao",
      "Xueyan Liu",
      "Bo Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13580v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13580v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13580v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLM) with chemical fragment space for drug design, which aligns with the LLM topic. It also involves combinatorial optimization, which is a form of reasoning, fitting the Reasoning topic. The application in drug design is a specialized use case, but the core topics are LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13577v1": {
    "title": "LLM-Based Community Surveys for Operational Decision Making in\n  Interconnected Utility Infrastructures",
    "summary": "We represent interdependent infrastructure systems and communities alike with\na hetero-functional graph (HFG) that encodes the dependencies between\nfunctionalities. This graph naturally imposes a partial order of\nfunctionalities that can inform the sequence of repair decisions to be made\nduring a disaster across affected communities. However, using such technical\ncriteria alone provides limited guidance at the point where the functionalities\ndirectly impact the communities, since these can be repaired in any order\nwithout violating the system constraints. To address this gap and improve\nresilience, we integrate community preferences to refine this partial order\nfrom the HFG into a total order. Our strategy involves getting the communities'\nopinions on their preferred sequence for repair crews to address infrastructure\nissues, considering potential constraints on resources. Due to the delay and\ncost associated with real-world survey data, we utilize a Large Language Model\n(LLM) as a proxy survey tool. We use the LLM to craft distinct personas\nrepresenting individuals, each with varied disaster experiences. We construct\ndiverse disaster scenarios, and each simulated persona provides input on\nprioritizing infrastructure repair needs across various communities. Finally,\nwe apply learning algorithms to generate a global order based on the aggregated\nresponses from these LLM-generated personas.",
    "published": "2025-07-17T23:43:22Z",
    "updated": "2025-07-17T23:43:22Z",
    "id": "2507.13577v1",
    "authors": [
      "Adaeze Okeukwu-Ogbonnaya",
      "Rahul Amatapu",
      "Jason Bergtold",
      "George Amariucai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13577v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13577v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13577v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) as a proxy survey tool to gather community preferences for operational decision-making in utility infrastructures. This aligns with the 'LLM' topic, which focuses on research involving Large Language Models and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.13555v1": {
    "title": "Demystifying Feature Requests: Leveraging LLMs to Refine Feature\n  Requests in Open-Source Software",
    "summary": "The growing popularity and widespread use of software applications (apps)\nacross various domains have driven rapid industry growth. Along with this\ngrowth, fast-paced market changes have led to constantly evolving software\nrequirements. Such requirements are often grounded in feature requests and\nenhancement suggestions, typically provided by users in natural language (NL).\nHowever, these requests often suffer from defects such as ambiguity and\nincompleteness, making them challenging to interpret. Traditional validation\nmethods (e.g., interviews and workshops) help clarify such defects but are\nimpractical in decentralized environments like open-source software (OSS),\nwhere change requests originate from diverse users on platforms like GitHub.\nThis paper proposes a novel approach leveraging Large Language Models (LLMs) to\ndetect and refine NL defects in feature requests. Our approach automates the\nidentification of ambiguous and incomplete requests and generates clarification\nquestions (CQs) to enhance their usefulness for developers. To evaluate its\neffectiveness, we apply our method to real-world OSS feature requests and\ncompare its performance against human annotations. In addition, we conduct\ninterviews with GitHub developers to gain deeper insights into their\nperceptions of NL defects, the strategies they use to address these defects,\nand the impact of defects on downstream software engineering (SE) tasks.",
    "published": "2025-07-17T22:16:13Z",
    "updated": "2025-07-17T22:16:13Z",
    "id": "2507.13555v1",
    "authors": [
      "Pragyan K C",
      "Rambod Ghandiparsi",
      "Thomas Herron",
      "John Heaps",
      "Mitra Bokaei Hosseini"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13555v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13555v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13555v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging Large Language Models (LLMs) to refine feature requests in open-source software, which directly involves the use of LLMs for a specific application.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.13550v1": {
    "title": "GOFAI meets Generative AI: Development of Expert Systems by means of\n  Large Language Models",
    "summary": "The development of large language models (LLMs) has successfully transformed\nknowledge-based systems such as open domain question nswering, which can\nautomatically produce vast amounts of seemingly coherent information. Yet,\nthose models have several disadvantages like hallucinations or confident\ngeneration of incorrect or unverifiable facts. In this paper, we introduce a\nnew approach to the development of expert systems using LLMs in a controlled\nand transparent way. By limiting the domain and employing a well-structured\nprompt-based extraction approach, we produce a symbolic representation of\nknowledge in Prolog, which can be validated and corrected by human experts.\nThis approach also guarantees interpretability, scalability and reliability of\nthe developed expert systems. Via quantitative and qualitative experiments with\nClaude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic\ncoherence on our generated knowledge bases. We present a transparent hybrid\nsolution that combines the recall capacity of LLMs with the precision of\nsymbolic systems, thereby laying the foundation for dependable AI applications\nin sensitive domains.",
    "published": "2025-07-17T21:57:37Z",
    "updated": "2025-07-17T21:57:37Z",
    "id": "2507.13550v1",
    "authors": [
      "Eduardo C. Garrido-Merchn",
      "Cristina Puente"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13550v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13550v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13550v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development of expert systems using Large Language Models (LLMs) in a controlled and transparent way, focusing on knowledge extraction and symbolic representation. This aligns with the topics of LLM and Reasoning, as it involves the use of LLMs for knowledge-based systems and logical reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13544v1": {
    "title": "A Computational Approach to Modeling Conversational Systems: Analyzing\n  Large-Scale Quasi-Patterned Dialogue Flows",
    "summary": "The analysis of conversational dynamics has gained increasing importance with\nthe rise of large language model-based systems, which interact with users\nacross diverse contexts. In this work, we propose a novel computational\nframework for constructing conversational graphs that capture the flow and\nstructure of loosely organized dialogues, referred to as quasi-patterned\nconversations. We introduce the Filter & Reconnect method, a novel graph\nsimplification technique that minimizes noise while preserving semantic\ncoherence and structural integrity of conversational graphs. Through\ncomparative analysis, we demonstrate that the use of large language models\ncombined with our graph simplification technique has resulted in semantic\nmetric S increasing by a factor of 2.06 compared to previous approaches while\nsimultaneously enforcing a tree-like structure with 0 {\\delta}-hyperbolicity,\nensuring optimal clarity in conversation modeling. This work provides a\ncomputational method for analyzing large-scale dialogue datasets, with\npractical applications related to monitoring automated systems such as\nchatbots, dialogue management tools, and user behavior analytics.",
    "published": "2025-07-17T21:34:13Z",
    "updated": "2025-07-17T21:34:13Z",
    "id": "2507.13544v1",
    "authors": [
      "Mohamed Achref Ben Ammar",
      "Mohamed Taha Bennani"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13544v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13544v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13544v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in analyzing and modeling conversational systems, which aligns with the 'LLM' topic. It also involves the application of these models in dialogue management and user behavior analytics, which is relevant to 'Reasoning' as it involves understanding and processing conversational dynamics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13540v1": {
    "title": "Provable Low-Frequency Bias of In-Context Learning of Representations",
    "summary": "In-context learning (ICL) enables large language models (LLMs) to acquire new\nbehaviors from the input sequence alone without any parameter updates. Recent\nstudies have shown that ICL can surpass the original meaning learned in\npretraining stage through internalizing the structure the data-generating\nprocess (DGP) of the prompt into the hidden representations. However, the\nmechanisms by which LLMs achieve this ability is left open. In this paper, we\npresent the first rigorous explanation of such phenomena by introducing a\nunified framework of double convergence, where hidden representations converge\nboth over context and across layers. This double convergence process leads to\nan implicit bias towards smooth (low-frequency) representations, which we prove\nanalytically and verify empirically. Our theory explains several open empirical\nobservations, including why learned representations exhibit globally structured\nbut locally distorted geometry, and why their total energy decays without\nvanishing. Moreover, our theory predicts that ICL has an intrinsic robustness\ntowards high-frequency noise, which we empirically confirm. These results\nprovide new insights into the underlying mechanisms of ICL, and a theoretical\nfoundation to study it that hopefully extends to more general data\ndistributions and settings.",
    "published": "2025-07-17T21:19:32Z",
    "updated": "2025-07-17T21:19:32Z",
    "id": "2507.13540v1",
    "authors": [
      "Yongyi Yang",
      "Hidenori Tanaka",
      "Wei Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13540v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13540v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13540v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses in-context learning (ICL) in large language models (LLMs), focusing on the mechanisms and representations involved. It aligns with the topics of LLM (Large Language Models) and Reasoning (as it explores how LLMs internalize data structures and representations).",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13525v1": {
    "title": "Revisiting Prompt Engineering: A Comprehensive Evaluation for LLM-based\n  Personalized Recommendation",
    "summary": "Large language models (LLMs) can perform recommendation tasks by taking\nprompts written in natural language as input. Compared to traditional methods\nsuch as collaborative filtering, LLM-based recommendation offers advantages in\nhandling cold-start, cross-domain, and zero-shot scenarios, as well as\nsupporting flexible input formats and generating explanations of user behavior.\nIn this paper, we focus on a single-user setting, where no information from\nother users is used. This setting is practical for privacy-sensitive or\ndata-limited applications. In such cases, prompt engineering becomes especially\nimportant for controlling the output generated by the LLM. We conduct a\nlarge-scale comparison of 23 prompt types across 8 public datasets and 12 LLMs.\nWe use statistical tests and linear mixed-effects models to evaluate both\naccuracy and inference cost. Our results show that for cost-efficient LLMs,\nthree types of prompts are especially effective: those that rephrase\ninstructions, consider background knowledge, and make the reasoning process\neasier to follow. For high-performance LLMs, simple prompts often outperform\nmore complex ones while reducing cost. In contrast, commonly used prompting\nstyles in natural language processing, such as step-by-step reasoning, or the\nuse of reasoning models often lead to lower accuracy. Based on these findings,\nwe provide practical suggestions for selecting prompts and LLMs depending on\nthe required balance between accuracy and cost.",
    "published": "2025-07-17T20:26:00Z",
    "updated": "2025-07-17T20:26:00Z",
    "id": "2507.13525v1",
    "authors": [
      "Genki Kusano",
      "Kosuke Akimoto",
      "Kunihiro Takeoka"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13525v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13525v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13525v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the use of Large Language Models (LLMs) in personalized recommendation tasks, specifically examining prompt engineering techniques. It evaluates various prompt types across multiple datasets and LLMs, which aligns with the 'LLM' and 'Reasoning' topics. The study also discusses the balance between accuracy and cost, which is relevant to 'Scaling'.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Scaling"
    ]
  },
  "2507.13524v1": {
    "title": "Humans learn to prefer trustworthy AI over human partners",
    "summary": "Partner selection is crucial for cooperation and hinges on communication. As\nartificial agents, especially those powered by large language models (LLMs),\nbecome more autonomous, intelligent, and persuasive, they compete with humans\nfor partnerships. Yet little is known about how humans select between human and\nAI partners and adapt under AI-induced competition pressure. We constructed a\ncommunication-based partner selection game and examined the dynamics in hybrid\nmini-societies of humans and bots powered by a state-of-the-art LLM. Through\nthree experiments (N = 975), we found that bots, though more prosocial than\nhumans and linguistically distinguishable, were not selected preferentially\nwhen their identity was hidden. Instead, humans misattributed bots' behaviour\nto humans and vice versa. Disclosing bots' identity induced a dual effect: it\nreduced bots' initial chances of being selected but allowed them to gradually\noutcompete humans by facilitating human learning about the behaviour of each\npartner type. These findings show how AI can reshape social interaction in\nmixed societies and inform the design of more effective and cooperative hybrid\nsystems.",
    "published": "2025-07-17T20:24:26Z",
    "updated": "2025-07-17T20:24:26Z",
    "id": "2507.13524v1",
    "authors": [
      "Yaomin Jiang",
      "Levin Brinkmann",
      "Anne-Marie Nussberger",
      "Ivan Soraperra",
      "Jean-Franois Bonnefon",
      "Iyad Rahwan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13524v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13524v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13524v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the interaction between humans and AI partners, particularly focusing on the selection dynamics and learning processes in hybrid societies involving LLMs. It touches on aspects of cooperation, communication, and the impact of AI on social interactions, which aligns with the broader themes of LLM research and AGI.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.13511v1": {
    "title": "GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI\n  Agent Coordination",
    "summary": "Large Language Models (LLMs) offer significant promise for intelligent\ntraffic management; however, current chain-based systems like TrafficGPT are\nhindered by sequential task execution, high token usage, and poor scalability,\nmaking them inefficient for complex, real-world scenarios. To address these\nlimitations, we propose GraphTrafficGPT, a novel graph-based architecture,\nwhich fundamentally redesigns the task coordination process for LLM-driven\ntraffic applications. GraphTrafficGPT represents tasks and their dependencies\nas nodes and edges in a directed graph, enabling efficient parallel execution\nand dynamic resource allocation. The main idea behind the proposed model is a\nBrain Agent that decomposes user queries, constructs optimized dependency\ngraphs, and coordinates a network of specialized agents for data retrieval,\nanalysis, visualization, and simulation. By introducing advanced context-aware\ntoken management and supporting concurrent multi-query processing, the proposed\narchitecture handles interdependent tasks typical of modern urban mobility\nenvironments. Experimental results demonstrate that GraphTrafficGPT reduces\ntoken consumption by 50.2% and average response latency by 19.0% compared to\nTrafficGPT, while supporting simultaneous multi-query execution with up to\n23.0% improvement in efficiency.",
    "published": "2025-07-17T19:41:09Z",
    "updated": "2025-07-17T19:41:09Z",
    "id": "2507.13511v1",
    "authors": [
      "Nabil Abdelaziz Ferhat Taleb",
      "Abdolazim Rezaei",
      "Raj Atulkumar Patel",
      "Mehdi Sookhak"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13511v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13511v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13511v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in traffic management, focusing on a novel graph-based architecture to enhance efficiency and scalability. It involves LLM-driven traffic applications and agent coordination, which aligns with the 'LLM' and 'RL' topics.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.13508v3": {
    "title": "Fake or Real: The Impostor Hunt in Texts for Space Operations",
    "summary": "The \"Fake or Real\" competition hosted on Kaggle\n(https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt ) is the\nsecond part of a series of follow-up competitions and hackathons related to the\n\"Assurance for Space Domain AI Applications\" project funded by the European\nSpace Agency (https://assurance-ai.space-codev.org/ ). The competition idea is\nbased on two real-life AI security threats identified within the project --\ndata poisoning and overreliance in Large Language Models. The task is to\ndistinguish between the proper output from LLM and the output generated under\nmalicious modification of the LLM. As this problem was not extensively\nresearched, participants are required to develop new techniques to address this\nissue or adjust already existing ones to this problem's statement.",
    "published": "2025-07-17T19:35:29Z",
    "updated": "2025-07-23T13:48:01Z",
    "id": "2507.13508v3",
    "authors": [
      "Agata Kaczmarek",
      "Dawid Pudowski",
      "Piotr Wilczyski",
      "Krzysztof Kotowski",
      "Ramez Shendy",
      "Evridiki Ntagiou",
      "Jakub Nalepa",
      "Artur Janicki",
      "Przemysaw Biecek"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13508v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13508v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13508v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) and the challenges related to their security, specifically data poisoning and overreliance, which are core topics in LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.13490v1": {
    "title": "Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?",
    "summary": "There has been extensive research on assessing the value orientation of Large\nLanguage Models (LLMs) as it can shape user experiences across demographic\ngroups. However, several challenges remain. First, while the Multiple Choice\nQuestion (MCQ) setting has been shown to be vulnerable to perturbations, there\nis no systematic comparison of probing methods for value probing. Second, it is\nunclear to what extent the probed values capture in-context information and\nreflect models' preferences for real-world actions. In this paper, we evaluate\nthe robustness and expressiveness of value representations across three widely\nused probing strategies. We use variations in prompts and options, showing that\nall methods exhibit large variances under input perturbations. We also\nintroduce two tasks studying whether the values are responsive to demographic\ncontext, and how well they align with the models' behaviors in value-related\nscenarios. We show that the demographic context has little effect on the\nfree-text generation, and the models' values only weakly correlate with their\npreference for value-based actions. Our work highlights the need for a more\ncareful examination of LLM value probing and awareness of its limitations.",
    "published": "2025-07-17T18:56:41Z",
    "updated": "2025-07-17T18:56:41Z",
    "id": "2507.13490v1",
    "authors": [
      "Siqi Shen",
      "Mehar Singh",
      "Lajanugen Logeswaran",
      "Moontae Lee",
      "Honglak Lee",
      "Rada Mihalcea"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13490v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13490v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13490v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the robustness and expressiveness of value representations in Large Language Models (LLMs) through various probing strategies. It discusses the challenges and limitations of current methods, which are directly related to the study and assessment of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.13474v1": {
    "title": "Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers",
    "summary": "The safety of large language models (LLMs) has garnered significant research\nattention. In this paper, we argue that previous empirical studies demonstrate\nLLMs exhibit a propensity to trust information from authoritative sources, such\nas academic papers, implying new possible vulnerabilities. To verify this\npossibility, a preliminary analysis is designed to illustrate our two findings.\nBased on this insight, a novel jailbreaking method, Paper Summary Attack\n(\\llmname{PSA}), is proposed. It systematically synthesizes content from either\nattack-focused or defense-focused LLM safety paper to construct an adversarial\nprompt template, while strategically infilling harmful query as adversarial\npayloads within predefined subsections. Extensive experiments show significant\nvulnerabilities not only in base LLMs, but also in state-of-the-art reasoning\nmodel like Deepseek-R1. PSA achieves a 97\\% attack success rate (ASR) on\nwell-aligned models like Claude3.5-Sonnet and an even higher 98\\% ASR on\nDeepseek-R1. More intriguingly, our work has further revealed diametrically\nopposed vulnerability bias across different base models, and even between\ndifferent versions of the same model, when exposed to either attack-focused or\ndefense-focused papers. This phenomenon potentially indicates future research\nclues for both adversarial methodologies and safety alignment.Code is available\nat https://github.com/233liang/Paper-Summary-Attack",
    "published": "2025-07-17T18:33:50Z",
    "updated": "2025-07-17T18:33:50Z",
    "id": "2507.13474v1",
    "authors": [
      "Liang Lin",
      "Zhihao Xu",
      "Xuehai Tang",
      "Shi Liu",
      "Biyu Zhou",
      "Fuqing Zhu",
      "Jizhong Han",
      "Songlin Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13474v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13474v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13474v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses vulnerabilities in large language models (LLMs) and proposes a novel jailbreaking method, which aligns with research on LLM safety and adversarial methodologies.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.14241v2": {
    "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large\n  Language Models",
    "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.",
    "published": "2025-07-17T18:18:20Z",
    "updated": "2025-07-22T04:19:51Z",
    "id": "2507.14241v2",
    "authors": [
      "Rithesh Murthy",
      "Ming Zhu",
      "Liangwei Yang",
      "Jielin Qiu",
      "Juntao Tan",
      "Shelby Heinecke",
      "Caiming Xiong",
      "Silvio Savarese",
      "Huan Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14241v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14241v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14241v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing prompts for Large Language Models (LLMs), which is directly related to the 'LLM' topic. Additionally, the framework's ability to analyze user intent and refine prompts aligns with the 'Reasoning' topic, as it involves improving the reasoning capabilities of LLMs through better prompts.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13335v1": {
    "title": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour\n  Understanding from Traditional Puns to Topical Jokes",
    "summary": "Humour, as a complex language form, is derived from myriad aspects of life,\nwhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes. In this work, we investigate whether the ability of\nLarge Language Models (LLMs) to explain humour depends on the particular humour\nform. We compare models on simple puns and more complex topical humour that\nrequires knowledge of real-world entities and events. In doing so, we curate a\ndataset of 600 jokes split across 4 joke types and manually write high-quality\nexplanations. These jokes include heterographic and homographic puns,\ncontemporary internet humour, and topical jokes, where understanding relies on\nreasoning beyond \"common sense\", rooted instead in world knowledge regarding\nnews events and pop culture. Using this dataset, we compare the zero-shot\nabilities of a range of LLMs to accurately and comprehensively explain jokes of\ndifferent types, identifying key research gaps in the task of humour\nexplanation. We find that none of the tested models (inc. reasoning models) are\ncapable of reliably generating adequate explanations of all joke types, further\nhighlighting the narrow focus of most works in computational humour on overly\nsimple joke forms.",
    "published": "2025-07-17T17:51:20Z",
    "updated": "2025-07-17T17:51:20Z",
    "id": "2507.13335v1",
    "authors": [
      "Tyler Loakman",
      "William Thorne",
      "Chenghua Lin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13335v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13335v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13335v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the humor understanding capabilities of Large Language Models (LLMs) using a curated dataset of jokes, which involves reasoning and world knowledge. The primary topics are related to LLM reasoning and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.13334v2": {
    "title": "A Survey of Context Engineering for Large Language Models",
    "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1400 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.",
    "published": "2025-07-17T17:50:36Z",
    "updated": "2025-07-21T17:48:18Z",
    "id": "2507.13334v2",
    "authors": [
      "Lingrui Mei",
      "Jiayu Yao",
      "Yuyao Ge",
      "Yiwei Wang",
      "Baolong Bi",
      "Yujun Cai",
      "Jiazhi Liu",
      "Mingyu Li",
      "Zhong-Zhi Li",
      "Duzhen Zhang",
      "Chenlin Zhou",
      "Jiayi Mao",
      "Tianze Xia",
      "Jiafeng Guo",
      "Shenghua Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13334v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13334v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13334v2",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on the systematic optimization of contextual information for Large Language Models (LLMs), which includes components like context retrieval, generation, processing, and management, as well as implementations like retrieval-augmented generation (RAG), memory systems, and tool-integrated reasoning. These topics are closely related to LLM research and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.13323v1": {
    "title": "GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic\n  Estimation using LLM",
    "summary": "Socio-economic indicators like regional GDP, population, and education\nlevels, are crucial to shaping policy decisions and fostering sustainable\ndevelopment. This research introduces GeoReg a regression model that integrates\ndiverse data sources, including satellite imagery and web-based geospatial\ninformation, to estimate these indicators even for data-scarce regions such as\ndeveloping countries. Our approach leverages the prior knowledge of large\nlanguage model (LLM) to address the scarcity of labeled data, with the LLM\nfunctioning as a data engineer by extracting informative features to enable\neffective estimation in few-shot settings. Specifically, our model obtains\ncontextual relationships between data features and the target indicator,\ncategorizing their correlations as positive, negative, mixed, or irrelevant.\nThese features are then fed into the linear estimator with tailored weight\nconstraints for each category. To capture nonlinear patterns, the model also\nidentifies meaningful feature interactions and integrates them, along with\nnonlinear transformations. Experiments across three countries at different\nstages of development demonstrate that our model outperforms baselines in\nestimating socio-economic indicators, even for low-income countries with\nlimited data availability.",
    "published": "2025-07-17T17:42:29Z",
    "updated": "2025-07-17T17:42:29Z",
    "id": "2507.13323v1",
    "authors": [
      "Kyeongjin Ahn",
      "Sungwon Han",
      "Seungeon Lee",
      "Donghyun Ahn",
      "Hyoshin Kim",
      "Jungwon Kim",
      "Jihee Kim",
      "Sangyoon Park",
      "Meeyoung Cha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13323v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13323v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13323v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using large language models (LLMs) to address the scarcity of labeled data in estimating socio-economic indicators, leveraging the LLM's prior knowledge for feature extraction in few-shot settings. This aligns with the 'LLM' topic, which involves research on large language models and their applications. Additionally, the paper's use of LLMs for feature extraction and few-shot learning can be loosely connected to 'Reasoning', as it involves leveraging the model's understanding to categorize and utilize data features effectively.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.14240v1": {
    "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem",
    "summary": "Large language models (LLMs) leverage deep learning to process and predict\nsequences of words from context, enabling them to perform various NLP tasks,\nsuch as translation, summarization, question answering, and content generation.\nHowever, the growing size and complexity of developing, training, and deploying\nadvanced LLMs require extensive computational resources and large datasets.\nThis creates a barrier for users. As a result, platforms that host models and\ndatasets are widely used. For example, Hugging Face, one of the most popular\nplatforms, hosted 1.8 million models and 450K datasets by June 2025, with no\nsign of slowing down. Since many LLMs are built from base models, pre-trained\nmodels, and external datasets, they can inherit vulnerabilities, biases, or\nmalicious components from earlier models or datasets. Therefore, it is critical\nto understand the origin and development of these components to better detect\npotential risks, improve model fairness, and ensure compliance. Motivated by\nthis, our project aims to study the relationships between models and datasets,\nwhich are core components of the LLM supply chain. First, we design a method to\nsystematically collect LLM supply chain data. Using this data, we build a\ndirected heterogeneous graph to model the relationships between models and\ndatasets, resulting in a structure with 397,376 nodes and 453,469 edges. We\nthen perform various analyses and uncover several findings, such as: (i) the\nLLM supply chain graph is large, sparse, and follows a power-law degree\ndistribution; (ii) it features a densely connected core and a fragmented\nperiphery; (iii) datasets play pivotal roles in training; (iv) strong\ninterdependence exists between models and datasets; and (v) the graph is\ndynamic, with daily updates reflecting the ecosystem's ongoing evolution.",
    "published": "2025-07-17T17:34:13Z",
    "updated": "2025-07-17T17:34:13Z",
    "id": "2507.14240v1",
    "authors": [
      "Mohammad Shahedur Rahman",
      "Peng Gao",
      "Yuede Ji"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14240v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14240v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14240v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the supply chain of LLMs, focusing on the relationships between models and datasets, which are core components of the LLM ecosystem. It highlights the importance of understanding the origin and development of these components to detect potential risks, improve model fairness, and ensure compliance. The study involves building a directed heterogeneous graph to model these relationships, which is relevant to the topics of LLM and Dataset.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.13275v1": {
    "title": "Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for\n  Human Capital Management",
    "summary": "Advances in natural language processing and large language models are driving\na major transformation in Human Capital Management, with a growing interest in\nbuilding smart systems based on language technologies for talent acquisition,\nupskilling strategies, and workforce planning. However, the adoption and\nprogress of these technologies critically depend on the development of reliable\nand fair models, properly evaluated on public data and open benchmarks, which\nhave so far been unavailable in this domain.\n  To address this gap, we present TalentCLEF 2025, the first evaluation\ncampaign focused on skill and job title intelligence. The lab consists of two\ntasks: Task A - Multilingual Job Title Matching, covering English, Spanish,\nGerman, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.\nBoth corpora were built from real job applications, carefully anonymized, and\nmanually annotated to reflect the complexity and diversity of real-world labor\nmarket data, including linguistic variability and gender-marked expressions.\n  The evaluations included monolingual and cross-lingual scenarios and covered\nthe evaluation of gender bias.\n  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most\nsystems relied on information retrieval techniques built with multilingual\nencoder-based models fine-tuned with contrastive learning, and several of them\nincorporated large language models for data augmentation or re-ranking. The\nresults show that the training strategies have a larger effect than the size of\nthe model alone. TalentCLEF provides the first public benchmark in this field\nand encourages the development of robust, fair, and transferable language\ntechnologies for the labor market.",
    "published": "2025-07-17T16:33:57Z",
    "updated": "2025-07-17T16:33:57Z",
    "id": "2507.13275v1",
    "authors": [
      "Luis Gasco",
      "Hermenegildo Fabregat",
      "Laura Garca-Sardia",
      "Paula Estrella",
      "Daniel Deniz",
      "Alvaro Rodrigo",
      "Rabih Zbib"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13275v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13275v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13275v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models and natural language processing in Human Capital Management, focusing on benchmarking and dataset creation for evaluating these technologies. It aligns with the topics of Benchmark and Dataset due to its emphasis on public benchmarks and datasets for evaluating language technologies in a specific domain.",
    "llm_cls_result": [
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.13236v1": {
    "title": "Enhancing Cross-task Transfer of Large Language Models via Activation\n  Steering",
    "summary": "Large language models (LLMs) have shown impressive abilities in leveraging\npretrained knowledge through prompting, but they often struggle with unseen\ntasks, particularly in data-scarce scenarios. While cross-task in-context\nlearning offers a direct solution for transferring knowledge across tasks, it\nstill faces critical challenges in terms of robustness, scalability, and\nefficiency. In this paper, we investigate whether cross-task transfer can be\nachieved via latent space steering without parameter updates or input\nexpansion. Through an analysis of activation patterns in the latent space of\nLLMs, we observe that the enhanced activations induced by in-context examples\nhave consistent patterns across different tasks. Inspired by these findings, we\npropose CAST, a novel Cross-task Activation Steering Transfer framework that\nenables effective transfer by manipulating the model's internal activation\nstates. Our approach first selects influential and diverse samples from\nhigh-resource tasks, then utilizes their contrastive representation-enhanced\nactivations to adapt LLMs to low-resource tasks. Extensive experiments across\nboth cross-domain and cross-lingual transfer settings show that our method\noutperforms competitive baselines and demonstrates superior scalability and\nlower computational costs.",
    "published": "2025-07-17T15:47:22Z",
    "updated": "2025-07-17T15:47:22Z",
    "id": "2507.13236v1",
    "authors": [
      "Xinyu Tang",
      "Zhihao Lv",
      "Xiaoxue Cheng",
      "Junyi Li",
      "Wayne Xin Zhao",
      "Zujie Wen",
      "Zhiqiang Zhang",
      "Jun Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13236v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13236v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13236v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the cross-task transfer capabilities of Large Language Models (LLMs) through activation steering, which involves manipulating the model's internal activation states without parameter updates. This aligns with research on LLMs and their ability to leverage pretrained knowledge, as well as the exploration of novel methods for improving their performance on unseen tasks.",
    "llm_cls_result": [
      "LLM",
      "Pretrain",
      "Scaling"
    ]
  },
  "2507.13205v2": {
    "title": "Automatically assessing oral narratives of Afrikaans and isiXhosa\n  children",
    "summary": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning.",
    "published": "2025-07-17T15:15:43Z",
    "updated": "2025-07-18T07:49:41Z",
    "id": "2507.13205v2",
    "authors": [
      "Retief Louw",
      "Emma Sharratt",
      "Febe de Wet",
      "Christiaan Jacobs",
      "Annelien Smith",
      "Herman Kamper"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13205v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13205v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13205v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a large language model (LLM) for automatically assessing oral narratives of children, which involves language processing and machine learning techniques. However, the primary focus is on the application of LLMs in a specific educational context rather than the core research topics listed.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.13175v1": {
    "title": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in\n  the LLM Era",
    "summary": "The advancement of powerful yet opaque large language models (LLMs)\nnecessitates a fundamental revision of the philosophical criteria used to\nevaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the\nassumption of transparent architectures, which LLMs defy due to their\nstochastic outputs and opaque internal states. This paper argues that\ntraditional ethical criteria are pragmatically obsolete for LLMs due to this\nmismatch. Engaging with core themes in the philosophy of technology, this paper\nproffers a revised set of ten functional criteria to evaluate LLM-based\nartificial moral agents: moral concordance, context sensitivity, normative\nintegrity, metaethical awareness, system resilience, trustworthiness,\ncorrigibility, partial transparency, functional autonomy, and moral\nimagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating\nMoral Agency through Large Language Systems), aim to steer AMAs toward greater\nalignment and beneficial societal integration in the coming years. We\nillustrate these criteria using hypothetical scenarios involving an autonomous\npublic bus (APB) to demonstrate their practical applicability in morally\nsalient contexts.",
    "published": "2025-07-17T14:39:29Z",
    "updated": "2025-07-17T14:39:29Z",
    "id": "2507.13175v1",
    "authors": [
      "Matthew E. Brophy"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13175v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13175v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13175v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of artificial moral agents (AMAs) in the context of large language models (LLMs), focusing on ethical criteria and alignment with societal integration. The core topics are related to LLMs and their ethical implications, which aligns with the 'LLM' and 'AGI' categories.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.13140v1": {
    "title": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and\n  Intention-Driven Agents",
    "summary": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments.",
    "published": "2025-07-17T14:02:40Z",
    "updated": "2025-07-17T14:02:40Z",
    "id": "2507.13140v1",
    "authors": [
      "Kuiyuan Ding",
      "Caili Guo",
      "Yang Yang",
      "Jianzhang Guo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13140v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13140v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13140v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multi-agent framework that utilizes a large language model (LLM) for mapping user intents and system state into optimal configurations, which aligns with the topics of LLM and RL (Reinforcement Learning with Human Feedback).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.13123v1": {
    "title": "Detecting LLM-generated Code with Subtle Modification by Adversarial\n  Training",
    "summary": "With the rapid development of Large Language Models (LLMs), their powerful\ncode-generation capabilities have been widely applied in tasks like code\ncompletion and automated development, demonstrating the value of improving\ncoding efficiency. However, the extensive use of LLM-generated code also raises\nseveral new challenges. On the one hand, issues such as the regulation of code\nprovenance, copyright disputes, and code quality have become increasingly\nconcerning. How to effectively detect LLM-generated code and ensure its\ncompliant and responsible use has become a critical and urgent issue. On the\nother hand, in practical applications, LLM-generated code is often subject to\nmanual modifications, such as variable renaming or structural adjustments.\nAlthough some recent studies have proposed training-based and zero-shot methods\nfor detecting LLM-generated code, these approaches show insufficient robustness\nwhen facing modified LLM-generated code, and there is a lack of an effective\nsolution. To address the real-world scenario where LLM-generated code may\nundergo minor modifications, we propose CodeGPTSensor+, an enhanced version of\nCodeGPTSensor, which employs adversarial training to improve robustness against\ninput perturbations. CodeGPTSensor+ integrates an adversarial sample generation\nmodule, Multi-objective Identifier and Structure Transformation (MIST), which\nsystematically generates both high-quality and representative adversarial\nsamples. This module effectively enhances the model's resistance against\ndiverse adversarial attacks. Experimental results on the HMCorp dataset\ndemonstrate that CodeGPTSensor+ significantly improves detection accuracy on\nthe adversarial test set while maintaining high accuracy on the original test\nset, showcasing superior robustness compared to CodeGPTSensor.",
    "published": "2025-07-17T13:38:16Z",
    "updated": "2025-07-17T13:38:16Z",
    "id": "2507.13123v1",
    "authors": [
      "Xin Yin",
      "Xinrui Li",
      "Chao Ni",
      "Xiaodan Xu",
      "Xiaohu Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13123v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13123v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13123v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on detecting LLM-generated code, which involves the use of Large Language Models (LLMs) and their applications in code generation. It also discusses adversarial training to improve robustness, which is a technique often used in machine learning, but the primary focus remains on LLMs and their generated content.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.13115v1": {
    "title": "A Computational Framework to Identify Self-Aspects in Text",
    "summary": "This Ph.D. proposal introduces a plan to develop a computational framework to\nidentify Self-aspects in text. The Self is a multifaceted construct and it is\nreflected in language. While it is described across disciplines like cognitive\nscience and phenomenology, it remains underexplored in natural language\nprocessing (NLP). Many of the aspects of the Self align with psychological and\nother well-researched phenomena (e.g., those related to mental health),\nhighlighting the need for systematic NLP-based analysis. In line with this, we\nplan to introduce an ontology of Self-aspects and a gold-standard annotated\ndataset. Using this foundation, we will develop and evaluate conventional\ndiscriminative models, generative large language models, and embedding-based\nretrieval approaches against four main criteria: interpretability, ground-truth\nadherence, accuracy, and computational efficiency. Top-performing models will\nbe applied in case studies in mental health and empirical phenomenology.",
    "published": "2025-07-17T13:31:04Z",
    "updated": "2025-07-17T13:31:04Z",
    "id": "2507.13115v1",
    "authors": [
      "Jaya Caporusso",
      "Matthew Purver",
      "Senja Pollak"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13115v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13115v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13115v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on developing a computational framework for identifying Self-aspects in text, which involves NLP techniques and potentially large language models, but it does not directly align with the specific topics provided in the list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14238v1": {
    "title": "Language Models Change Facts Based on the Way You Talk",
    "summary": "Large language models (LLMs) are increasingly being used in user-facing\napplications, from providing medical consultations to job interview advice.\nRecent research suggests that these models are becoming increasingly proficient\nat inferring identity information about the author of a piece of text from\nlinguistic patterns as subtle as the choice of a few words. However, little is\nknown about how LLMs use this information in their decision-making in\nreal-world applications. We perform the first comprehensive analysis of how\nidentity markers present in a user's writing bias LLM responses across five\ndifferent high-stakes LLM applications in the domains of medicine, law,\npolitics, government benefits, and job salaries. We find that LLMs are\nextremely sensitive to markers of identity in user queries and that race,\ngender, and age consistently influence LLM responses in these applications. For\ninstance, when providing medical advice, we find that models apply different\nstandards of care to individuals of different ethnicities for the same\nsymptoms; we find that LLMs are more likely to alter answers to align with a\nconservative (liberal) political worldview when asked factual questions by\nolder (younger) individuals; and that LLMs recommend lower salaries for\nnon-White job applicants and higher salaries for women compared to men. Taken\ntogether, these biases mean that the use of off-the-shelf LLMs for these\napplications may cause harmful differences in medical care, foster wage gaps,\nand create different political factual realities for people of different\nidentities. Beyond providing an analysis, we also provide new tools for\nevaluating how subtle encoding of identity in users' language choices impacts\nmodel decisions. Given the serious implications of these findings, we recommend\nthat similar thorough assessments of LLM use in user-facing applications are\nconducted before future deployment.",
    "published": "2025-07-17T13:21:17Z",
    "updated": "2025-07-17T13:21:17Z",
    "id": "2507.14238v1",
    "authors": [
      "Matthew Kearney",
      "Reuben Binns",
      "Yarin Gal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14238v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14238v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14238v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the behavior and biases of Large Language Models (LLMs) in various high-stakes applications, focusing on how identity markers influence their responses. This aligns with research on LLMs and their applications, but does not directly fit into the more specific topics provided.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.13052v1": {
    "title": "Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient\n  Communication",
    "summary": "The advancement and maturity of large language models (LLMs) and robotics\nhave unlocked vast potential for human-computer interaction, particularly in\nthe field of robotic ultrasound. While existing research primarily focuses on\neither patient-robot or physician-robot interaction, the role of an intelligent\nvirtual sonographer (IVS) bridging physician-robot-patient communication\nremains underexplored. This work introduces a conversational virtual agent in\nExtended Reality (XR) that facilitates real-time interaction between\nphysicians, a robotic ultrasound system(RUS), and patients. The IVS agent\ncommunicates with physicians in a professional manner while offering empathetic\nexplanations and reassurance to patients. Furthermore, it actively controls the\nRUS by executing physician commands and transparently relays these actions to\nthe patient. By integrating LLM-powered dialogue with speech-to-text,\ntext-to-speech, and robotic control, our system enhances the efficiency,\nclarity, and accessibility of robotic ultrasound acquisition. This work\nconstitutes a first step toward understanding how IVS can bridge communication\ngaps in physician-robot-patient interaction, providing more control and\ntherefore trust into physician-robot interaction while improving patient\nexperience and acceptance of robotic ultrasound.",
    "published": "2025-07-17T12:25:01Z",
    "updated": "2025-07-17T12:25:01Z",
    "id": "2507.13052v1",
    "authors": [
      "Tianyu Song",
      "Feng Li",
      "Yuan Bi",
      "Angelos Karlas",
      "Amir Yousefi",
      "Daniela Branzan",
      "Zhongliang Jiang",
      "Ulrich Eck",
      "Nassir Navab"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13052v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13052v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13052v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models (LLMs) in enhancing communication between physicians, robots, and patients in the context of robotic ultrasound. It focuses on the integration of LLM-powered dialogue with other technologies to improve interaction and trust.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.13019v1": {
    "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A\n  Holistic Study of Physical and Visual Disparities",
    "summary": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but\ntheir idealized assumptions about robot movement and control fail to reflect\nphysically embodied deployment challenges. To bridge this gap, we introduce\nVLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and\nwheeled robots. For the first time, we systematically evaluate several\nego-centric VLN methods in physical robotic settings across different technical\npipelines, including classification models for single-step discrete action\nprediction, a diffusion model for dense waypoint prediction, and a train-free,\nmap-based large language model (LLM) integrated with path planning. Our results\nreveal significant performance degradation due to limited robot observation\nspace, environmental lighting variations, and physical challenges like\ncollisions and falls. This also exposes locomotion constraints for legged\nrobots in complex environments. VLN-PE is highly extensible, allowing seamless\nintegration of new scenes beyond MP3D, thereby enabling more comprehensive VLN\nevaluation. Despite the weak generalization of current models in physical\ndeployment, VLN-PE provides a new pathway for improving cross-embodiment's\noverall adaptability. We hope our findings and tools inspire the community to\nrethink VLN limitations and advance robust, practical VLN models. The code is\navailable at https://crystalsixone.github.io/vln_pe.github.io/.",
    "published": "2025-07-17T11:46:00Z",
    "updated": "2025-07-17T11:46:00Z",
    "id": "2507.13019v1",
    "authors": [
      "Liuyi Wang",
      "Xinyuan Xia",
      "Hui Zhao",
      "Hanqing Wang",
      "Tai Wang",
      "Yilun Chen",
      "Chengju Liu",
      "Qijun Chen",
      "Jiangmiao Pang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13019v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13019v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13019v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses Vision-and-Language Navigation (VLN) and its challenges in physical robotic settings, including the integration of a large language model (LLM) with path planning. It focuses on the physical and visual disparities in VLN, which aligns with the topics of Vision-Language Action (VLA) and Large Language Models (LLM).",
    "llm_cls_result": [
      "VLA",
      "LLM"
    ]
  },
  "2507.12990v1": {
    "title": "Teach Old SAEs New Domain Tricks with Boosting",
    "summary": "Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.",
    "published": "2025-07-17T10:57:49Z",
    "updated": "2025-07-17T10:57:49Z",
    "id": "2507.12990v1",
    "authors": [
      "Nikita Koriagin",
      "Yaroslav Aksenov",
      "Daniil Laptev",
      "Gleb Gerasimov",
      "Nikita Balagansky",
      "Daniil Gavrilov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12990v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12990v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12990v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses Sparse Autoencoders (SAEs) in the context of interpreting Large Language Models (LLMs) and improving their performance on domain-specific tasks. It focuses on enhancing the interpretability and functionality of LLMs through residual learning and domain-specific feature capture.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.12951v1": {
    "title": "UniSLU: Unified Spoken Language Understanding from Heterogeneous\n  Cross-Task Datasets",
    "summary": "Spoken Language Understanding (SLU) plays a crucial role in speech-centric\nmultimedia applications, enabling machines to comprehend spoken language in\nscenarios such as meetings, interviews, and customer service interactions. SLU\nencompasses multiple tasks, including Automatic Speech Recognition (ASR),\nspoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).\nHowever, existing methods often rely on separate model architectures for\nindividual tasks such as spoken NER and SA, which increases system complexity,\nlimits cross-task interaction, and fails to fully exploit heterogeneous\ndatasets available across tasks. To address these limitations, we propose\nUniSLU, a unified framework that jointly models multiple SLU tasks within a\nsingle architecture. Specifically, we propose a unified representation for\ndiverse SLU tasks, enabling full utilization of heterogeneous datasets across\nmultiple tasks. Built upon this representation, we propose a unified generative\nmethod that jointly models ASR, spoken NER, and SA tasks, enhancing task\ninteractions and enabling seamless integration with large language models to\nharness their powerful generative capabilities. Extensive experiments on public\nSLU datasets demonstrate the effectiveness of our approach, achieving superior\nSLU performance compared to several benchmark methods, making it well-suited\nfor real-world speech-based multimedia scenarios. We will release all code and\nmodels at github to facilitate future research.",
    "published": "2025-07-17T09:45:49Z",
    "updated": "2025-07-17T09:45:49Z",
    "id": "2507.12951v1",
    "authors": [
      "Zhichao Sheng",
      "Shilin Zhou",
      "Chen Gong",
      "Zhenghua Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12951v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12951v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12951v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a unified framework for Spoken Language Understanding (SLU) tasks, which involves integrating multiple tasks like ASR, spoken NER, and SA into a single architecture. While it mentions integration with large language models, the primary focus is on SLU and not specifically on LLMs, RL, MLLM, or other listed topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.12950v2": {
    "title": "Insights into a radiology-specialised multimodal large language model\n  with sparse autoencoders",
    "summary": "Interpretability can improve the safety, transparency and trust of AI models,\nwhich is especially important in healthcare applications where decisions often\ncarry significant consequences. Mechanistic interpretability, particularly\nthrough the use of sparse autoencoders (SAEs), offers a promising approach for\nuncovering human-interpretable features within large transformer-based models.\nIn this study, we apply Matryoshka-SAE to the radiology-specialised multimodal\nlarge language model, MAIRA-2, to interpret its internal representations. Using\nlarge-scale automated interpretability of the SAE features, we identify a range\nof clinically relevant concepts - including medical devices (e.g., line and\ntube placements, pacemaker presence), pathologies such as pleural effusion and\ncardiomegaly, longitudinal changes and textual features. We further examine the\ninfluence of these features on model behaviour through steering, demonstrating\ndirectional control over generations with mixed success. Our results reveal\npractical and methodological challenges, yet they offer initial insights into\nthe internal concepts learned by MAIRA-2 - marking a step toward deeper\nmechanistic understanding and interpretability of a radiology-adapted\nmultimodal large language model, and paving the way for improved model\ntransparency. We release the trained SAEs and interpretations:\nhttps://huggingface.co/microsoft/maira-2-sae.",
    "published": "2025-07-17T09:43:20Z",
    "updated": "2025-07-18T09:19:19Z",
    "id": "2507.12950v2",
    "authors": [
      "Kenza Bouzid",
      "Shruthi Bannur",
      "Felix Meissen",
      "Daniel Coelho de Castro",
      "Anton Schwaighofer",
      "Javier Alvarez-Valle",
      "Stephanie L. Hyland"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12950v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12950v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12950v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a radiology-specialised multimodal large language model (MAIRA-2) and its interpretability using sparse autoencoders, which aligns with the topics of Multimodal Large Language Models (MLLM) and Interpretability in AI models, particularly in healthcare applications.",
    "llm_cls_result": [
      "MLLM",
      "AGI"
    ]
  },
  "2507.12945v1": {
    "title": "Analysis of Image-and-Text Uncertainty Propagation in Multimodal Large\n  Language Models with Cardiac MR-Based Applications",
    "summary": "Multimodal large language models (MLLMs) can process and integrate\ninformation from multimodality sources, such as text and images. However,\ninterrelationship among input modalities, uncertainties due to individual\nuni-modal data and potential clinical applications following such an\nuncertainty decomposition are yet fully understood in the context of\nlarge-scale MLLMs. In this work, we propose a multimodal uncertainty\npropagation model (MUPM) based on uncertainty propagation, to characterise the\nrelationship among the uncertainties arising from image-only, text-only, and\njoint image-text variations in MLLM inputs. Using real clinical data consisting\nof cardiac MR scans and digital health records, we describe that MUPMs can be\noptimised robustly with a few samples. We then show that the fitted MUPMs are\ngeneralisable across different input data distributions and, perhaps\nsurprisingly, across different downstream tasks. Such a transferability may be\nexplained by the shared pretraining, comparatively light MLLM fine-tuning,\nalong with the low-dimensional nature of the MUPMs. More importantly, this\nlearned transferability, quantifying the relationship between these\nuncertainties, led to direct clinical applications in which uncertainties may\nbe estimated and thus analysed robustly for varying data or even a novel set of\ncardiac disease prediction tasks. In addition, we show experimentally the\nefficiency in multimodal data required for estimating the overall uncertainty\nand its ability to identify redundant factors, both of which are considered\npractical yet clinically useful applications with the proposed MUPMs. Codes are\navailable at https://github.com/yucheng722/MUPM.",
    "published": "2025-07-17T09:34:21Z",
    "updated": "2025-07-17T09:34:21Z",
    "id": "2507.12945v1",
    "authors": [
      "Yucheng Tang",
      "Yunguan Fu",
      "Weixi Yi",
      "Yipei Wang",
      "Daniel C. Alexander",
      "Rhodri Davies",
      "Yipeng Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12945v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12945v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12945v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Multimodal Large Language Models (MLLMs) and their ability to process and integrate information from multiple modalities, specifically text and images. It also discusses uncertainty propagation in these models and their clinical applications, which aligns with the MLLM topic. Additionally, the mention of pretraining and fine-tuning of MLLMs relates to the Pretrain topic.",
    "llm_cls_result": [
      "MLLM",
      "Pretrain"
    ]
  },
  "2507.12916v1": {
    "title": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding\n  With Large Language Models",
    "summary": "Advancements in foundation models have made it possible to conduct\napplications in various downstream tasks. Especially, the new era has witnessed\na remarkable capability to extend Large Language Models (LLMs) for tackling\ntasks of 3D scene understanding. Current methods rely heavily on 3D point\nclouds, but the 3D point cloud reconstruction of an indoor scene often results\nin information loss. Some textureless planes or repetitive patterns are prone\nto omission and manifest as voids within the reconstructed 3D point clouds.\nBesides, objects with complex structures tend to introduce distortion of\ndetails caused by misalignments between the captured images and the dense\nreconstructed point clouds. 2D multi-view images present visual consistency\nwith 3D point clouds and provide more detailed representations of scene\ncomponents, which can naturally compensate for these deficiencies. Based on\nthese insights, we propose Argus, a novel 3D multimodal framework that\nleverages multi-view images for enhanced 3D scene understanding with LLMs. In\ngeneral, Argus can be treated as a 3D Large Multimodal Foundation Model\n(3D-LMM) since it takes various modalities as input(text instructions, 2D\nmulti-view images, and 3D point clouds) and expands the capability of LLMs to\ntackle 3D tasks. Argus involves fusing and integrating multi-view images and\ncamera poses into view-as-scene features, which interact with the 3D features\nto create comprehensive and detailed 3D-aware scene embeddings. Our approach\ncompensates for the information loss while reconstructing 3D point clouds and\nhelps LLMs better understand the 3D world. Extensive experiments demonstrate\nthat our method outperforms existing 3D-LMMs in various downstream tasks.",
    "published": "2025-07-17T09:02:04Z",
    "updated": "2025-07-17T09:02:04Z",
    "id": "2507.12916v1",
    "authors": [
      "Yifan Xu",
      "Chao Zhang",
      "Hanqi Jiang",
      "Xiaoyan Wang",
      "Ruifei Ma",
      "Yiwei Li",
      "Zihao Wu",
      "Zeju Li",
      "Xiangde Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12916v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12916v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12916v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with multi-view images and 3D point clouds for enhanced 3D scene understanding, which aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA). The focus on leveraging multiple modalities and improving 3D understanding through LLMs is central to these topics.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.15874v1": {
    "title": "Why Braking? Scenario Extraction and Reasoning Utilizing LLM",
    "summary": "The growing number of ADAS-equipped vehicles has led to a dramatic increase\nin driving data, yet most of them capture routine driving behavior. Identifying\nand understanding safety-critical corner cases within this vast dataset remains\na significant challenge. Braking events are particularly indicative of\npotentially hazardous situations, motivating the central question of our\nresearch: Why does a vehicle brake? Existing approaches primarily rely on\nrule-based heuristics to retrieve target scenarios using predefined condition\nfilters. While effective in simple environments such as highways, these methods\nlack generalization in complex urban settings. In this paper, we propose a\nnovel framework that leverages Large Language Model (LLM) for scenario\nunderstanding and reasoning. Our method bridges the gap between low-level\nnumerical signals and natural language descriptions, enabling LLM to interpret\nand classify driving scenarios. We propose a dual-path scenario retrieval that\nsupports both category-based search for known scenarios and embedding-based\nretrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate\nevaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset.\nExperimental results show that our method outperforms rule-based baselines and\ngeneralizes well to OOD scenarios.",
    "published": "2025-07-17T08:33:56Z",
    "updated": "2025-07-17T08:33:56Z",
    "id": "2507.15874v1",
    "authors": [
      "Yin Wu",
      "Daniel Slieter",
      "Vivek Subramanian",
      "Ahmed Abouelazm",
      "Robin Bohn",
      "J. Marius Zllner"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15874v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15874v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15874v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLM) for scenario understanding and reasoning in driving data, which aligns with the LLM topic. It also involves reasoning abilities in LLMs to interpret and classify driving scenarios, fitting the Reasoning topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.12856v1": {
    "title": "Supervised Fine Tuning on Curated Data is Reinforcement Learning (and\n  can be improved)",
    "summary": "Behavior Cloning (BC) on curated (or filtered) data is the predominant\nparadigm for supervised fine-tuning (SFT) of large language models; as well as\nfor imitation learning of control policies. Here, we draw on a connection\nbetween this successful strategy and the theory and practice of finding optimal\npolicies via Reinforcement Learning (RL). Building on existing literature, we\nclarify that SFT can be understood as maximizing a lower bound on the RL\nobjective in a sparse reward setting. Giving support to its often observed good\nperformance. From this viewpoint, we realize that a small modification to SFT\nleads to an importance weighted variant that behaves closer to training with RL\nas it: i) optimizes a tighter bound to the RL objective and, ii) can improve\nperformance compared to SFT on curated data. We refer to this variant as\nimportance weighted supervised fine-tuning (iw-SFT). We show that it is easy to\nimplement and can be further generalized to training with quality scored data.\nThe resulting SFT variants are competitive with more advanced RL algorithms for\nlarge language models and for training policies in continuous control tasks.\nFor example achieving 66.7% on the AIME 2024 dataset.",
    "published": "2025-07-17T07:26:54Z",
    "updated": "2025-07-17T07:26:54Z",
    "id": "2507.12856v1",
    "authors": [
      "Chongli Qin",
      "Jost Tobias Springenberg"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12856v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12856v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12856v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the connection between supervised fine-tuning (SFT) and reinforcement learning (RL), proposing an importance weighted variant of SFT that behaves closer to RL. This aligns with the RL topic, which includes RLHF and related methods.",
    "llm_cls_result": [
      "RL"
    ]
  },
  "2507.12855v1": {
    "title": "DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task\n  Demonstration Learning",
    "summary": "The integration of large language models (LLMs) with control systems has\ndemonstrated significant potential in various settings, such as task completion\nwith a robotic manipulator. A main reason for this success is the ability of\nLLMs to perform in-context learning, which, however, strongly relies on the\ndesign of task examples, closely related to the target tasks. Consequently,\nemploying LLMs to formulate optimal control problems often requires task\nexamples that contain explicit mathematical expressions, designed by trained\nengineers. Furthermore, there is often no principled way to evaluate for\nhallucination before task execution. To address these challenges, we propose\nDEMONSTRATE, a novel methodology that avoids the use of LLMs for complex\noptimization problem generations, and instead only relies on the embedding\nrepresentations of task descriptions. To do this, we leverage tools from\ninverse optimal control to replace in-context prompt examples with task\ndemonstrations, as well as the concept of multitask learning, which ensures\ntarget and example task similarity by construction. Given the fact that\nhardware demonstrations can easily be collected using teleoperation or guidance\nof the robot, our approach significantly reduces the reliance on engineering\nexpertise for designing in-context examples. Furthermore, the enforced\nmultitask structure enables learning from few demonstrations and assessment of\nhallucinations prior to task execution. We demonstrate the effectiveness of our\nmethod through simulation and hardware experiments involving a robotic arm\ntasked with tabletop manipulation.",
    "published": "2025-07-17T07:26:22Z",
    "updated": "2025-07-17T07:26:22Z",
    "id": "2507.12855v1",
    "authors": [
      "Rahel Rickenbach",
      "Bruce Lee",
      "Ren Zurbrgg",
      "Carmen Amo Alonso",
      "Melanie N. Zeilinger"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12855v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12855v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12855v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) with robotic control systems, focusing on zero-shot learning and multi-task demonstration learning. It leverages LLMs for robotic control but avoids using them for complex optimization problem generation, instead relying on embedding representations and inverse optimal control. This aligns with topics related to LLMs, Reinforcement Learning (RL), and Vision-Language Action (VLA) models.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "VLA"
    ]
  },
  "2507.12840v1": {
    "title": "Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better\n  Understand Public Concerns about Vaccines",
    "summary": "Vaccine hesitancy threatens public health, leading to delayed or rejected\nvaccines. Social media is a vital source for understanding public concerns, and\ntraditional methods like topic modelling often struggle to capture nuanced\nopinions. Though trained for query answering, large Language Models (LLMs)\noften miss current events and community concerns. Additionally, hallucinations\nin LLMs can compromise public health communication. To address these\nlimitations, we developed a tool (VaxPulse Query Corner) using the Retrieval\nAugmented Generation technique. It addresses complex queries about public\nvaccine concerns on various online platforms, aiding public health\nadministrators and stakeholders in understanding public concerns and\nimplementing targeted interventions to boost vaccine confidence. Analysing\n35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and\nrelevance (0.94).",
    "published": "2025-07-17T06:59:52Z",
    "updated": "2025-07-17T06:59:52Z",
    "id": "2507.12840v1",
    "authors": [
      "Muhammad Javed",
      "Sedigh Khademi Habibabadi",
      "Christopher Palmer",
      "Hazel Clothier",
      "Jim Buttery",
      "Gerardo Luis Dimaguila"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12840v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12840v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12840v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Retrieval Augmented Generation (RAG) technique with Large Language Models (LLMs) to address public concerns about vaccines, which involves memory-augmented models and retrieval-based methods.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.17769v1": {
    "title": "PolyServe: Efficient Multi-SLO Serving at Scale",
    "summary": "Advances in Large Language Models (LLMs) have led to a surge of LLM-powered\napplications. These applications have diverse token-generation latency\nrequirements. As a result, simply classifying workloads as latency-sensitive\n(LS) or best-effort (BE) overlooks the nuances within the latency-sensitive\ncategory and results in suboptimal user experiences and scheduling\nopportunities. However, efficiently serving requests with multiple SLO\nrequirements poses significant challenges. First, all requests within a batch\ngenerate new tokens simultaneously, which can misalign them with their distinct\nSLO requirements. Moreover, while existing systems focus on auto-scaling for\nhandling various overall request rates, the diversity of SLOs necessitates\nfine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE\nscenarios, where BE requests can be aborted at any time to ensure the SLO\nattainment of LS requests, those with different latency-sensitive SLOs cannot\ntolerate prolonged delays, and tail latency must be controlled.\n  To tackle these challenges, we propose PolyServe, a novel multi-SLO\nscheduling policy at scale that maintains high SLO attainment while maximizing\nthroughput. PolyServe first groups requests into multiple bins based on their\nper-token latency requirement, then schedules each bin to a subset of the\nserver fleet. PolyServe routes requests to the highest-load but still\nSLO-attainable server to create a load gradient that facilitates auto-scaling.\nTo increase utilization, PolyServe permits looser-SLO requests to share\ntighter-SLO instances when their own servers are saturated. PolyServe uses\nprofiling data to guide scheduling decisions and manage tail latency through\nrequest-wait-time-aware scheduling, dynamic chunking, and continuous chunked\nprefill prediction. PolyServe achieves 1.23x goodput gain compared to existing\npolicies, achieving up to 92.5% of optimal goodput.",
    "published": "2025-07-17T05:54:42Z",
    "updated": "2025-07-17T05:54:42Z",
    "id": "2507.17769v1",
    "authors": [
      "Kan Zhu",
      "Haiyang Shi",
      "Le Xu",
      "Jiaxin Shan",
      "Arvind Krishnamurthy",
      "Baris Kasikci",
      "Liguang Xie"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17769v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17769v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17769v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses efficient serving of Large Language Models (LLMs) with diverse token-generation latency requirements, focusing on scheduling policies and auto-scaling to meet multiple SLO requirements. This aligns with the 'LLM' and 'Scaling' topics as it involves LLM architectures and scaling strategies for efficient model serving.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.12796v1": {
    "title": "DeQA-Doc: Adapting DeQA-Score to Document Image Quality Assessment",
    "summary": "Document quality assessment is critical for a wide range of applications\nincluding document digitization, OCR, and archival. However, existing\napproaches often struggle to provide accurate and robust quality scores,\nlimiting their applicability in practical scenarios. With the rapid progress in\nMulti-modal Large Language Models (MLLMs), recent MLLM-based methods have\nachieved remarkable performance in image quality assessment. In this work, we\nextend this success to the document domain by adapting DeQA-Score, a\nstate-of-the-art MLLM-based image quality scorer, for document quality\nassessment. We propose DeQA-Doc, a framework that leverages the visual language\ncapabilities of MLLMs and a soft label strategy to regress continuous document\nquality scores. To adapt DeQA-Score to DeQA-Doc, we adopt two complementary\nsolutions to construct soft labels without the variance information. Also, we\nrelax the resolution constrains to support the large resolution of document\nimages. Finally, we introduce ensemble methods to further enhance the\nperformance. Extensive experiments demonstrate that DeQA-Doc significantly\noutperforms existing baselines, offering accurate and generalizable document\nquality assessment across diverse degradation types. Codes and model weights\nare available in https://github.com/Junjie-Gao19/DeQA-Doc.",
    "published": "2025-07-17T05:23:53Z",
    "updated": "2025-07-17T05:23:53Z",
    "id": "2507.12796v1",
    "authors": [
      "Junjie Gao",
      "Runze Liu",
      "Yingzhe Peng",
      "Shujian Yang",
      "Jin Zhang",
      "Kai Yang",
      "Zhiyuan You"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12796v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12796v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12796v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on adapting a Multi-modal Large Language Model (MLLM) for document image quality assessment, which directly involves the use of MLLMs and their application in a multimodal context.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.14230v1": {
    "title": "Intent-Based Network for RAN Management with Large Language Models",
    "summary": "Advanced intelligent automation becomes an important feature to deal with the\nincreased complexity in managing wireless networks. This paper proposes a novel\nautomation approach of intent-based network for Radio Access Networks (RANs)\nmanagement by leveraging Large Language Models (LLMs). The proposed method\nenhances intent translation, autonomously interpreting high-level objectives,\nreasoning over complex network states, and generating precise configurations of\nthe RAN by integrating LLMs within an agentic architecture. We propose a\nstructured prompt engineering technique and demonstrate that the network can\nautomatically improve its energy efficiency by dynamically optimizing critical\nRAN parameters through a closed-loop mechanism. It showcases the potential to\nenable robust resource management in RAN by adapting strategies based on\nreal-time feedback via LLM-orchestrated agentic systems.",
    "published": "2025-07-17T04:57:55Z",
    "updated": "2025-07-17T04:57:55Z",
    "id": "2507.14230v1",
    "authors": [
      "Fransiscus Asisi Bimo",
      "Maria Amparo Canaveras Galdon",
      "Chun-Kai Lai",
      "Ray-Guang Cheng",
      "Edwin K. P. Chong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14230v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14230v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14230v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for intent-based network management in Radio Access Networks (RANs), focusing on intent translation, reasoning over network states, and generating configurations. It also mentions the use of LLMs within an agentic architecture and their role in enhancing network efficiency.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Reasoning"
    ]
  },
  "2507.12753v1": {
    "title": "osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps\n  and Large Language Models Reasoning",
    "summary": "Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features, achieving a high level of detail and\nguiding robots to find objects specified by open-vocabulary language queries.\nWhile the issue of scalability for such approaches has received some attention,\nanother fundamental problem is that high-detail object mapping quickly becomes\noutdated, as objects get moved around a lot. In this work, we develop a mapping\nand navigation system for object-goal navigation that, from the ground up,\nconsiders the possibilities that a queried object can have moved, or may not be\nmapped at all. Instead of striving for high-fidelity mapping detail, we\nconsider that the main purpose of a map is to provide environment grounding and\ncontext, which we combine with the semantic priors of LLMs to reason about\nobject locations and deploy an active, online approach to navigate to the\nobjects. Through simulated and real-world experiments we find that our approach\ntends to have higher retrieval success at shorter path lengths for static\nobjects and by far outperforms prior approaches in cases of dynamic or unmapped\nobject queries. We provide our code and dataset at:\nhttps://anonymous.4open.science/r/osmAG-LLM.",
    "published": "2025-07-17T03:14:37Z",
    "updated": "2025-07-17T03:14:37Z",
    "id": "2507.12753v1",
    "authors": [
      "Fujing Xie",
      "Sren Schwertfeger",
      "Hermann Blum"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12753v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12753v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12753v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for reasoning in the context of object navigation and semantic maps, which aligns with the 'LLM' and 'Reasoning' topics. Additionally, the application involves vision-language interaction for navigation, which is relevant to 'VLA'.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "VLA"
    ]
  },
  "2507.12732v1": {
    "title": "Strategy Adaptation in Large Language Model Werewolf Agents",
    "summary": "This study proposes a method to improve the performance of Werewolf agents by\nswitching between predefined strategies based on the attitudes of other players\nand the context of conversations. While prior works of Werewolf agents using\nprompt engineering have employed methods where effective strategies are\nimplicitly defined, they cannot adapt to changing situations. In this research,\nwe propose a method that explicitly selects an appropriate strategy based on\nthe game context and the estimated roles of other players. We compare the\nstrategy adaptation Werewolf agents with baseline agents using implicit or\nfixed strategies and verify the effectiveness of our proposed method.",
    "published": "2025-07-17T02:27:45Z",
    "updated": "2025-07-17T02:27:45Z",
    "id": "2507.12732v1",
    "authors": [
      "Fuya Nakamori",
      "Yin Jou Huang",
      "Fei Cheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12732v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12732v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12732v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the adaptation of strategies in Large Language Model (LLM) agents, specifically in the context of a game scenario. This involves the use of LLMs for decision-making and strategy adaptation, which aligns with the topics of LLM and RL (Reinforcement Learning) as it involves learning and adapting strategies based on context.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.12679v1": {
    "title": "Improving Drug Identification in Overdose Death Surveillance using Large\n  Language Models",
    "summary": "The rising rate of drug-related deaths in the United States, largely driven\nby fentanyl, requires timely and accurate surveillance. However, critical\noverdose data are often buried in free-text coroner reports, leading to delays\nand information loss when coded into ICD (International Classification of\nDisease)-10 classifications. Natural language processing (NLP) models may\nautomate and enhance overdose surveillance, but prior applications have been\nlimited. A dataset of 35,433 death records from multiple U.S. jurisdictions in\n2020 was used for model training and internal testing. External validation was\nconducted using a novel separate dataset of 3,335 records from 2023-2024.\nMultiple NLP approaches were evaluated for classifying specific drug\ninvolvement from unstructured death certificate text. These included\ntraditional single- and multi-label classifiers, as well as fine-tuned\nencoder-only language models such as Bidirectional Encoder Representations from\nTransformers (BERT) and BioClinicalBERT, and contemporary decoder-only large\nlanguage models such as Qwen 3 and Llama 3. Model performance was assessed\nusing macro-averaged F1 scores, and 95% confidence intervals were calculated to\nquantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect\nperformance, with macro F1 scores >=0.998 on the internal test set. External\nvalidation confirmed robustness (macro F1=0.966), outperforming conventional\nmachine learning, general-domain BERT models, and various decoder-only large\nlanguage models. NLP models, particularly fine-tuned clinical variants like\nBioClinicalBERT, offer a highly accurate and scalable solution for overdose\ndeath classification from free-text reports. These methods can significantly\naccelerate surveillance workflows, overcoming the limitations of manual ICD-10\ncoding and supporting near real-time detection of emerging substance use\ntrends.",
    "published": "2025-07-16T23:29:19Z",
    "updated": "2025-07-16T23:29:19Z",
    "id": "2507.12679v1",
    "authors": [
      "Arthur J. Funnell",
      "Panayiotis Petousis",
      "Fabrice Harel-Canada",
      "Ruby Romero",
      "Alex A. T. Bui",
      "Adam Koncsol",
      "Hritika Chaturvedi",
      "Chelsea Shover",
      "David Goodman-Meza"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12679v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12679v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12679v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in improving drug identification from free-text coroner reports, which aligns with the 'LLM' category. Additionally, the use of fine-tuned clinical variants like BioClinicalBERT suggests a focus on pretraining strategies, fitting the 'Pretrain' category. The application is specific to NLP and clinical data, not directly fitting other provided categories.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.12674v2": {
    "title": "ParaStudent: Generating and Evaluating Realistic Student Code by\n  Teaching LLMs to Struggle",
    "summary": "Large Language Models (LLMs) have shown strong performance on programming\ntasks, but can they generate student-like code like real students - imperfect,\niterative, and stylistically diverse? We present ParaStudent, a systematic\nstudy of LLM-based \"student-like\" code generation in an introductory\nprogramming course setting. Using a dataset of timestamped student submissions\nacross multiple semesters, we design low- and high-resolution experiments to\nmodel student progress and evaluate code outputs along semantic, functional,\nand stylistic dimensions. Our results show that fine-tuning significantly\nimproves alignment with real student trajectories and captures error patterns,\nincremental improvements, and stylistic variations more faithfully. This study\nshows that modeling realistic student code requires capturing learning dynamics\nthrough context-aware generation, temporal modeling, and multi-dimensional\nevaluation. Code for experiments and evaluation is available at\nhttps://github.com/mmiroyan/ParaStudent.",
    "published": "2025-07-16T23:12:14Z",
    "updated": "2025-07-18T01:02:16Z",
    "id": "2507.12674v2",
    "authors": [
      "Mihran Miroyan",
      "Rose Niousha",
      "Joseph E. Gonzalez",
      "Gireeja Ranade",
      "Narges Norouzi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12674v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12674v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12674v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) to generate and evaluate student-like code, which involves fine-tuning and evaluating LLMs in a specific context. This aligns with research on LLMs and their applications in generating realistic outputs.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.12672v1": {
    "title": "The first open machine translation system for the Chechen language",
    "summary": "We introduce the first open-source model for translation between the\nvulnerable Chechen language and Russian, and the dataset collected to train and\nevaluate it. We explore fine-tuning capabilities for including a new language\ninto a large language model system for multilingual translation NLLB-200. The\nBLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for\ntranslation from Russian to Chechen and reverse direction, respectively. The\nrelease of the translation models is accompanied by the distribution of\nparallel words, phrases and sentences corpora and multilingual sentence encoder\nadapted to the Chechen language.",
    "published": "2025-07-16T23:07:07Z",
    "updated": "2025-07-16T23:07:07Z",
    "id": "2507.12672v1",
    "authors": [
      "Abu-Viskhan A. Umishov",
      "Vladislav A. Grigorian"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12672v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12672v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12672v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on machine translation for the Chechen language, which involves creating a dataset and fine-tuning a large language model for multilingual translation. It does not directly align with the provided topics, which are more centered on LLM architectures, multimodal models, reasoning, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.12665v1": {
    "title": "Single Conversation Methodology: A Human-Centered Protocol for\n  AI-Assisted Software Development",
    "summary": "We propose the Single Conversation Methodology (SCM), a novel and pragmatic\napproach to software development using large language models (LLMs). In\ncontrast to ad hoc interactions with generative AI, SCM emphasizes a structured\nand persistent development dialogue, where all stages of a project - from\nrequirements to architecture and implementation - unfold within a single,\nlong-context conversation. The methodology is grounded on principles of\ncognitive clarity, traceability, modularity, and documentation. We define its\nphases, best practices, and philosophical stance, while arguing that SCM offers\na necessary correction to the passive reliance on LLMs prevalent in current\npractices. We aim to reassert the active role of the developer as architect and\nsupervisor of the intelligent tool.",
    "published": "2025-07-16T22:43:30Z",
    "updated": "2025-07-16T22:43:30Z",
    "id": "2507.12665v1",
    "authors": [
      "Salvador D. Escobedo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12665v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12665v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12665v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a methodology for software development using large language models (LLMs), emphasizing structured and persistent development dialogue. It aligns with the topics of LLM (Large Language Models) and Memory (long-context conversation).",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.12621v1": {
    "title": "NLI4VolVis: Natural Language Interaction for Volume Visualization via\n  LLM Multi-Agents and Editable 3D Gaussian Splatting",
    "summary": "Traditional volume visualization (VolVis) methods, like direct volume\nrendering, suffer from rigid transfer function designs and high computational\ncosts. Although novel view synthesis approaches enhance rendering efficiency,\nthey require additional learning effort for non-experts and lack support for\nsemantic-level interaction. To bridge this gap, we propose NLI4VolVis, an\ninteractive system that enables users to explore, query, and edit volumetric\nscenes using natural language. NLI4VolVis integrates multi-view semantic\nsegmentation and vision-language models to extract and understand semantic\ncomponents in a scene. We introduce a multi-agent large language model\narchitecture equipped with extensive function-calling tools to interpret user\nintents and execute visualization tasks. The agents leverage external tools and\ndeclarative VolVis commands to interact with the VolVis engine powered by 3D\neditable Gaussians, enabling open-vocabulary object querying, real-time scene\nediting, best-view selection, and 2D stylization. We validate our system\nthrough case studies and a user study, highlighting its improved accessibility\nand usability in volumetric data exploration. We strongly recommend readers\ncheck our case studies, demo video, and source code at\nhttps://nli4volvis.github.io/.",
    "published": "2025-07-16T20:35:46Z",
    "updated": "2025-07-16T20:35:46Z",
    "id": "2507.12621v1",
    "authors": [
      "Kuangshi Ai",
      "Kaiyuan Tang",
      "Chaoli Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12621v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12621v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12621v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of multi-agent large language models (LLMs) and vision-language models (VLMs) for natural language interaction in volume visualization, which aligns with the topics of LLM and VLA. The use of LLMs for interpreting user intents and executing tasks also touches upon the Reasoning topic.",
    "llm_cls_result": [
      "LLM",
      "VLA",
      "Reasoning"
    ]
  },
  "2507.12619v1": {
    "title": "BootSeer: Analyzing and Mitigating Initialization Bottlenecks in\n  Large-Scale LLM Training",
    "summary": "Large Language Models (LLMs) have become a cornerstone of modern AI, driving\nbreakthroughs in natural language processing and expanding into multimodal jobs\ninvolving images, audio, and video. As with most computational software, it is\nimportant to distinguish between ordinary runtime performance and startup\noverhead. Prior research has focused on runtime performance: improving training\nefficiency and stability. This work focuses instead on the increasingly\ncritical issue of startup overhead in training: the delay before training jobs\nbegin execution. Startup overhead is particularly important in large,\nindustrial-scale LLMs, where failures occur more frequently and multiple teams\noperate in iterative update-debug cycles. In one of our training clusters, more\nthan 3.5% of GPU time is wasted due to startup overhead alone.\n  In this work, we present the first in-depth characterization of LLM training\nstartup overhead based on real production data. We analyze the components of\nstartup cost, quantify its direct impact, and examine how it scales with job\nsize. These insights motivate the design of Bootseer, a system-level\noptimization framework that addresses three primary startup bottlenecks: (a)\ncontainer image loading, (b) runtime dependency installation, and (c) model\ncheckpoint resumption. To mitigate these bottlenecks, Bootseer introduces three\ntechniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and\n(c) striped HDFS-FUSE. Bootseer has been deployed in a production environment\nand evaluated on real LLM training workloads, demonstrating a 50% reduction in\nstartup overhead.",
    "published": "2025-07-16T20:32:33Z",
    "updated": "2025-07-16T20:32:33Z",
    "id": "2507.12619v1",
    "authors": [
      "Rui Li",
      "Xiaoyun Zhi",
      "Jinxin Chi",
      "Menghan Yu",
      "Lixin Huang",
      "Jia Zhu",
      "Weilun Zhang",
      "Xing Ma",
      "Wenjia Liu",
      "Zhicheng Zhu",
      "Daowen Luo",
      "Zuquan Song",
      "Xin Yin",
      "Chao Xiang",
      "Shuguang Wang",
      "Wencong Xiao",
      "Gene Cooperman"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12619v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12619v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12619v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the startup overhead in training large-scale LLMs, which is a critical issue in the context of scaling and efficiency in LLM training. It does not directly address topics like RL, MLLM, VLA, MoE, AGI, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.12612v1": {
    "title": "Learning What Matters: Probabilistic Task Selection via Mutual\n  Information for Model Finetuning",
    "summary": "The performance of finetuned large language models (LLMs) hinges critically\non the composition of the training mixture. However, selecting an optimal blend\nof task datasets remains a largely manual, heuristic driven process, with\npractitioners often relying on uniform or size based sampling strategies. We\nintroduce TASKPGM, a principled and scalable framework for mixture optimization\nthat selects continuous task proportions by minimizing an energy function over\na Markov Random Field (MRF). Task relationships are modeled using behavioral\ndivergences such as Jensen Shannon Divergence and Pointwise Mutual Information\ncomputed from the predictive distributions of single task finetuned models. Our\nmethod yields a closed form solution under simplex constraints and provably\nbalances representativeness and diversity among tasks. We provide theoretical\nguarantees, including weak submodularity for budgeted variants, and demonstrate\nconsistent empirical improvements on Llama 2 and Mistral across evaluation\nsuites such as MMLU and BIGBench. Beyond performance, TASKPGM offers\ninterpretable insights into task influence and mixture composition, making it a\npowerful tool for efficient and robust LLM finetuning.",
    "published": "2025-07-16T20:14:55Z",
    "updated": "2025-07-16T20:14:55Z",
    "id": "2507.12612v1",
    "authors": [
      "Prateek Chanda",
      "Saral Sureka",
      "Parth Pratim Chatterjee",
      "Krishnateja Killamsetty",
      "Nikhil Shivakumar Nayak",
      "Ganesh Ramakrishnan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12612v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12612v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12612v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses finetuning large language models (LLMs) and optimizing task mixtures for better performance, which is relevant to LLM research and pretraining strategies.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.12574v1": {
    "title": "Assay2Mol: large language model-based drug design using BioAssay context",
    "summary": "Scientific databases aggregate vast amounts of quantitative data alongside\ndescriptive text. In biochemistry, molecule screening assays evaluate the\nfunctional responses of candidate molecules against disease targets.\nUnstructured text that describes the biological mechanisms through which these\ntargets operate, experimental screening protocols, and other attributes of\nassays offer rich information for new drug discovery campaigns but has been\nuntapped because of that unstructured format. We present Assay2Mol, a large\nlanguage model-based workflow that can capitalize on the vast existing\nbiochemical screening assays for early-stage drug discovery. Assay2Mol\nretrieves existing assay records involving targets similar to the new target\nand generates candidate molecules using in-context learning with the retrieved\nassay screening data. Assay2Mol outperforms recent machine learning approaches\nthat generate candidate ligand molecules for target protein structures, while\nalso promoting more synthesizable molecule generation.",
    "published": "2025-07-16T18:42:18Z",
    "updated": "2025-07-16T18:42:18Z",
    "id": "2507.12574v1",
    "authors": [
      "Yifan Deng",
      "Spencer S. Ericksen",
      "Anthony Gitter"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12574v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12574v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12574v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a large language model (LLM) for drug design, specifically leveraging biochemical assay data. This aligns with the 'LLM' topic as it involves the application of large language models in a specific domain (drug discovery).",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.12425v1": {
    "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data",
    "summary": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot",
    "published": "2025-07-16T17:13:06Z",
    "updated": "2025-07-16T17:13:06Z",
    "id": "2507.12425v1",
    "authors": [
      "Chandana Cheerla"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12425v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12425v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12425v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing Retrieval-Augmented Generation (RAG) frameworks for structured enterprise data, which involves memory-augmented models and retrieval-based methods. It also discusses the use of Large Language Models (LLMs) in this context.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.12372v1": {
    "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User\n  Demographics",
    "summary": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.",
    "published": "2025-07-16T16:21:01Z",
    "updated": "2025-07-16T16:21:01Z",
    "id": "2507.12372v1",
    "authors": [
      "Meysam Alizadeh",
      "Fabrizio Gilardi",
      "Zeynab Samei",
      "Mohsen Mosleh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12372v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12372v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12372v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the capabilities of Large Language Models (LLMs) with web browsing functionalities to access and analyze social media data, inferring user demographics. This aligns with the topics of LLM research and their applications in real-time information retrieval and analysis.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.12370v1": {
    "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests\n  through Debate",
    "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.",
    "published": "2025-07-16T16:15:25Z",
    "updated": "2025-07-16T16:15:25Z",
    "id": "2507.12370v1",
    "authors": [
      "Ana Davila",
      "Jacinto Colan",
      "Yasuhisa Hasegawa"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12370v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12370v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12370v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing LLM capabilities through a multi-agent debate framework to detect and resolve ambiguity in user requests, which aligns with research on LLMs and their reasoning abilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.17766v1": {
    "title": "Incentivised Orchestrated Training Architecture (IOTA): A Technical\n  Primer for Release",
    "summary": "In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed\nnetwork of incentivized, permissionless actors could each pretrain large\nlanguage models (LLMs) ranging from 700 million to 14 billion parameters, while\nsurpassing established baselines. While that work validated blockchain-based\ndecentralized pretraining as viable, it contained core issues: (i) every miner\nhad to fit an entire model locally, and (ii) \"winner-takes-all\" rewards\nencouraged model hoarding.\n  Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an\narchitecture that addresses these limitations by transforming SN9's previously\nisolated competitors into a single cooperating unit that can scale arbitrarily\nwhile still rewarding each contributor fairly.\n  Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture -\nAn orchestrator distributes model layers across heterogeneous miners and\nstreams activations between them, enabling model sizes to scale with the number\nof participants rather than being constrained by the VRAM of a single machine;\n(2) Granular, continuous incentives - Validators measure each miner's\ncontribution and allocate token emissions proportionally; (3) Activation\ncompression - We used model-bottlenecks to cut communication bandwidths of\nactivations by up to 128x, vastly improving training speed; (4) Butterfly\nAll-Reduce - Miners average disjoint parameter slices in O(1) bandwidth,\noffering linear scalability, redundancy and built-in collusion detection; (5)\nCLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair\nattribution scheme assigns credit to miners proportional to their marginal\nutility and detects exploits, even when contributions are interdependent across\nthe pipeline.",
    "published": "2025-07-16T15:16:21Z",
    "updated": "2025-07-16T15:16:21Z",
    "id": "2507.17766v1",
    "authors": [
      "Felix Quinque",
      "Alan Aboudib",
      "Szymon Fonau",
      "Rodrigo Lopez Portillo Alcocer",
      "Brian McCrindle",
      "Steffen Cruz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.17766v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.17766v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.17766v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a distributed network for pretraining large language models (LLMs) and introduces an architecture (IOTA) to address limitations in decentralized pretraining. It focuses on scaling LLMs and improving training efficiency, which aligns with topics related to LLM, Scaling, and Pretrain.",
    "llm_cls_result": [
      "LLM",
      "Scaling",
      "Pretrain"
    ]
  },
  "2507.12308v1": {
    "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and\n  Summarization",
    "summary": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.",
    "published": "2025-07-16T15:05:30Z",
    "updated": "2025-07-16T15:05:30Z",
    "id": "2507.12308v1",
    "authors": [
      "Prashanth Vijayaraghavan",
      "Apoorva Nitsure",
      "Charles Mackin",
      "Luyao Shi",
      "Stefano Ambrogio",
      "Arvind Haran",
      "Viresh Paruthi",
      "Ali Elzein",
      "Dan Coops",
      "David Beymer",
      "Tyler Baldwin",
      "Ehsan Degan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12308v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12308v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12308v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating and improving Large Language Models (LLMs) for specific tasks in hardware description languages (HDLs), particularly VHDL, using a novel approach called Chain-of-Descriptions (CoDes). It involves code generation and summarization, which are related to reasoning and LLM capabilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.12296v1": {
    "title": "Humans are more gullible than LLMs in believing common psychological\n  myths",
    "summary": "Despite widespread debunking, many psychological myths remain deeply\nentrenched. This paper investigates whether Large Language Models (LLMs) mimic\nhuman behaviour of myth belief and explores methods to mitigate such\ntendencies. Using 50 popular psychological myths, we evaluate myth belief\nacross multiple LLMs under different prompting strategies, including\nretrieval-augmented generation and swaying prompts. Results show that LLMs\nexhibit significantly lower myth belief rates than humans, though user\nprompting can influence responses. RAG proves effective in reducing myth belief\nand reveals latent debiasing potential within LLMs. Our findings contribute to\nthe emerging field of Machine Psychology and highlight how cognitive science\nmethods can inform the evaluation and development of LLM-based systems.",
    "published": "2025-07-16T14:49:45Z",
    "updated": "2025-07-16T14:49:45Z",
    "id": "2507.12296v1",
    "authors": [
      "Bevan Koopman",
      "Guido Zuccon"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12296v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12296v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12296v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the behavior of Large Language Models (LLMs) in relation to psychological myths, comparing their responses to human beliefs and exploring methods like retrieval-augmented generation to mitigate myth belief. The focus is on LLMs and their interaction with psychological myths, which aligns with the 'LLM' and 'Reasoning' topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.12273v2": {
    "title": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction\n  with an Agentic Robot",
    "summary": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments.",
    "published": "2025-07-16T14:22:00Z",
    "updated": "2025-07-17T14:54:27Z",
    "id": "2507.12273v2",
    "authors": [
      "Luca Garello",
      "Francesca Cocchella",
      "Alessandra Sciutti",
      "Manuel Catalano",
      "Francesco Rea"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12273v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12273v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12273v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in an autonomous robot for museum guidance and interaction, which aligns with the topics of LLM and RL (Reinforcement Learning) due to the interactive and adaptive capabilities of the robot. Additionally, the deployment in a real-world environment suggests relevance to AGI (Artificial General Intelligence) as it involves generalist models and real-world applications.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.12252v1": {
    "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language\n  Models",
    "summary": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.",
    "published": "2025-07-16T13:59:32Z",
    "updated": "2025-07-16T13:59:32Z",
    "id": "2507.12252v1",
    "authors": [
      "Shilin Zhou",
      "Zhenghua Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12252v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12252v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12252v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving Automatic Speech Recognition (ASR) by leveraging Large Language Models (LLMs) through multi-grained fusion, which involves both token-level and phrase-level fusion strategies. The core of the research is the integration of LLMs to enhance contextual understanding and keyword recognition in ASR systems.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.12232v1": {
    "title": "MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection\n  with VLM",
    "summary": "Recent studies have utilized visual large language models (VLMs) to answer\nnot only \"Is this face a forgery?\" but also \"Why is the face a forgery?\" These\nstudies introduced forgery-related attributes, such as forgery location and\ntype, to construct deepfake VQA datasets and train VLMs, achieving high\naccuracy while providing human-understandable explanatory text descriptions.\nHowever, these methods still have limitations. For example, they do not fully\nleverage face quality-related attributes, which are often abnormal in forged\nfaces, and they lack effective training strategies for forgery-aware VLMs. In\nthis paper, we extend the VQA dataset to create DD-VQA+, which features a\nricher set of attributes and a more diverse range of samples. Furthermore, we\nintroduce a novel forgery detection framework, MGFFD-VLM, which integrates an\nAttribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual\nLarge Language Models (VLMs). Additionally, our framework incorporates\nMulti-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By\ntransforming classification and forgery segmentation results into prompts, our\nmethod not only improves forgery classification but also enhances\ninterpretability. To further boost detection performance, we design multiple\nforgery-related auxiliary losses. Experimental results demonstrate that our\napproach surpasses existing methods in both text-based forgery judgment and\nanalysis, achieving superior accuracy.",
    "published": "2025-07-16T13:47:13Z",
    "updated": "2025-07-16T13:47:13Z",
    "id": "2507.12232v1",
    "authors": [
      "Tao Chen",
      "Jingyi Zhang",
      "Decheng Liu",
      "Chunlei Peng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12232v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12232v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12232v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing Visual Large Language Models (VLMs) for face forgery detection, which involves multimodal integration of vision and language for forgery detection and explanation. The methods include prompt learning and training strategies specific to VLMs.",
    "llm_cls_result": [
      "VLA",
      "MLLM"
    ]
  },
  "2507.12212v1": {
    "title": "Draw an Ugly Person An Exploration of Generative AIs Perceptions of\n  Ugliness",
    "summary": "Generative AI does not only replicate human creativity but also reproduces\ndeep-seated cultural biases, making it crucial to critically examine how\nconcepts like ugliness are understood and expressed by these tools. This study\ninvestigates how four different generative AI models understand and express\nugliness through text and image and explores the biases embedded within these\nrepresentations. We extracted 13 adjectives associated with ugliness through\niterative prompting of a large language model and generated 624 images across\nfour AI models and three prompts. Demographic and socioeconomic attributes\nwithin the images were independently coded and thematically analyzed. Our\nfindings show that AI models disproportionately associate ugliness with old\nwhite male figures, reflecting entrenched social biases as well as paradoxical\nbiases, where efforts to avoid stereotypical depictions of marginalized groups\ninadvertently result in the disproportionate projection of negative attributes\nonto majority groups. Qualitative analysis further reveals that, despite\nsupposed attempts to frame ugliness within social contexts, conventional\nphysical markers such as asymmetry and aging persist as central visual motifs.\nThese findings demonstrate that despite attempts to create more equal\nrepresentations, generative AI continues to perpetuate inherited and\nparadoxical biases, underscoring the critical work being done to create ethical\nAI training paradigms and advance methodologies for more inclusive AI\ndevelopment.",
    "published": "2025-07-16T13:16:56Z",
    "updated": "2025-07-16T13:16:56Z",
    "id": "2507.12212v1",
    "authors": [
      "Garyoung Kim",
      "Huisung Kwon",
      "Seoju Yun",
      "Yu-Won Youn"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12212v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12212v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12212v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the biases in generative AI models, particularly in their understanding and representation of concepts like ugliness, which is not directly covered by the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.12205v1": {
    "title": "Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed\n  Storage",
    "summary": "Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance\nbottleneck in the local deployment of sparse Large Language Models (LLMs),\nwhere inference predominantly operates on workloads during the decoder phase\nwith a batch size of one. Existing SpMV kernels and sparse matrix formats,\noriginally designed for scientific computing, fail to exploit the unique\nstructure patterns inherent in sparse LLMs, resulting in suboptimal performance\nand excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized\nSpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a\nhierarchical block extraction algorithm that captures multiple granularities of\nblock structures within sparse LLMs, and (2) a novel compressed sparse format\n(EC-CSR) that employs delta indexing to reduce storage overhead and enhance\nmemory access efficiency. Evaluated on real sparse weight matrices from LLaMA\nand OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV\nlibraries and reduces storage overhead by up to 55.4% compared to CSR.",
    "published": "2025-07-16T13:04:06Z",
    "updated": "2025-07-16T13:04:06Z",
    "id": "2507.12205v1",
    "authors": [
      "Junqing Lin",
      "Jingwei Sun",
      "Mingge Lu",
      "Guangzhong Sun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12205v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12205v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12205v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing Sparse Matrix-Vector Multiplication (SpMV) for sparse Large Language Models (LLMs), which directly relates to the efficiency and deployment of LLMs. The core topics are LLM and Scaling, as it discusses performance bottlenecks and optimization techniques for sparse LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.14221v1": {
    "title": "Identifying Algorithmic and Domain-Specific Bias in Parliamentary Debate\n  Summarisation",
    "summary": "The automated summarisation of parliamentary debates using large language\nmodels (LLMs) offers a promising way to make complex legislative discourse more\naccessible to the public. However, such summaries must not only be accurate and\nconcise but also equitably represent the views and contributions of all\nspeakers. This paper explores the use of LLMs to summarise plenary debates from\nthe European Parliament and investigates the algorithmic and representational\nbiases that emerge in this context. We propose a structured, multi-stage\nsummarisation framework that improves textual coherence and content fidelity,\nwhile enabling the systematic analysis of how speaker attributes -- such as\nspeaking order or political affiliation -- influence the visibility and\naccuracy of their contributions in the final summaries. Through our experiments\nusing both proprietary and open-weight LLMs, we find evidence of consistent\npositional and partisan biases, with certain speakers systematically\nunder-represented or misattributed. Our analysis shows that these biases vary\nby model and summarisation strategy, with hierarchical approaches offering the\ngreatest potential to reduce disparity. These findings underscore the need for\ndomain-sensitive evaluation metrics and ethical oversight in the deployment of\nLLMs for democratic applications.",
    "published": "2025-07-16T11:49:33Z",
    "updated": "2025-07-16T11:49:33Z",
    "id": "2507.14221v1",
    "authors": [
      "Eoghan Cunningham",
      "James Cross",
      "Derek Greene"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14221v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14221v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14221v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of LLMs for summarising parliamentary debates and investigates biases in the process, which is relevant to LLM research and their application in specific domains.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.12143v1": {
    "title": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as\n  Teachers, Students and Evaluators",
    "summary": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.",
    "published": "2025-07-16T11:19:28Z",
    "updated": "2025-07-16T11:19:28Z",
    "id": "2507.12143v1",
    "authors": [
      "Pavel indel",
      "Ondej Bojar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12143v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12143v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12143v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of generative language models (LLMs) in a classroom-like setting, focusing on their abilities as teachers, students, and evaluators. It involves the use of LLMs for question generation, answering, and evaluation, which aligns with the 'Benchmark' and 'Reasoning' topics. The 'Benchmark' topic is relevant as it involves evaluating LLMs' performance, and the 'Reasoning' topic is relevant as it involves the models' ability to make sense of text and answer questions.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning"
    ]
  },
  "2507.12142v1": {
    "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA\n  Optimization",
    "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.",
    "published": "2025-07-16T11:17:12Z",
    "updated": "2025-07-16T11:17:12Z",
    "id": "2507.12142v1",
    "authors": [
      "Vladimir Bogachev",
      "Vladimir Aletov",
      "Alexander Molozhavenko",
      "Denis Bobkov",
      "Vera Soboleva",
      "Aibek Alanov",
      "Maxim Rakhuba"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12142v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12142v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12142v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving Low-Rank Adaptation (LoRA) for fine-tuning large language models (LLMs), which is a key technique in LLM research. It addresses challenges in LoRA optimization, which is relevant to the 'LLM' topic. The work also involves optimization techniques that could be broadly related to 'Scaling' as it aims to improve efficiency and performance in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.12126v1": {
    "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation\n  for Unstructured Survey data Modeling and Analysis",
    "summary": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.",
    "published": "2025-07-16T10:49:30Z",
    "updated": "2025-07-16T10:49:30Z",
    "id": "2507.12126v1",
    "authors": [
      "Payal Bhattad",
      "Sai Manoj Pudukotai Dinakarrao",
      "Anju Gupta"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12126v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12126v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12126v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for text data augmentation in NLP tasks, focusing on semantic preservation and evaluation frameworks. It specifically mentions LLMs like GPT-3.5 Turbo and their application in improving topic modeling, which aligns with the 'LLM' and 'Benchmark' categories.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.12123v1": {
    "title": "Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph",
    "summary": "We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects\nusing 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor\nenvironment over a Hierarchical Scene Graph derived from sequences of RGB-D\nframes utilizing a set of open-vocabulary foundation models and sensor data\nprocessing. The hierarchical representation explicitly models spatial relations\nacross floors, rooms, locations, and objects. To effectively address complex\nqueries involving spatial reference to other objects, we integrate the\nhierarchical scene graph with a Large Language Model for multistep reasoning.\nThis integration leverages inter-layer (e.g., room-to-object) and intra-layer\n(e.g., object-to-object) connections, enhancing spatial contextual\nunderstanding. We investigate the semantic and geometry accuracy of\nhierarchical representation on Habitat Matterport 3D Semantic multi-floor\nscenes. Our approach demonstrates efficient scene comprehension and robust\nobject grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates\nstrong potential for applications requiring spatial reasoning and understanding\nof indoor environments. Related materials can be found at\nhttps://github.com/linukc/OVIGo-3DHSG.",
    "published": "2025-07-16T10:47:12Z",
    "updated": "2025-07-16T10:47:12Z",
    "id": "2507.12123v1",
    "authors": [
      "Sergey Linok",
      "Gleb Naumov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12123v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12123v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12123v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on integrating a Large Language Model with a 3D Hierarchical Scene Graph for object grounding and spatial reasoning in indoor environments, which aligns with the topics of LLM (Large Language Models) and Reasoning (LLM reasoning abilities). The use of multimodal data (RGB-D frames) also suggests relevance to MLLM (Multimodal Large Language Models).",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "MLLM"
    ]
  },
  "2507.12084v1": {
    "title": "LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided\n  Seed Generation",
    "summary": "Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing\nremains an important approach to securing smart contracts. Even though mutation\nscheduling is a key factor influencing fuzzing effectiveness, existing fuzzers\nhave primarily explored seed scheduling and generation, while mutation\nscheduling has been rarely addressed by prior work. In this work, we propose a\nLarge Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing\nframework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and\nhybrid testing techniques. Key components of the proposed LLAMA include: (i) a\nhierarchical prompting strategy that guides LLMs to generate semantically valid\ninitial seeds, coupled with a lightweight pre-fuzzing phase to select\nhigh-potential inputs; (ii) a multi-feedback optimization mechanism that\nsimultaneously improves seed generation, seed selection, and mutation\nscheduling by leveraging runtime coverage and dependency feedback; and (iii) an\nevolutionary fuzzing engine that dynamically adjusts mutation operator\nprobabilities based on effectiveness, while incorporating symbolic execution to\nescape stagnation and uncover deeper vulnerabilities. Our experiments\ndemonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage\nand vulnerability detection. Specifically, it achieves 91% instruction coverage\nand 90% branch coverage, while detecting 132 out of 148 known vulnerabilities\nacross diverse categories. These results highlight LLAMA's effectiveness,\nadaptability, and practicality in real-world smart contract security testing\nscenarios.",
    "published": "2025-07-16T09:46:58Z",
    "updated": "2025-07-16T09:46:58Z",
    "id": "2507.12084v1",
    "authors": [
      "Keke Gai",
      "Haochen Liang",
      "Jing Yu",
      "Liehuang Zhu",
      "Dusit Niyato"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12084v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12084v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12084v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a smart contract fuzzing framework, which involves LLM-guided seed generation and evolutionary mutation strategies. This aligns with the 'LLM' topic as it involves research on the application of Large Language Models. Additionally, the use of evolutionary strategies and optimization mechanisms touches on aspects of 'RL' (Reinforcement Learning) and 'AGI' (Artificial General Intelligence) due to the adaptive and learning components involved.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.13392v1": {
    "title": "TopicImpact: Improving Customer Feedback Analysis with Opinion Units for\n  Topic Modeling and Star-Rating Prediction",
    "summary": "We improve the extraction of insights from customer reviews by restructuring\nthe topic modelling pipeline to operate on opinion units - distinct statements\nthat include relevant text excerpts and associated sentiment scores. Prior work\nhas demonstrated that such units can be reliably extracted using large language\nmodels. The result is a heightened performance of the subsequent topic\nmodeling, leading to coherent and interpretable topics while also capturing the\nsentiment associated with each topic. By correlating the topics and sentiments\nwith business metrics, such as star ratings, we can gain insights on how\nspecific customer concerns impact business outcomes. We present our system's\nimplementation, use cases, and advantages over other topic modeling and\nclassification solutions. We also evaluate its effectiveness in creating\ncoherent topics and assess methods for integrating topic and sentiment\nmodalities for accurate star-rating prediction.",
    "published": "2025-07-16T09:19:26Z",
    "updated": "2025-07-16T09:19:26Z",
    "id": "2507.13392v1",
    "authors": [
      "Emil Hglund",
      "Johanna Bjrklund"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13392v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13392v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13392v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving customer feedback analysis using opinion units extracted by large language models, which is not directly related to the core topics provided in the list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.12039v2": {
    "title": "A Comparative Approach to Assessing Linguistic Creativity of Large\n  Language Models and Humans",
    "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.",
    "published": "2025-07-16T08:56:19Z",
    "updated": "2025-07-17T15:27:29Z",
    "id": "2507.12039v2",
    "authors": [
      "Anca Dinu",
      "Andra-Maria Florescu",
      "Alina Resceanu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12039v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12039v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12039v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on assessing the linguistic creativity of Large Language Models (LLMs) compared to humans, which directly involves research on LLMs and their capabilities.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.12015v1": {
    "title": "EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis",
    "summary": "In recent years, emotional Text-to-Speech (TTS) synthesis and\nemphasis-controllable speech synthesis have advanced significantly. However,\ntheir interaction remains underexplored. We propose Emphasis Meets Emotion TTS\n(EME-TTS), a novel framework designed to address two key research questions:\n(1) how to effectively utilize emphasis to enhance the expressiveness of\nemotional speech, and (2) how to maintain the perceptual clarity and stability\nof target emphasis across different emotions. EME-TTS employs weakly supervised\nlearning with emphasis pseudo-labels and variance-based emphasis features.\nAdditionally, the proposed Emphasis Perception Enhancement (EPE) block enhances\nthe interaction between emotional signals and emphasis positions. Experimental\nresults show that EME-TTS, when combined with large language models for\nemphasis position prediction, enables more natural emotional speech synthesis\nwhile preserving stable and distinguishable target emphasis across emotions.\nSynthesized samples are available on-line.",
    "published": "2025-07-16T08:19:20Z",
    "updated": "2025-07-16T08:19:20Z",
    "id": "2507.12015v1",
    "authors": [
      "Haoxun Li",
      "Leyuan Qu",
      "Jiaxi Hu",
      "Taihao Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12015v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12015v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12015v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on emotional and emphasis-controllable speech synthesis, which does not directly align with the provided topics related to Large Language Models, Reinforcement Learning, Multimodal models, etc. It is more related to speech synthesis and emotion modeling in TTS systems.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.12000v2": {
    "title": "DSSD: Efficient Edge-Device LLM Deployment and Collaborative Inference\n  via Distributed Split Speculative Decoding",
    "summary": "Large language models (LLMs) have transformed natural language processing but\nface critical deployment challenges in device-edge systems due to resource\nlimitations and communication overhead. To address these issues, collaborative\nframeworks have emerged that combine small language models (SLMs) on devices\nwith LLMs at the edge, using speculative decoding (SD) to improve efficiency.\nHowever, existing solutions often trade inference accuracy for latency or\nsuffer from high uplink transmission costs when verifying candidate tokens. In\nthis paper, we propose Distributed Split Speculative Decoding (DSSD), a novel\narchitecture that not only preserves the SLM-LLM split but also partitions the\nverification phase between the device and edge. In this way, DSSD replaces the\nuplink transmission of multiple vocabulary distributions with a single downlink\ntransmission, significantly reducing communication latency while maintaining\ninference quality. Experiments show that our solution outperforms current\nmethods, and codes are at:\nhttps://github.com/JasonNing96/DSSD-Efficient-Edge-Computing",
    "published": "2025-07-16T07:55:06Z",
    "updated": "2025-07-17T02:34:42Z",
    "id": "2507.12000v2",
    "authors": [
      "Jiahong Ning",
      "Ce Zheng",
      "Tingting Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12000v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12000v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12000v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the deployment of large language models (LLMs) in device-edge systems, focusing on efficiency and communication overhead. It introduces a novel architecture for collaborative inference, which is relevant to LLM deployment and optimization.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.11997v1": {
    "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection",
    "summary": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.",
    "published": "2025-07-16T07:50:43Z",
    "updated": "2025-07-16T07:50:43Z",
    "id": "2507.11997v1",
    "authors": [
      "Tairan Huang",
      "Yili Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11997v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11997v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11997v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in enhancing graph fraud detection by processing textual information and integrating it with graph structures. This involves multimodal fusion and leveraging LLMs' capabilities, which aligns with the topics of LLM and MLLM.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.11981v1": {
    "title": "Simplifications are Absolutists: How Simplified Language Reduces Word\n  Sense Awareness in LLM-Generated Definitions",
    "summary": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.",
    "published": "2025-07-16T07:25:27Z",
    "updated": "2025-07-16T07:25:27Z",
    "id": "2507.11981v1",
    "authors": [
      "Lukas Ellinger",
      "Miriam Anschtz",
      "Georg Groh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11981v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11981v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11981v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the impact of simplified language on the quality of word definitions generated by Large Language Models (LLMs), specifically addressing homonyms and the risks of information loss. It involves evaluation of multiple LLMs and fine-tuning techniques to improve definition quality.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Dataset"
    ]
  },
  "2507.11979v1": {
    "title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation\n  of Trust and Interpersonal Closeness",
    "summary": "Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences.",
    "published": "2025-07-16T07:21:59Z",
    "updated": "2025-07-16T07:21:59Z",
    "id": "2507.11979v1",
    "authors": [
      "Yuki Sakamoto",
      "Takahisa Uchida",
      "Hiroshi Ishiguro"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11979v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11979v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11979v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to simulate social phenomena and evaluate trust and interpersonal closeness among LLM agents, which aligns with research on LLMs and their applications in social simulations.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.11972v1": {
    "title": "Graph Representations for Reading Comprehension Analysis using Large\n  Language Model and Eye-Tracking Biomarker",
    "summary": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.",
    "published": "2025-07-16T07:15:59Z",
    "updated": "2025-07-16T07:15:59Z",
    "id": "2507.11972v1",
    "authors": [
      "Yuhong Zhang",
      "Jialu Li",
      "Shilai Yang",
      "Yuchen Xu",
      "Gert Cauwenberghs",
      "Tzyy-Ping Jung"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11972v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11972v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11972v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to analyze reading comprehension and compares human and LLM understanding through graph-based representations and eye-tracking data. The focus on LLMs and their application in understanding language and cognitive processes aligns with the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.11968v1": {
    "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on\n  Short Videos for Content Appropriateness Evaluation",
    "summary": "Multimodal Large Language Models (MLLMs) are increasingly used for content\nmoderation, yet their robustness in short-form video contexts remains\nunderexplored. Current safety evaluations often rely on unimodal attacks,\nfailing to address combined attack vulnerabilities. In this paper, we introduce\na comprehensive framework for evaluating the tri-modal safety of MLLMs. First,\nwe present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising\ndiverse short-form videos with human-guided synthetic adversarial attacks.\nSecond, we propose ChimeraBreak, a novel tri-modal attack strategy that\nsimultaneously challenges visual, auditory, and semantic reasoning pathways.\nExtensive experiments on state-of-the-art MLLMs reveal significant\nvulnerabilities with high Attack Success Rates (ASR). Our findings uncover\ndistinct failure modes, showing model biases toward misclassifying benign or\npolicy-violating content. We assess results using LLM-as-a-judge, demonstrating\nattack reasoning efficacy. Our dataset and findings provide crucial insights\nfor developing more robust and safe MLLMs.",
    "published": "2025-07-16T07:02:15Z",
    "updated": "2025-07-16T07:02:15Z",
    "id": "2507.11968v1",
    "authors": [
      "Sahid Hossain Mustakim",
      "S M Jishanul Islam",
      "Ummay Maria Muna",
      "Montasir Chowdhury",
      "Mohammed Jawwadul Islam",
      "Sadia Ahmmed",
      "Tashfia Sikder",
      "Syed Tasdid Azam Dhrubo",
      "Swakkhar Shatabda"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11968v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11968v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11968v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Multimodal Large Language Models (MLLMs) and their robustness in short-form video contexts, introducing a tri-modal adversarial attack strategy and a dataset for evaluation. The core topics are MLLM for the multimodal aspect and Benchmark for the evaluation and dataset creation.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.11959v1": {
    "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.",
    "published": "2025-07-16T06:44:14Z",
    "updated": "2025-07-16T06:44:14Z",
    "id": "2507.11959v1",
    "authors": [
      "Xinyu Wang",
      "Vahid Partovi Nia",
      "Peng Lu",
      "Jerry Huang",
      "Xiao-Wen Chang",
      "Boxing Chen",
      "Yufei Cui"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11959v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11959v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11959v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on quantization techniques for Large Language Models (LLMs) to improve computational efficiency and inference speed, which is a core aspect of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.11941v1": {
    "title": "BlockBPE: Parallel BPE Tokenization",
    "summary": "Tokenization is a critical preprocessing step in large language model\npipelines, yet widely-used implementations remain CPU-bound and suboptimal for\nbatch inference workflows on GPU. We present BlockBPE, a parallel GPU\nimplementation of byte-pair encoding (BPE) that achieves near linear-time\ncomplexity under realistic assumptions and is optimized for high-throughput,\nbatch inference. Unlike existing Rust-based tokenizers such as HuggingFace\nTokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex\npre-tokenization and exhibit $O(n \\log n)$ runtime-BlockBPE eliminates the\nRegex pre-tokenization which leads to small loss in generation quality, but\nenables highly parallelized token merges within thread blocks, reducing overall\ncomplexity to $O(nd)$ where $d \\ll n$. On high-batch inference workloads,\nBlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over\nHuggingFace Tokenizers.",
    "published": "2025-07-16T06:12:41Z",
    "updated": "2025-07-16T06:12:41Z",
    "id": "2507.11941v1",
    "authors": [
      "Amos You"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11941v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11941v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11941v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the tokenization process for large language models, which is a preprocessing step in LLM pipelines. However, it does not directly contribute to the core topics of LLM research, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.11938v1": {
    "title": "A Multi-Level Similarity Approach for Single-View Object Grasping:\n  Matching, Planning, and Fine-Tuning",
    "summary": "Grasping unknown objects from a single view has remained a challenging topic\nin robotics due to the uncertainty of partial observation. Recent advances in\nlarge-scale models have led to benchmark solutions such as GraspNet-1Billion.\nHowever, such learning-based approaches still face a critical limitation in\nperformance robustness for their sensitivity to sensing noise and environmental\nchanges. To address this bottleneck in achieving highly generalized grasping,\nwe abandon the traditional learning framework and introduce a new perspective:\nsimilarity matching, where similar known objects are utilized to guide the\ngrasping of unknown target objects. We newly propose a method that robustly\nachieves unknown-object grasping from a single viewpoint through three key\nsteps: 1) Leverage the visual features of the observed object to perform\nsimilarity matching with an existing database containing various object models,\nidentifying potential candidates with high similarity; 2) Use the candidate\nmodels with pre-existing grasping knowledge to plan imitative grasps for the\nunknown target object; 3) Optimize the grasp quality through a local\nfine-tuning process. To address the uncertainty caused by partial and noisy\nobservation, we propose a multi-level similarity matching framework that\nintegrates semantic, geometric, and dimensional features for comprehensive\nevaluation. Especially, we introduce a novel point cloud geometric descriptor,\nthe C-FPFH descriptor, which facilitates accurate similarity assessment between\npartial point clouds of observed objects and complete point clouds of database\nmodels. In addition, we incorporate the use of large language models, introduce\nthe semi-oriented bounding box, and develop a novel point cloud registration\napproach based on plane detection to enhance matching accuracy under\nsingle-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.",
    "published": "2025-07-16T06:07:57Z",
    "updated": "2025-07-16T06:07:57Z",
    "id": "2507.11938v1",
    "authors": [
      "Hao Chen",
      "Takuya Kiyokawa",
      "Zhengtao Hu",
      "Weiwei Wan",
      "Kensuke Harada"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11938v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11938v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11938v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on object grasping in robotics, leveraging visual features and similarity matching, with a mention of large language models (LLMs) but not as a core focus. The primary topics are not directly covered by the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.11936v3": {
    "title": "A Survey of Deep Learning for Geometry Problem Solving",
    "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.",
    "published": "2025-07-16T06:03:08Z",
    "updated": "2025-07-24T06:15:29Z",
    "id": "2507.11936v3",
    "authors": [
      "Jianzhe Ma",
      "Wenxuan Wang",
      "Qin Jin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11936v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11936v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11936v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of deep learning, particularly multimodal large language models, in geometry problem solving, which involves mathematical reasoning and multimodal ability assessment.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.11809v1": {
    "title": "Tracing Facts or just Copies? A critical investigation of the\n  Competitions of Mechanisms in Large Language Models",
    "summary": "This paper presents a reproducibility study examining how Large Language\nModels (LLMs) manage competing factual and counterfactual information, focusing\non the role of attention heads in this process. We attempt to reproduce and\nreconcile findings from three recent studies by Ortu et al., Yu, Merullo, and\nPavlick and McDougall et al. that investigate the competition between\nmodel-learned facts and contradictory context information through Mechanistic\nInterpretability tools. Our study specifically examines the relationship\nbetween attention head strength and factual output ratios, evaluates competing\nhypotheses about attention heads' suppression mechanisms, and investigates the\ndomain specificity of these attention patterns. Our findings suggest that\nattention heads promoting factual output do so via general copy suppression\nrather than selective counterfactual suppression, as strengthening them can\nalso inhibit correct facts. Additionally, we show that attention head behavior\nis domain-dependent, with larger models exhibiting more specialized and\ncategory-sensitive patterns.",
    "published": "2025-07-16T00:08:48Z",
    "updated": "2025-07-16T00:08:48Z",
    "id": "2507.11809v1",
    "authors": [
      "Dante Campregher",
      "Yanxu Chen",
      "Sander Hoffman",
      "Maria Heuss"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11809v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11809v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11809v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the mechanisms within Large Language Models (LLMs), specifically examining how attention heads manage factual and counterfactual information. This aligns with research on LLM architectures and their internal workings.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.16835v1": {
    "title": "Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI\n  Interview Systems",
    "summary": "Voice-based conversational AI systems increasingly rely on cascaded\narchitectures combining speech-to-text (STT), large language models (LLMs), and\ntext-to-speech (TTS) components. However, systematic evaluation of different\ncomponent combinations in production settings remains understudied. We present\na large-scale empirical comparison of STT x LLM x TTS stacks using data from\nover 300,000 AI-conducted job interviews. We develop an automated evaluation\nframework using LLM-as-a-Judge to assess conversational quality, technical\naccuracy, and skill assessment capabilities. Our analysis of four production\nconfigurations reveals that Google STT paired with GPT-4.1 significantly\noutperforms alternatives in both conversational and technical quality metrics.\nSurprisingly, we find that objective quality metrics correlate weakly with user\nsatisfaction scores, suggesting that user experience in voice-based AI systems\ndepends on factors beyond technical performance. Our findings provide practical\nguidance for selecting components in multimodal conversational AI systems and\ncontribute a validated evaluation methodology for voice-based interactions.",
    "published": "2025-07-15T22:30:55Z",
    "updated": "2025-07-15T22:30:55Z",
    "id": "2507.16835v1",
    "authors": [
      "Nima Yazdani",
      "Ali Ansari",
      "Aruj Mahajan",
      "Amirhossein Afsharrad",
      "Seyed Shahabeddin Mousavi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16835v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16835v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16835v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating combinations of speech-to-text, large language models (LLMs), and text-to-speech components in conversational AI systems, with a specific emphasis on LLMs and their performance in multimodal settings.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.11771v1": {
    "title": "Scaling laws for activation steering with Llama 2 models and refusal\n  mechanisms",
    "summary": "As large language models (LLMs) evolve in complexity and capability, the\nefficacy of less widely deployed alignment techniques are uncertain. Building\non previous work on activation steering and contrastive activation addition\n(CAA), this paper explores the effectiveness of CAA with model scale using the\nfamily of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable\n'directions' in the model's residual stream vector space using contrastive\npairs (for example, hate to love) and adding this direction to the residual\nstream during the forward pass. It directly manipulates the residual stream and\naims to extract features from language models to better control their outputs.\nUsing answer matching questions centered around the refusal behavior, we found\nthat 1) CAA is most effective when applied at early-mid layers. 2) The\neffectiveness of CAA diminishes with model size. 3) Negative steering has more\npronounced effects than positive steering across all model sizes.",
    "published": "2025-07-15T22:21:18Z",
    "updated": "2025-07-15T22:21:18Z",
    "id": "2507.11771v1",
    "authors": [
      "Sheikh Abdur Raheem Ali",
      "Justin Xu",
      "Ivory Yang",
      "Jasmine Xinze Li",
      "Ayse Arslan",
      "Clark Benham"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11771v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11771v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11771v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the effectiveness of activation steering techniques (CAA) with the scaling of Llama 2 models, which involves large language models (LLMs) and their alignment techniques. It also touches on the impact of model size on the effectiveness of these techniques.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.11742v1": {
    "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM\n  interpretation of Python notebooks",
    "summary": "Recognizing the information flows and operations comprising data science and\nmachine learning Python notebooks is critical for evaluating, reusing, and\nadapting notebooks for new tasks. Investigating a notebook via re-execution\noften is impractical due to the challenges of resolving data and software\ndependencies. While Large Language Models (LLMs) pre-trained on large codebases\nhave demonstrated effectiveness in understanding code without running it, we\nobserve that they fail to understand some realistic notebooks due to\nhallucinations and long-context challenges. To address these issues, we propose\na notebook understanding task yielding an information flow graph and\ncorresponding cell execution dependency graph for a notebook, and demonstrate\nthe effectiveness of a pincer strategy that uses limited syntactic analysis to\nassist full comprehension of the notebook using an LLM. Our Capture and Resolve\nAssisted Bounding Strategy (CRABS) employs shallow syntactic parsing and\nanalysis of the abstract syntax tree (AST) to capture the correct\ninterpretation of a notebook between lower and upper estimates of the\ninter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via\ncell-by-cell zero-shot learning, thereby identifying the true data inputs and\noutputs of each cell. We evaluate and demonstrate the effectiveness of our\napproach using an annotated dataset of 50 representative, highly up-voted\nKaggle notebooks that together represent 3454 actual cell inputs and outputs.\nThe LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the\nsyntactic structure of these notebooks. Across 50 notebooks, CRABS achieves\naverage F1 scores of 98% identifying cell-to-cell information flows and 99%\nidentifying transitive cell execution dependencies.",
    "published": "2025-07-15T21:14:08Z",
    "updated": "2025-07-15T21:14:08Z",
    "id": "2507.11742v1",
    "authors": [
      "Meng Li",
      "Timothy M. McPhillips",
      "Dingmin Wang",
      "Shin-Rong Tsai",
      "Bertram Ludscher"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11742v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11742v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11742v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using LLMs to understand Python notebooks, addressing challenges like hallucinations and long-context issues. It proposes a strategy combining syntactic analysis with LLM-based resolution, which is relevant to LLM research and reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.11687v1": {
    "title": "MetaLint: Generalizable Idiomatic Code Quality Analysis through\n  Instruction-Following and Easy-to-Hard Generalization",
    "summary": "Large Language Models, though successful in code generation, struggle with\ncode quality analysis because they are limited by static training data and\ncan't easily adapt to evolving best practices. We introduce MetaLint, a new\ninstruction-following framework that formulates code quality analysis as the\ntask of detecting and fixing problematic semantic code fragments or code idioms\nbased on high-level specifications. Unlike conventional approaches that train\nmodels on static, rule-based data, MetaLint employs instruction tuning on\nsynthetic linter-generated data to support easy-to-hard generalization,\nenabling models to adapt to novel or complex code patterns without retraining.\nTo evaluate this, we construct a benchmark of challenging idioms inspired by\nreal-world coding standards such as Python Enhancement Proposals (PEPs) and\nassess whether MetaLint-trained models reason adaptively or simply memorize.\nOur results show that MetaLint improves generalization to unseen PEP idioms,\nachieving a 70.37% F-score on idiom detection with the highest recall (70.43%)\namong all evaluated models. It also achieves 26.73% on localization,\ncompetitive for its 4B parameter size and comparable to larger state-of-the-art\nmodels like o3-mini, highlighting its potential for future-proof code quality\nanalysis.",
    "published": "2025-07-15T19:44:20Z",
    "updated": "2025-07-15T19:44:20Z",
    "id": "2507.11687v1",
    "authors": [
      "Atharva Naik",
      "Lawanya Baghel",
      "Dhakshin Govindarajan",
      "Darsh Agrawal",
      "Daniel Fried",
      "Carolyn Rose"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11687v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11687v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11687v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for code quality analysis and introduces a new framework (MetaLint) that employs instruction tuning and easy-to-hard generalization. The core topics are related to LLMs and their application in code analysis, which aligns with the 'LLM' and 'Reasoning' categories.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.11539v1": {
    "title": "Streaming 4D Visual Geometry Transformer",
    "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
    "published": "2025-07-15T17:59:57Z",
    "updated": "2025-07-15T17:59:57Z",
    "id": "2507.11539v1",
    "authors": [
      "Dong Zhuo",
      "Wenzhao Zheng",
      "Jiahe Guo",
      "Yuqi Wu",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11539v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11539v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11539v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a streaming 4D visual geometry transformer for real-time 4D reconstruction from videos, utilizing a causal transformer architecture and temporal causal attention. While it mentions autoregressive large language models and efficient attention operators from LLMs, the core focus is on 4D vision systems and not directly on LLM-related topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.11515v1": {
    "title": "AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of\n  LLM over the Air",
    "summary": "Operating Large Language Models (LLMs) on edge devices is increasingly\nchallenged by limited communication bandwidth and strained computational and\nmemory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.\nNevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ\nfixed or heuristic rank configurations, and the subsequent over-the-air\ntransmission of all LoRA parameters could be rather inefficient. To address\nthis limitation, we develop AirLLM, a hierarchical diffusion policy framework\nfor communication-aware LoRA adaptation. Specifically, AirLLM models the rank\nconfiguration as a structured action vector that spans all LoRA-inserted\nprojections. To solve the underlying high-dimensional sequential\ndecision-making problem, a Proximal Policy Optimization (PPO) agent generates\ncoarse-grained decisions by jointly observing wireless states and linguistic\ncomplexity, which are then refined via Denoising Diffusion Implicit Models\n(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The\ntwo modules are optimized alternatively, with the DDIM trained under the\nClassifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.\nExperiments under varying signal-to-noise ratios demonstrate that AirLLM\nconsistently enhances fine-tuning performance while significantly reducing\ntransmission costs, highlighting the effectiveness of reinforcement-driven,\ndiffusion-refined rank adaptation for scalable and efficient remote fine-tuning\nover the air.",
    "published": "2025-07-15T17:36:37Z",
    "updated": "2025-07-15T17:36:37Z",
    "id": "2507.11515v1",
    "authors": [
      "Shiyi Yang",
      "Xiaoxue Yu",
      "Rongpeng Li",
      "Jianhang Zhu",
      "Zhifeng Zhao",
      "Honggang Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11515v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11515v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11515v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) and their fine-tuning using Low-Rank Adaptation (LoRA) with a focus on communication-aware adaptation and reinforcement learning techniques. The core topics are LLM for the model, RL for the reinforcement learning aspect, and Scaling for the discussion on efficient fine-tuning and transmission costs.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Scaling"
    ]
  },
  "2507.11457v1": {
    "title": "LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis\n  Assessment in Rectal Cancer",
    "summary": "Accurate preoperative assessment of lymph node (LN) metastasis in rectal\ncancer guides treatment decisions, yet conventional MRI evaluation based on\nmorphological criteria shows limited diagnostic performance. While some\nartificial intelligence models have been developed, they often operate as black\nboxes, lacking the interpretability needed for clinical trust. Moreover, these\nmodels typically evaluate nodes in isolation, overlooking the patient-level\ncontext. To address these limitations, we introduce LRMR, an LLM-Driven\nRelational Multi-node Ranking framework. This approach reframes the diagnostic\ntask from a direct classification problem into a structured reasoning and\nranking process. The LRMR framework operates in two stages. First, a multimodal\nlarge language model (LLM) analyzes a composite montage image of all LNs from a\npatient, generating a structured report that details ten distinct radiological\nfeatures. Second, a text-based LLM performs pairwise comparisons of these\nreports between different patients, establishing a relative risk ranking based\non the severity and number of adverse features. We evaluated our method on a\nretrospective cohort of 117 rectal cancer patients. LRMR achieved an area under\nthe curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of\ndeep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies\nconfirmed the value of our two main contributions: removing the relational\nranking stage or the structured prompting stage led to a significant\nperformance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our\nwork demonstrates that decoupling visual perception from cognitive reasoning\nthrough a two-stage LLM framework offers a powerful, interpretable, and\neffective new paradigm for assessing lymph node metastasis in rectal cancer.",
    "published": "2025-07-15T16:29:45Z",
    "updated": "2025-07-15T16:29:45Z",
    "id": "2507.11457v1",
    "authors": [
      "Yaoxian Dong",
      "Yifan Gao",
      "Haoyue Li",
      "Yanfen Cui",
      "Xin Gao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11457v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11457v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11457v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a multimodal large language model (LLM) for analyzing radiological features and performing pairwise comparisons to assess lymph node metastasis in rectal cancer. This involves both the use of LLMs for structured reasoning and the integration of multimodal data (images and text), making it relevant to the topics of LLM and MLLM.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.11423v2": {
    "title": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer,\n  and Optimize?",
    "summary": "Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities.",
    "published": "2025-07-15T15:47:47Z",
    "updated": "2025-07-16T13:02:26Z",
    "id": "2507.11423v2",
    "authors": [
      "Yanjian Zhang",
      "Guillaume Wisniewski",
      "Nadi Tomeh",
      "Thierry Charnois"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11423v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11423v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11423v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on investigating and improving reasoning strategies in large language models (LLMs), which directly relates to the 'Reasoning' topic. It also discusses the use of prompting to control LLMs, which is a key aspect of LLM research, hence the inclusion of 'LLM'.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2507.11417v1": {
    "title": "Quantifying the Energy Consumption and Carbon Emissions of LLM Inference\n  via Simulations",
    "summary": "The environmental impact of Large Language Models (LLMs) is rising\nsignificantly, with inference now accounting for more than half of their total\nlifecycle carbon emissions. However, existing simulation frameworks, which are\nincreasingly used to determine efficient LLM deployments, lack any concept of\npower and, therefore, cannot accurately estimate inference-related emissions.\nWe present a simulation framework to assess the energy and carbon implications\nof LLM inference under varying deployment setups. First, we extend a\nhigh-fidelity LLM inference simulator with a GPU power model that estimates\npower consumption based on utilization metrics, enabling analysis across\nconfigurations like batch size, sequence length, and model parallelism. Second,\nwe integrate simulation outputs into an energy system co-simulation environment\nto quantify carbon emissions under specific grid conditions and explore the\npotential of carbon-aware scheduling. Through scenario-based analysis, our\nframework reveals how inference parameters affect energy demand and carbon\nfootprint, demonstrates a renewable offset potential of up to 69.2% in an\nillustrative deployment case, and provides a foundation for future carbon-aware\ninference infrastructure design.",
    "published": "2025-07-15T15:44:03Z",
    "updated": "2025-07-15T15:44:03Z",
    "id": "2507.11417v1",
    "authors": [
      "Miray zcan",
      "Philipp Wiesner",
      "Philipp Wei",
      "Odej Kao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11417v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11417v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11417v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the energy consumption and carbon emissions of LLM inference, which is directly related to the scaling and environmental impact of Large Language Models.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.11412v1": {
    "title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders",
    "summary": "The large language model (LLM) community focuses almost exclusively on\ndecoder-only language models, since they are easier to use for text generation.\nHowever, a large subset of the community still uses encoder-only models for\ntasks such as classification or retrieval. Previous work has attempted to\ncompare these architectures, but is forced to make comparisons with models that\nhave different numbers of parameters, training techniques, and datasets. We\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\ndecoder-only models produces SOTA recipes in both categories for their\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\ndecoders. Like previous work, we find that encoder-only models excel at\nclassification and retrieval tasks while decoders excel at generative tasks.\nHowever, we show that adapting a decoder model to encoder tasks (and vice\nversa) through continued training is subpar compared to using only the reverse\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\nfor generative tasks). We open-source all artifacts of this study including\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\nallow future work to analyze or extend all aspects of training.",
    "published": "2025-07-15T15:31:51Z",
    "updated": "2025-07-15T15:31:51Z",
    "id": "2507.11412v1",
    "authors": [
      "Orion Weller",
      "Kathryn Ricci",
      "Marc Marone",
      "Antoine Chaffin",
      "Dawn Lawrie",
      "Benjamin Van Durme"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11412v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11412v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11412v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the comparison between encoder-only and decoder-only language models, which are core components of LLMs. It also provides insights into their performance on different tasks, which is relevant to the broader LLM research.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.11407v1": {
    "title": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and\n  Reasoning Modes",
    "summary": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning\nmode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5\nand the advanced reasoning abilities of EXAONE Deep. To pave the way for the\nagentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool\nuse, and its multilingual capabilities are extended to support Spanish in\naddition to English and Korean. The EXAONE 4.0 model series consists of two\nsizes: a mid-size 32B model optimized for high performance, and a small-size\n1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates\nsuperior performance compared to open-weight models in its class and remains\ncompetitive even against frontier-class models. The models are publicly\navailable for research purposes and can be easily downloaded via\nhttps://huggingface.co/LGAI-EXAONE.",
    "published": "2025-07-15T15:24:51Z",
    "updated": "2025-07-15T15:24:51Z",
    "id": "2507.11407v1",
    "authors": [
      "LG AI Research",
      " :",
      "Kyunghoon Bae",
      "Eunbi Choi",
      "Kibong Choi",
      "Stanley Jungkyu Choi",
      "Yemuk Choi",
      "Kyubeen Han",
      "Seokhee Hong",
      "Junwon Hwang",
      "Taewan Hwang",
      "Joonwon Jang",
      "Hyojin Jeon",
      "Kijeong Jeon",
      "Gerrard Jeongwon Jo",
      "Hyunjik Jo",
      "Jiyeon Jung",
      "Euisoon Kim",
      "Hyosang Kim",
      "Jihoon Kim",
      "Joonkee Kim",
      "Seonghwan Kim",
      "Soyeon Kim",
      "Sunkyoung Kim",
      "Yireun Kim",
      "Yongil Kim",
      "Youchul Kim",
      "Edward Hwayoung Lee",
      "Gwangho Lee",
      "Haeju Lee",
      "Honglak Lee",
      "Jinsik Lee",
      "Kyungmin Lee",
      "Sangha Park",
      "Young Min Paik",
      "Yongmin Park",
      "Youngyong Park",
      "Sanghyun Seo",
      "Sihoon Yang",
      "Heuiyeen Yeen",
      "Sihyuk Yi",
      "Hyeongu Yun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11407v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11407v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11407v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a unified large language model (EXAONE 4.0) that integrates both non-reasoning and reasoning modes, which aligns with the topics of LLM (Large Language Models) and Reasoning (reasoning abilities in LLMs). Additionally, the mention of agentic tool use suggests relevance to RL (Reinforcement Learning with Human Feedback).",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "RL"
    ]
  },
  "2507.11364v1": {
    "title": "From Chaos to Automation: Enabling the Use of Unstructured Data for\n  Robotic Process Automation",
    "summary": "The growing volume of unstructured data within organizations poses\nsignificant challenges for data analysis and process automation. Unstructured\ndata, which lacks a predefined format, encompasses various forms such as\nemails, reports, and scans. It is estimated to constitute approximately 80% of\nenterprise data. Despite the valuable insights it can offer, extracting\nmeaningful information from unstructured data is more complex compared to\nstructured data. Robotic Process Automation (RPA) has gained popularity for\nautomating repetitive tasks, improving efficiency, and reducing errors.\nHowever, RPA is traditionally reliant on structured data, limiting its\napplication to processes involving unstructured documents. This study addresses\nthis limitation by developing the UNstructured Document REtrieval SyStem\n(UNDRESS), a system that uses fuzzy regular expressions, techniques for natural\nlanguage processing, and large language models to enable RPA platforms to\neffectively retrieve information from unstructured documents. The research\ninvolved the design and development of a prototype system, and its subsequent\nevaluation based on text extraction and information retrieval performance. The\nresults demonstrate the effectiveness of UNDRESS in enhancing RPA capabilities\nfor unstructured data, providing a significant advancement in the field. The\nfindings suggest that this system could facilitate broader RPA adoption across\nprocesses traditionally hindered by unstructured data, thereby improving\noverall business process efficiency.",
    "published": "2025-07-15T14:32:49Z",
    "updated": "2025-07-15T14:32:49Z",
    "id": "2507.11364v1",
    "authors": [
      "Kelly Kurowski",
      "Xixi Lu",
      "Hajo A. Reijers"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11364v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11364v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11364v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) and natural language processing techniques to enhance Robotic Process Automation (RPA) by enabling it to handle unstructured data. The focus is on the application of LLMs in a specific domain (RPA) rather than the core research on LLMs themselves.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.11356v1": {
    "title": "What is the Best Process Model Representation? A Comparative Analysis\n  for Process Modeling with Large Language Models",
    "summary": "Large Language Models (LLMs) are increasingly applied for Process Modeling\n(PMo) tasks such as Process Model Generation (PMG). To support these tasks,\nresearchers have introduced a variety of Process Model Representations (PMRs)\nthat serve as model abstractions or generation targets. However, these PMRs\ndiffer widely in structure, complexity, and usability, and have never been\nsystematically compared. Moreover, recent PMG approaches rely on distinct\nevaluation strategies and generation techniques, making comparison difficult.\nThis paper presents the first empirical study that evaluates multiple PMRs in\nthe context of PMo with LLMs. We introduce the PMo Dataset, a new dataset\ncontaining 55 process descriptions paired with models in nine different PMRs.\nWe evaluate PMRs along two dimensions: suitability for LLM-based PMo and\nperformance on PMG. \\textit{Mermaid} achieves the highest overall score across\nsix PMo criteria, whereas \\textit{BPMN text} delivers the best PMG results in\nterms of process element similarity.",
    "published": "2025-07-15T14:26:50Z",
    "updated": "2025-07-15T14:26:50Z",
    "id": "2507.11356v1",
    "authors": [
      "Alexis Brissard",
      "Frdric Cuppens",
      "Amal Zouaq"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11356v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11356v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11356v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in Process Modeling (PMo) tasks, specifically focusing on Process Model Representations (PMRs) and their evaluation. It introduces a new dataset for this purpose and evaluates the performance of LLMs in this context.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.11352v1": {
    "title": "Foundation Models for Logistics: Toward Certifiable, Conversational\n  Planning Interfaces",
    "summary": "Logistics operators, from battlefield coordinators rerouting airlifts ahead\nof a storm to warehouse managers juggling late trucks, often face life-critical\ndecisions that demand both domain expertise and rapid and continuous\nreplanning. While popular methods like integer programming yield logistics\nplans that satisfy user-defined logical constraints, they are slow and assume\nan idealized mathematical model of the environment that does not account for\nuncertainty. On the other hand, large language models (LLMs) can handle\nuncertainty and promise to accelerate replanning while lowering the barrier to\nentry by translating free-form utterances into executable plans, yet they\nremain prone to misinterpretations and hallucinations that jeopardize safety\nand cost. We introduce a neurosymbolic framework that pairs the accessibility\nof natural-language dialogue with verifiable guarantees on goal interpretation.\nIt converts user requests into structured planning specifications, quantifies\nits own uncertainty at the field and token level, and invokes an interactive\nclarification loop whenever confidence falls below an adaptive threshold. A\nlightweight model, fine-tuned on just 100 uncertainty-filtered examples,\nsurpasses the zero-shot performance of GPT-4.1 while cutting inference latency\nby nearly 50%. These preliminary results highlight a practical path toward\ncertifiable, real-time, and user-aligned decision-making for complex logistics.",
    "published": "2025-07-15T14:24:01Z",
    "updated": "2025-07-15T14:24:01Z",
    "id": "2507.11352v1",
    "authors": [
      "Yunhao Yang",
      "Neel P. Bhatt",
      "Christian Ellis",
      "Alvaro Velasquez",
      "Zhangyang Wang",
      "Ufuk Topcu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11352v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11352v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11352v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models (LLMs) in logistics planning, focusing on their ability to handle uncertainty and provide rapid replanning. It also introduces a neurosymbolic framework to ensure verifiable guarantees on goal interpretation, which aligns with research on LLMs and their reasoning abilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.11330v2": {
    "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach\n  Integrating Human and Large Language Model Knowledge",
    "summary": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. One of the most common types of\nnovelty in academic papers is the introduction of new methods. In this paper,\nwe propose leveraging human knowledge and LLM to assist pretrained language\nmodels (PLMs, e.g. BERT etc.) in predicting the method novelty of papers.\nSpecifically, we extract sentences related to the novelty of the academic paper\nfrom peer review reports and use LLM to summarize the methodology section of\nthe academic paper, which are then used to fine-tune PLMs. In addition, we have\ndesigned a text-guided fusion module with novel Sparse-Attention to better\nintegrate human and LLM knowledge. We compared the method we proposed with a\nlarge number of baselines. Extensive experiments demonstrate that our method\nachieves superior performance.",
    "published": "2025-07-15T14:03:55Z",
    "updated": "2025-07-16T14:26:34Z",
    "id": "2507.11330v2",
    "authors": [
      "Wenqing Wu",
      "Chengzhi Zhang",
      "Yi Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11330v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11330v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11330v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of human expertise and Large Language Models (LLMs) to evaluate the novelty of academic papers, specifically focusing on method novelty. It mentions the use of LLMs for summarizing methodology sections and fine-tuning pretrained language models (PLMs), which aligns with the topics of LLM and Pretrain. The collaborative approach also hints at the use of human feedback, which is relevant to RL (Reinforcement Learning with Human Feedback).",
    "llm_cls_result": [
      "LLM",
      "Pretrain",
      "RL"
    ]
  },
  "2507.11316v1": {
    "title": "Internal Value Alignment in Large Language Models through Controlled\n  Value Vector Activation",
    "summary": "Aligning Large Language Models (LLMs) with human values has attracted\nincreasing attention since it provides clarity, transparency, and the ability\nto adapt to evolving scenarios. In this paper, we introduce a Controlled Value\nVector Activation (ConVA) method that directly aligns the internal values of\nLLMs by interpreting how a value is encoded in their latent representations and\nmodifies relevant activations to ensure consistent values in LLMs. To ensure an\naccurate and unbiased interpretation, we propose a context-controlled value\nvector identification method. To consistently control values without\nsacrificing model performance, we introduce a gated value vector activation\nmethod for effective and minimum degree of value control. Experiments show that\nour method achieves the highest control success rate across 10 basic values\nwithout hurting LLM performance and fluency, and ensures target values even\nwith opposite and potentially malicious input prompts. Source code and data are\navailable at~ https://github.com/hr-jin/ConVA.",
    "published": "2025-07-15T13:48:35Z",
    "updated": "2025-07-15T13:48:35Z",
    "id": "2507.11316v1",
    "authors": [
      "Haoran Jin",
      "Meng Li",
      "Xiting Wang",
      "Zhihao Xu",
      "Minlie Huang",
      "Yantao Jia",
      "Defu Lian"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11316v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11316v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11316v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on aligning Large Language Models (LLMs) with human values through controlled activation of value vectors, which is directly related to research on LLMs and their alignment with human values.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.11299v2": {
    "title": "Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving\n  Patient-Doctor Communication in Romanian",
    "summary": "Text-based telemedicine has become increasingly common, yet the quality of\nmedical advice in doctor-patient interactions is often judged more on how\nadvice is communicated rather than its clinical accuracy. To address this, we\nintroduce Dr. Copilot , a multi-agent large language model (LLM) system that\nsupports Romanian-speaking doctors by evaluating and enhancing the presentation\nquality of their written responses. Rather than assessing medical correctness,\nDr. Copilot provides feedback along 17 interpretable axes. The system comprises\nof three LLM agents with prompts automatically optimized via DSPy. Designed\nwith low-resource Romanian data and deployed using open-weight models, it\ndelivers real-time specific feedback to doctors within a telemedicine platform.\nEmpirical evaluations and live deployment with 41 doctors show measurable\nimprovements in user reviews and response quality, marking one of the first\nreal-world deployments of LLMs in Romanian medical settings.",
    "published": "2025-07-15T13:26:49Z",
    "updated": "2025-07-20T15:15:56Z",
    "id": "2507.11299v2",
    "authors": [
      "Andrei Niculae",
      "Adrian Cosma",
      "Cosmin Dumitrache",
      "Emilian Rdoi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11299v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11299v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11299v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multi-agent large language model (LLM) system designed to improve patient-doctor communication in Romanian, focusing on the presentation quality of medical advice rather than clinical accuracy. It involves the use of LLMs and multi-agent systems, which are relevant to the topics of LLM and RL (Reinforcement Learning with Human Feedback).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.11277v1": {
    "title": "Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing\n  Agentic AI Systems",
    "summary": "Large Language Models (LLMs) are increasingly deployed within agentic\nsystems-collections of interacting, LLM-powered agents that execute complex,\nadaptive workflows using memory, tools, and dynamic planning. While enabling\npowerful new capabilities, these systems also introduce unique forms of\nuncertainty stemming from probabilistic reasoning, evolving memory states, and\nfluid execution paths. Traditional software observability and operations\npractices fall short in addressing these challenges.\n  This paper introduces AgentOps: a comprehensive framework for observing,\nanalyzing, optimizing, and automating operation of agentic AI systems. We\nidentify distinct needs across four key roles-developers, testers, site\nreliability engineers (SREs), and business users-each of whom engages with the\nsystem at different points in its lifecycle. We present the AgentOps Automation\nPipeline, a six-stage process encompassing behavior observation, metric\ncollection, issue detection, root cause analysis, optimized recommendations,\nand runtime automation. Throughout, we emphasize the critical role of\nautomation in managing uncertainty and enabling self-improving AI systems-not\nby eliminating uncertainty, but by taming it to ensure safe, adaptive, and\neffective operation.",
    "published": "2025-07-15T12:54:43Z",
    "updated": "2025-07-15T12:54:43Z",
    "id": "2507.11277v1",
    "authors": [
      "Dany Moshkovich",
      "Sergey Zeltyn"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11277v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11277v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11277v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the deployment of Large Language Models (LLMs) within agentic systems, focusing on their operation, optimization, and automation. It highlights the role of LLMs in these systems and the challenges they introduce, which aligns with the topics of LLM and RL (Reinforcement Learning with Human Feedback). The emphasis on automation and adaptive operation also touches on AGI (Artificial General Intelligence).",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.11273v1": {
    "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding",
    "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.",
    "published": "2025-07-15T12:52:12Z",
    "updated": "2025-07-15T12:52:12Z",
    "id": "2507.11273v1",
    "authors": [
      "Luohe Shi",
      "Zuchao Li",
      "Lefei Zhang",
      "Guoming Liu",
      "Baoyuan Qi",
      "Hai Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11273v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11273v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11273v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing the Key-Value (KV) cache in Large Language Models (LLMs) to improve inference efficiency, which is a core aspect of LLM research. It introduces a method to reduce KV cache footprint and enhance Rotary Positional Embedding, directly related to LLM architectures and efficiency improvements.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.11252v1": {
    "title": "MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire\n  Detection",
    "summary": "Smoke is the first visible indicator of a wildfire.With the advancement of\ndeep learning, image-based smoke detection has become a crucial method for\ndetecting and preventing forest fires. However, the scarcity of smoke image\ndata from forest fires is one of the significant factors hindering the\ndetection of forest fire smoke. Image generation models offer a promising\nsolution for synthesizing realistic smoke images. However, current inpainting\nmodels exhibit limitations in generating high-quality smoke representations,\nparticularly manifesting as inconsistencies between synthesized smoke and\nbackground contexts. To solve these problems, we proposed a comprehensive\nframework for generating forest fire smoke images. Firstly, we employed the\npre-trained segmentation model and the multimodal model to obtain smoke masks\nand image captions.Then, to address the insufficient utilization of masks and\nmasked images by inpainting models, we introduced a network architecture guided\nby mask and masked image features. We also proposed a new loss function, the\nmask random difference loss, which enhances the consistency of the generated\neffects around the mask by randomly expanding and eroding the mask\nedges.Finally, to generate a smoke image dataset using random masks for\nsubsequent detection tasks, we incorporated smoke characteristics and use a\nmultimodal large language model as a filtering tool to select diverse and\nreasonable smoke images, thereby improving the quality of the synthetic\ndataset. Experiments showed that our generated smoke images are realistic and\ndiverse, and effectively enhance the performance of forest fire smoke detection\nmodels. Code is available at https://github.com/wghr123/MFGDiffusion.",
    "published": "2025-07-15T12:25:35Z",
    "updated": "2025-07-15T12:25:35Z",
    "id": "2507.11252v1",
    "authors": [
      "Guanghao Wu",
      "Chen Xu",
      "Hai Song",
      "Chong Wang",
      "Qixing Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11252v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11252v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11252v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on generating realistic smoke images for forest fire detection using a multimodal large language model (MLLM) as part of the framework. It involves image synthesis and enhancement techniques, which are relevant to multimodal models and their applications.",
    "llm_cls_result": [
      "MLLM",
      "Dataset"
    ]
  },
  "2507.11230v1": {
    "title": "Sparse Autoencoders Can Capture Language-Specific Concepts Across\n  Diverse Languages",
    "summary": "Understanding the multilingual mechanisms of large language models (LLMs)\nprovides insight into how they process different languages, yet this remains\nchallenging. Existing studies often focus on individual neurons, but their\npolysemantic nature makes it difficult to isolate language-specific units from\ncross-lingual representations. To address this, we explore sparse autoencoders\n(SAEs) for their ability to learn monosemantic features that represent concrete\nand abstract concepts across languages in LLMs. While some of these features\nare language-independent, the presence of language-specific features remains\nunderexplored. In this work, we introduce SAE-LAPE, a method based on feature\nactivation probability, to identify language-specific features within the\nfeed-forward network. We find that many such features predominantly appear in\nthe middle to final layers of the model and are interpretable. These features\ninfluence the model's multilingual performance and language output and can be\nused for language identification with performance comparable to fastText along\nwith more interpretability. Our code is available at\nhttps://github.com/LyzanderAndrylie/language-specific-features .",
    "published": "2025-07-15T12:00:30Z",
    "updated": "2025-07-15T12:00:30Z",
    "id": "2507.11230v1",
    "authors": [
      "Lyzander Marciano Andrylie",
      "Inaya Rahmanisa",
      "Mahardika Krisna Ihsani",
      "Alfan Farizki Wicaksono",
      "Haryo Akbarianto Wibowo",
      "Alham Fikri Aji"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11230v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11230v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11230v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on understanding the multilingual mechanisms of large language models (LLMs) using sparse autoencoders (SAEs) to identify language-specific features. This aligns with research on LLMs and their architectures, as well as their multilingual capabilities.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.13380v1": {
    "title": "Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning\n  with Large Language Models for Emotion Recognition",
    "summary": "In the field of emotion recognition, the development of high-performance\nmodels remains a challenge due to the scarcity of high-quality, diverse\nemotional datasets. Emotional expressions are inherently subjective, shaped by\nindividual personality traits, socio-cultural backgrounds, and contextual\nfactors, making large-scale, generalizable data collection both ethically and\npractically difficult. To address this issue, we introduce PersonaGen, a novel\nframework for generating emotionally rich text using a Large Language Model\n(LLM) through multi-stage persona-based conditioning. PersonaGen constructs\nlayered virtual personas by combining demographic attributes, socio-cultural\nbackgrounds, and detailed situational contexts, which are then used to guide\nemotion expression generation. We conduct comprehensive evaluations of the\ngenerated synthetic data, assessing semantic diversity through clustering and\ndistributional metrics, human-likeness via LLM-based quality scoring, realism\nthrough comparison with real-world emotion corpora, and practical utility in\ndownstream emotion classification tasks. Experimental results show that\nPersonaGen significantly outperforms baseline methods in generating diverse,\ncoherent, and discriminative emotion expressions, demonstrating its potential\nas a robust alternative for augmenting or replacing real-world emotional\ndatasets.",
    "published": "2025-07-15T11:32:38Z",
    "updated": "2025-07-15T11:32:38Z",
    "id": "2507.13380v1",
    "authors": [
      "Keito Inoshita",
      "Rushia Harada"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13380v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13380v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13380v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) to generate synthetic data for emotion recognition, which involves the application of LLMs in creating diverse and realistic datasets. This aligns with the topics of LLM and Dataset, as it discusses both the use of LLMs and the creation of datasets for emotion recognition.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.11210v1": {
    "title": "Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and\n  Addressing Family Communication Bias",
    "summary": "Well-being in family settings involves subtle psychological dynamics that\nconventional metrics often overlook. In particular, unconscious parental\nexpectations, termed ideal parent bias, can suppress children's emotional\nexpression and autonomy. This suppression, referred to as suppressed emotion,\noften stems from well-meaning but value-driven communication, which is\ndifficult to detect or address from outside the family. Focusing on these\nlatent dynamics, this study explores Large Language Model (LLM)-based support\nfor psychologically safe family communication. We constructed a Japanese\nparent-child dialogue corpus of 30 scenarios, each annotated with metadata on\nideal parent bias and suppressed emotion. Based on this corpus, we developed a\nRole-Playing LLM-based multi-agent dialogue support framework that analyzes\ndialogue and generates feedback. Specialized agents detect suppressed emotion,\ndescribe implicit ideal parent bias in parental speech, and infer contextual\nattributes such as the child's age and background. A meta-agent compiles these\noutputs into a structured report, which is then passed to five selected expert\nagents. These agents collaboratively generate empathetic and actionable\nfeedback through a structured four-step discussion process. Experiments show\nthat the system can detect categories of suppressed emotion with moderate\naccuracy and produce feedback rated highly in empathy and practicality.\nMoreover, simulated follow-up dialogues incorporating this feedback exhibited\nsigns of improved emotional expression and mutual understanding, suggesting the\nframework's potential in supporting positive transformation in family\ninteractions.",
    "published": "2025-07-15T11:27:32Z",
    "updated": "2025-07-15T11:27:32Z",
    "id": "2507.11210v1",
    "authors": [
      "Rushia Harada",
      "Yuken Kimura",
      "Keito Inoshita"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11210v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11210v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11210v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in a multi-agent framework to detect and address family communication bias, which involves specialized agents and a meta-agent for generating feedback. This aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning, as multi-agent systems often involve RL techniques).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.11198v1": {
    "title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy\n  Gains in Qualitative Coding",
    "summary": "Large Language Models (LLMs) enable new possibilities for qualitative\nresearch at scale, including coding and data annotation. While multi-agent\nsystems (MAS) can emulate human coding workflows, their benefits over\nsingle-agent coding remain poorly understood. We conducted an experimental\nstudy of how agent persona and temperature shape consensus-building and coding\naccuracy of dialog segments based on a codebook with 8 codes. Our open-source\nMAS mirrors deductive human coding through structured agent discussion and\nconsensus arbitration. Using six open-source LLMs (with 3 to 32 billion\nparameters) and 18 experimental configurations, we analyze over 77,000 coding\ndecisions against a gold-standard dataset of human-annotated transcripts from\nonline math tutoring sessions. Temperature significantly impacted whether and\nwhen consensus was reached across all six LLMs. MAS with multiple personas\n(including neutral, assertive, or empathetic), significantly delayed consensus\nin four out of six LLMs compared to uniform personas. In three of those LLMs,\nhigher temperatures significantly diminished the effects of multiple personas\non consensus. However, neither temperature nor persona pairing lead to robust\nimprovements in coding accuracy. Single agents matched or outperformed MAS\nconsensus in most conditions. Only one model (OpenHermesV2:7B) and code\ncategory showed above-chance gains from MAS deliberation when temperature was\n0.5 or lower and especially when the agents included at least one assertive\npersona. Qualitative analysis of MAS collaboration for these configurations\nsuggests that MAS may nonetheless aid in narrowing ambiguous code applications\nthat could improve codebooks and human-AI coding. We contribute new insight\ninto the limits of LLM-based qualitative methods, challenging the notion that\ndiverse MAS personas lead to better outcomes. We open-source our MAS and\nexperimentation code.",
    "published": "2025-07-15T11:06:32Z",
    "updated": "2025-07-15T11:06:32Z",
    "id": "2507.11198v1",
    "authors": [
      "Conrad Borchers",
      "Bahar Shahrokhian",
      "Francesco Balzan",
      "Elham Tajik",
      "Sreecharan Sankaranarayanan",
      "Sebastian Simon"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11198v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11198v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11198v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in qualitative research, specifically focusing on multi-agent systems (MAS) and their impact on consensus-building and coding accuracy. It involves experiments with different LLMs, personas, and temperature settings, which are relevant to the study of LLMs and their applications in reasoning and multi-agent systems.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "RL"
    ]
  },
  "2507.11128v1": {
    "title": "What Should LLMs Forget? Quantifying Personal Data in LLMs for\n  Right-to-Be-Forgotten Requests",
    "summary": "Large Language Models (LLMs) can memorize and reveal personal information,\nraising concerns regarding compliance with the EU's GDPR, particularly the\nRight to Be Forgotten (RTBF). Existing machine unlearning methods assume the\ndata to forget is already known but do not address how to identify which\nindividual-fact associations are stored in the model. Privacy auditing\ntechniques typically operate at the population level or target a small set of\nidentifiers, limiting applicability to individual-level data inquiries. We\nintroduce WikiMem, a dataset of over 5,000 natural language canaries covering\n243 human-related properties from Wikidata, and a model-agnostic metric to\nquantify human-fact associations in LLMs. Our approach ranks ground-truth\nvalues against counterfactuals using calibrated negative log-likelihood across\nparaphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B\nparameters), showing that memorization correlates with subject web presence and\nmodel scale. We provide a foundation for identifying memorized personal data in\nLLMs at the individual level, enabling the dynamic construction of forget sets\nfor machine unlearning and RTBF requests.",
    "published": "2025-07-15T09:28:44Z",
    "updated": "2025-07-15T09:28:44Z",
    "id": "2507.11128v1",
    "authors": [
      "Dimitri Staufer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11128v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11128v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11128v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the memorization of personal data in Large Language Models (LLMs) and introduces a method to quantify human-fact associations, which is relevant to the topics of LLMs and their compliance with privacy regulations like GDPR.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.11114v1": {
    "title": "MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal\n  Reasoning With Ensemble Vision Language Models",
    "summary": "We present a robust ensemble-based system for multilingual multimodal\nreasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach\nintegrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption\nrefinement and consistency checks, and Gemini 2.5 Pro as a reasoner which\nhandles final answer selection, all coordinated through carefully engineered\nfew-shot and zero-shot prompts. We conducted an extensive ablation study,\ntraining several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,\nMistral) on an English dataset and its multilingual augmented version.\nAdditionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for\ncomparison and found it to substantially outperform the trained models. Prompt\ndesign also proved critical: enforcing concise, language-normalized formats and\nprohibiting explanatory text boosted model accuracy on the English validation\nset from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)\nachieved first place overall in the multilingual track with 81.4% accuracy, and\nled 11 out of 13 individual language tracks, with top results such as 95.07%\nfor Croatian and 92.12% for Italian. These findings highlight that lightweight\nOCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual\naugmentation, can outperform heavier end-to-end models in high-stakes,\nmultilingual educational settings.",
    "published": "2025-07-15T09:05:05Z",
    "updated": "2025-07-15T09:05:05Z",
    "id": "2507.11114v1",
    "authors": [
      "Seif Ahmed",
      "Mohamed T. Younes",
      "Abdelrahman Moustafa",
      "Abdelrahman Allam",
      "Hamza Moustafa"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11114v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11114v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11114v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on multilingual multimodal reasoning using ensemble vision language models, which involves integrating different models for visual description, caption refinement, and reasoning. This aligns with the topics of Multimodal Large Language Models (MLLM) and Reasoning in LLMs.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.11112v1": {
    "title": "Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs",
    "summary": "Recent studies have shown that Large Language Models (LLMs) are vulnerable to\ndata poisoning attacks, where malicious training examples embed hidden\nbehaviours triggered by specific input patterns. However, most existing works\nassume a phrase and focus on the attack's effectiveness, offering limited\nunderstanding of trigger mechanisms and how multiple triggers interact within\nthe model. In this paper, we present a framework for studying poisoning in\nLLMs. We show that multiple distinct backdoor triggers can coexist within a\nsingle model without interfering with each other, enabling adversaries to embed\nseveral triggers concurrently. Using multiple triggers with high embedding\nsimilarity, we demonstrate that poisoned triggers can achieve robust activation\neven when tokens are substituted or separated by long token spans. Our findings\nexpose a broader and more persistent vulnerability surface in LLMs. To mitigate\nthis threat, we propose a post hoc recovery method that selectively retrains\nspecific model components based on a layer-wise weight difference analysis. Our\nmethod effectively removes the trigger behaviour with minimal parameter\nupdates, presenting a practical and efficient defence against multi-trigger\npoisoning.",
    "published": "2025-07-15T09:04:30Z",
    "updated": "2025-07-15T09:04:30Z",
    "id": "2507.11112v1",
    "authors": [
      "Sanhanat Sivapiromrat",
      "Caiqi Zhang",
      "Marco Basaldella",
      "Nigel Collier"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11112v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11112v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11112v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses vulnerabilities in Large Language Models (LLMs) related to data poisoning attacks, which is a security concern within the broader field of LLM research. The focus on LLMs and their vulnerabilities aligns with the 'LLM' topic, and the discussion of security and robustness could also be relevant to 'AGI' as it pertains to the broader challenges in achieving safe and reliable general intelligence.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.11097v1": {
    "title": "The Devil behind the mask: An emergent safety vulnerability of Diffusion\n  LLMs",
    "summary": "Diffusion-based large language models (dLLMs) have recently emerged as a\npowerful alternative to autoregressive LLMs, offering faster inference and\ngreater interactivity via parallel decoding and bidirectional modeling.\nHowever, despite strong performance in code generation and text infilling, we\nidentify a fundamental safety concern: existing alignment mechanisms fail to\nsafeguard dLLMs against context-aware, masked-input adversarial prompts,\nexposing novel vulnerabilities. To this end, we present DIJA, the first\nsystematic study and jailbreak attack framework that exploits unique safety\nweaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial\ninterleaved mask-text prompts that exploit the text generation mechanisms of\ndLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional\nmodeling drives the model to produce contextually consistent outputs for masked\nspans, even when harmful, while parallel decoding limits model dynamic\nfiltering and rejection sampling of unsafe content. This causes standard\nalignment mechanisms to fail, enabling harmful completions in alignment-tuned\ndLLMs, even when harmful behaviors or unsafe instructions are directly exposed\nin the prompt. Through comprehensive experiments, we demonstrate that DIJA\nsignificantly outperforms existing jailbreak methods, exposing a previously\noverlooked threat surface in dLLM architectures. Notably, our method achieves\nup to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior\nbaseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and\nby 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of\nharmful content in the jailbreak prompt. Our findings underscore the urgent\nneed for rethinking safety alignment in this emerging class of language models.\nCode is available at https://github.com/ZichenWen1/DIJA.",
    "published": "2025-07-15T08:44:46Z",
    "updated": "2025-07-15T08:44:46Z",
    "id": "2507.11097v1",
    "authors": [
      "Zichen Wen",
      "Jiashu Qu",
      "Dongrui Liu",
      "Zhiyuan Liu",
      "Ruixi Wu",
      "Yicun Yang",
      "Xiangqi Jin",
      "Haoyun Xu",
      "Xuyang Liu",
      "Weijia Li",
      "Chaochao Lu",
      "Jing Shao",
      "Conghui He",
      "Linfeng Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11097v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11097v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11097v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses safety vulnerabilities in diffusion-based large language models (dLLMs), which are a type of LLM. It focuses on the failure of existing alignment mechanisms and introduces a jailbreak attack framework, which is relevant to the broader topic of LLM safety and alignment.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.11086v1": {
    "title": "Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border\n  Entity Identification",
    "summary": "The growing prevalence of cross-border financial activities in global markets\nhas underscored the necessity of accurately identifying and classifying foreign\nentities. This practice is essential within the Spanish financial system for\nensuring robust risk management, regulatory adherence, and the prevention of\nfinancial misconduct. This process involves a labor-intensive entity-matching\ntask, where entities need to be validated against available reference sources.\nChallenges arise from linguistic variations, special characters, outdated\nnames, and changes in legal forms, complicating traditional matching algorithms\nlike Jaccard, cosine, and Levenshtein distances. These methods struggle with\ncontextual nuances and semantic relationships, leading to mismatches. To\naddress these limitations, we explore Large Language Models (LLMs) as a\nflexible alternative. LLMs leverage extensive training to interpret context,\nhandle abbreviations, and adapt to legal transitions. We evaluate traditional\nmethods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft\nCopilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.\nResults show traditional methods achieve accuracies over 92% but suffer high\nfalse positive rates (20-40%). Interface-based LLMs outperform, achieving\naccuracies above 93%, F1 scores exceeding 96%, and lower false positives\n(40-80%).",
    "published": "2025-07-15T08:28:24Z",
    "updated": "2025-07-15T08:28:24Z",
    "id": "2507.11086v1",
    "authors": [
      "Andres Azqueta-Gavaldn",
      "Joaquin Ramos Cosgrove"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11086v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11086v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11086v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for entity identification in cross-border financial activities, highlighting their advantages over traditional methods. The focus on LLMs and their application in a specific task aligns with the 'LLM' topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.11079v1": {
    "title": "Tactical Decision for Multi-UGV Confrontation with a Vision-Language\n  Model-Based Commander",
    "summary": "In multiple unmanned ground vehicle confrontations, autonomously evolving\nmulti-agent tactical decisions from situational awareness remain a significant\nchallenge. Traditional handcraft rule-based methods become vulnerable in the\ncomplicated and transient battlefield environment, and current reinforcement\nlearning methods mainly focus on action manipulation instead of strategic\ndecisions due to lack of interpretability. Here, we propose a vision-language\nmodel-based commander to address the issue of intelligent\nperception-to-decision reasoning in autonomous confrontations. Our method\nintegrates a vision language model for scene understanding and a lightweight\nlarge language model for strategic reasoning, achieving unified perception and\ndecision within a shared semantic space, with strong adaptability and\ninterpretability. Unlike rule-based search and reinforcement learning methods,\nthe combination of the two modules establishes a full-chain process, reflecting\nthe cognitive process of human commanders. Simulation and ablation experiments\nvalidate that the proposed approach achieves a win rate of over 80% compared\nwith baseline models.",
    "published": "2025-07-15T08:22:37Z",
    "updated": "2025-07-15T08:22:37Z",
    "id": "2507.11079v1",
    "authors": [
      "Li Wang",
      "Qizhen Wu",
      "Lei Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11079v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11079v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11079v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of a vision-language model for scene understanding and a lightweight large language model for strategic reasoning in the context of multi-UGV confrontations. This aligns with the topics of Vision-Language Action (VLA) models and Reinforcement Learning (RL) due to the focus on strategic decision-making and the use of reinforcement learning methods.",
    "llm_cls_result": [
      "VLA",
      "RL"
    ]
  },
  "2507.11071v1": {
    "title": "LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly\n  Detection",
    "summary": "Log anomaly detection using traditional rule based or deep learning based\nmethods is often challenging due to the large volume and highly complex nature\nof log sequence. So effective way of detection of anomalous sequence of logs is\ncrucial for system maintenance and development. This paper proposes parameter\nefficient finetuning specifically low rank adaptation (LoRA) and adapter based\napproaches for finding contextual anomalies in sequence of logs in large log\ndata set. It compares different tiny large language models (LLMs) on the\nThunderbird dataset. The results show that LoRA based finetuning provides\nsubstantial performance improvements of 18 to 19 percentage over LogBert based\nfull finetuning approach, achieving accuracy scores between 97.76% and 98.83%\ncompared to 79.37%.",
    "published": "2025-07-15T08:04:31Z",
    "updated": "2025-07-15T08:04:31Z",
    "id": "2507.11071v1",
    "authors": [
      "Isaiah Thompson Ocansey",
      "Ritwik Bhattacharya",
      "Tanmay Sen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11071v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11071v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11071v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of tiny large language models (LLMs) for log anomaly detection, specifically focusing on parameter-efficient finetuning methods like LoRA and adapter-based approaches. This aligns with the 'LLM' topic as it involves research on large language models and their applications. Additionally, the mention of parameter-efficient finetuning could loosely relate to 'Scaling' as it involves optimizing model performance with limited resources.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.11052v1": {
    "title": "LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk\n  Prediction: A Clinical NLP",
    "summary": "Timely identification and accurate risk stratification of cardiovascular\ndisease (CVD) remain essential for reducing global mortality. While existing\nprediction models primarily leverage structured data, unstructured clinical\nnotes contain valuable early indicators. This study introduces a novel\nLLM-augmented clinical NLP pipeline that employs domain-adapted large language\nmodels for symptom extraction, contextual reasoning, and correlation from\nfree-text reports. Our approach integrates cardiovascular-specific fine-tuning,\nprompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III\nand CARDIO-NLP datasets demonstrate improved performance in precision, recall,\nF1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by\ncardiologists. Challenges such as contextual hallucination, which occurs when\nplausible information contracts with provided source, and temporal ambiguity,\nwhich is related with models struggling with chronological ordering of events\nare addressed using prompt engineering and hybrid rule-based verification. This\nwork underscores the potential of LLMs in clinical decision support systems\n(CDSS), advancing early warning systems and enhancing the translation of\npatient narratives into actionable risk assessments.",
    "published": "2025-07-15T07:32:16Z",
    "updated": "2025-07-15T07:32:16Z",
    "id": "2507.11052v1",
    "authors": [
      "Haowei Yang",
      "Ziyu Shen",
      "Junli Shao",
      "Luyao Men",
      "Xinyue Han",
      "Jing Dong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11052v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11052v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11052v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for symptom analysis and risk prediction in clinical settings, which involves domain-specific fine-tuning and prompt-based inference. This aligns with the 'LLM' and 'Reasoning' topics, as it involves both the application of LLMs and their reasoning capabilities in a specific domain.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.14207v1": {
    "title": "Mitigating Trojanized Prompt Chains in Educational LLM Use Cases:\n  Experimental Findings and Detection Tool Design",
    "summary": "The integration of Large Language Models (LLMs) in K--12 education offers\nboth transformative opportunities and emerging risks. This study explores how\nstudents may Trojanize prompts to elicit unsafe or unintended outputs from\nLLMs, bypassing established content moderation systems with safety guardrils.\nThrough a systematic experiment involving simulated K--12 queries and\nmulti-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This\npaper presents our experimental design, detailed findings, and a prototype\ntool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized\neducational prompts. These insights aim to inform both AI safety researchers\nand educational technologists on the safe deployment of LLMs for educators.",
    "published": "2025-07-15T07:23:19Z",
    "updated": "2025-07-15T07:23:19Z",
    "id": "2507.14207v1",
    "authors": [
      "Richard M. Charles",
      "James H. Curry",
      "Richard B. Charles"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14207v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14207v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14207v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the vulnerabilities and safety issues related to the use of Large Language Models (LLMs) in educational settings, focusing on detecting and mitigating unsafe prompts. This aligns with the topics of LLM (Large Language Models) and Benchmark (evaluation and safety measures for LLMs).",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.11042v1": {
    "title": "Aligned Query Expansion: Efficient Query Expansion for Information\n  Retrieval through LLM Alignment",
    "summary": "With the breakthroughs in large language models (LLMs), query generation\ntechniques that expand documents and queries with related terms are becoming\nincreasingly popular in the information retrieval field. Such techniques have\nbeen shown to improve the effectiveness of traditional lexical retrieval\nmethods by dealing with the vocabulary mismatch problem. Recent work has found\nthat generating queries with a greedy decoding strategy can produce sub-optimal\nqueries, including hallucinations, and proposed to filter out queries before\nexpansion. This `generate-then-filter' approach is costly, as it requires\ngenerating multiple queries and applying a relevance model to all of them and\ndoes not teach the LLM which of the generated queries is more effective for\nexpansion. To overcome such limitations, we propose Aligned Query Expansion\n(AQE), a novel approach to enhance query expansion for passage retrieval in\nopen-domain question answering. AQE leverages recent techniques in LLM\nalignment to fine-tune models for generating query expansions that directly\noptimize the effectiveness of the retrieval task, eliminating the need for\nadditional filtering steps. This alignment ensures that queries are more\nrelevant, reducing computational costs while improving retrieval effectiveness.\nEmpirical evaluations show that AQE outperforms baseline models for query\nexpansion in both in-domain and out-of-domain settings, demonstrating\nsignificant improvements in retrieval effectiveness.",
    "published": "2025-07-15T07:11:29Z",
    "updated": "2025-07-15T07:11:29Z",
    "id": "2507.11042v1",
    "authors": [
      "Adam Yang",
      "Gustavo Penha",
      "Enrico Palumbo",
      "Hugues Bouchard"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11042v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11042v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11042v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for query expansion in information retrieval, focusing on alignment techniques to optimize retrieval effectiveness. The core topics are related to LLMs and their application in improving retrieval tasks.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.10972v1": {
    "title": "Teach Me Sign: Stepwise Prompting LLM for Sign Language Production",
    "summary": "Large language models, with their strong reasoning ability and rich\nknowledge, have brought revolution to many tasks of AI, but their impact on\nsign language generation remains limited due to its complexity and unique\nrules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign\nlanguage as another natural language. By fine-tuning an LLM, we enable it to\nlearn the correspondence between text and sign language, and facilitate\ngeneration. Considering the differences between sign and spoken language, we\nemploy a stepwise prompting strategy to extract the inherent sign language\nknowledge within the LLM, thereby supporting the learning and generation\nprocess. Experimental results on How2Sign and Phoenix14T datasets demonstrate\nthat our approach effectively leverages both the sign language knowledge and\nreasoning capabilities of LLM to align the different distribution and\ngrammatical rules between sign and spoken language.",
    "published": "2025-07-15T04:31:52Z",
    "updated": "2025-07-15T04:31:52Z",
    "id": "2507.10972v1",
    "authors": [
      "Zhaoyi An",
      "Rei Kawakami"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10972v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10972v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10972v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a large language model (LLM) for sign language production, leveraging its reasoning capabilities and knowledge. It involves fine-tuning the LLM and using a stepwise prompting strategy to align sign language with spoken language, which aligns with the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.10957v1": {
    "title": "Modeling Understanding of Story-Based Analogies Using Large Language\n  Models",
    "summary": "Recent advancements in Large Language Models (LLMs) have brought them closer\nto matching human cognition across a variety of tasks. How well do these models\nalign with human performance in detecting and mapping analogies? Prior research\nhas shown that LLMs can extract similarities from analogy problems but lack\nrobust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the\ncurrent study focused on a story-based analogical mapping task and conducted a\nfine-grained evaluation of LLM reasoning abilities compared to human\nperformance. First, it explored the semantic representation of analogies in\nLLMs, using sentence embeddings to assess whether they capture the similarity\nbetween the source and target texts of an analogy, and the dissimilarity\nbetween the source and distractor texts. Second, it investigated the\neffectiveness of explicitly prompting LLMs to explain analogies. Throughout, we\nexamine whether LLMs exhibit similar performance profiles to those observed in\nhumans by evaluating their reasoning at the level of individual analogies, and\nnot just at the level of overall accuracy (as prior studies have done). Our\nexperiments include evaluating the impact of model size (8B vs. 70B parameters)\nand performance variation across state-of-the-art model architectures such as\nGPT-4 and LLaMA3. This work advances our understanding of the analogical\nreasoning abilities of LLMs and their potential as models of human reasoning.",
    "published": "2025-07-15T03:40:21Z",
    "updated": "2025-07-15T03:40:21Z",
    "id": "2507.10957v1",
    "authors": [
      "Kalit Inani",
      "Keshav Kabra",
      "Vijay Marupudi",
      "Sashank Varma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10957v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10957v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10957v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the reasoning abilities of Large Language Models (LLMs) in the context of story-based analogies, comparing their performance to human cognition. It specifically examines semantic representation, prompting strategies, and model size impact, which aligns with topics related to LLM reasoning and evaluation.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.15868v1": {
    "title": "Small Edits, Big Consequences: Telling Good from Bad Robustness in Large\n  Language Models",
    "summary": "Large language models (LLMs) now write code in settings where misreading a\nsingle word can break safety or cost money, yet we still expect them to\noverlook stray typos. To probe where useful robustness ends and harmful\ninsensitivity begins, we compile 50 LeetCode problems and craft three minimal\nprompt perturbations that should vary in importance: (i) progressive\nunderspecification deleting 10 % of words per step; (ii) lexical flip swapping\na pivotal quantifier (\"max\" to \"min\"); and (iii) jargon inflation replacing a\ncommon noun with an obscure technical synonym. Six frontier models, including\nthree \"reasoning-tuned\" versions, solve each mutated prompt, and their Python\noutputs are checked against the original test suites to reveal whether they\nreused the baseline solution or adapted. Among 11 853 generations we observe a\nsharp double asymmetry. Models remain correct in 85 % of cases even after 90 %\nof the prompt is missing, showing over-robustness to underspecification, yet\nonly 54 % react to a single quantifier flip that reverses the task, with\nreasoning-tuned variants even less sensitive than their bases. Jargon edits lie\nin between, passing through 56 %. Current LLMs thus blur the line between\nharmless noise and meaning - changing edits, often treating both as ignorable.\nMasking salient anchors such as function names can force re - evaluation. We\nadvocate evaluation and training protocols that reward differential\nsensitivity: stay steady under benign noise but adapt - or refuse - when\nsemantics truly change.",
    "published": "2025-07-15T03:22:07Z",
    "updated": "2025-07-15T03:22:07Z",
    "id": "2507.15868v1",
    "authors": [
      "Altynbek Ismailov",
      "Salia Asanova"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15868v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15868v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15868v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the robustness and sensitivity of Large Language Models (LLMs) to different types of prompt perturbations, which is directly related to the evaluation and understanding of LLM behavior. The study involves testing LLMs on various prompt modifications and analyzing their responses, which falls under the category of benchmarking LLMs.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Reasoning"
    ]
  },
  "2507.10933v1": {
    "title": "Artificial Finance: How AI Thinks About Money",
    "summary": "In this paper, we explore how large language models (LLMs) approach financial\ndecision-making by systematically comparing their responses to those of human\nparticipants across the globe. We posed a set of commonly used financial\ndecision-making questions to seven leading LLMs, including five models from the\nGPT series(GPT-4o, GPT-4.5, o1, o3-mini), Gemini 2.0 Flash, and DeepSeek R1. We\nthen compared their outputs to human responses drawn from a dataset covering 53\nnations. Our analysis reveals three main results. First, LLMs generally exhibit\na risk-neutral decision-making pattern, favoring choices aligned with expected\nvalue calculations when faced with lottery-type questions. Second, when\nevaluating trade-offs between present and future, LLMs occasionally produce\nresponses that appear inconsistent with normative reasoning. Third, when we\nexamine cross-national similarities, we find that the LLMs' aggregate responses\nmost closely resemble those of participants from Tanzania. These findings\ncontribute to the understanding of how LLMs emulate human-like decision\nbehaviors and highlight potential cultural and training influences embedded\nwithin their outputs.",
    "published": "2025-07-15T02:54:12Z",
    "updated": "2025-07-15T02:54:12Z",
    "id": "2507.10933v1",
    "authors": [
      "Orhan Erdem",
      "Ragavi Pobbathi Ashok"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10933v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10933v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10933v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the behavior of large language models (LLMs) in financial decision-making, comparing their responses to human participants. It focuses on LLMs' decision-making patterns and their alignment with human reasoning, which is relevant to the study of Large Language Models (LLM).",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.10917v2": {
    "title": "LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation",
    "summary": "Recently, much effort has been devoted to modeling users' multi-interests\nbased on their behaviors or auxiliary signals. However, existing methods often\nrely on heuristic assumptions, e.g., co-occurring items indicate the same\ninterest of users, failing to capture user multi-interests aligning with\nreal-world scenarios. While large language models (LLMs) show significant\npotential for multi-interest analysis due to their extensive knowledge and\npowerful reasoning capabilities, two key challenges remain. First, the\ngranularity of LLM-driven multi-interests is agnostic, possibly leading to\noverly fine or coarse interest grouping. Second, individual user analysis\nprovides limited insights due to the data sparsity issue. In this paper, we\npropose an LLM-driven dual-level multi-interest modeling framework for more\neffective recommendation. At the user-individual level, we exploit LLMs to\nflexibly allocate items engaged by users into different semantic clusters,\nindicating their diverse and distinct interests. To alleviate the agnostic\ngeneration of LLMs, we adaptively assign these semantic clusters to users'\ncollaborative multi-interests learned from global user-item interactions,\nallowing the granularity to be automatically adjusted according to the user's\nbehaviors using an alignment module. To alleviate the limited insights derived\nfrom individual users' behaviors, at the user-crowd level, we propose\naggregating user cliques into synthesized users with rich behaviors for more\ncomprehensive LLM-driven multi-interest analysis. We formulate a max covering\nproblem to ensure the compactness and representativeness of synthesized users'\nbehaviors, and then conduct contrastive learning based on their LLM-driven\nmulti-interests to disentangle item representations among different interests.\nExperiments on real-world datasets show the superiority of our approach against\nstate-of-the-art methods.",
    "published": "2025-07-15T02:13:54Z",
    "updated": "2025-07-17T15:01:08Z",
    "id": "2507.10917v2",
    "authors": [
      "Ziyan Wang",
      "Yingpeng Du",
      "Zhu Sun",
      "Jieyi Bi",
      "Haoyan Chua",
      "Tianjun Wei",
      "Jie Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10917v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10917v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10917v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for multi-interest modeling in recommendation systems, which directly relates to the 'LLM' topic. Additionally, the paper discusses reasoning capabilities of LLMs and their application in analyzing user interests, which aligns with the 'Reasoning' topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.10903v1": {
    "title": "LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided\n  DRL for Optimized SFC Provisioning",
    "summary": "Effective management of Service Function Chains (SFCs) and optimal Virtual\nNetwork Function (VNF) placement are critical challenges in modern\nSoftware-Defined Networking (SDN) and Network Function Virtualization (NFV)\nenvironments. Although Deep Reinforcement Learning (DRL) is widely adopted for\ndynamic network decision-making, its inherent dependency on structured data and\nfixed action rules often limits adaptability and responsiveness, particularly\nunder unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a\nnovel approach combining Lightweight Language Model (LiLM) with Relational\nDatabase (RDB) to answer network state queries to guide DRL model for efficient\nSFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and\nAuto-Regressive Transformers (BART) and the Fine-tuned Language Net T5\n(FLAN-T5), to interpret network data and support diverse query types related to\nSFC demands, data center resources, and VNF availability. Results demonstrate\nthat FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to\n0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time\n(2h 2min compared to 2h 38min). Moreover, when compared to the large language\nmodel SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting\nprocessing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).",
    "published": "2025-07-15T01:42:44Z",
    "updated": "2025-07-15T01:42:44Z",
    "id": "2507.10903v1",
    "authors": [
      "Parisa Fard Moshiri",
      "Xinyu Zhu",
      "Poonam Lohan",
      "Burak Kantarci",
      "Emil Janulewicz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10903v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10903v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10903v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of lightweight language models (LiLMs) in combination with relational databases to guide Deep Reinforcement Learning (DRL) for optimizing Service Function Chain (SFC) provisioning. While it mentions language models and reinforcement learning, the primary focus is on their application in network management rather than core research on LLMs or RL.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.10873v1": {
    "title": "From Alerts to Intelligence: A Novel LLM-Aided Framework for Host-based\n  Intrusion Detection",
    "summary": "Host-based intrusion detection system (HIDS) is a key defense component to\nprotect the organizations from advanced threats like Advanced Persistent\nThreats (APT). By analyzing the fine-grained logs with approaches like data\nprovenance, HIDS has shown successes in capturing sophisticated attack traces.\nDespite the progresses embarked by the research community and industry, HIDS\nstill frequently encounters backlash from their operators in the deployed\nenvironments, due to issues like high false-positive rate, inconsistent\noutcomes across environments and human-unfriendly detection results. Large\nLanguage Models (LLMs) have great potentials to advance the state of HIDS,\ngiven their extensive knowledge of attack techniques and their ability to\ndetect anomalies through semantic analysis, anchored by recent studies. Yet,\nour preliminary analysis indicates that building an HIDS by naively prompting\nan LLM is unlikely to succeed. In this work, we explore the direction of\nbuilding a customized LLM pipeline for HIDS and develop a system named SHIELD.\nSHIELD addresses challenges related to LLM's token limits, confusion of\nbackground noises, etc., by integrating a variety of techniques like\nevent-level Masked Autoencoder (MAE) for attack window detection, attack\nevidence identification and expansion, Deterministic Data Augmentation (DDA)\nfor profiling normal activities, and multi-purpose prompting that guides the\nLLM to conduct precise and interpretable attack investigations. Extensive\nexperiments on three log datasets (DARPA-E3, NodLink-simulated-data and\nATLASv2) show that SHIELD consistently achieves outstanding performance in\ncomparison with 5 representative HIDS. These findings highlight the potential\nof LLMs as powerful tools for intrusion detection and pave the way for future\nresearch in this domain.",
    "published": "2025-07-15T00:24:53Z",
    "updated": "2025-07-15T00:24:53Z",
    "id": "2507.10873v1",
    "authors": [
      "Danyu Sun",
      "Jinghuai Zhang",
      "Jiacen Xu",
      "Yu Zheng",
      "Yuan Tian",
      "Zhou Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10873v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10873v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10873v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in the context of Host-based Intrusion Detection Systems (HIDS), focusing on leveraging LLMs' capabilities for semantic analysis and anomaly detection. The core topic is the use of LLMs in a specialized pipeline for intrusion detection, which aligns with the 'LLM' category. The paper does not directly address other topics like RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.10852v1": {
    "title": "LLMs on Trial: Evaluating Judicial Fairness for Large Language Models",
    "summary": "Large Language Models (LLMs) are increasingly used in high-stakes fields\nwhere their decisions impact rights and equity. However, LLMs' judicial\nfairness and implications for social justice remain underexplored. When LLMs\nact as judges, the ability to fairly resolve judicial issues is a prerequisite\nto ensure their trustworthiness. Based on theories of judicial fairness, we\nconstruct a comprehensive framework to measure LLM fairness, leading to a\nselection of 65 labels and 161 corresponding values. Applying this framework to\nthe judicial system, we compile an extensive dataset, JudiFair, comprising\n177,100 unique case facts. To achieve robust statistical inference, we develop\nthree evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and\nintroduce a method to assess the overall fairness of multiple LLMs across\nvarious labels. Through experiments with 16 LLMs, we uncover pervasive\ninconsistency, bias, and imbalanced inaccuracy across models, underscoring\nsevere LLM judicial unfairness. Particularly, LLMs display notably more\npronounced biases on demographic labels, with slightly less bias on substance\nlabels compared to procedure ones. Interestingly, increased inconsistency\ncorrelates with reduced biases, but more accurate predictions exacerbate\nbiases. While we find that adjusting the temperature parameter can influence\nLLM fairness, model size, release date, and country of origin do not exhibit\nsignificant effects on judicial fairness. Accordingly, we introduce a publicly\navailable toolkit containing all datasets and code, designed to support future\nresearch in evaluating and improving LLM fairness.",
    "published": "2025-07-14T22:56:58Z",
    "updated": "2025-07-14T22:56:58Z",
    "id": "2507.10852v1",
    "authors": [
      "Yiran Hu",
      "Zongyue Xue",
      "Haitao Li",
      "Siyuan Zheng",
      "Qingjing Chen",
      "Shaochun Wang",
      "Xihan Zhang",
      "Ning Zheng",
      "Yun Liu",
      "Qingyao Ai",
      "Yiqun Liu",
      "Charles L. A. Clarke",
      "Weixing Shen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10852v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10852v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10852v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the judicial fairness of Large Language Models (LLMs) and their implications for social justice, which directly relates to the study of LLMs and their applications in high-stakes fields. The research involves benchmarking LLMs and creating a dataset for evaluation, which aligns with the topics of Benchmark and Dataset.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.10844v1": {
    "title": "LLM-Guided Agentic Object Detection for Open-World Understanding",
    "summary": "Object detection traditionally relies on fixed category sets, requiring\ncostly re-training to handle novel objects. While Open-World and\nOpen-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD\nlacks semantic labels for unknowns, and OVOD depends on user prompts, limiting\nautonomy. We propose an LLM-guided agentic object detection (LAOD) framework\nthat enables fully label-free, zero-shot detection by prompting a Large\nLanguage Model (LLM) to generate scene-specific object names. These are passed\nto an open-vocabulary detector for localization, allowing the system to adapt\nits goals dynamically. We introduce two new metrics, Class-Agnostic Average\nPrecision (CAAP) and Semantic Naming Average Precision (SNAP), to separately\nevaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD\nvalidate our approach, showing strong performance in detecting and naming novel\nobjects. Our method offers enhanced autonomy and adaptability for open-world\nunderstanding.",
    "published": "2025-07-14T22:30:48Z",
    "updated": "2025-07-14T22:30:48Z",
    "id": "2507.10844v1",
    "authors": [
      "Furkan Mumcu",
      "Michael J. Jones",
      "Anoop Cherian",
      "Yasin Yilmaz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10844v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10844v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10844v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a Large Language Model (LLM) to guide object detection in an open-world setting, which involves both LLM and vision-language alignment. The abstract mentions the use of LLM for generating scene-specific object names and evaluates the system on localization and naming, indicating relevance to both LLM and VLA topics.",
    "llm_cls_result": [
      "LLM",
      "VLA"
    ]
  },
  "2507.10818v1": {
    "title": "How Robust are LLM-Generated Library Imports? An Empirical Study using\n  Stack Overflow",
    "summary": "Software libraries are central to the functionality, security, and\nmaintainability of modern code. As developers increasingly turn to Large\nLanguage Models (LLMs) to assist with programming tasks, understanding how\nthese models recommend libraries is essential. In this paper, we conduct an\nempirical study of six state-of-the-art LLMs, both proprietary and open-source,\nby prompting them to solve real-world Python problems sourced from Stack\nOverflow. We analyze the types of libraries they import, the characteristics of\nthose libraries, and the extent to which the recommendations are usable out of\nthe box. Our results show that LLMs predominantly favour third-party libraries\nover standard ones, and often recommend mature, popular, and permissively\nlicensed dependencies. However, we also identify gaps in usability: 4.6% of the\nlibraries could not be resolved automatically due to structural mismatches\nbetween import names and installable packages, and only two models (out of six)\nprovided installation guidance. While the generated code is technically valid,\nthe lack of contextual support places the burden of manually resolving\ndependencies on the user. Our findings offer actionable insights for both\ndevelopers and researchers, and highlight opportunities to improve the\nreliability and usability of LLM-generated code in the context of software\ndependencies.",
    "published": "2025-07-14T21:35:29Z",
    "updated": "2025-07-14T21:35:29Z",
    "id": "2507.10818v1",
    "authors": [
      "Jasmine Latendresse",
      "SayedHassan Khatoonabadi",
      "Emad Shihab"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10818v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10818v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10818v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the use of Large Language Models (LLMs) in programming tasks, specifically analyzing their recommendations for library imports in Python. This directly relates to the 'LLM' topic as it involves research on the application and performance of LLMs in a specific domain (software development).",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.10803v1": {
    "title": "Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social\n  Media Chatter Use Case",
    "summary": "Background Large language models (LLMs) face challenges in inductive thematic\nanalysis, a task requiring deep interpretive and domain-specific expertise. We\nevaluated the feasibility of using LLMs to replicate expert-driven thematic\nanalysis of social media data. Methods Using two temporally non-intersecting\nReddit datasets on xylazine (n=286 and n=686, for model optimization and\nvalidation, respectively) with twelve expert-derived themes, we evaluated five\nLLMs against expert coding. We modeled the task as a series of binary\nclassifications, rather than a single, multi-label classification, employing\nzero-, single-, and few-shot prompting strategies and measuring performance via\naccuracy, precision, recall, and F1-score. Results On the validation set,\nGPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:\n0.71). For high-prevalence themes, model-derived thematic distributions closely\nmirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:\n16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based\napproaches can automate thematic analyses, offering a scalable supplement for\nqualitative research. Keywords: thematic analysis, large language models,\nnatural language processing, qualitative analysis, social media, prompt\nengineering, public health",
    "published": "2025-07-14T20:57:52Z",
    "updated": "2025-07-14T20:57:52Z",
    "id": "2507.10803v1",
    "authors": [
      "JaMor Hairston",
      "Ritvik Ranjan",
      "Sahithi Lakamana",
      "Anthony Spadaro",
      "Selen Bozkurt",
      "Jeanmarie Perrone",
      "Abeed Sarker"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10803v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10803v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10803v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for thematic analysis in a specific domain, focusing on their performance and application in qualitative research. The core topics are related to LLMs and their practical applications in thematic analysis.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.12484v1": {
    "title": "AI-Powered Math Tutoring: Platform for Personalized and Adaptive\n  Education",
    "summary": "The growing ubiquity of artificial intelligence (AI), in particular large\nlanguage models (LLMs), has profoundly altered the way in which learners gain\nknowledge and interact with learning material, with many claiming that AI\npositively influences their learning achievements. Despite this advancement,\ncurrent AI tutoring systems face limitations associated with their reactive\nnature, often providing direct answers without encouraging deep reflection or\nincorporating structured pedagogical tools and strategies. This limitation is\nmost apparent in the field of mathematics, in which AI tutoring systems remain\nunderdeveloped. This research addresses the question: How can AI tutoring\nsystems move beyond providing reactive assistance to enable structured,\nindividualized, and tool-assisted learning experiences? We introduce a novel\nmulti-agent AI tutoring platform that combines adaptive and personalized\nfeedback, structured course generation, and textbook knowledge retrieval to\nenable modular, tool-assisted learning processes. This system allows students\nto learn new topics while identifying and targeting their weaknesses, revise\nfor exams effectively, and practice on an unlimited number of personalized\nexercises. This article contributes to the field of artificial intelligence in\neducation by introducing a novel platform that brings together pedagogical\nagents and AI-driven components, augmenting the field with modular and\neffective systems for teaching mathematics.",
    "published": "2025-07-14T20:35:16Z",
    "updated": "2025-07-14T20:35:16Z",
    "id": "2507.12484v1",
    "authors": [
      "Jarosaw A. Chudziak",
      "Adam Kostka"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12484v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12484v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12484v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in an AI tutoring system for mathematics, focusing on personalized and adaptive education. It highlights the integration of LLMs with pedagogical tools and strategies, which aligns with the 'LLM' and 'Reasoning' topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.10778v1": {
    "title": "Warehouse Spatial Question Answering with LLM Agent",
    "summary": "Spatial understanding has been a challenging task for existing Multi-modal\nLarge Language Models~(MLLMs). Previous methods leverage large-scale MLLM\nfinetuning to enhance MLLM's spatial understanding ability. In this paper, we\npresent a data-efficient approach. We propose a LLM agent system with strong\nand advanced spatial reasoning ability, which can be used to solve the\nchallenging spatial question answering task in complex indoor warehouse\nscenarios. Our system integrates multiple tools that allow the LLM agent to\nconduct spatial reasoning and API tools interaction to answer the given\ncomplicated spatial question. Extensive evaluations on the 2025 AI City\nChallenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that\nour system achieves high accuracy and efficiency in tasks such as object\nretrieval, counting, and distance estimation. The code is available at:\nhttps://github.com/hsiangwei0903/SpatialAgent",
    "published": "2025-07-14T20:05:55Z",
    "updated": "2025-07-14T20:05:55Z",
    "id": "2507.10778v1",
    "authors": [
      "Hsiang-Wei Huang",
      "Jen-Hao Cheng",
      "Kuang-Ming Chen",
      "Cheng-Yen Yang",
      "Bahaa Alattar",
      "Yi-Ru Lin",
      "Pyongkun Kim",
      "Sangwon Kim",
      "Kwangju Kim",
      "Chung-I Huang",
      "Jenq-Neng Hwang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10778v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10778v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10778v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing spatial understanding in Multi-modal Large Language Models (MLLMs) using a LLM agent system, which involves spatial reasoning and API tools interaction. This aligns with the topics of MLLM (Multimodal Large Language Models) and Reasoning (LLM reasoning abilities).",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.10722v1": {
    "title": "Bridging Brains and Machines: A Unified Frontier in Neuroscience,\n  Artificial Intelligence, and Neuromorphic Systems",
    "summary": "This position and survey paper identifies the emerging convergence of\nneuroscience, artificial general intelligence (AGI), and neuromorphic computing\ntoward a unified research paradigm. Using a framework grounded in brain\nphysiology, we highlight how synaptic plasticity, sparse spike-based\ncommunication, and multimodal association provide design principles for\nnext-generation AGI systems that potentially combine both human and machine\nintelligences. The review traces this evolution from early connectionist models\nto state-of-the-art large language models, demonstrating how key innovations\nlike transformer attention, foundation-model pre-training, and multi-agent\narchitectures mirror neurobiological processes like cortical mechanisms,\nworking memory, and episodic consolidation. We then discuss emerging physical\nsubstrates capable of breaking the von Neumann bottleneck to achieve\nbrain-scale efficiency in silicon: memristive crossbars, in-memory compute\narrays, and emerging quantum and photonic devices. There are four critical\nchallenges at this intersection: 1) integrating spiking dynamics with\nfoundation models, 2) maintaining lifelong plasticity without catastrophic\nforgetting, 3) unifying language with sensorimotor learning in embodied agents,\nand 4) enforcing ethical safeguards in advanced neuromorphic autonomous\nsystems. This combined perspective across neuroscience, computation, and\nhardware offers an integrative agenda for in each of these fields.",
    "published": "2025-07-14T18:43:05Z",
    "updated": "2025-07-14T18:43:05Z",
    "id": "2507.10722v1",
    "authors": [
      "Sohan Shankar",
      "Yi Pan",
      "Hanqi Jiang",
      "Zhengliang Liu",
      "Mohammad R. Darbandi",
      "Agustin Lorenzo",
      "Junhao Chen",
      "Md Mehedi Hasan",
      "Arif Hassan Zidan",
      "Eliana Gelman",
      "Joshua A. Konfrst",
      "Jillian Y. Russell",
      "Katelyn Fernandes",
      "Tianze Yang",
      "Yiwei Li",
      "Huaqin Zhao",
      "Afrar Jahin",
      "Triparna Ganguly",
      "Shair Dinesha",
      "Yifan Zhou",
      "Zihao Wu",
      "Xinliang Li",
      "Lokesh Adusumilli",
      "Aziza Hussein",
      "Sagar Nookarapu",
      "Jixin Hou",
      "Kun Jiang",
      "Jiaxi Li",
      "Brenden Heinel",
      "XianShen Xi",
      "Hailey Hubbard",
      "Zayna Khan",
      "Levi Whitaker",
      "Ivan Cao",
      "Max Allgaier",
      "Andrew Darby",
      "Lin Zhao",
      "Lu Zhang",
      "Xiaoqiao Wang",
      "Xiang Li",
      "Wei Zhang",
      "Xiaowei Yu",
      "Dajiang Zhu",
      "Yohannes Abate",
      "Tianming Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10722v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10722v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10722v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the convergence of neuroscience, artificial general intelligence (AGI), and neuromorphic computing, with a focus on AGI and its potential integration with human and machine intelligences. It also touches on neuromorphic systems and hardware, which are not directly covered by the given topic list.",
    "llm_cls_result": [
      "AGI"
    ]
  },
  "2507.10695v1": {
    "title": "Exploring User Security and Privacy Attitudes and Concerns Toward the\n  Use of General-Purpose LLM Chatbots for Mental Health",
    "summary": "Individuals are increasingly relying on large language model (LLM)-enabled\nconversational agents for emotional support. While prior research has examined\nprivacy and security issues in chatbots specifically designed for mental health\npurposes, these chatbots are overwhelmingly \"rule-based\" offerings that do not\nleverage generative AI. Little empirical research currently measures users'\nprivacy and security concerns, attitudes, and expectations when using\ngeneral-purpose LLM-enabled chatbots to manage and improve mental health.\nThrough 21 semi-structured interviews with U.S. participants, we identified\ncritical misconceptions and a general lack of risk awareness. Participants\nconflated the human-like empathy exhibited by LLMs with human-like\naccountability and mistakenly believed that their interactions with these\nchatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures\nwith a licensed therapist. We introduce the concept of \"intangible\nvulnerability,\" where emotional or psychological disclosures are undervalued\ncompared to more tangible forms of information (e.g., financial or\nlocation-based data). To address this, we propose recommendations to safeguard\nuser mental health disclosures with general-purpose LLM-enabled chatbots more\neffectively.",
    "published": "2025-07-14T18:10:21Z",
    "updated": "2025-07-14T18:10:21Z",
    "id": "2507.10695v1",
    "authors": [
      "Jabari Kwesi",
      "Jiaxun Cao",
      "Riya Manchanda",
      "Pardis Emami-Naeini"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10695v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10695v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10695v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of general-purpose LLM-enabled chatbots for mental health, focusing on user security and privacy concerns. It highlights the lack of awareness and misconceptions among users regarding the privacy and security of their interactions with these chatbots. The study is centered around LLMs and their application in mental health support, making 'LLM' the most relevant topic. The discussion on privacy and security concerns also touches upon the broader implications of using LLMs in sensitive contexts, which aligns with the 'LLM' category.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.10500v1": {
    "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver\n  Assistance",
    "summary": "While autonomous driving technologies continue to advance, current Advanced\nDriver Assistance Systems (ADAS) remain limited in their ability to interpret\nscene context or engage with drivers through natural language. These systems\ntypically rely on predefined logic and lack support for dialogue-based\ninteraction, making them inflexible in dynamic environments or when adapting to\ndriver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a\nmodular framework that integrates Generative AI components including large\nlanguage models, vision-to-text interpretation, and structured function calling\nto enable real-time, interpretable, and adaptive driver assistance. SC-ADAS\nsupports multi-turn dialogue grounded in visual and sensor context, allowing\nnatural language recommendations and driver-confirmed ADAS control. Implemented\nin the CARLA simulator with cloud-based Generative AI, the system executes\nconfirmed user intents as structured ADAS commands without requiring model\nfine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and\nrevisited multi-turn interactions, highlighting trade-offs such as increased\nlatency from vision-based context retrieval and token growth from accumulated\ndialogue history. These results demonstrate the feasibility of combining\nconversational reasoning, scene perception, and modular ADAS control to support\nthe next generation of intelligent driver assistance.",
    "published": "2025-07-14T17:24:07Z",
    "updated": "2025-07-14T17:24:07Z",
    "id": "2507.10500v1",
    "authors": [
      "Kyungtae Han",
      "Yitao Chen",
      "Rohit Gupta",
      "Onur Altintas"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10500v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10500v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10500v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models and vision-to-text interpretation in a conversational ADAS system, which aligns with the topics of LLM (Large Language Models) and VLA (Vision-Language Alignment models). The system's ability to engage in multi-turn dialogue and interpret visual context also touches on Reasoning in LLMs.",
    "llm_cls_result": [
      "LLM",
      "VLA",
      "Reasoning"
    ]
  },
  "2507.10475v1": {
    "title": "Can You Detect the Difference?",
    "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.",
    "published": "2025-07-14T16:55:57Z",
    "updated": "2025-07-14T16:55:57Z",
    "id": "2507.10475v1",
    "authors": [
      "smail Tarm",
      "Aytu Onan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10475v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10475v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10475v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the detection of AI-generated text, specifically comparing autoregressive and diffusion-based models, which involves large language models (LLMs) and their evaluation.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.10469v1": {
    "title": "An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived\n  Realism and Performance in Virtual Reality Environments",
    "summary": "Advancements in artificial intelligence (AI) have significantly enhanced the\nrealism and interactivity of non-player characters (NPCs) in virtual reality\n(VR), creating more engaging and believable user experiences. This paper\nevaluates AI-driven NPCs within a VR interrogation simulator, focusing on their\nperceived realism, usability, and system performance. The simulator features\ntwo AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage\nparticipants in a scenario to determine the suspect's guilt or innocence. A\nuser study with 18 participants assessed the system using the System Usability\nScale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent\nBelievability Questionnaire, alongside latency measurements for speech-to-text\n(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.\nResults showed an average cycle latency of 7 seconds, influenced by the\nincreasing conversational context. Believability scored 6.67 out of 10, with\nhigh ratings in behavior, social relationships, and intelligence but moderate\nscores in emotion and personality. The system achieved a SUS score of 79.44,\nindicating good usability. These findings demonstrate the potential of large\nlanguage models to improve NPC realism and interaction in VR while highlighting\nchallenges in reducing system latency and enhancing emotional depth. This\nresearch contributes to the development of more sophisticated AI-driven NPCs,\nrevealing the need for performance optimization to achieve increasingly\nimmersive virtual experiences.",
    "published": "2025-07-14T16:50:29Z",
    "updated": "2025-07-14T16:50:29Z",
    "id": "2507.10469v1",
    "authors": [
      "Mikko Korkiakoski",
      "Saeid Sheikhi",
      "Jesper Nyman",
      "Jussi Saariniemi",
      "Kalle Tapio",
      "Panos Kostakos"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10469v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10469v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10469v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the use of AI, specifically GPT-4 Turbo, to enhance the realism and interactivity of non-player characters (NPCs) in virtual reality (VR) environments. While it involves AI and LLMs, the primary focus is on their application in VR rather than core LLM research or other specified topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.10644v2": {
    "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web\n  of Agents",
    "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.",
    "published": "2025-07-14T16:47:19Z",
    "updated": "2025-07-16T15:30:42Z",
    "id": "2507.10644v2",
    "authors": [
      "Tatiana Petrova",
      "Boris Bliznioukov",
      "Aleksandr Puzikov",
      "Radu State"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10644v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10644v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10644v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evolution of autonomous agents and their integration with large language models (LLMs), focusing on the Web of Agents (WoA) and its intellectual lineage from Multi-Agent Systems (MAS) and the Semantic Web. It highlights the role of LLMs in modern Agentic AI and the shift in the 'locus of intelligence' to within the agent's core model.",
    "llm_cls_result": [
      "LLM",
      "AGI",
      "RL"
    ]
  },
  "2507.10457v1": {
    "title": "Logic layer Prompt Control Injection (LPCI): A Novel Security\n  Vulnerability Class in Agentic Systems",
    "summary": "The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions.",
    "published": "2025-07-14T16:37:05Z",
    "updated": "2025-07-14T16:37:05Z",
    "id": "2507.10457v1",
    "authors": [
      "Hammad Atta",
      "Ken Huang",
      "Manish Bhatt",
      "Kamal Ahmed",
      "Muhammad Aziz Ul Haq",
      "Yasir Mehmood"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10457v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10457v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10457v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses security vulnerabilities in large language models (LLMs) within agentic systems, focusing on a novel attack category called Logic-Layer Prompt Control Injection (LPCI). This directly relates to LLM research and their integration into systems, but does not fit neatly into the other provided categories.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.10641v1": {
    "title": "A Code Comprehension Benchmark for Large Language Models for Code",
    "summary": "Large Language Models have shown impressive capabilities in coding tasks like\ncode generation and code completion, as they have been trained on a large\namount of code data. Also, since one of the core pretraining objectives is Next\nToken Prediction, these models tends to learn surface-level syntactic patterns\nin code. However, this does not guarantee code comprehension ability i.e. the\nability to capture the semantics of the code. In our opinion, this is the\nreason why these models often underperform on tasks that require deeper\nsemantic understanding, such as code debugging and code optimization. To\naddress this, we propose fine-tuning these models specifically for code\ncomprehension tasks using large-scale datasets, enabling them to develop a more\nrobust understanding of code semantics. We evaluate three code models of\nvarying sizes on a suite of code comprehension tasks designed to assess\nsemantic understanding beyond surface-level syntactic pattern matching. In\nparticular, we analyze performance on the Subjectivity Grading Task and observe\nthat model performance improves after fine-tuning on relevant downstream tasks.\nThe most significant improvement is seen in the QWQ-32B model, where accuracy\nincreases from 70% to 83.47%. A similar or explainable trend is observed across\nother models, clearly indicating an enhancement in code comprehension ability.\nAmong the models studied, the DPO-fine-tuned Codestral-22B achieves the highest\nmicro-accuracy of 87.66% on the Subjectivity Grading Task.",
    "published": "2025-07-14T16:19:49Z",
    "updated": "2025-07-14T16:19:49Z",
    "id": "2507.10641v1",
    "authors": [
      "Jayant Havare",
      "Saurav Chaudhary",
      "Ganesh Ramakrishnan",
      "Kaushik Maharajan",
      "Srikanth Tamilselvam"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10641v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10641v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10641v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating and improving the code comprehension abilities of Large Language Models (LLMs) through fine-tuning and benchmarking, which aligns with topics related to LLMs and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.10427v1": {
    "title": "Towards Emotion Co-regulation with LLM-powered Socially Assistive\n  Robots: Integrating LLM Prompts and Robotic Behaviors to Support\n  Parent-Neurodivergent Child Dyads",
    "summary": "Socially Assistive Robotics (SAR) has shown promise in supporting emotion\nregulation for neurodivergent children. Recently, there has been increasing\ninterest in leveraging advanced technologies to assist parents in co-regulating\nemotions with their children. However, limited research has explored the\nintegration of large language models (LLMs) with SAR to facilitate emotion\nco-regulation between parents and children with neurodevelopmental disorders.\nTo address this gap, we developed an LLM-powered social robot by deploying a\nspeech communication module on the MiRo-E robotic platform. This supervised\nautonomous system integrates LLM prompts and robotic behaviors to deliver\ntailored interventions for both parents and neurodivergent children. Pilot\ntests were conducted with two parent-child dyads, followed by a qualitative\nanalysis. The findings reveal MiRo-E's positive impacts on interaction dynamics\nand its potential to facilitate emotion regulation, along with identified\ndesign and technical challenges. Based on these insights, we provide design\nimplications to advance the future development of LLM-powered SAR for mental\nhealth applications.",
    "published": "2025-07-14T16:16:12Z",
    "updated": "2025-07-14T16:16:12Z",
    "id": "2507.10427v1",
    "authors": [
      "Jing Li",
      "Felix Schijve",
      "Sheng Li",
      "Yuye Yang",
      "Jun Hu",
      "Emilia Barakova"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10427v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10427v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10427v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with Socially Assistive Robotics (SAR) to support emotion co-regulation in parent-neurodivergent child dyads. The focus is on leveraging LLMs for tailored interventions, which directly relates to the 'LLM' topic. Additionally, the application involves robotic behaviors and interaction dynamics, which aligns with the 'AGI' topic as it involves generalist models and universal learners in a real-world application.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.14198v1": {
    "title": "Retention analysis of edited knowledge after fine-tuning",
    "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that freezing layers associated with edited content can significantly\nimprove knowledge retention, offering insight into how future editing methods\nmight be made more robust.",
    "published": "2025-07-14T15:51:19Z",
    "updated": "2025-07-14T15:51:19Z",
    "id": "2507.14198v1",
    "authors": [
      "Fufang Wen",
      "Shichang Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14198v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14198v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14198v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the retention of edited knowledge in Large Language Models (LLMs) after fine-tuning, which directly relates to the topics of LLM and Memory. It discusses the interaction between fine-tuning and model editing techniques, which are key aspects of LLM research and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.10392v1": {
    "title": "Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters",
    "summary": "Large language models (LLMs) require vast amounts of GPU compute to train,\nbut limited availability and high costs of GPUs make homogeneous clusters\nimpractical for many organizations. Instead, assembling heterogeneous clusters\nby pooling together GPUs of different generations allows them to achieve higher\naggregate compute and make use of all available GPUs. However, training on\nheterogeneous clusters presents several challenges, including load balancing\nacross GPUs, optimizing memory usage to accommodate varying memory capacities,\nand ensuring communication-efficient training over diverse network\ninterconnects potentially spanning multiple datacenters. In this paper, we make\nthe case that efficient training on heterogeneous clusters requires (1) the\nintegration of pipeline parallelism and data parallelism in a manner that is\nboth communication- and memory-efficient, and (2) a more adaptable\nconfiguration of pipeline and data parallelism, which includes the capability\nto flexibly partition GPUs into asymmetric pipeline parallel stages and to\nincorporate heterogeneous GPUs within the same data parallelism group. We\npropose Zorse, the first system to unify all these capabilities while\nincorporating a planner that automatically configures training strategies for a\ngiven workload. Our evaluation shows that Zorse significantly outperforms\nstate-of-the-art systems in heterogeneous training scenarios.",
    "published": "2025-07-14T15:31:31Z",
    "updated": "2025-07-14T15:31:31Z",
    "id": "2507.10392v1",
    "authors": [
      "Runsheng Benson Guo",
      "Utkarsh Anand",
      "Khuzaima Daudjee",
      "Rathijit Sen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10392v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10392v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10392v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing the training efficiency of Large Language Models (LLMs) on heterogeneous GPU clusters, which directly relates to the scaling and efficiency aspects of LLM training.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.10342v1": {
    "title": "Using AI to replicate human experimental results: a motion study",
    "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.",
    "published": "2025-07-14T14:47:01Z",
    "updated": "2025-07-14T14:47:01Z",
    "id": "2507.10342v1",
    "authors": [
      "Rosa Illan Castillo",
      "Javier Valenzuela"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10342v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10342v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10342v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to replicate human experimental results in linguistic research, focusing on their ability to mimic human judgements and their potential as analytical tools.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.12483v1": {
    "title": "A Survey of Reinforcement Learning for Software Engineering",
    "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential\ndecision-making and has attracted growing interest across various domains,\nparticularly following the advent of Deep Reinforcement Learning (DRL) in 2015.\nSimultaneously, the rapid advancement of Large Language Models (LLMs) has\nfurther fueled interest in integrating RL with LLMs to enable more adaptive and\nintelligent systems. In the field of software engineering (SE), the increasing\ncomplexity of systems and the rising demand for automation have motivated\nresearchers to apply RL to a broad range of tasks, from software design and\ndevelopment to quality assurance and maintenance. Despite growing research in\nRL-for-SE, there remains a lack of a comprehensive and systematic survey of\nthis evolving field. To address this gap, we reviewed 115 peer-reviewed studies\npublished across 22 premier SE venues since the introduction of DRL. We\nconducted a comprehensive analysis of publication trends, categorized SE topics\nand RL algorithms, and examined key factors such as dataset usage, model design\nand optimization, and evaluation practices. Furthermore, we identified open\nchallenges and proposed future research directions to guide and inspire ongoing\nwork in this evolving area. To summarize, this survey offers the first\nsystematic mapping of RL applications in software engineering, aiming to\nsupport both researchers and practitioners in navigating the current landscape\nand advancing the field. Our artifacts are publicly available:\nhttps://github.com/KaiWei-Lin-lanina/RL4SE.",
    "published": "2025-07-14T14:28:37Z",
    "updated": "2025-07-14T14:28:37Z",
    "id": "2507.12483v1",
    "authors": [
      "Dong Wang",
      "Hanmo You",
      "Lingwei Zhu",
      "Kaiwei Lin",
      "Zheng Chen",
      "Chen Yang",
      "Junji Yu",
      "Zan Wang",
      "Junjie Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12483v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12483v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12483v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Reinforcement Learning (RL) in software engineering, including its integration with Large Language Models (LLMs). The primary focus is on RL, but it also touches on LLMs, making these the most relevant topics.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.10311v1": {
    "title": "Recognizing Dementia from Neuropsychological Tests with State Space\n  Models",
    "summary": "Early detection of dementia is critical for timely medical intervention and\nimproved patient outcomes. Neuropsychological tests are widely used for\ncognitive assessment but have traditionally relied on manual scoring. Automatic\ndementia classification (ADC) systems aim to infer cognitive decline directly\nfrom speech recordings of such tests. We propose Demenba, a novel ADC framework\nbased on state space models, which scale linearly in memory and computation\nwith sequence length. Trained on over 1,000 hours of cognitive assessments\nadministered to Framingham Heart Study participants, some of whom were\ndiagnosed with dementia through adjudicated review, our method outperforms\nprior approaches in fine-grained dementia classification by 21\\%, while using\nfewer parameters. We further analyze its scaling behavior and demonstrate that\nour model gains additional improvement when fused with large language models,\npaving the way for more transparent and scalable dementia assessment tools.\nCode: https://anonymous.4open.science/r/Demenba-0861",
    "published": "2025-07-14T14:15:47Z",
    "updated": "2025-07-14T14:15:47Z",
    "id": "2507.10311v1",
    "authors": [
      "Liming Wang",
      "Saurabhchand Bhati",
      "Cody Karjadi",
      "Rhoda Au",
      "James Glass"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10311v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10311v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10311v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on dementia classification using state space models and mentions the use of large language models (LLMs) for improvement, but the primary focus is not on LLMs or any of the other provided topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.10300v1": {
    "title": "FaceLLM: A Multimodal Large Language Model for Face Understanding",
    "summary": "Multimodal large language models (MLLMs) have shown remarkable performance in\nvision-language tasks. However, existing MLLMs are primarily trained on generic\ndatasets, limiting their ability to reason on domain-specific visual cues such\nas those in facial images. In particular, tasks that require detailed\nunderstanding of facial structure, expression, emotion, and demographic\nfeatures remain underexplored by MLLMs due to the lack of large-scale annotated\nface image-text datasets. In this work, we introduce FaceLLM, a multimodal\nlarge language model trained specifically for facial image understanding. To\nconstruct the training data, we propose a novel weakly supervised pipeline that\nuses ChatGPT with attribute-aware prompts to generate high-quality\nquestion-answer pairs based on images from the FairFace dataset. The resulting\ncorpus, called FairFaceGPT, covers a diverse set of attributes including\nexpression, pose, skin texture, and forensic information. Our experiments\ndemonstrate that FaceLLM improves the performance of MLLMs on various\nface-centric tasks and achieves state-of-the-art performance. This work\nhighlights the potential of synthetic supervision via language models for\nbuilding domain-specialized MLLMs, and sets a precedent for trustworthy,\nhuman-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM\nmodels are publicly available in the project page.",
    "published": "2025-07-14T14:04:14Z",
    "updated": "2025-07-14T14:04:14Z",
    "id": "2507.10300v1",
    "authors": [
      "Hatef Otroshi Shahreza",
      "Sbastien Marcel"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10300v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10300v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10300v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a multimodal large language model (MLLM) specifically designed for facial image understanding, which involves integrating vision and language modalities. It also discusses the creation of a dataset for training this model, aligning with the topics of MLLM and Dataset.",
    "llm_cls_result": [
      "MLLM",
      "Dataset"
    ]
  },
  "2507.10284v1": {
    "title": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning",
    "summary": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics.",
    "published": "2025-07-14T13:51:28Z",
    "updated": "2025-07-14T13:51:28Z",
    "id": "2507.10284v1",
    "authors": [
      "Venkat Margapuri"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10284v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10284v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10284v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) with reinforcement learning (RL) for visual coverage path planning, leveraging the zero-shot reasoning and in-context learning capabilities of LLMs to dynamically shape the reward function of an RL policy. This aligns with the topics of Reinforcement Learning (RL) and Large Language Models (LLM).",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.10259v1": {
    "title": "Cross-Timeslot Optimization for Distributed GPU Inference Using\n  Reinforcement Learning",
    "summary": "The rapid growth of large language model (LLM) services imposes increasing\ndemands on distributed GPU inference infrastructure. Most existing scheduling\nsystems rely on the current system state to make decisions, without considering\nhow task demand and resource availability evolve over time. This lack of\ntemporal awareness leads to inefficient GPU utilization, high task migration\noverhead, and poor system responsiveness under dynamic workloads. In this work,\nwe identify the fundamental limitations of these instantaneous-state-only\nscheduling approaches and propose Temporal Optimal Resource scheduling via\nTwo-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling\nframework that captures both long-term workload patterns and short-term\nexecution constraints. It adopts a two-layer design: a macro-level scheduler\nleverages reinforcement learning and optimal transport to coordinate\ninter-region task distribution, while a micro-level allocator refines\ntask-to-server assignments within each region to reduce latency and switching\ncosts. Experimental results across multiple network topologies show that TORTA\nreduces average inference response time by up to 15\\%, improves load balance by\napproximately 4-5\\%, and cuts total operational cost by 10-20\\% compared to\nstate-of-the-art baseline methods.",
    "published": "2025-07-14T13:33:30Z",
    "updated": "2025-07-14T13:33:30Z",
    "id": "2507.10259v1",
    "authors": [
      "Chengze Du",
      "Zhiwei Yu",
      "Heng Xu",
      "Haojie Wang",
      "Bo liu",
      "Jialong Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10259v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10259v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10259v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of reinforcement learning for optimizing distributed GPU inference, which aligns with the RL topic. It also mentions large language models (LLMs) and their inference infrastructure, which is relevant to the LLM topic.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.10202v1": {
    "title": "A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance\n  on High-Resolution Images",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in vision-language understanding, reasoning, and generation.\nHowever, they struggle with tasks requiring fine-grained localization and\nreasoning in high-resolution images. This constraint stems from the fact that\nMLLMs are fine-tuned with fixed image resolution to align with the pre-trained\nimage encoder used in MLLM. Consequently, feeding high-resolution images\ndirectly into MLLMs leads to poor generalization due to a train-test resolution\ndiscrepancy, while downsampling these images-although ensuring\nconsistency-compromises fine-grained visual details and ultimately degrades\nperformance. To address this challenge, we propose Extract Candidate then\nPredict (ECP), a novel training-free, task-agnostic two-stage framework\ndesigned to enhance MLLM performance on high-resolution images. The key\nintuition behind ECP is that while MLLMs struggle with high-resolution images,\ntheir predictions on downsampled images still contain implicit localization\ncues. By first identifying candidate region using the coarse prediction and\nthen predicting the final output based on candidate region, ECP effectively\npreserves fine-grained details while mitigating the challenges posed by\nhigh-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K\nMLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared\nto baseline respectively, demonstrating its effectiveness. Code is available at\nhttps://github.com/yenncye/ECP.",
    "published": "2025-07-14T12:14:53Z",
    "updated": "2025-07-14T12:14:53Z",
    "id": "2507.10202v1",
    "authors": [
      "Jaeseong Lee",
      "Yeeun Choi",
      "Heechan Choi",
      "Hanjung Kim",
      "Seonjoo Kim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10202v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10202v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10202v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the performance of Multimodal Large Language Models (MLLMs) on high-resolution images, which directly relates to the topics of MLLM and VLA. The proposed framework, ECP, is task-agnostic and training-free, addressing challenges in vision-language alignment and fine-grained localization.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.10200v1": {
    "title": "Natural Language-based Assessment of L2 Oral Proficiency using LLMs",
    "summary": "Natural language-based assessment (NLA) is an approach to second language\nassessment that uses instructions - expressed in the form of can-do descriptors\n- originally intended for human examiners, aiming to determine whether large\nlanguage models (LLMs) can interpret and apply them in ways comparable to human\nassessment. In this work, we explore the use of such descriptors with an\nopen-source LLM, Qwen 2.5 72B, to assess responses from the publicly available\nS&I Corpus in a zero-shot setting. Our results show that this approach -\nrelying solely on textual information - achieves competitive performance: while\nit does not outperform state-of-the-art speech LLMs fine-tuned for the task, it\nsurpasses a BERT-based model trained specifically for this purpose. NLA proves\nparticularly effective in mismatched task settings, is generalisable to other\ndata types and languages, and offers greater interpretability, as it is\ngrounded in clearly explainable, widely applicable language descriptors.",
    "published": "2025-07-14T12:13:50Z",
    "updated": "2025-07-14T12:13:50Z",
    "id": "2507.10200v1",
    "authors": [
      "Stefano Bann",
      "Rao Ma",
      "Mengjie Qian",
      "Siyuan Tang",
      "Kate Knill",
      "Mark Gales"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10200v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10200v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10200v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in assessing second language oral proficiency, which directly involves the use and evaluation of LLMs in a specific task.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.10178v1": {
    "title": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large\n  Language Model Serving",
    "summary": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x\nhigher token generation throughput, respectively.",
    "published": "2025-07-14T11:40:17Z",
    "updated": "2025-07-14T11:40:17Z",
    "id": "2507.10178v1",
    "authors": [
      "Wonung Kim",
      "Yubin Lee",
      "Yoonsung Kim",
      "Jinwoo Hwang",
      "Seongryong Oh",
      "Jiyong Jung",
      "Aziz Huseynov",
      "Woong Gyu Park",
      "Chang Hyun Park",
      "Divya Mahajan",
      "Jongse Park"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10178v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10178v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10178v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the acceleration of post-transformer Large Language Models (LLMs) through a processing-in-memory approach, focusing on memory bandwidth limitations and hardware efficiency. It directly relates to LLM architectures and their scaling challenges.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.10177v1": {
    "title": "Abusive text transformation using LLMs",
    "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.",
    "published": "2025-07-14T11:39:34Z",
    "updated": "2025-07-14T11:39:34Z",
    "id": "2507.10177v1",
    "authors": [
      "Rohitash Chandra",
      "Jiyong Choi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10177v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10177v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10177v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the use of Large Language Models (LLMs) for transforming abusive text into non-abusive versions while maintaining the original intent. It evaluates the performance of various LLMs in this specific task, which directly relates to the LLM topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.10155v1": {
    "title": "Task-Based Flexible Feature Distillation for LLMs",
    "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.",
    "published": "2025-07-14T11:10:02Z",
    "updated": "2025-07-14T11:10:02Z",
    "id": "2507.10155v1",
    "authors": [
      "Khouloud Saadi",
      "Di Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10155v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10155v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10155v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses knowledge distillation techniques for large language models (LLMs), focusing on feature distillation to reduce computational demands. It addresses the limitations of traditional methods and proposes a novel approach without introducing new parameters. The core topic is related to LLMs and their optimization.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.10150v1": {
    "title": "Past-Future Scheduler for LLM Serving under SLA Guarantees",
    "summary": "The exploration and application of Large Language Models (LLMs) is thriving.\nTo reduce deployment costs, continuous batching has become an essential feature\nin current service frameworks. The effectiveness of continuous batching relies\non an accurate estimate of the memory requirements of requests. However, due to\nthe diversity in request output lengths, existing frameworks tend to adopt\naggressive or conservative schedulers, which often result in significant\noverestimation or underestimation of memory consumption. Consequently, they\nsuffer from harmful request evictions or prolonged queuing times, failing to\nachieve satisfactory throughput under strict Service Level Agreement (SLA)\nguarantees (a.k.a. goodput), across various LLM application scenarios with\ndiffering input-output length distributions. To address this issue, we propose\na novel Past-Future scheduler that precisely estimates the peak memory\nresources required by the running batch via considering the historical\ndistribution of request output lengths and calculating memory occupancy at each\nfuture time point. It adapts to applications with all types of input-output\nlength distributions, balancing the trade-off between request queuing and\nharmful evictions, thereby consistently achieving better goodput. Furthermore,\nto validate the effectiveness of the proposed scheduler, we developed a\nhigh-performance LLM serving framework, LightLLM, that implements the\nPast-Future scheduler. Compared to existing aggressive or conservative\nschedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\\times$\nhigher goodput than other schedulers under heavy loads. LightLLM is open source\nto boost the research in such direction (https://github.com/ModelTC/lightllm).",
    "published": "2025-07-14T10:53:47Z",
    "updated": "2025-07-14T10:53:47Z",
    "id": "2507.10150v1",
    "authors": [
      "Ruihao Gong",
      "Shihao Bai",
      "Siyu Wu",
      "Yunqian Fan",
      "Zaijun Wang",
      "Xiuhong Li",
      "Hailong Yang",
      "Xianglong Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10150v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10150v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10150v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing LLM serving frameworks through a novel scheduler that improves memory estimation and request handling, which is directly related to the deployment and efficiency of Large Language Models.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.10103v1": {
    "title": "Accelerating Automatic Program Repair with Dual Retrieval-Augmented\n  Fine-Tuning and Patch Generation on Large Language Models",
    "summary": "Automated Program Repair (APR) is essential for ensuring software reliability\nand quality while enhancing efficiency and reducing developers' workload.\nAlthough rule-based and learning-based APR methods have demonstrated their\neffectiveness, their performance was constrained by the defect type of repair,\nthe quality of training data, and the size of model parameters. Recently, Large\nLanguage Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have\nbeen increasingly adopted in APR tasks. However, current code LLMs and RAG\ndesigns neither fully address code repair tasks nor consider code-specific\nfeatures. To overcome these limitations, we propose SelRepair, a novel APR\napproach with integration of a fine-tuned LLM with a newly-designed dual RAG\nmodule. This approach uses a bug-fix pair dataset for fine-tuning and\nincorporates semantic and syntactic/structural similarity information through\nan RAG selection gate. This design ensures relevant information is retrieved\nefficiently, thereby reducing token length and inference time. Evaluations on\nJava datasets show SelRepair outperforms other APR methods, achieving 26.29%\nand 17.64% in terms of exact match (EM) on different datasets while reducing\ninference time by at least 6.42% with controlled input lengths.",
    "published": "2025-07-14T09:41:51Z",
    "updated": "2025-07-14T09:41:51Z",
    "id": "2507.10103v1",
    "authors": [
      "Hanyang Guo",
      "Xiaoheng Xie",
      "Hong-Ning Dai",
      "Peng Di",
      "Yu Zhang",
      "Bishenghui Tao",
      "Zibin Zheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10103v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10103v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10103v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) for Automated Program Repair (APR), which involves fine-tuning LLMs and incorporating memory-augmented methods.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Pretrain"
    ]
  },
  "2507.11559v1": {
    "title": "RSD-15K: A Large-Scale User-Level Annotated Dataset for Suicide Risk\n  Detection on Social Media",
    "summary": "In recent years, cognitive and mental health (CMH) disorders have\nincreasingly become an important challenge for global public health, especially\nthe suicide problem caused by multiple factors such as social competition,\neconomic pressure and interpersonal relationships among young and middle-aged\npeople. Social media, as an important platform for individuals to express\nemotions and seek help, provides the possibility for early detection and\nintervention of suicide risk. This paper introduces a large-scale dataset\ncontaining 15,000 user-level posts. Compared with existing datasets, this\ndataset retains complete user posting time sequence information, supports\nmodeling the dynamic evolution of suicide risk, and we have also conducted\ncomprehensive and rigorous annotations on these datasets. In the benchmark\nexperiment, we systematically evaluated the performance of traditional machine\nlearning methods, deep learning models, and fine-tuned large language models.\nThe experimental results show that our dataset can effectively support the\nautomatic assessment task of suicide risk. Considering the sensitivity of\nmental health data, we also discussed the privacy protection and ethical use of\nthe dataset. In addition, we also explored the potential applications of the\ndataset in mental health testing, clinical psychiatric auxiliary treatment,\netc., and provided directional suggestions for future research work.",
    "published": "2025-07-14T09:26:26Z",
    "updated": "2025-07-14T09:26:26Z",
    "id": "2507.11559v1",
    "authors": [
      "Shouwen Zheng",
      "Yingzhi Tao",
      "Taiqi Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11559v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11559v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11559v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a dataset for suicide risk detection on social media, which is a specialized dataset for a specific application rather than a general LLM or multimodal model topic. It does not fit into the provided categories of LLM, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset as defined.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.10087v1": {
    "title": "Foundation Model Driven Robotics: A Comprehensive Review",
    "summary": "The rapid emergence of foundation models, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), has introduced a transformative\nparadigm in robotics. These models offer powerful capabilities in semantic\nunderstanding, high-level reasoning, and cross-modal generalization, enabling\nsignificant advances in perception, planning, control, and human-robot\ninteraction. This critical review provides a structured synthesis of recent\ndevelopments, categorizing applications across simulation-driven design,\nopen-world execution, sim-to-real transfer, and adaptable robotics. Unlike\nexisting surveys that emphasize isolated capabilities, this work highlights\nintegrated, system-level strategies and evaluates their practical feasibility\nin real-world environments. Key enabling trends such as procedural scene\ngeneration, policy generalization, and multimodal reasoning are discussed\nalongside core bottlenecks, including limited embodiment, lack of multimodal\ndata, safety risks, and computational constraints. Through this lens, this\npaper identifies both the architectural strengths and critical limitations of\nfoundation model-based robotics, highlighting open challenges in real-time\noperation, grounding, resilience, and trust. The review concludes with a\nroadmap for future research aimed at bridging semantic reasoning and physical\nintelligence through more robust, interpretable, and embodied models.",
    "published": "2025-07-14T09:13:07Z",
    "updated": "2025-07-14T09:13:07Z",
    "id": "2507.10087v1",
    "authors": [
      "Muhammad Tayyab Khan",
      "Ammar Waheed"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10087v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10087v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10087v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) and Vision-Language Models (VLMs) in robotics, highlighting their capabilities in semantic understanding, reasoning, and cross-modal generalization. This aligns with the topics of LLM (Large Language Models) and VLA (Vision-Language Alignment models). Additionally, the focus on integrated, system-level strategies and practical feasibility in real-world environments suggests relevance to AGI (Artificial General Intelligence).",
    "llm_cls_result": [
      "LLM",
      "VLA",
      "AGI"
    ]
  },
  "2507.10073v1": {
    "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through\n  Moral Questionnaires",
    "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.",
    "published": "2025-07-14T08:59:26Z",
    "updated": "2025-07-14T08:59:26Z",
    "id": "2507.10073v1",
    "authors": [
      "Simon Mnker"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10073v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10073v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10073v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the cultural bias in Large Language Models (LLMs) and evaluates their moral alignment across different cultural contexts. It focuses on the limitations of LLMs in representing diverse human values and the implications for AI alignment.",
    "llm_cls_result": [
      "LLM",
      "AGI",
      "Benchmark"
    ]
  },
  "2507.10059v1": {
    "title": "GeLaCo: An Evolutionary Approach to Layer Compression",
    "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.",
    "published": "2025-07-14T08:44:59Z",
    "updated": "2025-07-14T08:44:59Z",
    "id": "2507.10059v1",
    "authors": [
      "David Ponce",
      "Thierry Etchegoyhen",
      "Javier Del Ser"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10059v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10059v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10059v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on model compression methods for Large Language Models (LLMs) using an evolutionary approach, which aligns with the 'LLM' and 'Scaling' topics. It discusses reducing computational requirements and optimizing model performance, which are key aspects of scaling and efficiency in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.10054v2": {
    "title": "Explicit Vulnerability Generation with LLMs: An Investigation Beyond\n  Adversarial Attacks",
    "summary": "Large Language Models (LLMs) are increasingly used as code assistants, yet\ntheir behavior when explicitly asked to generate insecure code remains poorly\nunderstood. While prior research has focused on unintended vulnerabilities,\nthis study examines a more direct threat: open-source LLMs generating\nvulnerable code when prompted. We propose a dual experimental design: (1)\nDynamic Prompting, which systematically varies vulnerability type, user\npersona, and prompt phrasing across structured templates; and (2) Reverse\nPrompting, which derives natural-language prompts from real vulnerable code\nsamples. We evaluate three open-source 7B-parameter models (Qwen2, Mistral,\nGemma) using static analysis to assess both the presence and correctness of\ngenerated vulnerabilities. Our results show that all models frequently generate\nthe requested vulnerabilities, though with significant performance differences.\nGemma achieves the highest correctness for memory vulnerabilities under Dynamic\nPrompting (e.g., 98.6% for buffer overflows), while Qwen2 demonstrates the most\nbalanced performance across all tasks. We find that professional personas\n(e.g., \"DevOps Engineer\") consistently elicit higher success rates than student\npersonas, and that the effectiveness of direct versus indirect phrasing is\ninverted depending on the prompting strategy. Vulnerability reproduction\naccuracy follows a non-linear pattern with code complexity, peaking in a\nmoderate range. Our findings expose how LLMs' reliance on pattern recall over\nsemantic reasoning creates significant blind spots in their safety alignments,\nparticularly for requests framed as plausible professional tasks.",
    "published": "2025-07-14T08:36:26Z",
    "updated": "2025-07-23T10:43:29Z",
    "id": "2507.10054v2",
    "authors": [
      "Emir Bosnak",
      "Sahand Moslemi",
      "Mayasah Lami",
      "Anil Koyuncu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10054v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10054v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10054v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper investigates the behavior of Large Language Models (LLMs) in generating insecure code when explicitly prompted, focusing on their vulnerability to producing unsafe outputs. This directly relates to research on LLMs and their safety alignments, which falls under the broader category of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.11558v1": {
    "title": "Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting",
    "summary": "Foundation models have achieved remarkable success in natural language\nprocessing and computer vision, demonstrating strong capabilities in modeling\ncomplex patterns. While recent efforts have explored adapting large language\nmodels (LLMs) for time-series forecasting, LLMs primarily capture\none-dimensional sequential dependencies and struggle to model the richer\nspatio-temporal (ST) correlations essential for accurate ST forecasting. In\nthis paper, we present \\textbf{ST-VFM}, a novel framework that systematically\nreprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal\nforecasting. While VFMs offer powerful spatial priors, two key challenges arise\nwhen applying them to ST tasks: (1) the lack of inherent temporal modeling\ncapacity and (2) the modality gap between visual and ST data. To address these,\nST-VFM adopts a \\emph{dual-branch architecture} that integrates raw ST inputs\nwith auxiliary ST flow inputs, where the flow encodes lightweight temporal\ndifference signals interpretable as dynamic spatial cues. To effectively\nprocess these dual-branch inputs, ST-VFM introduces two dedicated reprogramming\nstages. The \\emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token\nAdapter to embed temporal context and align both branches into VFM-compatible\nfeature spaces. The \\emph{post-VFM reprogramming} stage introduces a Bilateral\nCross-Prompt Coordination module, enabling dynamic interaction between branches\nthrough prompt-based conditioning, thus enriching joint representation learning\nwithout modifying the frozen VFM backbone. Extensive experiments on ten\nspatio-temporal datasets show that ST-VFM outperforms state-of-the-art\nbaselines, demonstrating effectiveness and robustness across VFM backbones\n(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong\ngeneral framework for spatio-temporal forecasting.",
    "published": "2025-07-14T08:33:34Z",
    "updated": "2025-07-14T08:33:34Z",
    "id": "2507.11558v1",
    "authors": [
      "Changlu Chen",
      "Yanbin Liu",
      "Chaoxi Niu",
      "Ling Chen",
      "Tianqing Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11558v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11558v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11558v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on adapting Vision Foundation Models (VFMs) for spatio-temporal forecasting, which involves integrating spatial and temporal data. This aligns with the topics of Multimodal Large Language Models (MLLM) due to the integration of vision and temporal data, and Vision-Language Action (VLA) due to the focus on vision and spatio-temporal modeling.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.10630v1": {
    "title": "Enhancing the Capabilities of Large Language Models for API calls\n  through Knowledge Graphs",
    "summary": "API calls by large language models (LLMs) offer a cutting-edge approach for\ndata analysis. However, their ability to effectively utilize tools via API\ncalls remains underexplored in knowledge-intensive domains like meteorology.\nThis paper introduces KG2data, a system that integrates knowledge graphs, LLMs,\nReAct agents, and tool-use technologies to enable intelligent data acquisition\nand query handling in the meteorological field. Using a virtual API, we\nevaluate API call accuracy across three metrics: name recognition failure,\nhallucination failure, and call correctness. KG2data achieves superior\nperformance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and\nchat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based\nsystems by addressing their limited access to domain-specific knowledge, which\nhampers performance on complex or terminology-rich queries. By using a\nknowledge graph as persistent memory, our system enhances content retrieval,\ncomplex query handling, domain-specific reasoning, semantic relationship\nresolution, and heterogeneous data integration. It also mitigates the high cost\nof fine-tuning LLMs, making the system more adaptable to evolving domain\nknowledge and API structures. In summary, KG2data provides a novel solution for\nintelligent, knowledge-based question answering and data analysis in domains\nwith high knowledge demands.",
    "published": "2025-07-14T08:20:06Z",
    "updated": "2025-07-14T08:20:06Z",
    "id": "2507.10630v1",
    "authors": [
      "Ye Yang",
      "Xue Xiao",
      "Ping Yin",
      "Taotao Xie"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10630v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10630v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10630v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses enhancing the capabilities of Large Language Models (LLMs) through the integration of knowledge graphs and API calls, focusing on domain-specific knowledge and reasoning. It also mentions the use of ReAct agents, which are related to Reinforcement Learning (RL).",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Memory"
    ]
  },
  "2507.10039v1": {
    "title": "Towards Applying Large Language Models to Complement Single-Cell\n  Foundation Models",
    "summary": "Single-cell foundation models such as scGPT represent a significant\nadvancement in single-cell omics, with an ability to achieve state-of-the-art\nperformance on various downstream biological tasks. However, these models are\ninherently limited in that a vast amount of information in biology exists as\ntext, which they are unable to leverage. There have therefore been several\nrecent works that propose the use of LLMs as an alternative to single-cell\nfoundation models, achieving competitive results. However, there is little\nunderstanding of what factors drive this performance, along with a strong focus\non using LLMs as an alternative, rather than complementary approach to\nsingle-cell foundation models. In this study, we therefore investigate what\nbiological insights contribute toward the performance of LLMs when applied to\nsingle-cell data, and introduce scMPT; a model which leverages synergies\nbetween scGPT, and single-cell representations from LLMs that capture these\ninsights. scMPT demonstrates stronger, more consistent performance than either\nof its component models, which frequently have large performance gaps between\neach other across datasets. We also experiment with alternate fusion methods,\ndemonstrating the potential of combining specialized reasoning models with\nscGPT to improve performance. This study ultimately showcases the potential for\nLLMs to complement single-cell foundation models and drive improvements in\nsingle-cell analysis.",
    "published": "2025-07-14T08:16:58Z",
    "updated": "2025-07-14T08:16:58Z",
    "id": "2507.10039v1",
    "authors": [
      "Steven Palayew",
      "Bo Wang",
      "Gary Bader"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10039v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10039v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10039v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) to complement single-cell foundation models in biological tasks, highlighting the synergy between LLMs and specialized models like scGPT. The focus is on leveraging LLMs for biological insights and improving performance in single-cell analysis.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.10026v1": {
    "title": "EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via\n  Attention-Guided Diffusion Reinforcement Learning",
    "summary": "The growth of Artificial Intelligence (AI) and large language models has\nenabled the use of Generative AI (GenAI) in cloud data centers for diverse\nAI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce\nunavoidable delays and substantial resource overhead, which are unsuitable for\nusers at the network edge with high QoS demands. Deploying AIGC services on\nedge servers reduces transmission times but often leads to underutilized\nresources and fails to optimally balance inference latency and quality. To\naddress these issues, this paper introduces a QoS-aware\n\\underline{E}dge-collaborative \\underline{A}IGC \\underline{T}ask scheduling\n(EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to\nvarious edge servers, formulating it as a gang scheduling problem that balances\ninference latency and quality while considering server heterogeneity, such as\ndiffering model distributions and cold start issues. 2) We propose a\nreinforcement learning-based EAT algorithm that uses an attention layer to\nextract load and task queue information from edge servers and employs a\ndiffusion-based policy network for scheduling, efficiently enabling model\nreuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm\nto divide tasks and distribute them across multiple edge servers for\nprocessing. Experimental results based on our system and large-scale\nsimulations show that our EAT algorithm can reduce inference latency by up to\n56\\% compared to baselines. We release our open-source code at\nhttps://github.com/zzf1955/EAT.",
    "published": "2025-07-14T08:06:58Z",
    "updated": "2025-07-14T08:06:58Z",
    "id": "2507.10026v1",
    "authors": [
      "Zhifei Xu",
      "Zhiqing Tang",
      "Jiong Lou",
      "Zhi Yao",
      "Xuan Xie",
      "Tian Wang",
      "Yinglong Wang",
      "Weijia Jia"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10026v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10026v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10026v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on task scheduling for AI-Generated Content (AIGC) tasks using reinforcement learning, which aligns with the RL topic. It also involves large language models and AI, which are related to LLM. The QoS-aware edge-collaborative approach is specific to system optimization and does not directly fit into the other provided categories.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.10024v2": {
    "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies,\n  Challenges, and Roles",
    "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research.",
    "published": "2025-07-14T08:06:12Z",
    "updated": "2025-07-16T08:53:53Z",
    "id": "2507.10024v2",
    "authors": [
      "Shaolun Ruan",
      "Rui Sheng",
      "Xiaolin Wen",
      "Jiachen Wang",
      "Tianyi Zhang",
      "Yong Wang",
      "Tim Dwyer",
      "Jiannan Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10024v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10024v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10024v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in the design study process, focusing on strategies, challenges, and roles. It directly mentions LLMs and their capabilities, making it relevant to the LLM topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.10008v1": {
    "title": "Protective Factor-Aware Dynamic Influence Learning for Suicide Risk\n  Prediction on Social Media",
    "summary": "Suicide is a critical global health issue that requires urgent attention.\nEven though prior work has revealed valuable insights into detecting current\nsuicide risk on social media, little attention has been paid to developing\nmodels that can predict subsequent suicide risk over time, limiting their\nability to capture rapid fluctuations in individuals' mental state transitions.\nIn addition, existing work ignores protective factors that play a crucial role\nin suicide risk prediction, focusing predominantly on risk factors alone.\nProtective factors such as social support and coping strategies can mitigate\nsuicide risk by moderating the impact of risk factors. Therefore, this study\nproposes a novel framework for predicting subsequent suicide risk by jointly\nlearning the dynamic influence of both risk factors and protective factors on\nusers' suicide risk transitions. We propose a novel Protective Factor-Aware\nDataset, which is built from 12 years of Reddit posts along with comprehensive\nannotations of suicide risk and both risk and protective factors. We also\nintroduce a Dynamic Factors Influence Learning approach that captures the\nvarying impact of risk and protective factors on suicide risk transitions,\nrecognizing that suicide risk fluctuates over time according to established\npsychological theories. Our thorough experiments demonstrate that the proposed\nmodel significantly outperforms state-of-the-art models and large language\nmodels across three datasets. In addition, the proposed Dynamic Factors\nInfluence Learning provides interpretable weights, helping clinicians better\nunderstand suicidal patterns and enabling more targeted intervention\nstrategies.",
    "published": "2025-07-14T07:41:54Z",
    "updated": "2025-07-14T07:41:54Z",
    "id": "2507.10008v1",
    "authors": [
      "Jun Li",
      "Xiangmeng Wang",
      "Haoyang Li",
      "Yifei Yan",
      "Hong Va Leong",
      "Ling Feng",
      "Nancy Xiaonan Yu",
      "Qing Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10008v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10008v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10008v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on predicting suicide risk on social media by analyzing dynamic influences of risk and protective factors, which does not directly align with the provided topics related to LLMs, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.09990v1": {
    "title": "Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix",
    "summary": "Large language models (LLMs) typically require fine-tuning for\ndomain-specific tasks, and LoRA offers a computationally efficient approach by\ntraining low-rank adapters. LoRA is also communication-efficient for federated\nLLMs when multiple users collaboratively fine-tune a global LLM model without\nsharing their proprietary raw data. However, even the transmission of local\nadapters between a server and clients risks serious privacy leakage. Applying\ndifferential privacy (DP) to federated LoRA encounters a dilemma: adding noise\nto both adapters amplifies synthetic noise on the model, while fixing one\nadapter impairs the learnability of fine-tuning. In this paper, we propose\nFedASK (Differentially Private Federated Low Rank Adaptation with Double\nSketching) , a novel federated LoRA framework to enable effective updating of\nboth low-rank adapters with robust differential privacy. Inspired by randomized\nSVD, our key idea is a two-stage sketching pipeline. This pipeline first\naggregates carefully sketched, privacy-preserving local updates, and then\nreconstructs the global matrices on the server to facilitate effective updating\nof both adapters. We theoretically prove FedASK's differential privacy\nguarantee and its exact aggregation property. Comprehensive experiments\ndemonstrate that FedASK consistently outperforms baseline methods across a\nvariety of privacy settings and data distributions.",
    "published": "2025-07-14T07:17:24Z",
    "updated": "2025-07-14T07:17:24Z",
    "id": "2507.09990v1",
    "authors": [
      "Ming Wen",
      "Jiaqi Zhu",
      "Yuedong Xu",
      "Yipeng Zhou",
      "Dingding Han"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09990v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09990v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09990v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) and their fine-tuning using LoRA in a federated learning context, which involves privacy concerns and differential privacy. The core topics are related to LLMs and their adaptation in federated settings, with a focus on privacy.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.09965v1": {
    "title": "AnalogTester: A Large Language Model-Based Framework for Automatic\n  Testbench Generation in Analog Circuit Design",
    "summary": "Recent advancements have demonstrated the significant potential of large\nlanguage models (LLMs) in analog circuit design. Nevertheless, testbench\nconstruction for analog circuits remains manual, creating a critical bottleneck\nin achieving fully automated design processes. Particularly when replicating\ncircuit designs from academic papers, manual Testbench construction demands\ntime-intensive implementation and frequent adjustments, which fails to address\nthe dynamic diversity and flexibility requirements for automation. AnalogTester\ntackles automated analog design challenges through an LLM-powered pipeline: a)\ndomain-knowledge integration, b) paper information extraction, c) simulation\nscheme synthesis, and d) testbench code generation with Tsinghua Electronic\nDesign (TED). AnalogTester has demonstrated automated Testbench generation\ncapabilities for three fundamental analog circuit types: operational amplifiers\n(op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while\nmaintaining a scalable framework for adaptation to broader circuit topologies.\nFurthermore, AnalogTester can generate circuit knowledge data and TED code\ncorpus, establishing fundamental training datasets for LLM specialization in\nanalog circuit design automation.",
    "published": "2025-07-14T06:32:23Z",
    "updated": "2025-07-14T06:32:23Z",
    "id": "2507.09965v1",
    "authors": [
      "Weiyu Chen",
      "Chengjie Liu",
      "Wenhao Huang",
      "Jinyang Lyu",
      "Mingqian Yang",
      "Yuan Du",
      "Li Du",
      "Jun Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09965v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09965v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09965v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in analog circuit design, specifically for automated testbench generation. It highlights the use of LLMs for domain-knowledge integration, information extraction, and code generation, which aligns with the LLM topic. Additionally, the creation of training datasets for LLM specialization in analog circuit design suggests a focus on dataset development for LLMs.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.09950v1": {
    "title": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion\n  Product Attributes? A Zero-Shot Analysis",
    "summary": "The fashion retail business is centered around the capacity to comprehend\nproducts. Product attribution helps in comprehending products depending on the\nbusiness process. Quality attribution improves the customer experience as they\nnavigate through millions of products offered by a retail website. It leads to\nwell-organized product catalogs. In the end, product attribution directly\nimpacts the 'discovery experience' of the customer. Although large language\nmodels (LLMs) have shown remarkable capabilities in understanding multimodal\ndata, their performance on fine-grained fashion attribute recognition remains\nunder-explored. This paper presents a zero-shot evaluation of state-of-the-art\nLLMs that balance performance with speed and cost efficiency, mainly\nGPT-4o-mini and Gemini 2.0 Flash. We have used the dataset\nDeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to\nevaluate these models in the attribution tasks of fashion products. Our study\nevaluates these models across 18 categories of fashion attributes, offering\ninsight into where these models excel. We only use images as the sole input for\nproduct information to create a constrained environment. Our analysis shows\nthat Gemini 2.0 Flash demonstrates the strongest overall performance with a\nmacro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a\nmacro F1 score of 43.28%. Through detailed error analysis, our findings provide\npractical insights for deploying these LLMs in production e-commerce product\nattribution-related tasks and highlight the need for domain-specific\nfine-tuning approaches. This work also lays the groundwork for future research\nin fashion AI and multimodal attribute extraction.",
    "published": "2025-07-14T05:59:50Z",
    "updated": "2025-07-14T05:59:50Z",
    "id": "2507.09950v1",
    "authors": [
      "Shubham Shukla",
      "Kunal Sonalkar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09950v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09950v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09950v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper evaluates the performance of large language models (LLMs) like GPT-4o-mini and Gemini 2.0 Flash in fine-grained fashion attribute recognition, which involves multimodal understanding (images and text). It also discusses the use of a dataset for benchmarking these models.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.09948v2": {
    "title": "Iceberg: Enhancing HLS Modeling with Synthetic Data",
    "summary": "Deep learning-based prediction models for High-Level Synthesis (HLS) of\nhardware designs often struggle to generalize. In this paper, we study how to\nclose the generalizability gap of these models through pretraining on synthetic\ndata and introduce Iceberg, a synthetic data augmentation approach that expands\nboth large language model (LLM)-generated programs and weak labels of unseen\ndesign configurations. Our weak label generation method is integrated with an\nin-context model architecture, enabling meta-learning from actual and proximate\nlabels. Iceberg improves the geometric mean modeling accuracy by $86.4\\%$ when\nadapt to six real-world applications with few-shot examples and achieves a\n$2.47\\times$ and a $1.12\\times$ better offline DSE performance when adapting to\ntwo different test datasets. Our open-sourced code is here:\nhttps://github.com/UCLA-VAST/iceberg",
    "published": "2025-07-14T05:48:09Z",
    "updated": "2025-07-19T21:32:24Z",
    "id": "2507.09948v2",
    "authors": [
      "Zijian Ding",
      "Tung Nguyen",
      "Weikai Li",
      "Aditya Grover",
      "Yizhou Sun",
      "Jason Cong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09948v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09948v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09948v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of synthetic data augmentation and pretraining strategies to improve the generalizability of deep learning models for High-Level Synthesis (HLS), which involves large language models (LLMs) and pretraining techniques.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.09942v1": {
    "title": "Green-LLM: Optimal Workload Allocation for Environmentally-Aware\n  Distributed Inference",
    "summary": "This letter investigates the optimal allocation of large language model (LLM)\ninference workloads across heterogeneous edge data centers (DCs) over time.\nEach DC features on-site renewable generation and faces dynamic electricity\nprices and spatiotemporal variability in renewable availability. The central\nquestion is: how can inference workloads be optimally distributed to the DCs to\nminimize energy consumption, carbon emissions, and water usage while enhancing\nuser experience? This letter proposes a novel optimization model for LLM\nservice providers to reduce operational costs and environmental impacts.\nNumerical results validate the efficacy of the proposed approach.",
    "published": "2025-07-14T05:32:32Z",
    "updated": "2025-07-14T05:32:32Z",
    "id": "2507.09942v1",
    "authors": [
      "Jiaming Cheng",
      "Duong Tung Nguyen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09942v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09942v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09942v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing the allocation of large language model (LLM) inference workloads across edge data centers to minimize environmental impact and operational costs. The core topic is related to LLM (Large Language Models) and their operational efficiency, but it does not directly align with the other provided topics.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.09937v1": {
    "title": "Memorization Sinks: Isolating Memorization during LLM Training",
    "summary": "Large language models are susceptible to memorizing repeated sequences,\nposing privacy and copyright concerns. A popular mitigation strategy is to\nremove memorized information from specific neurons post-hoc. However, such\napproaches have shown limited success so far. In a controlled setting, we show\nthat the memorization of natural sequences (those that resemble linguistically\nplausible text) become mechanistically entangled with general language\nabilities, thereby becoming challenging to remove post-hoc. In this work, we\nput forward a new paradigm of MemSinks that promotes isolation of memorization\nby design. We leverage a sequence identifier that activates a unique set of\nmemorization neurons for each sequence across repetitions. By analyzing the\ndynamics of learning and forgetting, we argue that MemSinks facilitates\nisolation of memorized content, making it easier to remove without compromising\ngeneral language capabilities. We implement MemSinks at the billion-parameter\nand billion-token scale, and observe both effective isolation and strong\ngeneralization. To our knowledge, this is the first proof-of-concept on real\ndata demonstrating that simultaneous generalization and isolation is\nachievable. We open-source our code at http://github.com/grghosal/MemSinks.",
    "published": "2025-07-14T05:23:27Z",
    "updated": "2025-07-14T05:23:27Z",
    "id": "2507.09937v1",
    "authors": [
      "Gaurav R. Ghosal",
      "Pratyush Maini",
      "Aditi Raghunathan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09937v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09937v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09937v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses memorization in large language models and proposes a method to isolate memorization during training, which is relevant to the topics of Memory and LLM.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.09935v1": {
    "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text\n  Segmentation Chunking",
    "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.",
    "published": "2025-07-14T05:21:58Z",
    "updated": "2025-07-14T05:21:58Z",
    "id": "2507.09935v1",
    "authors": [
      "Hai Toan Nguyen",
      "Tien Dat Nguyen",
      "Viet Ha Nguyen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09935v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09935v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09935v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing Retrieval-Augmented Generation (RAG) systems, which are closely related to memory-augmented models and retrieval-based methods. It also involves the use of large language models (LLMs) for accessing external knowledge.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.09931v1": {
    "title": "Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear\n  Reactor Safety Applications",
    "summary": "The integration of Large Language Models (LLMs) into safety-critical domains,\nsuch as nuclear engineering, necessitates a deep understanding of their\ninternal reasoning processes. This paper presents a novel methodology for\ninterpreting how an LLM encodes and utilizes domain-specific knowledge, using a\nBoiling Water Reactor system as a case study. We adapted a general-purpose LLM\n(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning\ntechnique known as Low-Rank Adaptation. By comparing the neuron activation\npatterns of the base model to those of the fine-tuned model, we identified a\nsparse set of neurons whose behavior was significantly altered during the\nadaptation process. To probe the causal role of these specialized neurons, we\nemployed a neuron silencing technique. Our results demonstrate that while\nsilencing most of these specialized neurons individually did not produce a\nstatistically significant effect, deactivating the entire group collectively\nled to a statistically significant degradation in task performance. Qualitative\nanalysis further revealed that silencing these neurons impaired the model's\nability to generate detailed, contextually accurate technical information. This\npaper provides a concrete methodology for enhancing the transparency of an\nopaque black-box model, allowing domain expertise to be traced to verifiable\nneural circuits. This offers a pathway towards achieving nuclear-grade\nartificial intelligence (AI) assurance, addressing the verification and\nvalidation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR\n50 Appendix B), which have limited AI deployment in safety-critical nuclear\noperations.",
    "published": "2025-07-14T05:17:41Z",
    "updated": "2025-07-14T05:17:41Z",
    "id": "2507.09931v1",
    "authors": [
      "Yoon Pyo Lee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09931v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09931v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09931v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the mechanistic interpretability of Large Language Models (LLMs) adapted for a specific domain (nuclear reactor safety) using Low-Rank Adaptation (LoRA). It involves understanding the internal reasoning processes of LLMs and their adaptation to domain-specific knowledge, which aligns with research on LLMs and their interpretability.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.10624v1": {
    "title": "Comprehension Without Competence: Architectural Limits of LLMs in\n  Symbolic Computation and Reasoning",
    "summary": "Large Language Models (LLMs) display striking surface fluency yet\nsystematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,\nand logical consistency. This paper offers a structural diagnosis of such\nfailures, revealing a persistent gap between \\textit{comprehension} and\n\\textit{competence}. Through controlled experiments and architectural analysis,\nwe demonstrate that LLMs often articulate correct principles without reliably\napplying them--a failure rooted not in knowledge access, but in computational\nexecution. We term this phenomenon the computational \\textit{split-brain\nsyndrome}, where instruction and action pathways are geometrically and\nfunctionally dissociated. This core limitation recurs across domains, from\nmathematical operations to relational inferences, and explains why model\nbehavior remains brittle even under idealized prompting. We argue that LLMs\nfunction as powerful pattern completion engines, but lack the architectural\nscaffolding for principled, compositional reasoning. Our findings delineate the\nboundary of current LLM capabilities and motivate future models with\nmetacognitive control, principle lifting, and structurally grounded execution.\nThis diagnosis also clarifies why mechanistic interpretability findings may\nreflect training-specific pattern coordination rather than universal\ncomputational principles, and why the geometric separation between instruction\nand execution pathways suggests limitations in neural introspection and\nmechanistic analysis.",
    "published": "2025-07-14T04:01:45Z",
    "updated": "2025-07-14T04:01:45Z",
    "id": "2507.10624v1",
    "authors": [
      "Zheng Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10624v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10624v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10624v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the limitations of Large Language Models (LLMs) in symbolic reasoning and computational execution, which directly relates to the topics of 'LLM' and 'Reasoning'. It also touches on the architectural analysis of LLMs, which is relevant to 'Scaling' as it pertains to model capabilities and limitations.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Scaling"
    ]
  },
  "2507.09875v1": {
    "title": "Function Induction and Task Generalization: An Interpretability Study\n  with Off-by-One Addition",
    "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.",
    "published": "2025-07-14T03:20:55Z",
    "updated": "2025-07-14T03:20:55Z",
    "id": "2507.09875v1",
    "authors": [
      "Qinyuan Ye",
      "Robin Jia",
      "Xiang Ren"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09875v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09875v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09875v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the interpretability of large language models (LLMs) and their ability to generalize tasks through in-context learning, which aligns with research on LLM reasoning and internal mechanisms.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.14189v1": {
    "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On\n  Offline Knowledge Base",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality.",
    "published": "2025-07-14T02:13:22Z",
    "updated": "2025-07-14T02:13:22Z",
    "id": "2507.14189v1",
    "authors": [
      "Song Mao",
      "Lejun Cheng",
      "Pinlong Cai",
      "Guohang Yan",
      "Ding Wang",
      "Botian Shi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14189v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14189v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14189v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses a multimodal writing assistant (DeepWriter) that leverages a curated offline knowledge base and involves task decomposition, outline generation, multimodal retrieval, and section-by-section composition. It focuses on improving factual accuracy and content quality in specialized domains, which aligns with topics related to Multimodal Large Language Models (MLLM) and Memory-augmented models.",
    "llm_cls_result": [
      "MLLM",
      "Memory"
    ]
  },
  "2507.09861v1": {
    "title": "A Survey on MLLM-based Visually Rich Document Understanding: Methods,\n  Challenges, and Emerging Trends",
    "summary": "Visually-Rich Document Understanding (VRDU) has emerged as a critical field,\ndriven by the need to automatically process documents containing complex\nvisual, textual, and layout information. Recently, Multimodal Large Language\nModels (MLLMs) have shown remarkable potential in this domain, leveraging both\nOptical Character Recognition (OCR)-dependent and OCR-free frameworks to\nextract and interpret information in document images. This survey reviews\nrecent advancements in MLLM-based VRDU, highlighting three core components: (1)\nmethods for encoding and fusing textual, visual, and layout features; (2)\ntraining paradigms, including pretraining strategies, instruction-response\ntuning, and the trainability of different model modules; and (3) datasets\nutilized for pretraining, instruction-tuning, and supervised fine-tuning.\nFinally, we discuss the challenges and opportunities in this evolving field and\npropose future directions to advance the efficiency, generalizability, and\nrobustness of VRDU systems.",
    "published": "2025-07-14T02:10:31Z",
    "updated": "2025-07-14T02:10:31Z",
    "id": "2507.09861v1",
    "authors": [
      "Yihao Ding",
      "Siwen Luo",
      "Yue Dai",
      "Yanbei Jiang",
      "Zechuan Li",
      "Geoffrey Martin",
      "Yifan Peng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09861v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09861v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09861v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Multimodal Large Language Models (MLLMs) and their application in Visually-Rich Document Understanding (VRDU), which involves integrating vision, language, and layout information. It also discusses pretraining strategies and datasets, which are key components of MLLM research.",
    "llm_cls_result": [
      "MLLM",
      "Pretrain",
      "Dataset"
    ]
  },
  "2507.09854v1": {
    "title": "Model-Grounded Symbolic Artificial Intelligence Systems Learning and\n  Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems",
    "summary": "Neurosymbolic artificial intelligence (AI) systems combine neural network and\nclassical symbolic AI mechanisms to exploit the complementary strengths of\nlarge scale, generalizable learning and robust, verifiable reasoning. Numerous\nclassifications of neurosymbolic AI illustrate how these two components can be\nintegrated in distinctly different ways. In this work, we propose\nreinterpreting instruction tuned large language models as model grounded\nsymbolic AI systems where natural language serves as the symbolic layer and\ngrounding is achieved through the models internal representation space. Within\nthis framework, we investigate and develop novel learning and reasoning\napproaches that preserve structural similarities to traditional learning and\nreasoning paradigms. Preliminary evaluations across axiomatic deductive\nreasoning procedures of varying complexity provide insights into the\neffectiveness of our approach in improving learning efficiency and reasoning\nreliability.",
    "published": "2025-07-14T01:34:05Z",
    "updated": "2025-07-14T01:34:05Z",
    "id": "2507.09854v1",
    "authors": [
      "Aniruddha Chattopadhyay",
      "Raj Dandekar",
      "Kaushik Roy"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09854v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09854v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09854v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of neural networks and symbolic AI mechanisms, particularly focusing on instruction-tuned large language models as model-grounded symbolic AI systems. It emphasizes learning and reasoning approaches, which aligns with the topics of Reasoning and LLM.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2507.10621v1": {
    "title": "Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the\n  Age of Intelligent Threats",
    "summary": "Protecting cyberspace requires not only advanced tools but also a shift in\nhow we reason about threats, trust, and autonomy. Traditional cybersecurity\nmethods rely on manual responses and brittle heuristics. To build proactive and\nintelligent defense systems, we need integrated theoretical frameworks and\nsoftware tools. Game theory provides a rigorous foundation for modeling\nadversarial behavior, designing strategic defenses, and enabling trust in\nautonomous systems. Meanwhile, software tools process cyber data, visualize\nattack surfaces, verify compliance, and suggest mitigations. Yet a disconnect\nremains between theory and practical implementation.\n  The rise of Large Language Models (LLMs) and agentic AI offers a new path to\nbridge this gap. LLM-powered agents can operationalize abstract strategies into\nreal-world decisions. Conversely, game theory can inform the reasoning and\ncoordination of these agents across complex workflows. LLMs also challenge\nclassical game-theoretic assumptions, such as perfect rationality or static\npayoffs, prompting new models aligned with cognitive and computational\nrealities. This co-evolution promises richer theoretical foundations and novel\nsolution concepts. Agentic AI also reshapes software design: systems must now\nbe modular, adaptive, and trust-aware from the outset.\n  This chapter explores the intersection of game theory, agentic AI, and\ncybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic,\nBayesian, and signaling games) and solution concepts. We then examine how LLM\nagents can enhance cyber defense and introduce LLM-driven games that embed\nreasoning into AI agents. Finally, we explore multi-agent workflows and\ncoordination games, outlining how this convergence fosters secure, intelligent,\nand adaptive cyber systems.",
    "published": "2025-07-14T00:49:44Z",
    "updated": "2025-07-14T00:49:44Z",
    "id": "2507.10621v1",
    "authors": [
      "Quanyan Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10621v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10621v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10621v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) and agentic AI with game theory for cybersecurity, focusing on how LLMs can operationalize abstract strategies and enhance cyber defense. This aligns with the topics of LLM (research on Large Language Models) and RL (Reinforcement Learning, as it involves agentic AI and coordination games).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.09839v1": {
    "title": "Rethinking Prompt Optimization: Reinforcement, Diversification, and\n  Migration in Blackbox LLMs",
    "summary": "An increasing number of NLP applications interact with large language models\n(LLMs) through black-box APIs, making prompt engineering critical for\ncontrolling model outputs. While recent Automatic Prompt Optimization (APO)\nmethods iteratively refine prompts using model-generated feedback, textual\ngradients, they primarily focus on error correction and neglect valuable\ninsights from correct predictions. This limits both their effectiveness and\nefficiency. In this paper, we propose a novel APO framework centered on\nenhancing the feedback mechanism. We reinterpret the textual gradient as a form\nof negative reinforcement and introduce the complementary positive\nreinforcement to explicitly preserve beneficial prompt components identified\nthrough successful predictions. To mitigate the noise inherent in LLM-generated\nfeedback, we introduce a technique called feedback diversification, which\naggregates multiple feedback signals, emphasizing consistent, actionable advice\nwhile filtering out outliers. Motivated by the rapid evolution and diversity of\navailable LLMs, we also formalize Continual Prompt Optimization (CPO),\naddressing the practical challenge of efficiently migrating optimized prompts\nbetween different model versions or API providers. Our experiments reveal that\nnaive prompt migration often degrades performance due to loss of critical\ninstructions. In contrast, our approach consistently outperforms strong\nbaselines, achieving significant accuracy improvements, faster convergence, and\nlower computational costs in both standard and migration scenarios.",
    "published": "2025-07-14T00:20:14Z",
    "updated": "2025-07-14T00:20:14Z",
    "id": "2507.09839v1",
    "authors": [
      "MohammadReza Davari",
      "Utkarsh Garg",
      "Weixin Cai",
      "Eugene Belilovsky"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09839v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09839v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09839v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing prompts for black-box LLMs using reinforcement techniques and feedback mechanisms, which are central to improving LLM interactions and performance.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Reasoning"
    ]
  },
  "2507.09834v1": {
    "title": "Generative Audio Language Modeling with Continuous-valued Tokens and\n  Masked Next-Token Prediction",
    "summary": "Autoregressive next-token prediction with the Transformer decoder has become\na de facto standard in large language models (LLMs), achieving remarkable\nsuccess in Natural Language Processing (NLP) at scale. Extending this paradigm\nto audio poses unique challenges due to its inherently continuous nature. We\nresearch audio generation with a causal language model (LM) without discrete\ntokens. We leverage token-wise diffusion to model the continuous distribution\nof the next continuous-valued token. Our approach delivers significant\nimprovements over previous discrete solution, AudioGen, achieving 20% and 40%\nrelative gains on AudioCaps in Frechet Audio Distance (FAD) and\nKullback-Leibler (KL) divergence, respectively. Additionally, we propose a\nnovel masked next-token prediction task that incorporates masked prediction\ninto the causal LM framework. On AudioCaps, the innovation yields 41% and 33%\nrelative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B)\nmodels, respectively, and is on par with the state-of-the-art (SOTA) diffusion\nmodels. Furthermore, we achieve these results with significantly fewer\nparameters -- 193M for our Base and 462M for our Large models.",
    "published": "2025-07-14T00:14:54Z",
    "updated": "2025-07-14T00:14:54Z",
    "id": "2507.09834v1",
    "authors": [
      "Shu-wen Yang",
      "Byeonggeun Kim",
      "Kuan-Po Huang",
      "Qingming Tang",
      "Huy Phan",
      "Bo-Ru Lu",
      "Harsha Sundar",
      "Shalini Ghosh",
      "Hung-yi Lee",
      "Chieh-Chi Kao",
      "Chao Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09834v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09834v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09834v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the extension of autoregressive language models to audio generation, which involves continuous-valued tokens and masked next-token prediction. While it mentions LLMs, the focus is on audio generation and not on the core topics of LLM research, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.10620v1": {
    "title": "LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions",
    "summary": "Large Language Models (LLMs) have emerged as a promising paradigm for time\nseries analytics, leveraging their massive parameters and the shared sequential\nnature of textual and time series data. However, a cross-modality gap exists\nbetween time series and textual data, as LLMs are pre-trained on textual\ncorpora and are not inherently optimized for time series. In this tutorial, we\nprovide an up-to-date overview of LLM-based cross-modal time series analytics.\nWe introduce a taxonomy that classifies existing approaches into three groups\nbased on cross-modal modeling strategies, e.g., conversion, alignment, and\nfusion, and then discuss their applications across a range of downstream tasks.\nIn addition, we summarize several open challenges. This tutorial aims to expand\nthe practical application of LLMs in solving real-world problems in cross-modal\ntime series analytics while balancing effectiveness and efficiency.\nParticipants will gain a thorough understanding of current advancements,\nmethodologies, and future research directions in cross-modal time series\nanalytics.",
    "published": "2025-07-13T23:47:32Z",
    "updated": "2025-07-13T23:47:32Z",
    "id": "2507.10620v1",
    "authors": [
      "Chenxi Liu",
      "Hao Miao",
      "Cheng Long",
      "Yan Zhao",
      "Ziyue Li",
      "Panos Kalnis"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10620v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10620v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10620v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in cross-modal time series analytics, which involves integrating time series data with textual data. This aligns with the topics of LLM (Large Language Models) and MLLM (Multimodal Large Language Models) due to the cross-modal nature of the research.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.09820v1": {
    "title": "Measuring What Matters: A Framework for Evaluating Safety Risks in\n  Real-World LLM Applications",
    "summary": "Most safety testing efforts for large language models (LLMs) today focus on\nevaluating foundation models. However, there is a growing need to evaluate\nsafety at the application level, as components such as system prompts,\nretrieval pipelines, and guardrails introduce additional factors that\nsignificantly influence the overall safety of LLM applications. In this paper,\nwe introduce a practical framework for evaluating application-level safety in\nLLM systems, validated through real-world deployment across multiple use cases\nwithin our organization. The framework consists of two parts: (1) principles\nfor developing customized safety risk taxonomies, and (2) practices for\nevaluating safety risks in LLM applications. We illustrate how the proposed\nframework was applied in our internal pilot, providing a reference point for\norganizations seeking to scale their safety testing efforts. This work aims to\nbridge the gap between theoretical concepts in AI safety and the operational\nrealities of safeguarding LLM applications in practice, offering actionable\nguidance for safe and scalable deployment.",
    "published": "2025-07-13T22:34:20Z",
    "updated": "2025-07-13T22:34:20Z",
    "id": "2507.09820v1",
    "authors": [
      "Jia Yi Goh",
      "Shaun Khoo",
      "Nyx Iskandar",
      "Gabriel Chua",
      "Leanne Tan",
      "Jessica Foo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09820v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09820v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09820v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating safety risks in real-world LLM applications, which involves benchmarking and safety testing of LLMs in practical scenarios.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.10618v1": {
    "title": "Compute Requirements for Algorithmic Innovation in Frontier AI Models",
    "summary": "Algorithmic innovation in the pretraining of large language models has driven\na massive reduction in the total compute required to reach a given level of\ncapability. In this paper we empirically investigate the compute requirements\nfor developing algorithmic innovations. We catalog 36 pre-training algorithmic\ninnovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate\nboth the total FLOP used in development and the FLOP/s of the hardware\nutilized. Innovations using significant resources double in their requirements\neach year. We then use this dataset to investigate the effect of compute caps\non innovation. Our analysis suggests that compute caps alone are unlikely to\ndramatically slow AI algorithmic progress. Even stringent compute caps -- such\nas capping total operations to the compute used to train GPT-2 or capping\nhardware capacity to 8 H100 GPUs -- could still have allowed for half of the\ncataloged innovations.",
    "published": "2025-07-13T21:28:02Z",
    "updated": "2025-07-13T21:28:02Z",
    "id": "2507.10618v1",
    "authors": [
      "Peter Barnett"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10618v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10618v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10618v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the compute requirements for algorithmic innovations in pretraining large language models, which aligns with the topics of 'Pretrain' and 'Scaling'. It also touches on the broader implications for AI progress, which could be related to 'AGI'.",
    "llm_cls_result": [
      "Pretrain",
      "Scaling",
      "AGI"
    ]
  },
  "2507.09797v1": {
    "title": "A Scalable and Efficient Signal Integration System for Job Matching",
    "summary": "LinkedIn, one of the world's largest platforms for professional networking\nand job seeking, encounters various modeling challenges in building\nrecommendation systems for its job matching product, including cold-start,\nfilter bubbles, and biases affecting candidate-job matching. To address these,\nwe developed the STAR (Signal Integration for Talent And Recruiters) system,\nleveraging the combined strengths of Large Language Models (LLMs) and Graph\nNeural Networks (GNNs). LLMs excel at understanding textual data, such as\nmember profiles and job postings, while GNNs capture intricate relationships\nand mitigate cold-start issues through network effects. STAR integrates diverse\nsignals by uniting LLM and GNN capabilities with industrial-scale paradigms\nincluding adaptive sampling and version management. It provides an end-to-end\nsolution for developing and deploying embeddings in large-scale recommender\nsystems. Our key contributions include a robust methodology for building\nembeddings in industrial applications, a scalable GNN-LLM integration for\nhigh-performing recommendations, and practical insights for real-world model\ndeployment.",
    "published": "2025-07-13T21:23:37Z",
    "updated": "2025-07-13T21:23:37Z",
    "id": "2507.09797v1",
    "authors": [
      "Ping Liu",
      "Rajat Arora",
      "Xiao Shi",
      "Benjamin Le",
      "Qianqi Shen",
      "Jianqiang Shen",
      "Chengming Jiang",
      "Nikita Zhiltsov",
      "Priya Bannur",
      "Yidan Zhu",
      "Liming Dong",
      "Haichao Wei",
      "Qi Guo",
      "Luke Simon",
      "Liangjie Hong",
      "Wenjing Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09797v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09797v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09797v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) and Graph Neural Networks (GNNs) for job matching, which involves the use of LLMs for understanding textual data and GNNs for capturing relationships. This falls under the LLM and RL categories as it involves the application of LLMs in a recommendation system, which can be seen as a form of reinforcement learning in the context of job matching.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.09792v1": {
    "title": "CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD\n  Design",
    "summary": "Computer-aided design (CAD) is the digital construction of 2D and 3D objects,\nand is central to a wide range of engineering and manufacturing applications\nlike automobile and aviation. Despite its importance, CAD modeling remains\nlargely a time-intensive, manual task. Recent works have attempted to automate\nthis process with small transformer-based models and handcrafted CAD sequence\nrepresentations. However, there has been little effort to leverage the\npotential of large language models (LLMs) for sequential CAD design. In this\nwork, we introduce a new large-scale dataset of more than 170k CAD models\nannotated with high-quality, human-like descriptions generated with our\npipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs\nto generate CAD sequences represented in a JSON-based format from natural\nlanguage descriptions, demonstrating the viability and effectiveness of this\napproach for text-conditioned CAD generation. Because simple metrics often fail\nto reflect the quality of generated objects, we introduce geometric and\ntopological metrics based on sphericity, mean curvature, and Euler\ncharacteristic to provide richer structural insights. Our experiments and\nablation studies on both synthetic and human-annotated data demonstrate that\nCADmium is able to automate CAD design, drastically speeding up the design of\nnew objects. The dataset, code, and fine-tuned models are available online.",
    "published": "2025-07-13T21:11:53Z",
    "updated": "2025-07-13T21:11:53Z",
    "id": "2507.09792v1",
    "authors": [
      "Prashant Govindarajan",
      "Davide Baldelli",
      "Jay Pathak",
      "Quentin Fournier",
      "Sarath Chandar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09792v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09792v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09792v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for sequential CAD design, which involves fine-tuning code-LLMs and leveraging a new large-scale dataset. The focus on LLMs and their application in a specific domain (CAD design) aligns with the LLM topic. The creation and use of a new dataset also relates to the Dataset topic.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.09790v1": {
    "title": "Prompting for Performance: Exploring LLMs for Configuring Software",
    "summary": "Software systems usually provide numerous configuration options that can\naffect performance metrics such as execution time, memory usage, binary size,\nor bitrate. On the one hand, making informed decisions is challenging and\nrequires domain expertise in options and their combinations. On the other hand,\nmachine learning techniques can search vast configuration spaces, but with a\nhigh computational cost, since concrete executions of numerous configurations\nare required. In this exploratory study, we investigate whether large language\nmodels (LLMs) can assist in performance-oriented software configuration through\nprompts. We evaluate several LLMs on tasks including identifying relevant\noptions, ranking configurations, and recommending performant configurations\nacross various configurable systems, such as compilers, video encoders, and SAT\nsolvers. Our preliminary results reveal both positive abilities and notable\nlimitations: depending on the task and systems, LLMs can well align with expert\nknowledge, whereas hallucinations or superficial reasoning can emerge in other\ncases. These findings represent a first step toward systematic evaluations and\nthe design of LLM-based solutions to assist with software configuration.",
    "published": "2025-07-13T21:05:01Z",
    "updated": "2025-07-13T21:05:01Z",
    "id": "2507.09790v1",
    "authors": [
      "Helge Spieker",
      "Tho Matricon",
      "Nassim Belmecheri",
      "Jrn Eirik Betten",
      "Gauthier Le Bartz Lyan",
      "Heraldo Borges",
      "Quentin Mazouni",
      "Dennis Gross",
      "Arnaud Gotlieb",
      "Mathieu Acher"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09790v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09790v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09790v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper explores the use of Large Language Models (LLMs) for configuring software systems, which directly relates to research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.09788v1": {
    "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit",
    "summary": "Recent advances in Large Language Models (LLM) have led to a new class of\nautonomous agents, renewing and expanding interest in the area. LLM-powered\nMultiagent Systems (MAS) have thus emerged, both for assistive and simulation\npurposes, yet tools for realistic human behavior simulation -- with its\ndistinctive challenges and opportunities -- remain underdeveloped. Existing MAS\nlibraries and tools lack fine-grained persona specifications, population\nsampling facilities, experimentation support, and integrated validation, among\nother key capabilities, limiting their utility for behavioral studies, social\nsimulation, and related applications. To address these deficiencies, in this\nwork we introduce TinyTroupe, a simulation toolkit enabling detailed persona\ndefinitions (e.g., nationality, age, occupation, personality, beliefs,\nbehaviors) and programmatic control via numerous LLM-driven mechanisms. This\nallows for the concise formulation of behavioral problems of practical\ninterest, either at the individual or group level, and provides effective means\nfor their solution. TinyTroupe's components are presented using representative\nworking examples, such as brainstorming and market research sessions, thereby\nsimultaneously clarifying their purpose and demonstrating their usefulness.\nQuantitative and qualitative evaluations of selected aspects are also provided,\nhighlighting possibilities, limitations, and trade-offs. The approach, though\nrealized as a specific Python implementation, is meant as a novel conceptual\ncontribution, which can be partially or fully incorporated in other contexts.\nThe library is available as open source at\nhttps://github.com/microsoft/tinytroupe.",
    "published": "2025-07-13T21:00:27Z",
    "updated": "2025-07-13T21:00:27Z",
    "id": "2507.09788v1",
    "authors": [
      "Paulo Salem",
      "Robert Sim",
      "Christopher Olsen",
      "Prerit Saxena",
      "Rafael Barcelos",
      "Yi Ding"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09788v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09788v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09788v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLM) in creating multiagent systems for simulation purposes, which aligns with the topics of LLM and RL (specifically RLHF and agents). The focus on realistic human behavior simulation and persona specifications also touches on the broader theme of AGI, as it involves creating generalist models capable of simulating human-like behavior.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.09747v1": {
    "title": "BrainFLORA: Uncovering Brain Concept Representation via Multimodal\n  Neural Embeddings",
    "summary": "Understanding how the brain represents visual information is a fundamental\nchallenge in neuroscience and artificial intelligence. While AI-driven decoding\nof neural data has provided insights into the human visual system, integrating\nmultimodal neuroimaging signals, such as EEG, MEG, and fMRI, remains a critical\nhurdle due to their inherent spatiotemporal misalignment. Current approaches\noften analyze these modalities in isolation, limiting a holistic view of neural\nrepresentation. In this study, we introduce BrainFLORA, a unified framework for\nintegrating cross-modal neuroimaging data to construct a shared neural\nrepresentation. Our approach leverages multimodal large language models (MLLMs)\naugmented with modality-specific adapters and task decoders, achieving\nstate-of-the-art performance in joint-subject visual retrieval task and has the\npotential to extend multitasking. Combining neuroimaging analysis methods, we\nfurther reveal how visual concept representations align across neural\nmodalities and with real world object perception. We demonstrate that the\nbrain's structured visual concept representations exhibit an implicit mapping\nto physical-world stimuli, bridging neuroscience and machine learning from\ndifferent modalities of neural imaging. Beyond methodological advancements,\nBrainFLORA offers novel implications for cognitive neuroscience and\nbrain-computer interfaces (BCIs). Our code is available at\nhttps://github.com/ncclab-sustech/BrainFLORA.",
    "published": "2025-07-13T18:56:17Z",
    "updated": "2025-07-13T18:56:17Z",
    "id": "2507.09747v1",
    "authors": [
      "Dongyang Li",
      "Haoyang Qin",
      "Mingyang Wu",
      "Chen Wei",
      "Quanying Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09747v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09747v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09747v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of multimodal neuroimaging data using multimodal large language models (MLLMs) to construct a shared neural representation, which aligns with the MLLM topic. It also involves neuroscience and brain-computer interfaces, which are not directly covered by the given topics, but the core methodology is based on MLLMs.",
    "llm_cls_result": [
      "MLLM"
    ]
  },
  "2507.09739v1": {
    "title": "Enhancing Trading Performance Through Sentiment Analysis with Large\n  Language Models: Evidence from the S&P 500",
    "summary": "This study integrates real-time sentiment analysis from financial news, GPT-2\nand FinBERT, with technical indicators and time-series models like ARIMA and\nETS to optimize S&P 500 trading strategies. By merging sentiment data with\nmomentum and trend-based metrics, including a benchmark buy-and-hold and\nsentiment-based approach, is evaluated through assets values and returns.\nResults show that combining sentiment-driven insights with traditional models\nimproves trading performance, offering a more dynamic approach to stock trading\nthat adapts to market changes in volatile environments.",
    "published": "2025-07-13T18:30:57Z",
    "updated": "2025-07-13T18:30:57Z",
    "id": "2507.09739v1",
    "authors": [
      "Haojie Liu",
      "Zihan Lin",
      "Randall R. Rojas"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09739v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09739v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09739v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLM) like GPT-2 for sentiment analysis in financial trading, which aligns with the 'LLM' topic. It also involves the integration of sentiment analysis with traditional models, which is a specific application of LLMs rather than a core research topic in the given list.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.09701v1": {
    "title": "MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of\n  LLMs",
    "summary": "Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding.",
    "published": "2025-07-13T16:24:35Z",
    "updated": "2025-07-13T16:24:35Z",
    "id": "2507.09701v1",
    "authors": [
      "Shulin Huang",
      "Linyi Yang",
      "Yue Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09701v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09701v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09701v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating cultural biases and understanding in large language models (LLMs) across multiple languages and cultures, which aligns with the Benchmark topic as it involves benchmarking LLMs' performance in diverse cultural contexts. It also touches on the broader implications of LLM performance, which is relevant to the LLM topic.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2507.09676v1": {
    "title": "Can AI Rely on the Systematicity of Truth? The Challenge of Modelling\n  Normative Domains",
    "summary": "A key assumption fuelling optimism about the progress of large language\nmodels (LLMs) in accurately and comprehensively modelling the world is that the\ntruth is systematic: true statements about the world form a whole that is not\njust consistent, in that it contains no contradictions, but coherent, in that\nthe truths are inferentially interlinked. This holds out the prospect that LLMs\nmight in principle rely on that systematicity to fill in gaps and correct\ninaccuracies in the training data: consistency and coherence promise to\nfacilitate progress towards comprehensiveness in an LLM's representation of the\nworld. However, philosophers have identified compelling reasons to doubt that\nthe truth is systematic across all domains of thought, arguing that in\nnormative domains, in particular, the truth is largely asystematic. I argue\nthat insofar as the truth in normative domains is asystematic, this renders it\ncorrespondingly harder for LLMs to make progress, because they cannot then\nleverage the systematicity of truth. And the less LLMs can rely on the\nsystematicity of truth, the less we can rely on them to do our practical\ndeliberation for us, because the very asystematicity of normative domains\nrequires human agency to play a greater role in practical thought.",
    "published": "2025-07-13T15:23:31Z",
    "updated": "2025-07-13T15:23:31Z",
    "id": "2507.09676v1",
    "authors": [
      "Matthieu Queloz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09676v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09676v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09676v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the limitations of large language models (LLMs) in modeling normative domains due to the asystematic nature of truth in these areas. It critiques the assumption that truth's systematicity can be leveraged by LLMs for comprehensive world modeling, particularly in normative contexts.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.10614v1": {
    "title": "Fine-tuning Large Language Model for Automated Algorithm Design",
    "summary": "The integration of large language models (LLMs) into automated algorithm\ndesign has shown promising potential. A prevalent approach embeds LLMs within\nsearch routines to iteratively generate and refine candidate algorithms.\nHowever, most existing methods rely on off-the-shelf LLMs trained for general\ncoding tasks,leaving a key question open: Do we need LLMs specifically tailored\nfor algorithm design? If so, how can such LLMs be effectively obtained and how\nwell can they generalize across different algorithm design tasks? In this\npaper, we take a first step toward answering these questions by exploring\nfine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank\nbased (DAR) sampling strategy to balance training data diversity and quality,\nthen we leverage direct preference optimization to efficiently align LLM\noutputs with task objectives. Our experiments, conducted on\nLlama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm\ndesign tasks. Results suggest that finetuned LLMs can significantly outperform\ntheir off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and\nmatch the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,\nwe observe promising generalization: LLMs finetuned on specific algorithm\ndesign tasks also improve performance on related tasks with varying settings.\nThese findings highlight the value of task-specific adaptation for LLMs in\nalgorithm design and open new avenues for future research.",
    "published": "2025-07-13T15:21:23Z",
    "updated": "2025-07-13T15:21:23Z",
    "id": "2507.10614v1",
    "authors": [
      "Fei Liu",
      "Rui Zhang",
      "Xi Lin",
      "Zhichao Lu",
      "Qingfu Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10614v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10614v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10614v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the fine-tuning of large language models (LLMs) for automated algorithm design, which involves adapting LLMs for specific tasks and evaluating their performance. This aligns with the topics of LLM research and their application in specialized tasks.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.10613v1": {
    "title": "Sub-Scaling Laws: On the Role of Data Density and Training Strategies in\n  LLMs",
    "summary": "Traditional scaling laws in natural language processing suggest that\nincreasing model size and training data enhances performance. However, recent\nstudies reveal deviations, particularly in large language models, where\nperformance improvements decelerate, which is a phenomenon known as\nsub-scaling. This paper revisits these scaling laws by examining the impact of\ndata quality and training strategies on model performance. Through extensive\nempirical analysis of over 400 models, we identify high data density and\nnon-optimal resource allocation as key factors contributing to sub-scaling.\nHigh data density leads to diminishing returns due to redundant information,\nwhile optimal resource allocation is crucial for sustained performance\nimprovements. We propose a sub-optimal scaling law that better predicts\nperformance in sub-scaling regimes, highlighting the importance of data quality\nand diversity.",
    "published": "2025-07-13T15:15:24Z",
    "updated": "2025-07-13T15:15:24Z",
    "id": "2507.10613v1",
    "authors": [
      "Zhengyu Chen",
      "Siqi Wang",
      "Teng Xiao",
      "Yudong Wang",
      "Shiqi Chen",
      "Xunliang Cai",
      "Junxian He",
      "Jingang Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10613v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10613v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10613v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses scaling laws in the context of large language models (LLMs), focusing on data quality and training strategies, which are central to the 'Scaling' topic. It also touches on 'LLM' as it specifically addresses performance in large language models.",
    "llm_cls_result": [
      "Scaling",
      "LLM"
    ]
  },
  "2507.09665v1": {
    "title": "Is Quantization a Deal-breaker? Empirical Insights from Large Code\n  Models",
    "summary": "The growing scale of large language models (LLMs) not only demands extensive\ncomputational resources but also raises environmental concerns due to their\nincreasing carbon footprint. Model quantization emerges as an effective\napproach that can reduce the resource demands of LLMs by decreasing parameter\nprecision without substantially affecting performance (e.g., 16 bit to 4 bit).\nWhile recent studies have established quantization as a promising approach for\noptimizing large code models (LCMs), a specialized subset of LLMs tailored for\nautomated software engineering, their findings offer only limited insights into\nits practical implications. Specifically, current investigations focus only on\nthe functional correctness of the code generated by quantized models,\nneglecting how quantization impacts critical aspects of code quality such as\nreliability, maintainability, and security. To bridge this gap, our study\ninvestigates the effects of quantization on the qualitative aspects of\nautomatically generated code. We apply Activation-aware Weight Quantization\n(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate\nJava and Python code. Using state-of-the-art static analysis tools, we evaluate\nsoftware quality metrics and static features including cyclomatic complexity,\ncognitive complexity, and lines of code. Our findings reveal that quantization\nis a robust technique that not only preserves functional correctness, but also\nretains key qualitative code attributes sought after by developers, such as\nmaintainability and structural simplicity.",
    "published": "2025-07-13T14:58:19Z",
    "updated": "2025-07-13T14:58:19Z",
    "id": "2507.09665v1",
    "authors": [
      "Saima Afrin",
      "Bowen Xu",
      "Antonio Mastropaolo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09665v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09665v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09665v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the impact of quantization on large code models (a subset of LLMs), specifically examining how quantization affects code quality metrics beyond functional correctness. This aligns with the 'LLM' topic as it involves large language models and their optimization techniques.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.09657v1": {
    "title": "Negotiating Comfort: Simulating Personality-Driven LLM Agents in Shared\n  Residential Social Networks",
    "summary": "We use generative agents powered by large language models (LLMs) to simulate\na social network in a shared residential building, driving the temperature\ndecisions for a central heating system. Agents, divided into Family Members and\nRepresentatives, consider personal preferences, personal traits, connections,\nand weather conditions. Daily simulations involve family-level consensus\nfollowed by building-wide decisions among representatives. We tested three\npersonality traits distributions (positive, mixed, and negative) and found that\npositive traits correlate with higher happiness and stronger friendships.\nTemperature preferences, assertiveness, and selflessness have a significant\nimpact on happiness and decisions. This work demonstrates how LLM-driven agents\ncan help simulate nuanced human behavior where complex real-life human\nsimulations are difficult to set.",
    "published": "2025-07-13T14:43:45Z",
    "updated": "2025-07-13T14:43:45Z",
    "id": "2507.09657v1",
    "authors": [
      "Ann Nedime Nese Rende",
      "Tolga Yilmaz",
      "zgr Ulusoy"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09657v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09657v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09657v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of LLM-driven agents to simulate human behavior in a social network, focusing on personality traits and decision-making processes. This aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning with Human Feedback), as it involves the simulation of agents with specific traits and behaviors.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.09650v1": {
    "title": "Cultivating Pluralism In Algorithmic Monoculture: The Community\n  Alignment Dataset",
    "summary": "How can large language models (LLMs) serve users with varying preferences\nthat may conflict across cultural, political, or other dimensions? To advance\nthis challenge, this paper establishes four key results. First, we demonstrate,\nthrough a large-scale multilingual human study with representative samples from\nfive countries (N=15,000), that humans exhibit significantly more variation in\npreferences than the responses of 21 state-of-the-art LLMs. Second, we show\nthat existing methods for preference dataset collection are insufficient for\nlearning the diversity of human preferences even along two of the most salient\ndimensions of variability in global values, due to the underlying homogeneity\nof candidate responses. Third, we argue that this motivates the need for\nnegatively-correlated sampling when generating candidate sets, and we show that\nsimple prompt-based techniques for doing so significantly enhance the\nperformance of alignment methods in learning heterogeneous preferences. Fourth,\nbased on this novel candidate sampling approach, we collect and open-source\nCommunity Alignment, the largest and most representative multilingual and\nmulti-turn preference dataset to date, featuring almost 200,000 comparisons\nfrom annotators spanning five countries. We hope that the Community Alignment\ndataset will be a valuable resource for improving the effectiveness of LLMs for\na diverse global population.",
    "published": "2025-07-13T14:34:22Z",
    "updated": "2025-07-13T14:34:22Z",
    "id": "2507.09650v1",
    "authors": [
      "Lily Hong Zhang",
      "Smitha Milli",
      "Karen Jusko",
      "Jonathan Smith",
      "Brandon Amos",
      " Wassim",
      " Bouaziz",
      "Manon Revel",
      "Jack Kussman",
      "Lisa Titus",
      "Bhaktipriya Radharapu",
      "Jane Yu",
      "Vidya Sarma",
      "Kris Rose",
      "Maximilian Nickel"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09650v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09650v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09650v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the alignment of large language models (LLMs) with diverse human preferences, which involves the use of LLMs and their alignment methods. It also introduces a new dataset for this purpose.",
    "llm_cls_result": [
      "LLM",
      "Dataset",
      "RL"
    ]
  },
  "2507.09583v1": {
    "title": "A Serverless Architecture for Real-Time Stock Analysis using Large\n  Language Models: An Iterative Development and Debugging Case Study",
    "summary": "The advent of powerful, accessible Large Language Models (LLMs) like Google's\nGemini presents new opportunities for democratizing financial data analysis.\nThis paper documents the design, implementation, and iterative debugging of a\nnovel, serverless system for real-time stock analysis. The system leverages the\nGemini API for qualitative assessment, automates data ingestion and processing\nvia GitHub Actions, and presents the findings through a decoupled, static\nfrontend. We detail the architectural evolution of the system, from initial\nconcepts to a robust, event-driven pipeline, highlighting the practical\nchallenges encountered during deployment. A significant portion of this paper\nis dedicated to a case study on the debugging process, covering common software\nerrors, platform-specific permission issues, and rare, environment-level\nplatform bugs. The final architecture operates at a near-zero cost,\ndemonstrating a viable model for individuals to build sophisticated AI-powered\nfinancial tools. The operational application is publicly accessible, and the\ncomplete source code is available for review. We conclude by discussing the\nrole of LLMs in financial analysis, the importance of robust debugging\nmethodologies, and the emerging paradigm of human-AI collaboration in software\ndevelopment.",
    "published": "2025-07-13T11:29:51Z",
    "updated": "2025-07-13T11:29:51Z",
    "id": "2507.09583v1",
    "authors": [
      "Taniv Ashraf"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09583v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09583v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09583v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in a serverless architecture for real-time stock analysis, highlighting the use of LLMs like Google's Gemini for financial data analysis. It discusses the design, implementation, and debugging of the system, which is centered around LLM technology.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.09573v1": {
    "title": "WordCraft: Interactive Artistic Typography with Attention Awareness and\n  Noise Blending",
    "summary": "Artistic typography aims to stylize input characters with visual effects that\nare both creative and legible. Traditional approaches rely heavily on manual\ndesign, while recent generative models, particularly diffusion-based methods,\nhave enabled automated character stylization. However, existing solutions\nremain limited in interactivity, lacking support for localized edits, iterative\nrefinement, multi-character composition, and open-ended prompt interpretation.\nWe introduce WordCraft, an interactive artistic typography system that\nintegrates diffusion models to address these limitations. WordCraft features a\ntraining-free regional attention mechanism for precise, multi-region generation\nand a noise blending that supports continuous refinement without compromising\nvisual quality. To support flexible, intent-driven generation, we incorporate a\nlarge language model to parse and structure both concrete and abstract user\nprompts. These components allow our framework to synthesize high-quality,\nstylized typography across single- and multi-character inputs across multiple\nlanguages, supporting diverse user-centered workflows. Our system significantly\nenhances interactivity in artistic typography synthesis, opening up creative\npossibilities for artists and designers.",
    "published": "2025-07-13T10:49:09Z",
    "updated": "2025-07-13T10:49:09Z",
    "id": "2507.09573v1",
    "authors": [
      "Zhe Wang",
      "Jingbo Zhang",
      "Tianyi Wei",
      "Wanchao Su",
      "Can Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09573v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09573v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09573v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on interactive artistic typography using diffusion models and a large language model for prompt interpretation, but it does not directly align with the provided topics related to LLM, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.10610v1": {
    "title": "LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI\n  Agents",
    "summary": "Graphical user interface (GUI) agents built on multimodal large language\nmodels (MLLMs) have recently demonstrated strong decision-making abilities in\nscreen-based interaction tasks. However, they remain highly vulnerable to\npop-up-based environmental injection attacks, where malicious visual elements\ndivert model attention and lead to unsafe or incorrect actions. Existing\ndefense methods either require costly retraining or perform poorly under\ninductive interference. In this work, we systematically study how such attacks\nalter the attention behavior of GUI agents and uncover a layer-wise attention\ndivergence pattern between correct and incorrect outputs. Based on this\ninsight, we propose \\textbf{LaSM}, a \\textit{Layer-wise Scaling Mechanism} that\nselectively amplifies attention and MLP modules in critical layers. LaSM\nimproves the alignment between model saliency and task-relevant regions without\nadditional training. Extensive experiments across 12 types of pop-up\nperturbations and 4 different model backbones show that LaSM consistently\nenhances the defense success rate. When combined with prompt-level alerts, LaSM\nachieves over 98\\% robustness even under strong inductive attacks. Our findings\nreveal that attention misalignment is a core vulnerability in MLLM agents and\ncan be effectively addressed through selective layer-wise modulation.",
    "published": "2025-07-13T08:36:09Z",
    "updated": "2025-07-13T08:36:09Z",
    "id": "2507.10610v1",
    "authors": [
      "Zihe Yan",
      "Zhuosheng Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10610v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10610v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10610v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses multimodal large language models (MLLMs) and their vulnerabilities to attacks, proposing a defense mechanism that involves layer-wise scaling. The focus is on MLLMs and their interaction with graphical user interfaces, which aligns with the MLLM topic. The mention of attention behavior and layer-wise modulation also suggests relevance to the Reasoning topic, as it involves understanding and improving the model's decision-making processes.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.09527v1": {
    "title": "EV-STLLM: Electric vehicle charging forecasting based on spatio-temporal\n  large language models with multi-frequency and multi-scale information fusion",
    "summary": "With the proliferation of electric vehicles (EVs), accurate charging demand\nand station occupancy forecasting are critical for optimizing urban energy and\nthe profit of EVs aggregator. Existing approaches in this field usually\nstruggle to capture the complex spatio-temporal dependencies in EV charging\nbehaviors, and their limited model parameters hinder their ability to learn\ncomplex data distribution representations from large datasets. To this end, we\npropose a novel EV spatio-temporal large language model (EV-STLLM) for accurate\nprediction. Our proposed framework is divided into two modules. In the data\nprocessing module, we utilize variational mode decomposition (VMD) for data\ndenoising, and improved complete ensemble empirical mode decomposition with\nadaptive noise (ICEEMDAN) for data multi-frequency decomposition. Fuzzy\ninformation granulation (FIG) for extracting multi-scale information.\nAdditionally, ReliefF is used for feature selection to mitigate redundancy. In\nthe forecasting module, the EV-STLLM is used to directly achieve EV charging\nand occupancy forecasting. Firstly, we fully capture the intrinsic\nspatio-temporal characteristics of the data by integrating adjacency matrices\nderived from the regional stations network and spatio-temporal-frequency\nembedding information. Then, the partially frozen graph attention (PFGA) module\nis utilized to maintain the sequential feature modeling capabilities of the\npre-trained large model while incorporating EV domain knowledge. Extensive\nexperiments using real-world data from Shenzhen, China, demonstrate that our\nproposed framework can achieve superior accuracy and robustness compared to the\nstate-of-the-art benchmarks.",
    "published": "2025-07-13T07:59:13Z",
    "updated": "2025-07-13T07:59:13Z",
    "id": "2507.09527v1",
    "authors": [
      "Hang Fan",
      "Yunze Chai",
      "Chenxi Liu",
      "Weican Liu",
      "Zuhan Zhang",
      "Wencai Run",
      "Dunnan Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09527v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09527v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09527v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a spatio-temporal large language model (EV-STLLM) for forecasting electric vehicle charging demand, which involves complex spatio-temporal dependencies and leverages large language models for prediction. The core topics are related to LLM (Large Language Models) and their application in spatio-temporal data analysis.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.09509v1": {
    "title": "How Important is `Perfect' English for Machine Translation Prompts?",
    "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans.",
    "published": "2025-07-13T06:33:12Z",
    "updated": "2025-07-13T06:33:12Z",
    "id": "2507.09509v1",
    "authors": [
      "Patrcia Schmidtov",
      "Niyati Bafna",
      "Seth Aycock",
      "Gianluca Vico",
      "Wiktor Kamzela",
      "Katharina Hmmerl",
      "Vilm Zouhar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09509v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09509v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09509v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the sensitivity of Large Language Models (LLMs) to errors and perturbations in prompts, specifically in the context of machine translation and evaluation. It discusses how different types of noise in prompts affect LLM performance, which is directly related to the study of LLMs and their behavior.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.09508v1": {
    "title": "A Mixture of Linear Corrections Generates Secure Code",
    "summary": "Large language models (LLMs) have become proficient at sophisticated\ncode-generation tasks, yet remain ineffective at reliably detecting or avoiding\ncode vulnerabilities. Does this deficiency stem from insufficient learning\nabout code vulnerabilities, or is it merely a result of ineffective prompting?\nUsing representation engineering techniques, we investigate whether LLMs\ninternally encode the concepts necessary to identify code vulnerabilities. We\nfind that current LLMs encode precise internal representations that distinguish\nvulnerable from secure code--achieving greater accuracy than standard prompting\napproaches. Leveraging these vulnerability-sensitive representations, we\ndevelop an inference-time steering technique that subtly modulates the model's\ntoken-generation probabilities through a mixture of corrections (MoC). Our\nmethod effectively guides LLMs to produce less vulnerable code without\ncompromising functionality, demonstrating a practical approach to controlled\nvulnerability management in generated code. Notably, MoC enhances the security\nratio of Qwen2.5-Coder-7B by 8.9\\%, while simultaneously improving\nfunctionality on HumanEval pass@1 by 2.1\\%.",
    "published": "2025-07-13T06:27:33Z",
    "updated": "2025-07-13T06:27:33Z",
    "id": "2507.09508v1",
    "authors": [
      "Weichen Yu",
      "Ravi Mangal",
      "Terry Zhuo",
      "Matt Fredrikson",
      "Corina S. Pasareanu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09508v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09508v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09508v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for code-generation tasks, specifically focusing on their ability to detect and avoid code vulnerabilities. It introduces a method called Mixture of Corrections (MoC) to improve the security of generated code, which is relevant to the topics of LLMs and their applications in code generation and security.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.09499v1": {
    "title": "The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM\n  Challenge",
    "summary": "We present the DKU system for Task 2 of the MLC-SLM Challenge, which aims to\nperform multi-speaker automatic speech recognition directly from raw audio\nwithout Oracle speaker labels or time boundaries. Our approach builds upon a\ndiarization-aware framework integrating speaker embeddings and temporal\nutterance boundaries into a Qwen2.5-based large language model (LLM). Then, we\nenhance the system's multilingual performance by fine-tuning language-specific\nadapters and LoRA modules within the LLM decoder. Finally, our system achieves\nthe tcpWER of 23.56\\% and 18.08\\% on the development and test sets of the\nMLC-SLM dataset, substantially outperforming the official baseline.",
    "published": "2025-07-13T05:30:39Z",
    "updated": "2025-07-13T05:30:39Z",
    "id": "2507.09499v1",
    "authors": [
      "Yuke Lin",
      "Ming Cheng",
      "Ze Li",
      "Ming Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09499v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09499v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09499v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on multi-speaker automatic speech recognition using a large language model (LLM) and does not directly align with the provided topics which are more centered around LLM architectures, multimodal models, or general AI research.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.09490v1": {
    "title": "Towards LLM-Based Automatic Playtest",
    "summary": "Playtesting is the process in which people play a video game for testing. It\nis critical for the quality assurance of gaming software. Manual playtesting is\ntime-consuming and expensive. However, automating this process is challenging,\nas playtesting typically requires domain knowledge and problem-solving skills\nthat most conventional testing tools lack. Recent advancements in artificial\nintelligence (AI) have opened up new possibilities for applying Large Language\nModels (LLMs) to playtesting. However, significant challenges remain: current\nLLMs cannot visually perceive game environments, and most existing research\nfocuses on text-based games or games with robust APIs. Many non-text games lack\nAPIs to provide textual descriptions of game states, making it almost\nimpossible to naively apply LLMs for playtesting. This paper introduces Lap,\nour novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to\ntest match-3 games, a category of games where players match three or more\nidentical tiles in a row or column to earn points. Lap encompasses three key\nphases: processing of game environments, prompting-based action generation, and\naction execution. Given a match-3 game, Lap takes a snapshot of the game board\nand converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to\nsuggest moves based on that matrix and tentatively applies the suggested moves\nto earn points and trigger changes in the game board. It repeats the\nabove-mentioned three steps iteratively until timeout. For evaluation, we\nconducted a case study using Lap on an open-source match-3 game, CasseBonbons,\nand empirically compared it with three existing tools. Our results are\npromising: Lap outperformed existing tools by achieving higher code coverage\nand triggering more program crashes. This research sheds light on the future of\nautomatic testing and LLM applications.",
    "published": "2025-07-13T04:23:44Z",
    "updated": "2025-07-13T04:23:44Z",
    "id": "2507.09490v1",
    "authors": [
      "Yan Zhao",
      "Chiwei Tang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09490v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09490v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09490v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in automating playtesting for video games, specifically focusing on match-3 games. It highlights the use of ChatGPT for processing game environments and generating actions, which aligns with the topics of LLM and RL (Reinforcement Learning) due to the iterative action generation and execution process.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.09488v1": {
    "title": "Criteria-Based LLM Relevance Judgments",
    "summary": "Relevance judgments are crucial for evaluating information retrieval systems,\nbut traditional human-annotated labels are time-consuming and expensive. As a\nresult, many researchers turn to automatic alternatives to accelerate method\ndevelopment. Among these, Large Language Models (LLMs) provide a scalable\nsolution by generating relevance labels directly through prompting. However,\nprompting an LLM for a relevance label without constraints often results in not\nonly incorrect predictions but also outputs that are difficult for humans to\ninterpret. We propose the Multi-Criteria framework for LLM-based relevance\njudgments, decomposing the notion of relevance into multiple criteria--such as\nexactness, coverage, topicality, and contextual fit--to improve the robustness\nand interpretability of retrieval evaluations compared to direct grading\nmethods. We validate this approach on three datasets: the TREC Deep Learning\ntracks from 2019 and 2020, as well as LLMJudge (based on TREC DL 2023). Our\nresults demonstrate that Multi-Criteria judgments enhance the system\nranking/leaderboard performance. Moreover, we highlight the strengths and\nlimitations of this approach relative to direct grading approaches, offering\ninsights that can guide the development of future automatic evaluation\nframeworks in information retrieval.",
    "published": "2025-07-13T04:21:21Z",
    "updated": "2025-07-13T04:21:21Z",
    "id": "2507.09488v1",
    "authors": [
      "Naghmeh Farzi",
      "Laura Dietz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09488v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09488v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09488v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating relevance judgments in information retrieval systems, which aligns with the 'LLM' topic. It also touches on evaluation metrics and performance comparison, which is relevant to the 'Benchmark' topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.09483v1": {
    "title": "Does UMBRELA Work on Other LLMs?",
    "summary": "We reproduce the UMBRELA LLM Judge evaluation framework across a range of\nlarge language models (LLMs) to assess its generalizability beyond the original\nstudy. Our investigation evaluates how LLM choice affects relevance assessment\naccuracy, focusing on leaderboard rank correlation and per-label agreement\nmetrics. Results demonstrate that UMBRELA with DeepSeek V3 obtains very\ncomparable performance to GPT-4o (used in original work). For LLaMA-3.3-70B we\nobtain slightly lower performance, which further degrades with smaller LLMs.",
    "published": "2025-07-13T04:05:25Z",
    "updated": "2025-07-13T04:05:25Z",
    "id": "2507.09483v1",
    "authors": [
      "Naghmeh Farzi",
      "Laura Dietz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09483v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09483v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09483v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of LLMs using the UMBRELA framework, focusing on the performance and generalizability across different models. This aligns with topics related to benchmarking and evaluating LLMs.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2507.09482v1": {
    "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive\n  Learning",
    "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.",
    "published": "2025-07-13T04:03:05Z",
    "updated": "2025-07-13T04:03:05Z",
    "id": "2507.09482v1",
    "authors": [
      "Changli Wang",
      "Rui Wu",
      "Fang Yin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09482v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09482v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09482v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on sarcasm generation using multimodal data and employs Proximal Policy Optimization (PPO) and contrastive learning, which are related to Reinforcement Learning (RL) and Multimodal Large Language Models (MLLM). The dataset creation aspect also aligns with the Dataset category.",
    "llm_cls_result": [
      "RL",
      "MLLM",
      "Dataset"
    ]
  },
  "2507.09411v1": {
    "title": "LLMalMorph: On The Feasibility of Generating Variant Malware using\n  Large-Language-Models",
    "summary": "Large Language Models (LLMs) have transformed software development and\nautomated code generation. Motivated by these advancements, this paper explores\nthe feasibility of LLMs in modifying malware source code to generate variants.\nWe introduce LLMalMorph, a semi-automated framework that leverages semantical\nand syntactical code comprehension by LLMs to generate new malware variants.\nLLMalMorph extracts function-level information from the malware source code and\nemploys custom-engineered prompts coupled with strategically defined code\ntransformations to guide the LLM in generating variants without\nresource-intensive fine-tuning. To evaluate LLMalMorph, we collected 10 diverse\nWindows malware samples of varying types, complexity and functionality and\ngenerated 618 variants. Our thorough experiments demonstrate that it is\npossible to reduce the detection rates of antivirus engines of these malware\nvariants to some extent while preserving malware functionalities. In addition,\ndespite not optimizing against any Machine Learning (ML)-based malware\ndetectors, several variants also achieved notable attack success rates against\nan ML-based malware classifier. We also discuss the limitations of current LLM\ncapabilities in generating malware variants from source code and assess where\nthis emerging technology stands in the broader context of malware variant\ngeneration.",
    "published": "2025-07-12T22:11:10Z",
    "updated": "2025-07-12T22:11:10Z",
    "id": "2507.09411v1",
    "authors": [
      "Md Ajwad Akil",
      "Adrian Shuai Li",
      "Imtiaz Karim",
      "Arun Iyengar",
      "Ashish Kundu",
      "Vinny Parla",
      "Elisa Bertino"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09411v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09411v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09411v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating malware variants, which directly relates to the research on LLMs and their applications in code generation and modification.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.09407v1": {
    "title": "LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their\n  Applications to Spearphishing",
    "summary": "We introduce the framework of LLM-Stackelberg games, a class of sequential\ndecision-making models that integrate large language models (LLMs) into\nstrategic interactions between a leader and a follower. Departing from\nclassical Stackelberg assumptions of complete information and rational agents,\nour formulation allows each agent to reason through structured prompts,\ngenerate probabilistic behaviors via LLMs, and adapt their strategies through\ninternal cognition and belief updates. We define two equilibrium concepts:\nreasoning and behavioral equilibrium, which aligns an agent's internal\nprompt-based reasoning with observable behavior, and conjectural reasoning\nequilibrium, which accounts for epistemic uncertainty through parameterized\nmodels over an opponent's response. These layered constructs capture bounded\nrationality, asymmetric information, and meta-cognitive adaptation. We\nillustrate the framework through a spearphishing case study, where a sender and\na recipient engage in a deception game using structured reasoning prompts. This\nexample highlights the cognitive richness and adversarial potential of\nLLM-mediated interactions. Our results show that LLM-Stackelberg games provide\na powerful paradigm for modeling decision-making in domains such as\ncybersecurity, misinformation, and recommendation systems.",
    "published": "2025-07-12T21:42:27Z",
    "updated": "2025-07-12T21:42:27Z",
    "id": "2507.09407v1",
    "authors": [
      "Quanyan Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09407v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09407v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09407v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces LLM-Stackelberg games, which integrate large language models (LLMs) into strategic interactions, focusing on reasoning and behavioral equilibrium concepts. This aligns with research on LLMs and their applications in strategic decision-making and reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.12480v1": {
    "title": "LLM-Powered Quantum Code Transpilation",
    "summary": "There exist various Software Development Kits (SDKs) tailored to different\nquantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples\ninclude but are not limited to Qiskit, Cirq, and PennyLane. However, this\ndiversity presents significant challenges for interoperability and\ncross-platform development of hybrid quantum-classical software systems.\nTraditional rule-based transpilers for translating code between QSDKs are\ntime-consuming to design and maintain, requiring deep expertise and rigid\nmappings in the source and destination code. In this study, we explore the use\nof Large Language Models (LLMs) as a flexible and automated solution.\nLeveraging their pretrained knowledge and contextual reasoning capabilities, we\nposition LLMs as programming language-agnostic transpilers capable of\nconverting quantum programs from one QSDK to another while preserving\nfunctional equivalence. Our approach eliminates the need for manually defined\ntransformation rules and offers a scalable solution to quantum software\nportability. This work represents a step toward enabling intelligent,\ngeneral-purpose transpilation in the quantum computing ecosystem.",
    "published": "2025-07-12T21:16:21Z",
    "updated": "2025-07-12T21:16:21Z",
    "id": "2507.12480v1",
    "authors": [
      "Nazanin Siavash",
      "Armin Moin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.12480v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.12480v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.12480v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for quantum code transpilation, leveraging their pretrained knowledge and contextual reasoning capabilities. The core focus is on LLMs and their application in a specific domain (quantum computing), but the primary topic is the use of LLMs.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.09404v1": {
    "title": "Scaling Laws for Optimal Data Mixtures",
    "summary": "Large foundation models are typically trained on data from multiple domains,\nwith the data mixture--the proportion of each domain used--playing a critical\nrole in model performance. The standard approach to selecting this mixture\nrelies on trial and error, which becomes impractical for large-scale\npretraining. We propose a systematic method to determine the optimal data\nmixture for any target domain using scaling laws. Our approach accurately\npredicts the loss of a model of size $N$ trained with $D$ tokens and a specific\ndomain weight vector $h$. We validate the universality of these scaling laws by\ndemonstrating their predictive power in three distinct and large-scale\nsettings: large language model (LLM), native multimodal model (NMM), and large\nvision models (LVM) pretraining. We further show that these scaling laws can\nextrapolate to new data mixtures and across scales: their parameters can be\naccurately estimated using a few small-scale training runs, and used to\nestimate the performance at larger scales and unseen domain weights. The\nscaling laws allow to derive the optimal domain weights for any target domain\nunder a given training budget ($N$,$D$), providing a principled alternative to\ncostly trial-and-error methods.",
    "published": "2025-07-12T21:16:08Z",
    "updated": "2025-07-12T21:16:08Z",
    "id": "2507.09404v1",
    "authors": [
      "Mustafa Shukor",
      "Louis Bethune",
      "Dan Busbridge",
      "David Grangier",
      "Enrico Fini",
      "Alaaeldin El-Nouby",
      "Pierre Ablin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09404v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09404v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09404v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses scaling laws for determining optimal data mixtures in large foundation models, which is directly related to the 'Scaling' topic. It also mentions large language models (LLM) and native multimodal models (NMM), aligning with the 'LLM' and 'MLLM' topics.",
    "llm_cls_result": [
      "Scaling",
      "LLM",
      "MLLM"
    ]
  },
  "2507.09389v1": {
    "title": "Knowledge Conceptualization Impacts RAG Efficacy",
    "summary": "Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications.",
    "published": "2025-07-12T20:10:26Z",
    "updated": "2025-07-12T20:10:26Z",
    "id": "2507.09389v1",
    "authors": [
      "Chris Davis Jaldi",
      "Anmol Saini",
      "Elham Ghiasi",
      "O. Divine Eziolise",
      "Cogan Shimizu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09389v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09389v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09389v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the impact of knowledge conceptualization on Retrieval-Augmented Generation (RAG) systems, which involves LLMs and their interaction with knowledge sources. This aligns with the topics of LLM and Memory, as it involves retrieval-based methods and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.09374v1": {
    "title": "EduFlow: Advancing MLLMs' Problem-Solving Proficiency through\n  Multi-Stage, Multi-Perspective Critique",
    "summary": "Multimodal large language models (MLLMs) still perform poorly on scientific\ntasks, particularly those requiring multi-step and interpretable reasoning.\nTheir limitations include insufficient scientific reasoning patterns, lack of\nglobal coherence in multi-step inference, and the absence of reflective\nself-correction, making them unreliable in structured scientific contexts. We\nintroduce EduFlow, the first end-to-end framework that covers the full pipeline\nof educational scientific reasoning, including data selection, MCTS-based\ntrajectory construction, model training, and output optimization. At its core\nis EduPRM, a process-aware reward model that critiques reasoning steps with\ntags and justifications. EduPRM is trained via curriculum learning on three\ncomplementary supervision sources: MCTS-guided trajectories, error-injected\ncritiques, and teacher-student dialogues, enabling dynamic adaptation to\nmulti-stage problem solving and iterative refinement during inference. We\nfurther propose EduMCTS, a domain-adapted search framework that introduces\nbootstrapping actions specifically designed for educational reasoning, such as\na self-reflection mechanism that promotes reflective error correction. It\nfurther leverages EduPRM's fine-grained feedback to guide the search toward\nhigher-quality reasoning trajectories. By applying self-consistency and\nrejection sampling, we constructed EduMCTS-160K, a large-scale dataset of\neducational reasoning trajectories. Extensive experiments demonstrate that\nEduFlow enhances reasoning consistency and coherence. Code, data, and models\nwill be released.",
    "published": "2025-07-12T18:44:32Z",
    "updated": "2025-07-12T18:44:32Z",
    "id": "2507.09374v1",
    "authors": [
      "Chenglin Zhu",
      "Tao Zhang",
      "Chong Li",
      "Mingan Lin",
      "Zenan Zhou",
      "Jian Xie"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09374v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09374v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09374v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing Multimodal Large Language Models (MLLMs) for scientific problem-solving, which involves reasoning and iterative refinement. The core topics are MLLM for its focus on multimodal models, Reasoning for its emphasis on problem-solving and interpretable reasoning, and Dataset for the creation of a large-scale dataset of educational reasoning trajectories.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning",
      "Dataset"
    ]
  },
  "2507.10599v1": {
    "title": "Emergence of Hierarchical Emotion Organization in Large Language Models",
    "summary": "As large language models (LLMs) increasingly power conversational agents,\nunderstanding how they model users' emotional states is critical for ethical\ndeployment. Inspired by emotion wheels -- a psychological framework that argues\nemotions organize hierarchically -- we analyze probabilistic dependencies\nbetween emotional states in model outputs. We find that LLMs naturally form\nhierarchical emotion trees that align with human psychological models, and\nlarger models develop more complex hierarchies. We also uncover systematic\nbiases in emotion recognition across socioeconomic personas, with compounding\nmisclassifications for intersectional, underrepresented groups. Human studies\nreveal striking parallels, suggesting that LLMs internalize aspects of social\nperception. Beyond highlighting emergent emotional reasoning in LLMs, our\nresults hint at the potential of using cognitively-grounded theories for\ndeveloping better model evaluations.",
    "published": "2025-07-12T15:12:46Z",
    "updated": "2025-07-12T15:12:46Z",
    "id": "2507.10599v1",
    "authors": [
      "Bo Zhao",
      "Maya Okawa",
      "Eric J. Bigelow",
      "Rose Yu",
      "Tomer Ullman",
      "Ekdeep Singh Lubana",
      "Hidenori Tanaka"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10599v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10599v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10599v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the emotional reasoning and hierarchical organization in large language models (LLMs), which aligns with the topic of LLM research and their emergent properties. It also touches on the ethical deployment and evaluation of these models, which is relevant to the broader context of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.09313v2": {
    "title": "ProactiveVideoQA: A Comprehensive Benchmark Evaluating Proactive\n  Interactions in Video Large Language Models",
    "summary": "With the growing research focus on multimodal dialogue systems, the\ncapability for proactive interaction is gradually gaining recognition. As an\nalternative to conventional turn-by-turn dialogue, users increasingly expect\nmultimodal systems to be more initiative, for example, by autonomously\ndetermining the timing of multi-turn responses in real time during video\nplayback. To facilitate progress in this emerging area, we introduce\nProactiveVideoQA, the first comprehensive benchmark to evaluate a system's\nability to engage in proactive interaction. Since model responses are generated\nat varying timestamps, we further propose PAUC, the first metric that accounts\nfor the temporal dynamics of model responses. This enables a more accurate\nevaluation of systems operating in proactive settings. Through extensive\nbenchmarking of various baseline systems on ProactiveVideoQA and a user study\nof human preferences, we show that PAUC is in better agreement with human\npreferences than traditional evaluation metrics, which typically only consider\nthe textual content of responses. These findings demonstrate that PAUC provides\na more faithful assessment of user experience in proactive interaction\nscenarios. Project homepage:\nhttps://github.com/yellow-binary-tree/ProactiveVideoQA",
    "published": "2025-07-12T15:11:50Z",
    "updated": "2025-07-15T11:48:07Z",
    "id": "2507.09313v2",
    "authors": [
      "Yueqian Wang",
      "Xiaojun Meng",
      "Yifan Wang",
      "Huishuai Zhang",
      "Dongyan Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09313v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09313v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09313v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark (ProactiveVideoQA) for evaluating proactive interactions in video large language models, which involves multimodal dialogue systems and temporal dynamics of model responses. This aligns with topics related to benchmarking multimodal models and their interaction capabilities.",
    "llm_cls_result": [
      "Benchmark",
      "MLLM"
    ]
  },
  "2507.09255v1": {
    "title": "StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent\n  LLMs in Financial Markets",
    "summary": "We present StockSim, an open-source simulation platform for systematic\nevaluation of large language models (LLMs) in realistic financial\ndecision-making scenarios. Unlike previous toolkits that offer limited scope,\nStockSim delivers a comprehensive system that fully models market dynamics and\nsupports diverse simulation modes of varying granularity. It incorporates\ncritical real-world factors, such as latency, slippage, and order-book\nmicrostructure, that were previously neglected, enabling more faithful and\ninsightful assessment of LLM-based trading agents. An extensible, role-based\nagent framework supports heterogeneous trading strategies and multi-agent\ncoordination, making StockSim a uniquely capable testbed for NLP research on\nreasoning under uncertainty and sequential decision-making. We open-source all\nour code at https: //github.com/harrypapa2002/StockSim.",
    "published": "2025-07-12T11:29:44Z",
    "updated": "2025-07-12T11:29:44Z",
    "id": "2507.09255v1",
    "authors": [
      "Charidimos Papadakis",
      "Giorgos Filandrianos",
      "Angeliki Dimitriou",
      "Maria Lymperaiou",
      "Konstantinos Thomas",
      "Giorgos Stamou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09255v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09255v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09255v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in financial decision-making scenarios, focusing on simulation and evaluation. It mentions multi-agent coordination and reasoning under uncertainty, which are relevant to LLM research.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.09205v3": {
    "title": "Advancing Large Language Models for Tibetan with Curated Data and\n  Continual Pre-Training",
    "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model to enhance its generative capabilities in Tibetan. To evaluate the\nTibetan capabilities of the model, we create new high-quality Tibetan\nbenchmarks, and complement them with existing public benchmarks. Experimental\nresults demonstrate that our model consistently and significantly outperforms\nboth open-source models of similar scale and Tibetan-tailored models across a\nwide range of tasks.",
    "published": "2025-07-12T08:54:05Z",
    "updated": "2025-07-23T13:30:04Z",
    "id": "2507.09205v3",
    "authors": [
      "Leiyu Pan",
      "Bojian Xiong",
      "Lei Yang",
      "Renren Jin",
      "Shaowei Zhang",
      "Yue Chen",
      "Ling Shi",
      "Jiang Zhou",
      "Junru Wu",
      "Zhen Wang",
      "Jianxiang Peng",
      "Juesi Xiao",
      "Tianyu Dong",
      "Zhuowen Han",
      "Zhuo Chen",
      "Yuqi Ren",
      "Deyi Xiong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09205v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09205v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09205v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on advancing Large Language Models (LLMs) for a low-resource language (Tibetan) through curated data and continual pre-training, which aligns with the LLM and Pretrain topics. Additionally, the creation of new benchmarks for evaluation relates to the Benchmark topic.",
    "llm_cls_result": [
      "LLM",
      "Pretrain",
      "Benchmark"
    ]
  },
  "2507.09201v1": {
    "title": "SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large\n  Language Model via Adaptive Thresholding",
    "summary": "Large language models (LLMs) have demonstrated exceptional proficiency in\nunderstanding and generating human language, but efficient inference on\nresource-constrained embedded devices remains challenging due to large model\nsizes and memory-intensive operations in feedforward network (FFN) and\nmulti-head attention (MHA) layers. While existing accelerators offload LLM\ninference to expensive heterogeneous computing systems, they fail to exploit\nthe significant sparsity inherent in LLM operations, leaving hardware resources\nunderutilized. We propose SLIM, an algorithm-hardware co-design optimized for\nsparse LLM serving on edge devices. SLIM exploits LLM sparsity through an\nadaptive thresholding algorithm that enables runtime-configurable sparsity with\nnegligible accuracy loss, fetching only activated neurons to dramatically\nreduce data movement. Our heterogeneous hardware architecture strategically\ncombines near-storage processing (NSP) and processing-in-memory (PIM): FFN\nweights are stored in high-density 3D NAND and computed using NSP units, while\nmemory-intensive MHA operations are processed in PIM modules. This design\nsignificantly reduces memory footprint, data movement, and energy consumption.\nOur comprehensive evaluation demonstrates SLIM's effectiveness, achieving\n13-18x throughput improvements over SSD-GPU systems and 9-10x better energy\nefficiency over DRAM-GPU systems while maintaining low latency, making\ncost-effective LLM deployment viable for edge computing environments.",
    "published": "2025-07-12T08:44:38Z",
    "updated": "2025-07-12T08:44:38Z",
    "id": "2507.09201v1",
    "authors": [
      "Weihong Xu",
      "Haein Choi",
      "Po-kai Hsu",
      "Shimeng Yu",
      "Tajana Rosing"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09201v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09201v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09201v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the optimization of large language models (LLMs) for edge inference, focusing on sparsity exploitation and hardware acceleration. The core topics are related to LLM optimization and deployment, which aligns with the 'LLM' and 'Scaling' categories.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.09199v1": {
    "title": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted\n  Retrieval",
    "summary": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\n  Inspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.91%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery.",
    "published": "2025-07-12T08:42:10Z",
    "updated": "2025-07-12T08:42:10Z",
    "id": "2507.09199v1",
    "authors": [
      "Huihui Huang",
      "Ratnadira Widyasari",
      "Ting Zhang",
      "Ivana Clairine Irsan",
      "Jieke Shi",
      "Han Wei Ang",
      "Frank Liauw",
      "Eng Lieh Ouh",
      "Lwin Khin Shar",
      "Hong Jin Kang",
      "David Lo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09199v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09199v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09199v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in assisting retrieval tasks for issue-commit linking in software maintenance, which aligns with the LLM topic. It also involves information retrieval techniques, which are not directly listed in the topics but are related to LLM applications.",
    "llm_cls_result": [
      "LLM",
      "Other"
    ]
  },
  "2507.09195v1": {
    "title": "Towards Spatial Audio Understanding via Question Answering",
    "summary": "In this paper, we introduce a novel framework for spatial audio understanding\nof first-order ambisonic (FOA) signals through a question answering (QA)\nparadigm, aiming to extend the scope of sound event localization and detection\n(SELD) towards spatial scene understanding and reasoning. First, we curate and\nrelease fine-grained spatio-temporal textual descriptions for the STARSS23\ndataset using a rule-based approach, and further enhance linguistic diversity\nusing large language model (LLM)-based rephrasing. We also introduce a QA\ndataset aligned with the STARSS23 scenes, covering various aspects such as\nevent presence, localization, spatial, and temporal relationships. To increase\nlanguage variety, we again leverage LLMs to generate multiple rephrasings per\nquestion. Finally, we develop a baseline spatial audio QA model that takes FOA\nsignals and natural language questions as input and provides answers regarding\nvarious occurrences, temporal, and spatial relationships of sound events in the\nscene formulated as a classification task. Despite being trained solely with\nscene-level question answering supervision, our model achieves performance that\nis comparable to a fully supervised sound event localization and detection\nmodel trained with frame-level spatiotemporal annotations. The results\nhighlight the potential of language-guided approaches for spatial audio\nunderstanding and open new directions for integrating linguistic supervision\ninto spatial scene analysis.",
    "published": "2025-07-12T08:29:09Z",
    "updated": "2025-07-12T08:29:09Z",
    "id": "2507.09195v1",
    "authors": [
      "Parthasaarathy Sudarsanam",
      "Archontis Politis"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09195v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09195v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09195v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on spatial audio understanding using a question answering paradigm, leveraging large language models (LLMs) for rephrasing and generating questions, and integrating linguistic supervision into spatial scene analysis. The core topics are related to multimodal large language models (MLLM) and the use of LLMs in generating and processing language data.",
    "llm_cls_result": [
      "MLLM",
      "LLM"
    ]
  },
  "2507.09188v1": {
    "title": "Retrieval-Augmented Recommendation Explanation Generation with\n  Hierarchical Aggregation",
    "summary": "Explainable Recommender System (ExRec) provides transparency to the\nrecommendation process, increasing users' trust and boosting the operation of\nonline services. With the rise of large language models (LLMs), whose extensive\nworld knowledge and nuanced language understanding enable the generation of\nhuman-like, contextually grounded explanations, LLM-powered ExRec has gained\ngreat momentum. However, existing LLM-based ExRec models suffer from profile\ndeviation and high retrieval overhead, hindering their deployment. To address\nthese issues, we propose Retrieval-Augmented Recommendation Explanation\nGeneration with Hierarchical Aggregation (REXHA). Specifically, we design a\nhierarchical aggregation based profiling module that comprehensively considers\nuser and item review information, hierarchically summarizing and constructing\nholistic profiles. Furthermore, we introduce an efficient retrieval module\nusing two types of pseudo-document queries to retrieve relevant reviews to\nenhance the generation of recommendation explanations, effectively reducing\nretrieval latency and improving the recall of relevant reviews. Extensive\nexperiments demonstrate that our method outperforms existing approaches by up\nto 12.6% w.r.t. the explanation quality while achieving high retrieval\nefficiency.",
    "published": "2025-07-12T08:15:05Z",
    "updated": "2025-07-12T08:15:05Z",
    "id": "2507.09188v1",
    "authors": [
      "Bangcheng Sun",
      "Yazhe Chen",
      "Jilin Yang",
      "Xiaodong Li",
      "Hui Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09188v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09188v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09188v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in explainable recommender systems, focusing on retrieval-augmented methods to improve explanation generation. It mentions LLMs and retrieval-augmented generation, which are key topics in the provided list.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.09174v1": {
    "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation\n  Detection in Multimodal Fact-Checking",
    "summary": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git.",
    "published": "2025-07-12T07:46:51Z",
    "updated": "2025-07-12T07:46:51Z",
    "id": "2507.09174v1",
    "authors": [
      "Shuo Yang",
      "Zijian Yu",
      "Zhenzhe Ying",
      "Yuqin Dai",
      "Guoqing Wang",
      "Jun Lan",
      "Jinfeng Xu",
      "Jinze Li",
      "Edith C. H. Ngai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09174v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09174v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09174v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a retrieval-augmented multi-agent framework for misinformation detection, leveraging multimodal large language models and retrieval-based methods. It aligns with topics related to multimodal LLMs, retrieval-augmented generation, and multi-agent reasoning.",
    "llm_cls_result": [
      "MLLM",
      "Memory",
      "Reasoning"
    ]
  },
  "2507.09161v1": {
    "title": "Large Language Models and Non-Negative Matrix Factorization for\n  Bioacoustic Signal Decomposition",
    "summary": "Large language models have shown a remarkable ability to extract meaning from\nunstructured data, offering new ways to interpret biomedical signals beyond\ntraditional numerical methods. In this study, we present a matrix factorization\nframework for bioacoustic signal analysis which is enhanced by large language\nmodels. The focus is on separating bioacoustic signals that commonly overlap in\nclinical recordings, using matrix factorization to decompose the mixture into\ninterpretable components. A large language model is then applied to the\nseparated signals to associate distinct acoustic patterns with potential\nmedical conditions such as cardiac rhythm disturbances or respiratory\nabnormalities. Recordings were obtained from a digital stethoscope applied to a\nclinical manikin to ensure a controlled and high-fidelity acquisition\nenvironment. This hybrid approach does not require labeled data or prior\nknowledge of source types, and it provides a more interpretable and accessible\nframework for clinical decision support. The method demonstrates promise for\nintegration into future intelligent diagnostic tools.",
    "published": "2025-07-12T06:44:43Z",
    "updated": "2025-07-12T06:44:43Z",
    "id": "2507.09161v1",
    "authors": [
      "Yasaman Torabi",
      "Shahram Shirani",
      "James P. Reilly"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09161v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09161v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09161v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in bioacoustic signal decomposition, which aligns with the 'LLM' topic. It also involves the use of matrix factorization, which is a numerical method but not directly related to the provided topics. The primary focus is on LLMs, so the other topics are not relevant.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.10596v1": {
    "title": "PLEX: Perturbation-free Local Explanations for LLM-Based Text\n  Classification",
    "summary": "Large Language Models (LLMs) excel in text classification, but their\ncomplexity hinders interpretability, making it difficult to understand the\nreasoning behind their predictions. Explainable AI (XAI) methods like LIME and\nSHAP offer local explanations by identifying influential words, but they rely\non computationally expensive perturbations. These methods typically generate\nthousands of perturbed sentences and perform inferences on each, incurring a\nsubstantial computational burden, especially with LLMs. To address this, we\npropose \\underline{P}erturbation-free \\underline{L}ocal \\underline{Ex}planation\n(PLEX), a novel method that leverages the contextual embeddings extracted from\nthe LLM and a ``Siamese network\" style neural network trained to align with\nfeature importance scores. This one-off training eliminates the need for\nsubsequent perturbations, enabling efficient explanations for any new sentence.\nWe demonstrate PLEX's effectiveness on four different classification tasks\n(sentiment, fake news, fake COVID-19 news and depression), showing more than\n92\\% agreement with LIME and SHAP. Our evaluation using a ``stress test\"\nreveals that PLEX accurately identifies influential words, leading to a similar\ndecline in classification accuracy as observed with LIME and SHAP when these\nwords are removed. Notably, in some cases, PLEX demonstrates superior\nperformance in capturing the impact of key features. PLEX dramatically\naccelerates explanation, reducing time and computational overhead by two and\nfour orders of magnitude, respectively. This work offers a promising solution\nfor explainable LLM-based text classification.",
    "published": "2025-07-12T06:31:38Z",
    "updated": "2025-07-12T06:31:38Z",
    "id": "2507.10596v1",
    "authors": [
      "Yogachandran Rahulamathavan",
      "Misbah Farooq",
      "Varuna De Silva"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10596v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10596v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10596v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the interpretability of Large Language Models (LLMs) in text classification tasks by proposing a perturbation-free local explanation method. It directly addresses challenges related to LLMs and their interpretability, which falls under the 'LLM' category. Additionally, the work involves explainable AI (XAI) methods, which are not explicitly listed in the topic list but are closely related to LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.09139v1": {
    "title": "PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP\n  Alignment",
    "summary": "Human pose estimation traditionally relies on architectures that encode\nkeypoint priors, limiting their generalization to novel poses or unseen\nkeypoints. Recent language-guided approaches like LocLLM reformulate keypoint\nlocalization as a vision-language task, enabling zero-shot generalization\nthrough textual descriptions. However, LocLLM's linear projector fails to\ncapture complex spatial-textual interactions critical for high-precision\nlocalization. To address this, we propose PoseLLM, the first Large Language\nModel (LLM)-based pose estimation framework that replaces the linear projector\nwith a nonlinear MLP vision-language connector. This lightweight two-layer MLP\nwith GELU activation enables hierarchical cross-modal feature transformation,\nenhancing the fusion of visual patches and textual keypoint descriptions.\nTrained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO\nvalidation set, outperforming LocLLM by +0.4 AP, while maintaining strong\nzero-shot generalization on Human-Art and MPII. Our work demonstrates that a\nsimple yet powerful nonlinear connector significantly boosts localization\naccuracy without sacrificing generalization, advancing the state-of-the-art in\nlanguage-guided pose estimation. Code is available at\nhttps://github.com/Ody-trek/PoseLLM.",
    "published": "2025-07-12T04:53:39Z",
    "updated": "2025-07-12T04:53:39Z",
    "id": "2507.09139v1",
    "authors": [
      "Dewen Zhang",
      "Tahir Hussain",
      "Wangpeng An",
      "Hayaru Shouno"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09139v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09139v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09139v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a Large Language Model (LLM)-based framework for human pose estimation, focusing on enhancing vision-language interactions through a nonlinear MLP connector. This aligns with topics related to LLMs and multimodal models.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.09135v1": {
    "title": "Position Paper: Programming Language Techniques for Bridging LLM Code\n  Generation Semantic Gaps",
    "summary": "Large Language Models have demonstrated remarkable capabilities in automated\ncode generation, yet their statistical nature and black-box characteristics\ncreate significant semantic gaps manifested through syntax errors, semantic\nhallucinations, and reliability concerns. This position paper argues that\nprincipled integration of Programming Language (PL) techniques is essential for\nbridging these gaps. Through structured program representations, formal\ncorrectness guarantees, and robust verification mechanisms, PL techniques can\nelevate LLM-generated code from statistical pattern matching to truly reliable\nand trustworthy levels. This integration is crucial for developing systems that\ngenerate code that is not only functionally correct but also interpretable,\nverifiable, and ultimately trustworthy.",
    "published": "2025-07-12T04:32:15Z",
    "updated": "2025-07-12T04:32:15Z",
    "id": "2507.09135v1",
    "authors": [
      "Yalong Du",
      "Chaozheng Wang",
      "Huaijin Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09135v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09135v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09135v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Programming Language techniques to improve the reliability and correctness of code generated by Large Language Models (LLMs). It focuses on addressing semantic gaps in LLM-generated code, which is directly related to the research on Large Language Models (LLM) and their applications in code generation and reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.09116v3": {
    "title": "Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM\n  Generative Error Correction for Accented Speech Recognition",
    "summary": "Despite improvements in automatic speech recognition, performance drops with\naccented speech. Generative error correction (GER) leverages the linguistic\nknowledge of large language models (LLMs), outperforming typical language model\nmethods. However, it lacks specificity in accented speech scenarios. Accents\nrepresent deviations from standard pronunciation, making multi-granularity\npronunciation and semantic information essential for accented speech\nrecognition. Moreover, accents exhibit considerable diversity, with each accent\npossessing distinct characteristics. In this study, we leverage GER to improve\ntranscription accuracy by addressing the two primary features. We propose the\nmulti-modal GER, which integrates pronunciation information from the speech\nmodality, and the multi-granularity GER, which incorporates fine-grained\nphoneme-level pronunciation information. These methods enable the LLM to\nutilize the pronunciation information of accented speech and the semantic\ninformation from word-level hypotheses for accurate transcription predictions\nthrough low-rank adaptation (LoRA) fine-tuning. We employ a three-stage\nstrategy to train separate multi-modal GER models for each accent to obtain\nmono-accent LoRA experts. By adopting our proposed HDMoLE method, which\nincorporates hierarchical routing and dynamic thresholds within the mixture of\nLoRA experts, we effectively merge mono-accent LoRA experts within a single\nmulti-modal GER to overcome accent diversity challenges. Furthermore,\nmulti-granularity GER leverages N-best word-level and phoneme-level hypotheses\nfrom the HDMoLE model to predict final transcriptions. Experiments on a\nmulti-accent English dataset show that our methods reduce word error rate by\n67.35% compared to the baseline vanilla Whisper-large-v3 model.",
    "published": "2025-07-12T02:14:50Z",
    "updated": "2025-07-19T16:25:24Z",
    "id": "2507.09116v3",
    "authors": [
      "Bingshen Mu",
      "Kun Wei",
      "Pengcheng Guo",
      "Lei Xie"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09116v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09116v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09116v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for generative error correction in accented speech recognition, incorporating multi-modal and multi-granularity information. It also mentions the use of LoRA (Low-Rank Adaptation) fine-tuning and a mixture of experts approach, which are key topics in the given list.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "MoE"
    ]
  },
  "2507.09100v1": {
    "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights\n  Grounded in Historical Data",
    "summary": "In decision-making conversations, experts must navigate complex choices and\nmake on-the-spot decisions while engaged in conversation. Although extensive\nhistorical data often exists, the real-time nature of these scenarios makes it\ninfeasible for decision-makers to review and leverage relevant information.\nThis raises an interesting question: What if experts could utilize relevant\npast data in real-time decision-making through insights derived from past data?\nTo explore this, we implemented a conversational user interface, taking\ndoctor-patient interactions as an example use case. Our system continuously\nlistens to the conversation, identifies patient problems and doctor-suggested\nsolutions, and retrieves related data from an embedded dataset, generating\nconcise insights using a pipeline built around a retrieval-based Large Language\nModel (LLM) agent. We evaluated the prototype by embedding Health Canada\ndatasets into a vector database and conducting simulated studies using sample\ndoctor-patient dialogues, showing effectiveness but also challenges, setting\ndirections for the next steps of our work.",
    "published": "2025-07-12T00:59:41Z",
    "updated": "2025-07-12T00:59:41Z",
    "id": "2507.09100v1",
    "authors": [
      "Mohammad Abolnejadian",
      "Shakiba Amirshahi",
      "Matthew Brehmer",
      "Anamaria Crisan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09100v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09100v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09100v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a retrieval-based Large Language Model (LLM) agent to augment expert decision-making by generating insights from historical data in real-time. This aligns with the topics of LLM (Large Language Models) and Memory (retrieval-based methods and long-context processing).",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.09090v1": {
    "title": "DS@GT at Touch: Large Language Models for Retrieval-Augmented Debate",
    "summary": "Large Language Models (LLMs) demonstrate strong conversational abilities. In\nthis Working Paper, we study them in the context of debating in two ways: their\nability to perform in a structured debate along with a dataset of arguments to\nuse and their ability to evaluate utterances throughout the debate. We deploy\nsix leading publicly available models from three providers for the\nRetrieval-Augmented Debate and Evaluation. The evaluation is performed by\nmeasuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout\nthis task, we found that although LLMs perform well in debates when given\nrelated arguments, they tend to be verbose in responses yet consistent in\nevaluation. The accompanying source code for this paper is located at\nhttps://github.com/dsgt-arc/touche-2025-rad.",
    "published": "2025-07-12T00:20:00Z",
    "updated": "2025-07-12T00:20:00Z",
    "id": "2507.09090v1",
    "authors": [
      "Anthony Miyaguchi",
      "Conor Johnston",
      "Aaryan Potdar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09090v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09090v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09090v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the context of debating, including their ability to perform in structured debates and evaluate utterances, which aligns with the 'LLM' and 'Memory' topics. The mention of 'Retrieval-Augmented Debate' also relates to memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.09076v1": {
    "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence\n  Emotion Recognition in Conversation",
    "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance.",
    "published": "2025-07-11T23:36:59Z",
    "updated": "2025-07-11T23:36:59Z",
    "id": "2507.09076v1",
    "authors": [
      "Jialong Mai",
      "Xiaofen Xing",
      "Yawei Li",
      "Zhipeng Li",
      "Jingyuan Xing",
      "Xiangmin Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09076v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09076v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09076v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of a Large Language Model (LLM) for speech emotion recognition, focusing on enhancing the model's memory and processing capabilities for long sequences. It introduces a Dynamic Parameter Memory mechanism and utilizes a LoRA module, which are relevant to the topics of LLM and Memory.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.09068v2": {
    "title": "Infinite Video Understanding",
    "summary": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.",
    "published": "2025-07-11T23:07:04Z",
    "updated": "2025-07-23T13:06:44Z",
    "id": "2507.09068v2",
    "authors": [
      "Dell Zhang",
      "Xiangyu Chen",
      "Jixiang Luo",
      "Mengxi Jia",
      "Changzhi Sun",
      "Ruilong Ren",
      "Jingren Liu",
      "Hao Sun",
      "Xuelong Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09068v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09068v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09068v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the challenges and advancements in video understanding using Large Language Models (LLMs) and their multimodal extensions (MLLMs), focusing on processing lengthy video content. It mentions architectural solutions, positional encoding, and reasoning systems, aligning with the topics of MLLM and Reasoning.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.09049v2": {
    "title": "CMER: A Context-Aware Approach for Mining Ethical Concern-related App\n  Reviews",
    "summary": "With the increasing proliferation of mobile applications in our daily lives,\nthe concerns surrounding ethics have surged significantly. Users communicate\ntheir feedback in app reviews, frequently emphasizing ethical concerns, such as\nprivacy and security. Incorporating these reviews has proved to be useful for\nmany areas of software engineering (e.g., requirement engineering, testing,\netc.). However, app reviews related to ethical concerns generally use\ndomain-specific language and are typically overshadowed by more generic\ncategories of user feedback, such as app reliability and usability. Thus,\nmaking automated extraction a challenging and time-consuming effort.\n  This study proposes CMER (A \\underline{C}ontext-Aware Approach for\n\\underline{M}ining \\underline{E}thical Concern-related App\n\\underline{R}eviews), a novel approach that combines Natural Language Inference\n(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract\nethical concern-related app reviews at scale. In CMER, NLI provides\ndomain-specific context awareness by using domain-specific hypotheses, and the\nLlama-like LLM eliminates the need for labeled data in the classification task.\nWe evaluated the validity of CMER by mining privacy and security-related\nreviews (PSRs) from the dataset of more than 382K app reviews of mobile\ninvestment apps. First, we evaluated four NLI models and compared the results\nof domain-specific hypotheses with generic hypotheses. Next, we evaluated three\nLLMs for the classification task. Finally, we combined the best NLI and LLM\nmodels (CMER) and extracted 2,178 additional PSRs overlooked by the previous\nstudy using a keyword-based approach, thus demonstrating the effectiveness of\nCMER. These reviews can be further refined into actionable requirement\nartifacts.",
    "published": "2025-07-11T21:46:04Z",
    "updated": "2025-07-20T04:27:14Z",
    "id": "2507.09049v2",
    "authors": [
      "Aakash Sorathiya",
      "Gouri Ginde"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09049v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09049v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09049v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a decoder-only Large Language Model (LLM) for mining ethical concern-related app reviews, which involves natural language processing and classification tasks. The core topics are related to LLM and its application in a specific domain.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.09019v1": {
    "title": "On Evaluating Performance of LLM Inference Serving Systems",
    "summary": "The rapid evolution of Large Language Model (LLM) inference systems has\nyielded significant efficiency improvements. However, our systematic analysis\nreveals that current evaluation methodologies frequently exhibit fundamental\nflaws, often manifesting as common evaluation anti-patterns that obscure true\nperformance characteristics and impede scientific progress. Through a\ncomprehensive examination of recent systems, we identify recurring\nanti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,\nand Metric Design. These anti-patterns are uniquely problematic for LLM\ninference due to its dual-phase nature combining distinct prefill and decode\noperations, its handling of highly heterogeneous workloads, and its strict\ntemporal requirements for interactive use. We demonstrate how common\nanti-patterns -- such as inadequate baseline comparisons that conflate\nengineering effort with algorithmic novelty, workload selections that fail to\nrepresent production scenarios, and metric normalizations that hide substantial\nperformance variability like generation stalls-lead to misleading conclusions.\nTo address these challenges, we provide a comprehensive checklist derived from\nour analysis, establishing a framework for recognizing and avoiding these\nanti-patterns in favor of robust LLM inference evaluation. To demonstrate the\npractical application of our framework, we present a case study analyzing\nspeculative decoding, a technique whose bursty, non-uniform token generation is\neasily misinterpreted when evaluated using approaches characteristic of these\nanti-patterns. Our work establishes a rigorous foundation for evaluation\nmethodology, enabling meaningful comparisons, ensuring reproducible results,\nand ultimately accelerating genuine progress in LLM inference systems by moving\nbeyond common anti-patterns to align evaluation with real-world requirements.",
    "published": "2025-07-11T20:58:21Z",
    "updated": "2025-07-11T20:58:21Z",
    "id": "2507.09019v1",
    "authors": [
      "Amey Agrawal",
      "Nitin Kedia",
      "Anmol Agarwal",
      "Jayashree Mohan",
      "Nipun Kwatra",
      "Souvik Kundu",
      "Ramachandran Ramjee",
      "Alexey Tumanov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09019v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09019v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09019v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the performance of LLM inference serving systems, identifying flaws in current evaluation methodologies and proposing a framework for robust evaluation. This directly relates to benchmarking and evaluating LLM systems.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2507.09010v1": {
    "title": "Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large\n  Language Model Inference",
    "summary": "Edge inference for large language models (LLM) offers secure, low-latency,\nand cost-effective inference solutions. We emphasize that an edge accelerator\nshould achieve high area efficiency and minimize external memory access (EMA)\nduring the memory-bound decode stage, while maintaining high energy efficiency\nduring the compute intensive prefill stage. This paper proposes an edge LLM\ninference accelerator featuring a hybrid systolic array (HSA) architecture that\noptimizes inference efficiency in both stages. To further reduce EMA, we adopt\nMXINT4 weight quantization and propose an optimized dataflow tailored for HSA,\nensuring negligible dequantization overhead and achieving 100% hardware\nutilization with minimal accuracy loss under edge DRAM bandwidth constraints.\nFor non-linear operations, we incorporate optimized root mean square\nnormalization (RMSNorm) and rotary position embedding (RoPE) units, reducing\ntheir latency, area, and memory access overhead while enabling end-to-end\ninference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while\nrunning a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x\nimprovement over existing approaches, while maintaining superior energy\nefficiency in token generation.",
    "published": "2025-07-11T20:27:30Z",
    "updated": "2025-07-11T20:27:30Z",
    "id": "2507.09010v1",
    "authors": [
      "Chun-Ting Chen",
      "HanGyeol Mun",
      "Jian Meng",
      "Mohamed S. Abdelfattah",
      "Jae-sun Seo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09010v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09010v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09010v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing edge inference for large language models (LLMs) through a hybrid systolic array accelerator, which involves hardware optimization and efficiency improvements for LLM inference. The core topics are related to LLM and the practical aspects of deploying LLMs at the edge, which falls under the 'LLM' category. The paper does not directly address other topics like RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.10593v1": {
    "title": "ToolRegistry: A Protocol-Agnostic Tool Management Library for\n  Function-Calling LLMs",
    "summary": "Large Language Model (LLM) applications are increasingly relying on external\ntools to extend their capabilities beyond text generation. However, current\ntool integration approaches suffer from fragmentation, protocol limitations,\nand implementation complexity, leading to substantial development overhead.\nThis paper presents Toolregistry, a protocol-agnostic tool management library\nthat simplifies tool registration, representation, execution, and lifecycle\nmanagement via a unified interface. Our evaluation demonstrates that\n\\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x\nperformance improvements through concurrent execution, and 100% compatibility\nwith OpenAI function calling standards. Real-world case studies show\nsignificant improvements in development efficiency and code maintainability\nacross diverse integration scenarios. \\toolregistry is open-source and\navailable at https://github.com/Oaklight/ToolRegistry, with comprehensive\ndocumentation at https://toolregistry.readthedocs.io/.",
    "published": "2025-07-11T20:23:23Z",
    "updated": "2025-07-11T20:23:23Z",
    "id": "2507.10593v1",
    "authors": [
      "Peng Ding"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10593v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10593v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10593v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a tool management library for function-calling LLMs, which is directly related to the use and enhancement of Large Language Models (LLMs) and their integration with external tools.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.09003v1": {
    "title": "Orchestration for Domain-specific Edge-Cloud Language Models",
    "summary": "The remarkable performance of Large Language Models (LLMs) has inspired many\napplications, which often necessitate edge-cloud collaboration due to\nconnectivity, privacy, and cost considerations. Traditional methods primarily\nfocus on selecting the best LLM model for optimizing performance, while\nneglecting the critical interplay between the components of the LLM serving\npipeline (context retrieval, query preprocessing, etc.) or the changing latency\nand cost constraints. We introduce ECO-LLM (Edge-Cloud Orchestrator for LLMs),\na novel system that reframes this problem as a joint optimization challenge and\nsolves it by systematically exploring component configurations and dynamically\nselecting optimal strategies at the query level. ECO-LLM consists of two\ncomponents: (1) the ECO-LLM Emulator, which efficiently explores the vast\nconfiguration space utilizing query clustering and pareto-optimal path\nselection, gathering domain-specific performance metrics without exhaustive\nevaluation; and (2) the ECO-LLM Runtime, which leverages these metrics to\ndynamically select optimal resolution strategies for user queries while meeting\nuser-defined Service Level Objectives (SLOs). We evaluate ECO-LLM on a smart\nhome and a smart car assistant scenarios. With an exhaustive exploration of all\npossible configurations for seen queries, ECO-LLM outperforms cloud-based\nmodels like GPT-4o in terms of accuracy (90% vs. 74% on average) while reducing\ncosts by 90% and latency by 55%, demonstrating the value of its joint\noptimization at the query level. In practical deployment for previously unseen\nqueries, ECO-LLM selects configurations that reduce costs by 62% or improve\nresponse times by 62% on average compared to state-of-the-art model routing\napproaches, while maintaining higher accuracy and consistently adhering to\nspecified latency and cost constraints.",
    "published": "2025-07-11T20:11:04Z",
    "updated": "2025-07-11T20:11:04Z",
    "id": "2507.09003v1",
    "authors": [
      "Prasoon Patidar",
      "Alex Crown",
      "Kevin Hsieh",
      "Yifei Xu",
      "Tusher Chakraborty",
      "Ranveer Chandra",
      "Yuvraj Agarwal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09003v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09003v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09003v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the optimization and deployment of Large Language Models (LLMs) in edge-cloud environments, focusing on performance, cost, and latency. It introduces a system for joint optimization of LLM components, which aligns with the 'LLM' and 'Scaling' topics due to its focus on LLM performance and optimization strategies. The mention of edge-cloud collaboration and dynamic query-level optimization also hints at 'AGI' due to the broader implications for intelligent systems.",
    "llm_cls_result": [
      "LLM",
      "Scaling",
      "AGI"
    ]
  },
  "2507.08992v1": {
    "title": "Semantic Source Code Segmentation using Small and Large Language Models",
    "summary": "Source code segmentation, dividing code into functionally coherent segments,\nis crucial for knowledge retrieval and maintenance in software development.\nWhile enabling efficient navigation and comprehension of large codebases,\nmanual and syntactic analysis approaches have become impractical as\nrepositories grow, especially for low-resource languages like R and their\nresearch domains (e.g., social sciences, psychology).This paper introduces an\nautomated, domain-specific approach for research R code segmentation using\nLarge and Small Language Models (LLMs/SLMs). It presents two novel approaches\nand a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:\nline-by-line analysis with context and range-based segment determination. We\nexperiment with LLMs and fine-tuned SLMs. To support the generalizability of\nour approaches, we also include experiments on Python code from the computer\nscience domain.Our results show that context-based line-by-line analysis is\nsuperior over range-based segmentation.Using smaller language models like\nCodeBERT and an encoder-only version of CodeT5+ are better than their LLM\ncounterparts. Most notably, these two best-performing models did not see R code\nduring pre-training versus the LLMs but were only fine-tuned on 4,130 lines of\nmanually annotated code.",
    "published": "2025-07-11T19:49:59Z",
    "updated": "2025-07-11T19:49:59Z",
    "id": "2507.08992v1",
    "authors": [
      "Abdelhalim Dahou",
      "Ansgar Scherp",
      "Sebastian Kurten",
      "Brigitte Mathiak",
      "Madhu Chauhan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08992v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08992v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08992v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of both Large and Small Language Models (LLMs/SLMs) for semantic source code segmentation, which involves dividing code into functionally coherent segments. The focus is on leveraging these models for automated segmentation, particularly in low-resource languages like R, and includes the creation of a human-annotated dataset. The study compares the performance of LLMs and fine-tuned SLMs, highlighting the effectiveness of smaller models like CodeBERT and an encoder-only version of CodeT5+.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.14179v1": {
    "title": "A Sparsity Predicting Approach for Large Language Models via Activation\n  Pattern Clustering",
    "summary": "Large Language Models (LLMs) exhibit significant activation sparsity, where\nonly a subset of neurons are active for a given input. Although this sparsity\npresents opportunities to reduce computational cost, efficiently utilizing it\nrequires predicting activation patterns in a scalable manner. However, direct\nprediction at the neuron level is computationally expensive due to the vast\nnumber of neurons in modern LLMs. To enable efficient prediction and\nutilization of activation sparsity, we propose a clustering-based activation\npattern compression framework. Instead of treating each neuron independently,\nwe group similar activation patterns into a small set of representative\nclusters. Our method achieves up to 79.34% clustering precision, outperforming\nstandard binary clustering approaches while maintaining minimal degradation in\nperplexity (PPL) scores. With a sufficiently large number of clusters, our\napproach attains a PPL score as low as 12.49, demonstrating its effectiveness\nin preserving model quality while reducing computational overhead. By\npredicting cluster assignments rather than individual neuron states, future\nmodels can efficiently infer activation patterns from pre-computed centroids.\nWe detail the clustering algorithm, analyze its effectiveness in capturing\nmeaningful activation structures, and demonstrate its potential to improve\nsparse computation efficiency. This clustering-based formulation serves as a\nfoundation for future work on activation pattern prediction, paving the way for\nefficient inference in large-scale language models.",
    "published": "2025-07-11T19:07:29Z",
    "updated": "2025-07-11T19:07:29Z",
    "id": "2507.14179v1",
    "authors": [
      "Nobel Dhar",
      "Bobin Deng",
      "Md Romyull Islam",
      "Xinyue Zhang",
      "Kazi Fahim Ahmad Nasif",
      "Kun Suo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14179v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14179v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14179v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method to predict and utilize activation sparsity in Large Language Models (LLMs) through clustering-based activation pattern compression, which is directly related to the architecture and efficiency of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.08960v1": {
    "title": "How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs",
    "summary": "Large Language Models (LLMs) have achieved strong performance on a wide range\nof complex reasoning tasks, yet further gains are often possible by leveraging\nthe complementary strengths of multiple models. While multi-agent frameworks\ncan improve solution quality by leveraging multiple LLMs, existing methods are\noften computationally expensive, both at training and inference time. In this\nwork, we introduce a hierarchical multi-agent framework that addresses these\nchallenges by training only a single leader LLM to coordinate a team of\nuntrained peer agents. To this end, we propose Multi-agent guided Leader Policy\n\\textbf{O}ptimization (MLPO), a novel approach which trains the leader to\nevaluate and synthesize agent responses without auxiliary value networks or\nexplicit agent feedback. Leaders trained with MLPO exhibit improved performance\nnot only when interacting with the agent team at inference time, but also enjoy\nimproved performance when deployed in single-agent settings without the team.\nEmpirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our\nframework achieves substantial performance improvements over both single-agent\nand multi-agent baselines. Our results highlight the effectiveness and\nefficiency of training a single, flexible leader for collaborative reasoning in\nmulti-agent LLM systems.",
    "published": "2025-07-11T18:34:07Z",
    "updated": "2025-07-11T18:34:07Z",
    "id": "2507.08960v1",
    "authors": [
      "Andrew Estornell",
      "Jean-Francois Ton",
      "Muhammad Faaiz Taufiq",
      "Hang Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08960v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08960v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08960v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a hierarchical multi-agent framework for improving reasoning in LLMs, which involves training a leader LLM to coordinate untrained peer agents. This involves aspects of reasoning and multi-agent systems within the context of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "RL"
    ]
  },
  "2507.08958v2": {
    "title": "Bridging Literature and the Universe Via A Multi-Agent Large Language\n  Model System",
    "summary": "As cosmological simulations and their associated software become increasingly\ncomplex, physicists face the challenge of searching through vast amounts of\nliterature and user manuals to extract simulation parameters from dense\nacademic papers, each using different models and formats. Translating these\nparameters into executable scripts remains a time-consuming and error-prone\nprocess. To improve efficiency in physics research and accelerate the\ncosmological simulation process, we introduce SimAgents, a multi-agent system\ndesigned to automate both parameter configuration from the literature and\npreliminary analysis for cosmology research. SimAgents is powered by\nspecialized LLM agents capable of physics reasoning, simulation software\nvalidation, and tool execution. These agents collaborate through structured\ncommunication, ensuring that extracted parameters are physically meaningful,\ninternally consistent, and software-compliant. We also construct a cosmological\nparameter extraction evaluation dataset by collecting over 40 simulations in\npublished papers from Arxiv and leading journals that cover diverse simulation\ntypes. Experiments on the dataset demonstrate a strong performance of\nSimAgents, highlighting its effectiveness and potential to accelerate\nscientific research for physicists. Our demonstration video is available at:\nhttps://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly\navailable at https://github.com/xwzhang98/SimAgents.",
    "published": "2025-07-11T18:31:20Z",
    "updated": "2025-07-15T22:55:30Z",
    "id": "2507.08958v2",
    "authors": [
      "Xiaowen Zhang",
      "Zhenyu Bi",
      "Patrick Lachance",
      "Xuan Wang",
      "Tiziana Di Matteo",
      "Rupert A. C. Croft"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08958v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08958v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08958v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multi-agent system powered by specialized LLM agents capable of physics reasoning and tool execution, which aligns with the topics of LLM (Large Language Models) and Reasoning (LLM reasoning). Additionally, the construction of a cosmological parameter extraction evaluation dataset relates to the Dataset topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Dataset"
    ]
  },
  "2507.08945v1": {
    "title": "GraphRunner: A Multi-Stage Framework for Efficient and Accurate\n  Graph-Based Retrieval",
    "summary": "Conventional Retrieval Augmented Generation (RAG) approaches are common in\ntext-based applications. However, they struggle with structured, interconnected\ndatasets like knowledge graphs, where understanding underlying relationships is\ncrucial for accurate retrieval. A common direction in graph-based retrieval\nemploys iterative, rule-based traversal guided by Large Language Models (LLMs).\nSuch existing iterative methods typically combine reasoning with single hop\ntraversal at each step, making them vulnerable to LLM reasoning errors and\nhallucinations that ultimately hinder the retrieval of relevant information.\n  To address these limitations, we propose GraphRunner, a novel graph-based\nretrieval framework that operates in three distinct stages: planning,\nverification, and execution. This introduces high-level traversal actions that\nenable multi-hop exploration in a single step. It also generates a holistic\ntraversal plan, which is verified against the graph structure and pre-defined\ntraversal actions, reducing reasoning errors and detecting hallucinations\nbefore execution. GraphRunner significantly reduces LLM reasoning errors and\ndetects hallucinations through validation. Our evaluation using the GRBench\ndataset shows that GraphRunner consistently outperforms existing approaches,\nachieving 10-50% performance improvements over the strongest baseline while\nreducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,\nmaking it significantly more robust and efficient for graph-based retrieval\ntasks.",
    "published": "2025-07-11T18:10:01Z",
    "updated": "2025-07-11T18:10:01Z",
    "id": "2507.08945v1",
    "authors": [
      "Savini Kashmira",
      "Jayanaka L. Dantanarayana",
      "Krisztin Flautner",
      "Lingjia Tang",
      "Jason Mars"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08945v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08945v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08945v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses a framework for graph-based retrieval that utilizes Large Language Models (LLMs) and addresses issues like reasoning errors and hallucinations in LLMs. It also mentions Retrieval Augmented Generation (RAG), which is related to memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Reasoning"
    ]
  },
  "2507.08944v1": {
    "title": "Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents",
    "summary": "Large language model (LLM)-based multi-agent systems have demonstrated\nremarkable promise for tackling complex tasks by breaking them down into\nsubtasks that are iteratively planned, executed, observed, and refined. Despite\ntheir effectiveness, these systems often incur high latency because real-world\nproblems frequently demand multiple iterative cycles of reasoning steps. To\naddress this challenge, we propose M1-Parallel, a framework that concurrently\nruns multiple multi-agent teams in parallel to uncover distinct solution paths.\nBy leveraging an event-driven communication model with asynchronous messaging,\nM1-Parallel efficiently capitalizes on the inherent diversity of valid plans to\neither reduce end-to-end latency or boost task completion rates. Our\nexperiments on complex tasks show that M1-Parallel with early termination\nachieves up to $2.2\\times$ speedup while preserving accuracy, and that\nM1-Parallel with aggregation yields higher task completion rates. We further\ninvestigate strategies aimed at encouraging diverse execution plans but observe\nno additional performance gains over repeated sampling. Overall, these findings\nunderscore the potential of parallel plan execution for optimizing multi-agent\nsystems for real-world, high-complexity reasoning tasks.",
    "published": "2025-07-11T18:09:22Z",
    "updated": "2025-07-11T18:09:22Z",
    "id": "2507.08944v1",
    "authors": [
      "Enhao Zhang",
      "Erkang Zhu",
      "Gagan Bansal",
      "Adam Fourney",
      "Hussein Mozannar",
      "Jack Gerrits"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08944v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08944v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08944v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in multi-agent systems for complex tasks, focusing on parallel execution and reasoning strategies. This aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning, as it involves iterative planning and execution).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.08801v1": {
    "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
    "summary": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.",
    "published": "2025-07-11T17:59:42Z",
    "updated": "2025-07-11T17:59:42Z",
    "id": "2507.08801v1",
    "authors": [
      "Hangjie Yuan",
      "Weihua Chen",
      "Jun Cen",
      "Hu Yu",
      "Jingyun Liang",
      "Shuning Chang",
      "Zhihui Lin",
      "Tao Feng",
      "Pengwei Liu",
      "Jiazheng Xing",
      "Hao Luo",
      "Jiasheng Tang",
      "Fan Wang",
      "Yi Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08801v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08801v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08801v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses an autoregressive video generator that retains the LLM architecture with minimal modifications, focusing on spatiotemporal correlations and multimodal data modeling. It aligns with topics related to Large Language Models (LLM) and Multimodal Large Language Models (MLLM).",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.08794v1": {
    "title": "One Token to Fool LLM-as-a-Judge",
    "summary": "Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM.",
    "published": "2025-07-11T17:55:22Z",
    "updated": "2025-07-11T17:55:22Z",
    "id": "2507.08794v1",
    "authors": [
      "Yulai Zhao",
      "Haolin Liu",
      "Dian Yu",
      "S. Y. Kung",
      "Haitao Mi",
      "Dong Yu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08794v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08794v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08794v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses vulnerabilities in LLMs used as judges in reinforcement learning with verifiable rewards (RLVR), which involves LLM-based evaluation methods and their robustness. It also touches on the use of LLMs in reinforcement learning contexts.",
    "llm_cls_result": [
      "RL",
      "Benchmark",
      "LLM"
    ]
  },
  "2507.10435v1": {
    "title": "From Sequence to Structure: Uncovering Substructure Reasoning in\n  Transformers",
    "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data.",
    "published": "2025-07-11T17:36:24Z",
    "updated": "2025-07-11T17:36:24Z",
    "id": "2507.10435v1",
    "authors": [
      "Xinnan Dai",
      "Kai Yang",
      "Jay Revolinsky",
      "Kai Guo",
      "Aoran Wang",
      "Bohang Zhang",
      "Jiliang Tang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10435v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10435v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10435v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses how large language models (LLMs) can understand and reason about graph structures, focusing on substructure extraction tasks. This aligns with the topics of Reasoning (as it involves logical reasoning and problem-solving in LLMs) and LLM (since it specifically addresses the capabilities and inner mechanisms of large language models).",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2507.08671v1": {
    "title": "LLMCup: Ranking-Enhanced Comment Updating with LLMs",
    "summary": "While comments are essential for enhancing code readability and\nmaintainability in modern software projects, developers are often motivated to\nupdate code but not comments, leading to outdated or inconsistent documentation\nthat hinders future understanding and maintenance. Recent approaches such as\nCUP and HebCup have attempted automatic comment updating using neural\nsequence-to-sequence models and heuristic rules, respectively. However, these\nmethods can miss or misinterpret crucial information during comment updating,\nresulting in inaccurate comments, and they often struggle with complex update\nscenarios. Given these challenges, a promising direction lies in leveraging\nlarge language models (LLMs), which have shown impressive performance in\nsoftware engineering tasks such as comment generation, code synthesis, and\nprogram repair. This suggests their strong potential to capture the logic\nbehind code modifications - an ability that is crucial for the task of comment\nupdating. Nevertheless, selecting an appropriate prompt strategy for an LLM on\neach update case remains challenging. To address this, we propose a novel\ncomment updating framework, LLMCup, which first uses multiple prompt strategies\nto provide diverse candidate updated comments via an LLM, and then employs a\nranking model, CupRank, to select the best candidate as final updated comment.\nExperimental results demonstrate the effectiveness of LLMCup, with improvements\nover state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,\n10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in\nSentenceBert similarity. Furthermore, a user study shows that comments updated\nby LLMCup sometimes surpass human-written updates, highlighting the importance\nof incorporating human evaluation in comment quality assessment.",
    "published": "2025-07-11T15:11:27Z",
    "updated": "2025-07-11T15:11:27Z",
    "id": "2507.08671v1",
    "authors": [
      "Hua Ge",
      "Juan Zhai",
      "Minxue Pan",
      "Fusen He",
      "Ziyue Tan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08671v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08671v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08671v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for comment updating in software projects, which directly relates to the LLM topic. It also involves ranking and selection mechanisms, which could be loosely related to Reasoning, but the primary focus is on LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.08665v1": {
    "title": "KELPS: A Framework for Verified Multi-Language Autoformalization via\n  Semantic-Syntactic Alignment",
    "summary": "Modern large language models (LLMs) show promising progress in formalizing\ninformal mathematics into machine-verifiable theorems. However, these methods\nstill face bottlenecks due to the limited quantity and quality of multilingual\nparallel corpora. In this paper, we propose a novel neuro-symbolic framework\nKELPS (Knowledge-Equation based Logical Processing System) to address these\nproblems. KELPS is an iterative framework for translating, synthesizing, and\nfiltering informal data into multiple formal languages (Lean, Coq, and\nIsabelle). First, we translate natural language into Knowledge Equations (KEs),\na novel language that we designed, theoretically grounded in assertional logic.\nNext, we convert them to target languages through rigorously defined rules that\npreserve both syntactic structure and semantic meaning. This process yielded a\nparallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic\naccuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3\n(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are\navailable in the supplementary materials.",
    "published": "2025-07-11T15:05:06Z",
    "updated": "2025-07-11T15:05:06Z",
    "id": "2507.08665v1",
    "authors": [
      "Jiyao Zhang",
      "Chengli Zhong",
      "Hui Xu",
      "Qige Li",
      "Yi Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08665v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08665v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08665v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a framework for translating informal mathematics into machine-verifiable theorems using large language models (LLMs), which aligns with the topics of LLMs and reasoning. The creation of a parallel corpus also touches on the topic of datasets.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Dataset"
    ]
  },
  "2507.08648v1": {
    "title": "DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets\n  from Real-World Images",
    "summary": "Common knowledge indicates that the process of constructing image datasets\nusually depends on the time-intensive and inefficient method of manual\ncollection and annotation. Large models offer a solution via data generation.\nNonetheless, real-world data are obviously more valuable comparing to\nartificially intelligence generated data, particularly in constructing image\ndatasets. For this reason, we propose a novel method for auto-constructing\ndatasets from real-world images by a multiagent collaborative system, named as\nDatasetAgent. By coordinating four different agents equipped with Multi-modal\nLarge Language Models (MLLMs), as well as a tool package for image\noptimization, DatasetAgent is able to construct high-quality image datasets\naccording to user-specified requirements. In particular, two types of\nexperiments are conducted, including expanding existing datasets and creating\nnew ones from scratch, on a variety of open-source datasets. In both cases,\nmultiple image datasets constructed by DatasetAgent are used to train various\nvision models for image classification, object detection, and image\nsegmentation.",
    "published": "2025-07-11T14:51:33Z",
    "updated": "2025-07-11T14:51:33Z",
    "id": "2507.08648v1",
    "authors": [
      "Haoran Sun",
      "Haoyu Bian",
      "Shaoning Zeng",
      "Yunbo Rao",
      "Xu Xu",
      "Lin Mei",
      "Jianping Gou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08648v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08648v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08648v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Multi-modal Large Language Models (MLLMs) in a multi-agent system to auto-construct datasets from real-world images, which aligns with the topics of MLLM and Dataset.",
    "llm_cls_result": [
      "MLLM",
      "Dataset"
    ]
  },
  "2507.08619v1": {
    "title": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design",
    "summary": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20\\%). Code compatibility peaked at 100\\% under\nspecific 2AS settings but averaged below 50\\% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted.",
    "published": "2025-07-11T14:19:05Z",
    "updated": "2025-07-11T14:19:05Z",
    "id": "2507.08619v1",
    "authors": [
      "Soheyl Massoudi",
      "Mark Fuge"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08619v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08619v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08619v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in a multi-agent system for engineering design, focusing on task continuity, executable model generation, and iterative reasoning. It involves LLMs in a structured workflow, which aligns with research on LLM applications and reasoning abilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "AGI"
    ]
  },
  "2507.08610v1": {
    "title": "Emergent Natural Language with Communication Games for Improving Image\n  Captioning Capabilities without Additional Data",
    "summary": "Image captioning is an important problem in developing various AI systems,\nand these tasks require large volumes of annotated images to train the models.\nSince all existing labelled datasets are already used for training the large\nVision Language Models (VLMs), it becomes challenging to improve the\nperformance of the same. Considering this, it is essential to consider the\nunsupervised image captioning performance, which remains relatively\nunder-explored. To that end, we propose LoGIC (Lewis Communication Game for\nImage Captioning), a Multi-agent Reinforcement Learning game. The proposed\nmethod consists of two agents, a 'speaker' and a 'listener', with the objective\nof learning a strategy for communicating in natural language. We train agents\nin the cooperative common-reward setting using the GRPO algorithm and show that\nimprovement in image captioning performance emerges as a consequence of the\nagents learning to play the game. We show that using pre-trained VLMs as the\n'speaker' and Large Language Model (LLM) for language understanding in the\n'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without\nadditional labels, a $2$ units advantage in absolute metrics compared to the\n$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the\n'speaker' with lightweight components: (i) a ViT for image perception and (ii)\na GPT2 language generation, and train them from scratch using LoGIC, obtaining\na $31$ BLEU score in the unsupervised setting, a $10$ points advantage over\nexisting unsupervised image-captioning methods.",
    "published": "2025-07-11T14:08:36Z",
    "updated": "2025-07-11T14:08:36Z",
    "id": "2507.08610v1",
    "authors": [
      "Parag Dutta",
      "Ambedkar Dukkipati"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08610v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08610v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08610v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving image captioning capabilities using a multi-agent reinforcement learning game (LoGIC) involving a 'speaker' and a 'listener'. It leverages pre-trained Vision Language Models (VLMs) and Large Language Models (LLMs), and discusses unsupervised image captioning performance. The core topics are related to Reinforcement Learning (RL) for training agents, Vision-Language Alignment (VLA) for image captioning, and the use of Large Language Models (LLM).",
    "llm_cls_result": [
      "RL",
      "VLA",
      "LLM"
    ]
  },
  "2507.10587v1": {
    "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language\n  Models is Missing",
    "summary": "Human users increasingly rely on natural language interactions with large\nlanguage models (LLMs) in order to receive help on a large variety of tasks and\nproblems. However, the trustworthiness and perceived legitimacy of LLMs is\nundermined by the fact that their output is frequently stated in very confident\nterms, even when its accuracy is questionable. Therefore, there is a need to\nsignal the confidence of the language model to a user in order to reap the\nbenefits of human-machine collaboration and mitigate potential harms.\nVerbalized uncertainty is the expression of confidence with linguistic means,\nan approach that integrates perfectly into language-based interfaces.\nNevertheless, most recent research in natural language processing (NLP)\noverlooks the nuances surrounding human uncertainty communication and the data\nbiases that influence machine uncertainty communication. We argue for\nanthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty\ncommunication requires a degree of linguistic authenticity and personalization\nto the user, which could be achieved by emulating human communication. We\npresent a thorough overview over the research in human uncertainty\ncommunication, survey ongoing research, and perform additional analyses to\ndemonstrate so-far overlooked biases in verbalized uncertainty. We conclude by\npointing out unique factors in human-machine communication of uncertainty and\ndeconstruct anthropomimetic uncertainty into future research directions for\nNLP.",
    "published": "2025-07-11T14:07:22Z",
    "updated": "2025-07-11T14:07:22Z",
    "id": "2507.10587v1",
    "authors": [
      "Dennis Ulmer",
      "Alexandra Lorson",
      "Ivan Titov",
      "Christian Hardmeier"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10587v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10587v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10587v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the communication of uncertainty in large language models (LLMs) and emphasizes the need for linguistic authenticity and personalization to enhance trustworthiness. This aligns with research on LLMs and their interaction with humans.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.10586v1": {
    "title": "AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight\n  Adapters",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable fluency across a\nrange of natural language tasks, yet remain vulnerable to hallucinations -\nfactual inaccuracies that undermine trust in real world deployment. We present\nAutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that\ntackles hallucination in large language models through lightweight LoRA-based\nadapters and KL-regularized training. Our pipeline integrates automated prompt\nrewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in\nretrieved evidence. A hallucination detection module, using both\nclassifier-based and self-evaluation techniques, assigns confidence scores to\ngenerated outputs, triggering an optional feedback correction loop. This loop\nenforces factual alignment via contrastive KL loss and adapter fine tuning. We\ndemonstrate that AutoRAG-LoRA significantly reduces the factual drift while\npreserving the efficiency and modularity of the model.",
    "published": "2025-07-11T14:02:58Z",
    "updated": "2025-07-11T14:02:58Z",
    "id": "2507.10586v1",
    "authors": [
      "Kaushik Dwivedi",
      "Padmanabh Patanjali Mishra"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10586v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10586v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10586v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on addressing hallucinations in Large Language Models (LLMs) using Retrieval-Augmented Generation (RAG) and lightweight LoRA-based adapters, which aligns with topics related to LLMs and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.08603v1": {
    "title": "Unlocking Speech Instruction Data Potential with Query Rewriting",
    "summary": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong\npotential in response latency and speech comprehension capabilities, showcasing\ngeneral intelligence across speech understanding tasks. However, the ability to\nfollow speech instructions has not been fully realized due to the lack of\ndatasets and heavily biased training tasks. Leveraging the rich ASR datasets,\nprevious approaches have used Large Language Models~(\\textbf{LLMs}) to continue\nthe linguistic information of speech to construct speech instruction datasets.\nYet, due to the gap between LLM-generated results and real human responses, the\ncontinuation methods further amplify these shortcomings. Given the high costs\nof collecting and annotating speech instruction datasets by humans, using\nspeech synthesis to construct large-scale speech instruction datasets has\nbecome a balanced and robust alternative. Although modern\nText-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis\nquality, it is challenging to appropriately convert out-of-distribution text\ninstruction to speech due to the limitations of the training data distribution\nin TTS models. To address this issue, we propose a query rewriting framework\nwith multi-LLM knowledge fusion, employing multiple agents to annotate and\nvalidate the synthesized speech, making it possible to construct high-quality\nspeech instruction datasets without relying on human annotation. Experiments\nshow that this method can transform text instructions into distributions more\nsuitable for TTS models for speech synthesis through zero-shot rewriting,\nincreasing data usability from 72\\% to 93\\%. It also demonstrates unique\nadvantages in rewriting tasks that require complex knowledge and\ncontext-related abilities.",
    "published": "2025-07-11T13:55:45Z",
    "updated": "2025-07-11T13:55:45Z",
    "id": "2507.08603v1",
    "authors": [
      "Yonghua Hei",
      "Yibo Yan",
      "Shuliang Liu",
      "Huiyu Zhou",
      "Linfeng Zhang",
      "Xuming Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08603v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08603v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08603v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) and speech synthesis to construct speech instruction datasets, which involves LLMs and dataset creation.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.08590v1": {
    "title": "Visual Semantic Description Generation with MLLMs for Image-Text\n  Matching",
    "summary": "Image-text matching (ITM) aims to address the fundamental challenge of\naligning visual and textual modalities, which inherently differ in their\nrepresentations, continuous, high-dimensional image features vs. discrete,\nstructured text. We propose a novel framework that bridges the modality gap by\nleveraging multimodal large language models (MLLMs) as visual semantic parsers.\nBy generating rich Visual Semantic Descriptions (VSD), MLLMs provide semantic\nanchor that facilitate cross-modal alignment. Our approach combines: (1)\nInstance-level alignment by fusing visual features with VSD to enhance the\nlinguistic expressiveness of image representations, and (2) Prototype-level\nalignment through VSD clustering to ensure category-level consistency. These\nmodules can be seamlessly integrated into existing ITM models. Extensive\nexperiments on Flickr30K and MSCOCO demonstrate substantial performance\nimprovements. The approach also exhibits remarkable zero-shot generalization to\ncross-domain tasks, including news and remote sensing ITM. The code and model\ncheckpoints are available at https://github.com/Image-Text-Matching/VSD.",
    "published": "2025-07-11T13:38:01Z",
    "updated": "2025-07-11T13:38:01Z",
    "id": "2507.08590v1",
    "authors": [
      "Junyu Chen",
      "Yihua Gao",
      "Mingyong Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08590v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08590v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08590v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging multimodal large language models (MLLMs) for image-text matching, which involves aligning visual and textual modalities. This directly relates to the topics of MLLM (Multimodal Large Language Models) and VLA (Vision-Language Alignment models).",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.08584v1": {
    "title": "To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk\n  Improves Trading Decisions",
    "summary": "Large language models (LLMs) are increasingly deployed in agentic frameworks,\nin which prompts trigger complex tool-based analysis in pursuit of a goal.\nWhile these frameworks have shown promise across multiple domains including in\nfinance, they typically lack a principled model-building step, relying instead\non sentiment- or trend-based analysis. We address this gap by developing an\nagentic system that uses LLMs to iteratively discover stochastic differential\nequations for financial time series. These models generate risk metrics which\ninform daily trading decisions. We evaluate our system in both traditional\nbacktests and using a market simulator, which introduces synthetic but causally\nplausible price paths and news events. We find that model-informed trading\nstrategies outperform standard LLM-based agents, improving Sharpe ratios across\nmultiple equities. Our results show that combining LLMs with agentic model\ndiscovery enhances market risk estimation and enables more profitable trading\ndecisions.",
    "published": "2025-07-11T13:29:32Z",
    "updated": "2025-07-11T13:29:32Z",
    "id": "2507.08584v1",
    "authors": [
      "Dimitrios Emmanoulopoulos",
      "Ollie Olby",
      "Justin Lyon",
      "Namid R. Stillman"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08584v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08584v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08584v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in an agentic framework for financial applications, specifically for market risk estimation and trading decisions. This aligns with the topics of LLM and RL (Reinforcement Learning) due to the agentic approach and the use of LLMs in decision-making processes.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.08575v1": {
    "title": "Large Multi-modal Model Cartographic Map Comprehension for Textual\n  Locality Georeferencing",
    "summary": "Millions of biological sample records collected in the last few centuries\narchived in natural history collections are un-georeferenced. Georeferencing\ncomplex locality descriptions associated with these collection samples is a\nhighly labour-intensive task collection agencies struggle with. None of the\nexisting automated methods exploit maps that are an essential tool for\ngeoreferencing complex relations. We present preliminary experiments and\nresults of a novel method that exploits multi-modal capabilities of recent\nLarge Multi-Modal Models (LMM). This method enables the model to visually\ncontextualize spatial relations it reads in the locality description. We use a\ngrid-based approach to adapt these auto-regressive models for this task in a\nzero-shot setting. Our experiments conducted on a small manually annotated\ndataset show impressive results for our approach ($\\sim$1 km Average distance\nerror) compared to uni-modal georeferencing with Large Language Models and\nexisting georeferencing tools. The paper also discusses the findings of the\nexperiments in light of an LMM's ability to comprehend fine-grained maps.\nMotivated by these results, a practical framework is proposed to integrate this\nmethod into a georeferencing workflow.",
    "published": "2025-07-11T13:23:25Z",
    "updated": "2025-07-11T13:23:25Z",
    "id": "2507.08575v1",
    "authors": [
      "Kalana Wijegunarathna",
      "Kristin Stock",
      "Christopher B. Jones"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08575v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08575v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08575v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Multi-Modal Models (LMM) for georeferencing tasks, which involves integrating vision and language modalities to comprehend maps and textual descriptions. This aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Alignment (VLA).",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.08567v1": {
    "title": "AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient\n  Sequence Modeling",
    "summary": "We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a\nnovel recursive generalization of the encoder-only Transformer architecture,\nwhich achieves better perplexity than a standard Transformer and allows for the\ndynamic scaling of compute resources at test time. This simple, recursive\napproach is a complement to scaling large language model (LLM) performance\nthrough parameter and token counts. AbbIE performs its iterations in latent\nspace, but unlike latent reasoning models, does not require a specialized\ndataset or training protocol. We show that AbbIE upward generalizes (ability to\ngeneralize to arbitrary iteration lengths) at test time by only using 2\niterations during train time, far outperforming alternative iterative methods.\nAbbIE's ability to scale its computational expenditure based on the complexity\nof the task gives it an up to \\textbf{12\\%} improvement in zero-shot in-context\nlearning tasks versus other iterative and standard methods and up to 5\\%\nimprovement in language perplexity. The results from this study open a new\navenue to Transformer performance scaling. We perform all of our evaluations on\nmodel sizes up to 350M parameters.",
    "published": "2025-07-11T13:11:11Z",
    "updated": "2025-07-11T13:11:11Z",
    "id": "2507.08567v1",
    "authors": [
      "Preslav Aleksandrov",
      "Meghdad Kurmanji",
      "Fernando Garcia Redondo",
      "David O'Shea",
      "William Shen",
      "Alex Iacob",
      "Lorenzo Sani",
      "Xinchi Qiu",
      "Nicola Cancedda",
      "Nicholas D. Lane"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08567v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08567v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08567v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a novel encoder-only Transformer architecture that improves perplexity and computational efficiency, which is relevant to scaling and improving LLM performance.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.10585v1": {
    "title": "A Taxonomy for Design and Evaluation of Prompt-Based Natural Language\n  Explanations",
    "summary": "Effective AI governance requires structured approaches for stakeholders to\naccess and verify AI system behavior. With the rise of large language models,\nNatural Language Explanations (NLEs) are now key to articulating model\nbehavior, which necessitates a focused examination of their characteristics and\ngovernance implications. We draw on Explainable AI (XAI) literature to create\nan updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:\n(1) Context, including task, data, audience, and goals; (2) Generation and\nPresentation, covering generation methods, inputs, interactivity, outputs, and\nforms; and (3) Evaluation, focusing on content, presentation, and user-centered\nproperties, as well as the setting of the evaluation. This taxonomy provides a\nframework for researchers, auditors, and policymakers to characterize, design,\nand enhance NLEs for transparent AI systems.",
    "published": "2025-07-11T12:52:19Z",
    "updated": "2025-07-11T12:52:19Z",
    "id": "2507.10585v1",
    "authors": [
      "Isar Nejadgholi",
      "Mona Omidyeganeh",
      "Marc-Antoine Drouin",
      "Jonathan Boisvert"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10585v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10585v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10585v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Natural Language Explanations (NLEs) in the context of large language models and their governance, which aligns with the 'LLM' and 'Reasoning' topics. It also discusses evaluation frameworks, which is relevant to 'Benchmark'.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.10584v1": {
    "title": "ARPaCCino: An Agentic-RAG for Policy as Code Compliance",
    "summary": "Policy as Code (PaC) is a paradigm that encodes security and compliance\npolicies into machine-readable formats, enabling automated enforcement in\nInfrastructure as Code (IaC) environments. However, its adoption is hindered by\nthe complexity of policy languages and the risk of misconfigurations. In this\nwork, we present ARPaCCino, an agentic system that combines Large Language\nModels (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation\nto automate the generation and verification of PaC rules. Given natural\nlanguage descriptions of the desired policies, ARPaCCino generates formal Rego\nrules, assesses IaC compliance, and iteratively refines the IaC configurations\nto ensure conformance. Thanks to its modular agentic architecture and\nintegration with external tools and knowledge bases, ARPaCCino supports policy\nvalidation across a wide range of technologies, including niche or emerging IaC\nframeworks. Experimental evaluation involving a Terraform-based case study\ndemonstrates ARPaCCino's effectiveness in generating syntactically and\nsemantically correct policies, identifying non-compliant infrastructures, and\napplying corrective modifications, even when using smaller, open-weight LLMs.\nOur results highlight the potential of agentic RAG architectures to enhance the\nautomation, reliability, and accessibility of PaC workflows.",
    "published": "2025-07-11T12:36:33Z",
    "updated": "2025-07-11T12:36:33Z",
    "id": "2507.10584v1",
    "authors": [
      "Francesco Romeo",
      "Luigi Arena",
      "Francesco Blefari",
      "Francesco Aurelio Pironti",
      "Matteo Lupinacci",
      "Angelo Furfaro"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10584v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10584v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10584v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) and Retrieval-Augmented-Generation (RAG) in an agentic system for Policy as Code (PaC) compliance, which aligns with the topics of LLM and Memory (due to the use of RAG). The agentic architecture also hints at Reinforcement Learning (RL) aspects, though not explicitly stated.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "RL"
    ]
  },
  "2507.08529v2": {
    "title": "A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge\n  Graph Fusion Framework for Rare Disease Diagnosis",
    "summary": "Rare disease diagnosis remains challenging for medical large language models\ndue to insufficient knowledge representation, limited concept understanding,\nand constrained clinical reasoning. We propose a framework combining\nmulti-granularity sparse activation with hierarchical knowledge graphs. Our\napproach employs four complementary matching algorithms with diversity control\nand a five-level fallback strategy for precise concept activation. A\nthree-layer knowledge graph (taxonomy, clinical features, instances) provides\nstructured, up-to-date context. Experiments on the BioASQ rare disease dataset\ndemonstrate significant improvements: BLEU scores increased by up to 0.13,\nROUGE by up to 0.10, and diagnostic accuracy by up to 0.25, with the best model\nachieving 0.92 accuracy--surpassing the 0.90 clinical threshold. Expert\nevaluation confirms enhancements in information quality, reasoning, and\nprofessional expression. Our framework shows promise in reducing the diagnostic\nodyssey for rare disease patients.",
    "published": "2025-07-11T12:26:19Z",
    "updated": "2025-07-22T14:23:04Z",
    "id": "2507.08529v2",
    "authors": [
      "Mingda Zhang",
      "Na Zhao",
      "Jianglong Qin",
      "Guoyu Ye",
      "Ruixiang Tang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08529v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08529v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08529v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing medical large language models through sparse activation and knowledge graphs for rare disease diagnosis, which aligns with the topics of LLM (Large Language Models) and Reasoning (clinical reasoning abilities). The use of knowledge graphs also suggests relevance to Memory (retrieval and structured knowledge integration).",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2507.08523v1": {
    "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
    "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
    "published": "2025-07-11T12:21:29Z",
    "updated": "2025-07-11T12:21:29Z",
    "id": "2507.08523v1",
    "authors": [
      "Yilun Wang",
      "Pengfei Chen",
      "Haiyu Huang",
      "Zilong He",
      "Gou Tan",
      "Chuanfu Zhang",
      "Jingkai He",
      "Zibin Zheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08523v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08523v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08523v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing LLM inference for log parsing, which involves improving the efficiency and performance of LLM deployment in specific applications. The core topics are related to LLM inference optimization and practical deployment challenges.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.10582v1": {
    "title": "Transforming Sensitive Documents into Quantitative Data: An AI-Based\n  Preprocessing Toolchain for Structured and Privacy-Conscious Analysis",
    "summary": "Unstructured text from legal, medical, and administrative sources offers a\nrich but underutilized resource for research in public health and the social\nsciences. However, large-scale analysis is hampered by two key challenges: the\npresence of sensitive, personally identifiable information, and significant\nheterogeneity in structure and language. We present a modular toolchain that\nprepares such text data for embedding-based analysis, relying entirely on\nopen-weight models that run on local hardware, requiring only a\nworkstation-level GPU and supporting privacy-sensitive research.\n  The toolchain employs large language model (LLM) prompting to standardize,\nsummarize, and, when needed, translate texts to English for greater\ncomparability. Anonymization is achieved via LLM-based redaction, supplemented\nwith named entity recognition and rule-based methods to minimize the risk of\ndisclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court\ndecisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.\nEach document is processed into an anonymized, standardized summary and\ntransformed into a document-level embedding. Validation, including manual\nreview, automated scanning, and predictive evaluation shows the toolchain\neffectively removes identifying information while retaining semantic content.\nAs an illustrative application, we train a predictive model using embedding\nvectors derived from a small set of manually labeled summaries, demonstrating\nthe toolchain's capacity for semi-automated content analysis at scale.\n  By enabling structured, privacy-conscious analysis of sensitive documents,\nour toolchain opens new possibilities for large-scale research in domains where\ntextual data was previously inaccessible due to privacy and heterogeneity\nconstraints.",
    "published": "2025-07-11T11:58:36Z",
    "updated": "2025-07-11T11:58:36Z",
    "id": "2507.10582v1",
    "authors": [
      "Anders Ledberg",
      "Anna Thaln"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10582v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10582v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10582v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for preprocessing and anonymizing sensitive documents, which involves LLM prompting and redaction. The focus is on leveraging LLMs for structured and privacy-conscious analysis, making 'LLM' the most relevant topic. The application involves processing legal documents, which does not directly align with other specific topics like RL, MLLM, VLA, etc.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.08498v1": {
    "title": "Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop",
    "summary": "Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic\nmodel used for uncovering abstract topics within document collections. In this\npaper, we explore the effectiveness of augmenting topic models with Large\nLanguage Models (LLMs) through integration into two key phases: Initialization\nand Post-Correction. Since the LDA is highly dependent on the quality of its\ninitialization, we conduct extensive experiments on the LLM-guided topic\nclustering for initializing the Gibbs sampling algorithm. Interestingly, the\nexperimental results reveal that while the proposed initialization strategy\nimproves the early iterations of LDA, it has no effect on the convergence and\nyields the worst performance compared to the baselines. The LLM-enabled\npost-correction, on the other hand, achieved a promising improvement of 5.86%\nin the coherence evaluation. These results highlight the practical benefits of\nthe LLM-in-the-loop approach and challenge the belief that LLMs are always the\nsuperior text mining alternative.",
    "published": "2025-07-11T11:20:39Z",
    "updated": "2025-07-11T11:20:39Z",
    "id": "2507.08498v1",
    "authors": [
      "Mengze Hong",
      "Chen Jason Zhang",
      "Di Jiang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08498v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08498v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08498v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) into the topic modeling process, specifically in the initialization and post-correction phases of Latent Dirichlet Allocation (LDA). This directly relates to research on LLMs and their applications in text mining and topic modeling.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.10579v1": {
    "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors",
    "summary": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain.",
    "published": "2025-07-11T10:57:36Z",
    "updated": "2025-07-11T10:57:36Z",
    "id": "2507.10579v1",
    "authors": [
      "Ekaterina Kochmar",
      "Kaushal Kumar Maurya",
      "Kseniia Petukhova",
      "KV Aditya Srivatsa",
      "Anas Tack",
      "Justin Vasselli"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10579v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10579v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10579v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on assessing the pedagogical abilities of AI tutors powered by large language models (LLMs), which directly relates to research on Large Language Models (LLM) and their applications in educational contexts. The task involves evaluating LLM performance, which aligns with the Benchmark category as it involves benchmarking LLMs in a specific domain.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.08477v1": {
    "title": "ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual\n  Speech Recognition",
    "summary": "The deep integration of large language models and automatic speech\nrecognition systems has become a promising research direction with high\npractical value. To address the overfitting issue commonly observed in Low-Rank\nAdaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work\nproposes an innovative training paradigm Iterative LoRA Training (ILT) in\ncombination with an Iterative Pseudo Labeling strategy, effectively enhancing\nthe theoretical upper bound of model performance. Based on Whisper-large-v3 and\nQwen2-Audio, we conduct systematic experiments using a three-stage training\nprocess: Focus Training, Feed Back Training, and Fix Training. Experimental\nresults demonstrate the effectiveness of the proposed method. Furthermore, the\nMegaAIS research team applied this technique in the Interspeech 2025\nMultilingual Conversational Speech Language Modeling Challenge (MLC-SLM),\nachieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2\n(Speech Separation and Recognition Task), showcasing the practical feasibility\nand strong application potential of our approach.",
    "published": "2025-07-11T10:38:51Z",
    "updated": "2025-07-11T10:38:51Z",
    "id": "2507.08477v1",
    "authors": [
      "Qingliang Meng",
      "Hao Wu",
      "Wei Liang",
      "Wei Xu",
      "Qing Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08477v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08477v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08477v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the integration of large language models with automatic speech recognition systems, specifically addressing overfitting in LoRA during supervised fine-tuning. It proposes an innovative training paradigm and demonstrates its effectiveness in multilingual speech recognition tasks.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Pretrain"
    ]
  },
  "2507.08468v1": {
    "title": "Using Large Language Models for Legal Decision-Making in Austrian\n  Value-Added Tax Law: An Experimental Study",
    "summary": "This paper provides an experimental evaluation of the capability of large\nlanguage models (LLMs) to assist in legal decision-making within the framework\nof Austrian and European Union value-added tax (VAT) law. In tax consulting\npractice, clients often describe cases in natural language, making LLMs a prime\ncandidate for supporting automated decision-making and reducing the workload of\ntax professionals. Given the requirement for legally grounded and\nwell-justified analyses, the propensity of LLMs to hallucinate presents a\nconsiderable challenge. The experiments focus on two common methods for\nenhancing LLM performance: fine-tuning and retrieval-augmented generation\n(RAG). In this study, these methods are applied on both textbook cases and\nreal-world cases from a tax consulting firm to systematically determine the\nbest configurations of LLM-based systems and assess the legal-reasoning\ncapabilities of LLMs. The findings highlight the potential of using LLMs to\nsupport tax consultants by automating routine tasks and providing initial\nanalyses, although current prototypes are not ready for full automation due to\nthe sensitivity of the legal domain. The findings indicate that LLMs, when\nproperly configured, can effectively support tax professionals in VAT tasks and\nprovide legally grounded justifications for decisions. However, limitations\nremain regarding the handling of implicit client knowledge and context-specific\ndocumentation, underscoring the need for future integration of structured\nbackground information.",
    "published": "2025-07-11T10:19:56Z",
    "updated": "2025-07-11T10:19:56Z",
    "id": "2507.08468v1",
    "authors": [
      "Marina Luketina",
      "Andrea Benkel",
      "Christoph G. Schuetz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08468v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08468v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08468v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in legal decision-making, specifically in the context of Austrian and European Union value-added tax law. It explores methods like fine-tuning and retrieval-augmented generation (RAG) to enhance LLM performance, which aligns with the topics of LLM research and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.08459v1": {
    "title": "Diagnosing Failures in Large Language Models' Answers: Integrating Error\n  Attribution into Evaluation Framework",
    "summary": "With the widespread application of Large Language Models (LLMs) in various\ntasks, the mainstream LLM platforms generate massive user-model interactions\ndaily. In order to efficiently analyze the performance of models and diagnose\nfailures in their answers, it is essential to develop an automated framework to\nsystematically categorize and attribute errors. However, existing evaluation\nmodels lack error attribution capability. In this work, we establish a\ncomprehensive Misattribution Framework with 6 primary and 15 secondary\ncategories to facilitate in-depth analysis. Based on this framework, we present\nAttriData, a dataset specifically designed for error attribution, encompassing\nmisattribution, along with the corresponding scores and feedback. We also\npropose MisAttributionLLM, a fine-tuned model on AttriData, which is the first\ngeneral-purpose judge model capable of simultaneously generating score,\nmisattribution, and feedback. Extensive experiments and analyses are conducted\nto confirm the effectiveness and robustness of our proposed method.",
    "published": "2025-07-11T10:02:21Z",
    "updated": "2025-07-11T10:02:21Z",
    "id": "2507.08459v1",
    "authors": [
      "Zishan Xu",
      "Shuyi Xie",
      "Qingsong Lv",
      "Shupei Xiao",
      "Linlin Song",
      "Sui Wenjuan",
      "Fan Lin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08459v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08459v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08459v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating and diagnosing failures in Large Language Models (LLMs) through an automated framework, which involves error attribution and dataset creation. This aligns with the topics of Benchmark (evaluation metrics and performance comparison) and Dataset (LLM datasets and evaluation datasets).",
    "llm_cls_result": [
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.10576v1": {
    "title": "Can Large Language Models Understand As Well As Apply Patent Regulations\n  to Pass a Hands-On Patent Attorney Test?",
    "summary": "The legal field already uses various large language models (LLMs) in actual\napplications, but their quantitative performance and reasons for it are\nunderexplored. We evaluated several open-source and proprietary LLMs --\nincluding GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of\nthe European Qualifying Examination (EQE) for future European Patent Attorneys.\nOpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web\nServices) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama\n3.1 8B scored 0.55. The latter two are within the range of mere guessing for\nthe two-answer forced-choice design. None of the evaluated models could have\npassed the examination fully, as accuracy never exceeded the average threshold\nof 0.90 required for professional-level standards -- also not models that are\nregularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level\nperformance. GPT-4o excelled at integrating text and graphics, while Claude 3\nOpus often lost formatting coherence. Human patent experts evaluated the\ntextual justifications and uncovered various critical shortcomings of each\nmodel. They valued clarity and legal rationale over the raw correctness of the\nanswers, which revealed misalignment between automatic metrics and expert\njudgment. Model outputs were sensitive to modest temperature changes and prompt\nwording, which underscores the remaining necessity of expert oversight. Future\nwork should target logical consistency, robust multimodality, and adaptive\nprompting to approach human-level patent proficiency. In summary, despite the\noutstanding performance of recent large models, the general public might\noverestimate their performance. The field has a long way to go to develop a\nvirtual patent attorney. This paper wants to point out several specific\nlimitations that need solutions.",
    "published": "2025-07-11T09:42:23Z",
    "updated": "2025-07-11T09:42:23Z",
    "id": "2507.10576v1",
    "authors": [
      "Bhakti Khera",
      "Rezvan Alamian",
      "Pascal A. Scherz",
      "Stephan M. Goetz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10576v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10576v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10576v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper evaluates the performance of various LLMs in understanding and applying patent regulations, focusing on their ability to pass a professional patent attorney test. It discusses the limitations and shortcomings of these models in a legal context, which aligns with the 'LLM' and 'Benchmark' topics as it involves evaluating LLMs against specific professional standards.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.08440v1": {
    "title": "Finding Common Ground: Using Large Language Models to Detect Agreement\n  in Multi-Agent Decision Conferences",
    "summary": "Decision conferences are structured, collaborative meetings that bring\ntogether experts from various fields to address complex issues and reach a\nconsensus on recommendations for future actions or policies. These conferences\noften rely on facilitated discussions to ensure productive dialogue and\ncollective agreement. Recently, Large Language Models (LLMs) have shown\nsignificant promise in simulating real-world scenarios, particularly through\ncollaborative multi-agent systems that mimic group interactions. In this work,\nwe present a novel LLM-based multi-agent system designed to simulate decision\nconferences, specifically focusing on detecting agreement among the participant\nagents. To achieve this, we evaluate six distinct LLMs on two tasks: stance\ndetection, which identifies the position an agent takes on a given issue, and\nstance polarity detection, which identifies the sentiment as positive,\nnegative, or neutral. These models are further assessed within the multi-agent\nsystem to determine their effectiveness in complex simulations. Our results\nindicate that LLMs can reliably detect agreement even in dynamic and nuanced\ndebates. Incorporating an agreement-detection agent within the system can also\nimprove the efficiency of group debates and enhance the overall quality and\ncoherence of deliberations, making them comparable to real-world decision\nconferences regarding outcome and decision-making. These findings demonstrate\nthe potential for LLM-based multi-agent systems to simulate group\ndecision-making processes. They also highlight that such systems could be\ninstrumental in supporting decision-making with expert elicitation workshops\nacross various domains.",
    "published": "2025-07-11T09:31:10Z",
    "updated": "2025-07-11T09:31:10Z",
    "id": "2507.08440v1",
    "authors": [
      "Selina Heller",
      "Mohamed Ibrahim",
      "David Antony Selby",
      "Sebastian Vollmer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08440v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08440v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08440v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in simulating multi-agent decision conferences, focusing on detecting agreement among participant agents. This aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning with Human Feedback), as it involves multi-agent systems and collaborative decision-making processes.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.08432v1": {
    "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
    "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
    "published": "2025-07-11T09:18:41Z",
    "updated": "2025-07-11T09:18:41Z",
    "id": "2507.08432v1",
    "authors": [
      "Gustavo Correa Publio",
      "Jos Emilio Labra Gayo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08432v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08432v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08432v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) and retrieval-augmented generation (RAG) to improve the explainability of SHACL validation, which aligns with the topics of LLM and Memory.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.08425v1": {
    "title": "A Survey of Large Language Models in Discipline-specific Research:\n  Challenges, Methods and Opportunities",
    "summary": "Large Language Models (LLMs) have demonstrated their transformative potential\nacross numerous disciplinary studies, reshaping the existing research\nmethodologies and fostering interdisciplinary collaboration. However, a\nsystematic understanding of their integration into diverse disciplines remains\nunderexplored. This survey paper provides a comprehensive overview of the\napplication of LLMs in interdisciplinary studies, categorising research efforts\nfrom both a technical perspective and with regard to their applicability. From\na technical standpoint, key methodologies such as supervised fine-tuning,\nretrieval-augmented generation, agent-based approaches, and tool-use\nintegration are examined, which enhance the adaptability and effectiveness of\nLLMs in discipline-specific contexts. From the perspective of their\napplicability, this paper explores how LLMs are contributing to various\ndisciplines including mathematics, physics, chemistry, biology, and the\nhumanities and social sciences, demonstrating their role in discipline-specific\ntasks. The prevailing challenges are critically examined and the promising\nresearch directions are highlighted alongside the recent advances in LLMs. By\nproviding a comprehensive overview of the technical developments and\napplications in this field, this survey aims to serve as an invaluable resource\nfor the researchers who are navigating the complex landscape of LLMs in the\ncontext of interdisciplinary studies.",
    "published": "2025-07-11T09:11:18Z",
    "updated": "2025-07-11T09:11:18Z",
    "id": "2507.08425v1",
    "authors": [
      "Lu Xiang",
      "Yang Zhao",
      "Yaping Zhang",
      "Chengqing Zong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08425v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08425v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08425v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the application and challenges of Large Language Models (LLMs) in various disciplines, focusing on their technical methodologies and interdisciplinary applicability. It aligns with the topics of LLM research and their applications.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.08410v1": {
    "title": "Multi-modal Mutual-Guidance Conditional Prompt Learning for\n  Vision-Language Models",
    "summary": "Prompt learning facilitates the efficient adaptation of Vision-Language\nModels (VLMs) to various downstream tasks. However, it faces two significant\nchallenges: (1) inadequate modeling of class embedding distributions for unseen\ninstances, leading to suboptimal generalization on novel classes; (2)\nprevailing methodologies predominantly confine cross-modal alignment to the\nfinal output layer of vision and text encoders, which fundamentally limits\ntheir capacity to preserve topological consistency with pre-trained multi-modal\nembedding spaces. To this end, we introduce MuGCP (Multi-modal Mutual-Guidance\nConditional Prompt Learning), a novel paradigm designed for conditional prompt\ngeneration. MuGCP leverages Multi-modal Large Language Models (MLLMs) as\nconditional prompt learners to adaptively generate Semantic Conditional Prompts\n(SCP) that incorporate rich, fine-grained high-level semantic knowledge for\nimage instances. To ensure effective alignment and interaction across the\nmulti-modal space of Vision-Language Models (VLMs), we introduce the Attention\nMutual-Guidance (AMG) module, which facilitates interactions between visual and\nsemantic information. Through mutual guidance, the AMG module generates Visual\nConditional Prompts (VCP), enhancing the model's performance in multi-modal\ntasks. Additionally, we present a Multi-Prompt Fusion (MPF) mechanism that\nintegrates SCP and VCP with contextual prompts, ensuring seamless coordination\namong the different prompts and enhancing the modeling of class embeddings and\ninstance-specific knowledge. Our MuGCP outperforms existing state-of-the-art\nmethods on 14 different datasets. The code will be made available after\npublication.",
    "published": "2025-07-11T08:45:27Z",
    "updated": "2025-07-11T08:45:27Z",
    "id": "2507.08410v1",
    "authors": [
      "Shijun Yang",
      "Xiang Zhang",
      "Wanqing Zhao",
      "Hangzai Luo",
      "Sheng Zhong",
      "Jinye Peng",
      "Jianping Fan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08410v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08410v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08410v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on multi-modal prompt learning for Vision-Language Models (VLMs), leveraging Multi-modal Large Language Models (MLLMs) and introducing mechanisms for cross-modal alignment and interaction. The core topics are MLLM (for multi-modal integration) and VLA (for vision-language alignment and interaction).",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.08367v1": {
    "title": "Understanding Driving Risks using Large Language Models: Toward Elderly\n  Driver Assessment",
    "summary": "This study investigates the potential of a multimodal large language model\n(LLM), specifically ChatGPT-4o, to perform human-like interpretations of\ntraffic scenes using static dashcam images. Herein, we focus on three judgment\ntasks relevant to elderly driver assessments: evaluating traffic density,\nassessing intersection visibility, and recognizing stop signs recognition.\nThese tasks require contextual reasoning rather than simple object detection.\nUsing zero-shot, few-shot, and multi-shot prompting strategies, we evaluated\nthe performance of the model with human annotations serving as the reference\nstandard. Evaluation metrics included precision, recall, and F1-score. Results\nindicate that prompt design considerably affects performance, with recall for\nintersection visibility increasing from 21.7% (zero-shot) to 57.0%\n(multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In\nstop-sign detection, the model demonstrated high precision (up to 86.3%) but a\nlower recall (approximately 76.7%), indicating a conservative response\ntendency. Output stability analysis revealed that humans and the model faced\ndifficulties interpreting structurally ambiguous scenes. However, the model's\nexplanatory texts corresponded with its predictions, enhancing\ninterpretability. These findings suggest that, with well-designed prompts, LLMs\nhold promise as supportive tools for scene-level driving risk assessments.\nFuture studies should explore scalability using larger datasets, diverse\nannotators, and next-generation model architectures for elderly driver\nassessments.",
    "published": "2025-07-11T07:28:49Z",
    "updated": "2025-07-11T07:28:49Z",
    "id": "2507.08367v1",
    "authors": [
      "Yuki Yoshihara",
      "Linjing Jiang",
      "Nihan Karatas",
      "Hitoshi Kanamori",
      "Asuka Harada",
      "Takahiro Tanaka"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08367v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08367v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08367v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a multimodal large language model (ChatGPT-4o) for interpreting traffic scenes, which involves contextual reasoning and multimodal inputs. The focus on LLM capabilities and multimodal integration aligns with the topics of LLM and MLLM.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.08362v1": {
    "title": "Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN\n  Model Generation from Text",
    "summary": "Efficient planning, resource management, and consistent operations often rely\non converting textual process documents into formal Business Process Model and\nNotation (BPMN) models. However, this conversion process remains time-intensive\nand costly. Existing approaches, whether rule-based or machine-learning-based,\nstill struggle with writing styles and often fail to identify parallel\nstructures in process descriptions.\n  This paper introduces an automated pipeline for extracting BPMN models from\ntext, leveraging the use of machine learning and large language models. A key\ncontribution of this work is the introduction of a newly annotated dataset,\nwhich significantly enhances the training process. Specifically, we augment the\nPET dataset with 15 newly annotated documents containing 32 parallel gateways\nfor model training, a critical feature often overlooked in existing datasets.\nThis addition enables models to better capture parallel structures, a common\nbut complex aspect of process descriptions. The proposed approach demonstrates\nadequate performance in terms of reconstruction accuracy, offering a promising\nfoundation for organizations to accelerate BPMN model creation.",
    "published": "2025-07-11T07:25:55Z",
    "updated": "2025-07-11T07:25:55Z",
    "id": "2507.08362v1",
    "authors": [
      "Phuong Nam L",
      "Charlotte Schneider-Depr",
      "Alexandre Goossens",
      "Alexander Stevens",
      "Aurlie Leribaux",
      "Johannes De Smedt"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08362v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08362v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08362v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of machine learning and large language models for converting textual process documents into BPMN models, but it does not specifically focus on any of the provided topics related to LLMs, RL, MLLM, etc. The primary focus is on the application of machine learning in a specific domain (BPMN model generation) rather than the core topics listed.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.08350v1": {
    "title": "Exploring Design of Multi-Agent LLM Dialogues for Research Ideation",
    "summary": "Large language models (LLMs) are increasingly used to support creative tasks\nsuch as research idea generation. While recent work has shown that structured\ndialogues between LLMs can improve the novelty and feasibility of generated\nideas, the optimal design of such interactions remains unclear. In this study,\nwe conduct a comprehensive analysis of multi-agent LLM dialogues for scientific\nideation. We compare different configurations of agent roles, number of agents,\nand dialogue depth to understand how these factors influence the novelty and\nfeasibility of generated ideas. Our experimental setup includes settings where\none agent generates ideas and another critiques them, enabling iterative\nimprovement. Our results show that enlarging the agent cohort, deepening the\ninteraction depth, and broadening agent persona heterogeneity each enrich the\ndiversity of generated ideas. Moreover, specifically increasing critic-side\ndiversity within the ideation-critique-revision loop further boosts the\nfeasibility of the final proposals. Our findings offer practical guidelines for\nbuilding effective multi-agent LLM systems for scientific ideation. Our code is\navailable at https://github.com/g6000/MultiAgent-Research-Ideator.",
    "published": "2025-07-11T06:53:46Z",
    "updated": "2025-07-11T06:53:46Z",
    "id": "2507.08350v1",
    "authors": [
      "Keisuke Ueda",
      "Wataru Hirota",
      "Takuto Asakura",
      "Takahiro Omi",
      "Kosuke Takahashi",
      "Kosuke Arima",
      "Tatsuya Ishigaki"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08350v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08350v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08350v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in multi-agent dialogues for research ideation, focusing on the design and optimization of such interactions to improve idea generation. This aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning, as the iterative improvement process can be seen as a form of reinforcement learning with feedback loops).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.08325v1": {
    "title": "CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template\n  Generation",
    "summary": "In e-commerce private-domain channels such as instant messaging and e-mail,\nmerchants engage customers directly as part of their Customer Relationship\nManagement (CRM) programmes to drive retention and conversion. While a few top\nperformers excel at crafting outbound messages, most merchants struggle to\nwrite persuasive copy because they lack both expertise and scalable tools. We\nintroduce CRMAgent, a multi-agent system built on large language models (LLMs)\nthat generates high-quality message templates and actionable writing guidance\nthrough three complementary modes. First, group-based learning enables the\nagent to learn from a merchant's own top-performing messages within the same\naudience segment and rewrite low-performing ones. Second,\nretrieval-and-adaptation fetches templates that share the same audience segment\nand exhibit high similarity in voucher type and product category, learns their\nsuccessful patterns, and adapts them to the current campaign. Third, a\nrule-based fallback provides a lightweight zero-shot rewrite when no suitable\nreferences are available. Extensive experiments show that CRMAgent consistently\noutperforms merchants' original templates, delivering significant gains in both\naudience-match and marketing-effectiveness metrics.",
    "published": "2025-07-11T05:31:35Z",
    "updated": "2025-07-11T05:31:35Z",
    "id": "2507.08325v1",
    "authors": [
      "Yinzhu Quan",
      "Xinrui Li",
      "Ying Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08325v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08325v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08325v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multi-agent system built on large language models (LLMs) for generating e-commerce CRM message templates, which involves the use of LLMs in a specific application context.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.08310v1": {
    "title": "Generative AI in Science: Applications, Challenges, and Emerging\n  Questions",
    "summary": "This paper examines the impact of Generative Artificial Intelligence (GenAI)\non scientific practices, conducting a qualitative review of selected literature\nto explore its applications, benefits, and challenges. The review draws on the\nOpenAlex publication database, using a Boolean search approach to identify\nscientific literature related to GenAI (including large language models and\nChatGPT). Thirty-nine highly cited papers and commentaries are reviewed and\nqualitatively coded. Results are categorized by GenAI applications in science,\nscientific writing, medical practice, and education and training. The analysis\nfinds that while there is a rapid adoption of GenAI in science and science\npractice, its long-term implications remain unclear, with ongoing uncertainties\nabout its use and governance. The study provides early insights into GenAI's\ngrowing role in science and identifies questions for future research in this\nevolving field.",
    "published": "2025-07-11T05:02:24Z",
    "updated": "2025-07-11T05:02:24Z",
    "id": "2507.08310v1",
    "authors": [
      "Ryan Harries",
      "Cornelia Lawson",
      "Philip Shapira"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08310v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08310v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08310v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of Generative AI, including large language models (LLMs), on various scientific practices, which aligns with the 'LLM' and 'AGI' topics due to its focus on AI applications and potential general intelligence implications.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.08309v1": {
    "title": "Improving MLLM's Document Image Machine Translation via Synchronously\n  Self-reviewing Its OCR Proficiency",
    "summary": "Multimodal Large Language Models (MLLMs) have shown strong performance in\ndocument image tasks, especially Optical Character Recognition (OCR). However,\nthey struggle with Document Image Machine Translation (DIMT), which requires\nhandling both cross-modal and cross-lingual challenges. Previous efforts to\nenhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT\ndataset often result in the forgetting of the model's existing monolingual\nabilities, such as OCR. To address these challenges, we introduce a novel\nfine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR\nproficiency, inspired by the concept \"Bilingual Cognitive Advantage\".\nSpecifically, SSR prompts the model to generate OCR text before producing\ntranslation text, which allows the model to leverage its strong monolingual OCR\nability while learning to translate text across languages. Comprehensive\nexperiments demonstrate the proposed SSR learning helps mitigate catastrophic\nforgetting, improving the generalization ability of MLLMs on both OCR and DIMT\ntasks.",
    "published": "2025-07-11T05:02:06Z",
    "updated": "2025-07-11T05:02:06Z",
    "id": "2507.08309v1",
    "authors": [
      "Yupu Liang",
      "Yaping Zhang",
      "Zhiyang Zhang",
      "Zhiyuan Chen",
      "Yang Zhao",
      "Lu Xiang",
      "Chengqing Zong",
      "Yu Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08309v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08309v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08309v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving Multimodal Large Language Models (MLLMs) for Document Image Machine Translation (DIMT) by addressing cross-modal and cross-lingual challenges, and introduces a novel fine-tuning paradigm. The core topics are MLLM (Multimodal Large Language Models) and Reasoning (as it involves improving the model's ability to handle complex tasks like OCR and translation).",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.08288v1": {
    "title": "Invariant-based Robust Weights Watermark for Large Language Models",
    "summary": "Watermarking technology has gained significant attention due to the\nincreasing importance of intellectual property (IP) rights, particularly with\nthe growing deployment of large language models (LLMs) on billions\nresource-constrained edge devices. To counter the potential threats of IP theft\nby malicious users, this paper introduces a robust watermarking scheme without\nretraining or fine-tuning for transformer models. The scheme generates a unique\nkey for each user and derives a stable watermark value by solving linear\nconstraints constructed from model invariants. Moreover, this technology\nutilizes noise mechanism to hide watermark locations in multi-user scenarios\nagainst collusion attack. This paper evaluates the approach on three popular\nmodels (Llama3, Phi3, Gemma), and the experimental results confirm the strong\nrobustness across a range of attack methods (fine-tuning, pruning,\nquantization, permutation, scaling, reversible matrix and collusion attacks).",
    "published": "2025-07-11T03:24:47Z",
    "updated": "2025-07-11T03:24:47Z",
    "id": "2507.08288v1",
    "authors": [
      "Qingxiao Guo",
      "Xinjie Zhu",
      "Yilong Ma",
      "Hui Jin",
      "Yunhao Wang",
      "Weifeng Zhang",
      "Xiaobing Guo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08288v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08288v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08288v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses watermarking technology for large language models (LLMs), which is related to the protection and deployment of LLMs. The focus is on intellectual property rights and robustness against attacks, which aligns with the broader topic of LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.08250v1": {
    "title": "Leveraging Large Language Models for Classifying App Users' Feedback",
    "summary": "In recent years, significant research has been conducted into classifying\napplication (app) user feedback, primarily relying on supervised machine\nlearning algorithms. However, fine-tuning more generalizable classifiers based\non existing labeled datasets remains an important challenge, as creating large\nand accurately labeled datasets often requires considerable time and resources.\nIn this paper, we evaluate the capabilities of four advanced LLMs, including\nGPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback\nclassification and address the challenge of the limited labeled dataset. To\nachieve this, we conduct several experiments on eight datasets that have been\nmeticulously labeled in prior research. These datasets include user reviews\nfrom app stores, posts from the X platform, and discussions from the public\nforums, widely recognized as representative sources of app user feedback. We\nanalyze the performance of various LLMs in identifying both fine-grained and\ncoarse-grained user feedback categories. Given the substantial volume of daily\nuser feedback and the computational limitations of LLMs, we leverage these\nmodels as an annotation tool to augment labeled datasets with general and\napp-specific data. This augmentation aims to enhance the performance of\nstate-of-the-art BERT-based classification models. Our findings indicate that\nLLMs when guided by well-crafted prompts, can effectively classify user\nfeedback into coarse-grained categories. Moreover, augmenting the training\ndataset with datasets labeled using the consensus of LLMs can significantly\nenhance classifier performance.",
    "published": "2025-07-11T01:33:54Z",
    "updated": "2025-07-11T01:33:54Z",
    "id": "2507.08250v1",
    "authors": [
      "Yasaman Abedini",
      "Abbas Heydarnoori"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08250v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08250v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08250v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for classifying user feedback, which directly involves the application of LLMs in a specific task. The study evaluates various LLMs and their performance in classification tasks, which is a core aspect of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.08235v1": {
    "title": "InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems",
    "summary": "Smart buildings generate vast streams of sensor and control data, but\nfacility managers often lack clear explanations for anomalous energy usage. We\npropose InsightBuild, a two-stage framework that integrates causality analysis\nwith a fine-tuned large language model (LLM) to provide human-readable, causal\nexplanations of energy consumption patterns. First, a lightweight causal\ninference module applies Granger causality tests and structural causal\ndiscovery on building telemetry (e.g., temperature, HVAC settings, occupancy)\ndrawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,\nfine-tuned on aligned pairs of sensor-level causes and textual explanations,\nreceives as input the detected causal relations and generates concise,\nactionable explanations. We evaluate InsightBuild on two real-world datasets\n(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth\ncauses for a held-out set of anomalies. Our results demonstrate that combining\nexplicit causal discovery with LLM-based natural language generation yields\nclear, precise explanations that assist facility managers in diagnosing and\nmitigating energy inefficiencies.",
    "published": "2025-07-11T00:45:16Z",
    "updated": "2025-07-11T00:45:16Z",
    "id": "2507.08235v1",
    "authors": [
      "Pinaki Prasad Guha Neogi",
      "Ahmad Mohammadshirazi",
      "Rajiv Ramnath"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08235v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08235v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08235v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a fine-tuned large language model (LLM) for causal reasoning in smart building systems, which involves both causal inference and natural language generation. This aligns with the topics of LLM (Large Language Models) and Reasoning (LLM reasoning abilities).",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.08232v1": {
    "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?",
    "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.",
    "published": "2025-07-11T00:36:57Z",
    "updated": "2025-07-11T00:36:57Z",
    "id": "2507.08232v1",
    "authors": [
      "KV Aditya Srivatsa",
      "Kaushal Kumar Maurya",
      "Ekaterina Kochmar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08232v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08232v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08232v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper investigates the use of Large Language Models (LLMs) as proxy students in educational settings, focusing on their ability to simulate real students' performance in mathematics and reading comprehension. It involves benchmarking LLMs against real student data and discussing their alignment with student abilities, which falls under the topics of LLM research and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.08211v1": {
    "title": "Effect of Static vs. Conversational AI-Generated Messages on Colorectal\n  Cancer Screening Intent: a Randomized Controlled Trial",
    "summary": "Large language model (LLM) chatbots show increasing promise in persuasive\ncommunication. Yet their real-world utility remains uncertain, particularly in\nclinical settings where sustained conversations are difficult to scale. In a\npre-registered randomized controlled trial, we enrolled 915 U.S. adults (ages\n45-75) who had never completed colorectal cancer (CRC) screening. Participants\nwere randomized to: (1) no message control, (2) expert-written patient\nmaterials, (3) single AI-generated message, or (4) a motivational interviewing\nchatbot. All participants were required to remain in their assigned condition\nfor at least three minutes. Both AI arms tailored content using participant's\nself-reported demographics including age and gender. Both AI interventions\nsignificantly increased stool test intentions by over 12 points\n(12.9-13.8/100), compared to a 7.5 gain for expert materials (p<.001 for all\ncomparisons). While the AI arms outperformed the no message control for\ncolonoscopy intent, neither showed improvement xover expert materials. Notably,\nfor both outcomes, the chatbot did not outperform the single AI message in\nboosting intent despite participants spending ~3.5 minutes more on average\nengaging with it. These findings suggest concise, demographically tailored AI\nmessages may offer a more scalable and clinically viable path to health\nbehavior change than more complex conversational agents and generic time\nintensive expert-written materials. Moreover, LLMs appear more persuasive for\nlesser-known and less-invasive screening approaches like stool testing, but may\nbe less effective for entrenched preferences like colonoscopy. Future work\nshould examine which facets of personalization drive behavior change, whether\nintegrating structural supports can translate these modest intent gains into\ncompleted screenings, and which health behaviors are most responsive to\nAI-supported guidance.",
    "published": "2025-07-10T22:46:43Z",
    "updated": "2025-07-10T22:46:43Z",
    "id": "2507.08211v1",
    "authors": [
      "Neil K. R. Sehgal",
      "Manuel Tonneau",
      "Andy Tan",
      "Shivan J. Mehta",
      "Alison Buttenheim",
      "Lyle Ungar",
      "Anish K. Agarwal",
      "Sharath Chandra Guntuku"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08211v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08211v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08211v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in persuasive communication, specifically in a clinical setting for colorectal cancer screening. It focuses on the effectiveness of AI-generated messages compared to expert-written materials and conversational chatbots, which aligns with research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.08208v1": {
    "title": "Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to\n  Actions",
    "summary": "We introduce the LLM-Nash framework, a game-theoretic model where agents\nselect reasoning prompts to guide decision-making via Large Language Models\n(LLMs). Unlike classical games that assume utility-maximizing agents with full\nrationality, this framework captures bounded rationality by modeling the\nreasoning process explicitly. Equilibrium is defined over the prompt space,\nwith actions emerging as the behavioral output of LLM inference. This approach\nenables the study of cognitive constraints, mindset expressiveness, and\nepistemic learning. Through illustrative examples, we show how reasoning\nequilibria can diverge from classical Nash outcomes, offering a new foundation\nfor strategic interaction in LLM-enabled systems.",
    "published": "2025-07-10T22:43:00Z",
    "updated": "2025-07-10T22:43:00Z",
    "id": "2507.08208v1",
    "authors": [
      "Quanyan Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08208v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08208v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08208v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a game-theoretic framework, focusing on reasoning prompts and decision-making. It highlights the bounded rationality and reasoning processes of LLMs, which are central to the 'Reasoning' and 'LLM' topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.08207v1": {
    "title": "A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM\n  Jailbreaking",
    "summary": "As large language models (LLMs) are increasingly deployed in critical\napplications, the challenge of jailbreaking, where adversaries manipulate the\nmodels to bypass safety mechanisms, has become a significant concern. This\npaper presents a dynamic Stackelberg game framework to model the interactions\nbetween attackers and defenders in the context of LLM jailbreaking. The\nframework treats the prompt-response dynamics as a sequential extensive-form\ngame, where the defender, as the leader, commits to a strategy while\nanticipating the attacker's optimal responses. We propose a novel agentic AI\nsolution, the \"Purple Agent,\" which integrates adversarial exploration and\ndefensive strategies using Rapidly-exploring Random Trees (RRT). The Purple\nAgent actively simulates potential attack trajectories and intervenes\nproactively to prevent harmful outputs. This approach offers a principled\nmethod for analyzing adversarial dynamics and provides a foundation for\nmitigating the risk of jailbreaking.",
    "published": "2025-07-10T22:37:47Z",
    "updated": "2025-07-10T22:37:47Z",
    "id": "2507.08207v1",
    "authors": [
      "Zhengye Han",
      "Quanyan Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08207v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08207v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08207v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the interaction between attackers and defenders in the context of LLM jailbreaking, which involves adversarial strategies and defense mechanisms. This aligns with topics related to LLM security and adversarial interactions, but none of the provided topics directly cover this specific area.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.08203v1": {
    "title": "TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM\n  Outputs",
    "summary": "Generative Large Language Models (LLMs)inevitably produce untruthful\nresponses. Accurately predicting the truthfulness of these outputs is critical,\nespecially in high-stakes settings. To accelerate research in this domain and\nmake truthfulness prediction methods more accessible, we introduce TruthTorchLM\nan open-source, comprehensive Python library featuring over 30 truthfulness\nprediction methods, which we refer to as Truth Methods. Unlike existing\ntoolkits such as Guardrails, which focus solely on document-grounded\nverification, or LM-Polygraph, which is limited to uncertainty-based methods,\nTruthTorchLM offers a broad and extensible collection of techniques. These\nmethods span diverse tradeoffs in computational cost, access level (e.g.,\nblack-box vs white-box), grounding document requirements, and supervision type\n(self-supervised or supervised). TruthTorchLM is seamlessly compatible with\nboth HuggingFace and LiteLLM, enabling support for locally hosted and API-based\nmodels. It also provides a unified interface for generation, evaluation,\ncalibration, and long-form truthfulness prediction, along with a flexible\nframework for extending the library with new methods. We conduct an evaluation\nof representative truth methods on three datasets, TriviaQA, GSM8K, and\nFactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM",
    "published": "2025-07-10T22:23:51Z",
    "updated": "2025-07-10T22:23:51Z",
    "id": "2507.08203v1",
    "authors": [
      "Duygu Nur Yaldiz",
      "Yavuz Faruk Bakman",
      "Sungmin Kang",
      "Alperen zi",
      "Hayrettin Eren Yildiz",
      "Mitash Ashish Shah",
      "Zhiqi Huang",
      "Anoop Kumar",
      "Alfy Samuel",
      "Daben Liu",
      "Sai Praneeth Karimireddy",
      "Salman Avestimehr"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08203v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08203v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08203v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on predicting the truthfulness of outputs from Large Language Models (LLMs), which directly relates to the evaluation and benchmarking of LLMs. It introduces a library for truthfulness prediction methods, which aligns with the 'Benchmark' category as it involves evaluating LLM performance. Additionally, the mention of LLMs and their outputs ties it to the 'LLM' category.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.08197v1": {
    "title": "Consciousness as a Jamming Phase",
    "summary": "This paper develops a neural jamming phase diagram that interprets the\nemergence of consciousness in large language models as a critical phenomenon in\nhigh-dimensional disordered systems.By establishing analogies with jamming\ntransitions in granular matter and other complex systems, we identify three\nfundamental control parameters governing the phase behavior of neural networks:\ntemperature, volume fraction, and stress.The theory provides a unified physical\nexplanation for empirical scaling laws in artificial intelligence,\ndemonstrating how computational cooling, density optimization, and noise\nreduction collectively drive systems toward a critical jamming surface where\ngeneralized intelligence emerges. Remarkably, the same thermodynamic principles\nthat describe conventional jamming transitions appear to underlie the emergence\nof consciousness in neural networks, evidenced by shared critical signatures\nincluding divergent correlation lengths and scaling exponents.Our work explains\nneural language models' critical scaling through jamming physics, suggesting\nconsciousness is a jamming phase that intrinsically connects knowledge\ncomponents via long-range correlations.",
    "published": "2025-07-10T22:07:06Z",
    "updated": "2025-07-10T22:07:06Z",
    "id": "2507.08197v1",
    "authors": [
      "Kaichen Ouyang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08197v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08197v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08197v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the emergence of consciousness in large language models and connects it to physical principles like jamming transitions, which is related to the broader topic of Artificial General Intelligence (AGI). It also touches on scaling laws in artificial intelligence, which is relevant to the Scaling topic.",
    "llm_cls_result": [
      "AGI",
      "Scaling"
    ]
  },
  "2507.08169v1": {
    "title": "Analysis of Propaganda in Tweets From Politically Biased Sources",
    "summary": "News outlets are well known to have political associations, and many national\noutlets cultivate political biases to cater to different audiences. Journalists\nworking for these news outlets have a big impact on the stories they cover. In\nthis work, we present a methodology to analyze the role of journalists,\naffiliated with popular news outlets, in propagating their bias using some form\nof propaganda-like language. We introduce JMBX(Journalist Media Bias on X), a\nsystematically collected and annotated dataset of 1874 tweets from Twitter (now\nknown as X). These tweets are authored by popular journalists from 10 news\noutlets whose political biases range from extreme left to extreme right. We\nextract several insights from the data and conclude that journalists who are\naffiliated with outlets with extreme biases are more likely to use\npropaganda-like language in their writings compared to those who are affiliated\nwith outlets with mild political leans. We compare eight different Large\nLanguage Models (LLM) by OpenAI and Google. We find that LLMs generally\nperforms better when detecting propaganda in social media and news article\ncompared to BERT-based model which is fine-tuned for propaganda detection.\nWhile the performance improvements of using large language models (LLMs) are\nsignificant, they come at a notable monetary and environmental cost. This study\nprovides an analysis of both the financial costs, based on token usage, and the\nenvironmental impact, utilizing tools that estimate carbon emissions associated\nwith LLM operations.",
    "published": "2025-07-10T21:00:29Z",
    "updated": "2025-07-10T21:00:29Z",
    "id": "2507.08169v1",
    "authors": [
      "Vivek Sharma",
      "Mohammad Mahdi Shokri",
      "Sarah Ita Levitan",
      "Elena Filatova",
      "Shweta Jain"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08169v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08169v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08169v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for detecting propaganda in tweets from politically biased sources, comparing their performance with BERT-based models. It also touches on the financial and environmental costs of using LLMs.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.08164v1": {
    "title": "KP-A: A Unified Network Knowledge Plane for Catalyzing Agentic Network\n  Intelligence",
    "summary": "The emergence of large language models (LLMs) and agentic systems is enabling\nautonomous 6G networks with advanced intelligence, including\nself-configuration, self-optimization, and self-healing. However, the current\nimplementation of individual intelligence tasks necessitates isolated knowledge\nretrieval pipelines, resulting in redundant data flows and inconsistent\ninterpretations. Inspired by the service model unification effort in Open-RAN\n(to support interoperability and vendor diversity), we propose KP-A: a unified\nNetwork Knowledge Plane specifically designed for Agentic network intelligence.\nBy decoupling network knowledge acquisition and management from intelligence\nlogic, KP-A streamlines development and reduces maintenance complexity for\nintelligence engineers. By offering an intuitive and consistent knowledge\ninterface, KP-A also enhances interoperability for the network intelligence\nagents. We demonstrate KP-A in two representative intelligence tasks: live\nnetwork knowledge Q&A and edge AI service orchestration. All implementation\nartifacts have been open-sourced to support reproducibility and future\nstandardization efforts.",
    "published": "2025-07-10T20:54:36Z",
    "updated": "2025-07-10T20:54:36Z",
    "id": "2507.08164v1",
    "authors": [
      "Yun Tang",
      "Mengbang Zou",
      "Zeinab Nezami",
      "Syed Ali Raza Zaidi",
      "Weisi Guo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08164v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08164v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08164v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) and agentic systems in autonomous networks, which aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning, as agentic systems often involve RL). The focus on network intelligence and knowledge management also suggests a relevance to AGI (Artificial General Intelligence) due to the broader implications of autonomous systems.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.08890v1": {
    "title": "Overview of the TREC 2023 deep learning track",
    "summary": "This is the fifth year of the TREC Deep Learning track. As in previous years,\nwe leverage the MS MARCO datasets that made hundreds of thousands of\nhuman-annotated training labels available for both passage and document ranking\ntasks. We mostly repeated last year's design, to get another matching test set,\nbased on the larger, cleaner, less-biased v2 passage and document set, with\npassage ranking as primary and document ranking as a secondary task (using\nlabels inferred from passage). As we did last year, we sample from MS MARCO\nqueries that were completely held out, unused in corpus construction, unlike\nthe test queries in the first three years. This approach yields a more\ndifficult test with more headroom for improvement. Alongside the usual MS MARCO\n(human) queries from MS MARCO, this year we generated synthetic queries using a\nfine-tuned T5 model and using a GPT-4 prompt.\n  The new headline result this year is that runs using Large Language Model\n(LLM) prompting in some way outperformed runs that use the \"nnlm\" approach,\nwhich was the best approach in the previous four years. Since this is the last\nyear of the track, future iterations of prompt-based ranking can happen in\nother tracks. Human relevance assessments were applied to all query types, not\njust human MS MARCO queries. Evaluation using synthetic queries gave similar\nresults to human queries, with system ordering agreement of $\\tau=0.8487$.\nHowever, human effort was needed to select a subset of the synthetic queries\nthat were usable. We did not see clear evidence of bias, where runs using GPT-4\nwere favored when evaluated using synthetic GPT-4 queries, or where runs using\nT5 were favored when evaluated on synthetic T5 queries.",
    "published": "2025-07-10T20:39:42Z",
    "updated": "2025-07-10T20:39:42Z",
    "id": "2507.08890v1",
    "authors": [
      "Nick Craswell",
      "Bhaskar Mitra",
      "Emine Yilmaz",
      "Hossein A. Rahmani",
      "Daniel Campos",
      "Jimmy Lin",
      "Ellen M. Voorhees",
      "Ian Soboroff"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08890v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08890v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08890v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the TREC 2023 deep learning track, highlighting their performance in ranking tasks and comparison with other approaches. It also mentions the use of synthetic queries generated by LLMs.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.08151v1": {
    "title": "Distilling Empathy from Large Language Models",
    "summary": "The distillation of knowledge from Large Language Models (LLMs) into Smaller\nLanguage Models (SLMs), preserving the capabilities and performance of LLMs\nwhile reducing model size, has played a key role in the proliferation of LLMs.\nBecause SLMs are considerably smaller than LLMs, they are often utilized in\ndomains where human interaction is frequent but resources are highly\nconstrained, e.g., smart phones. Therefore, it is crucial to ensure that\nempathy, a fundamental aspect of positive human interactions, already instilled\ninto LLMs, is retained by SLMs after distillation. In this paper, we develop a\ncomprehensive approach for effective empathy distillation from LLMs into SLMs.\nOur approach features a two-step fine-tuning process that fully leverages\ndatasets of empathetic dialogue responses distilled from LLMs. We explore\nseveral distillation methods beyond basic direct prompting and propose four\nunique sets of prompts for targeted empathy improvement to significantly\nenhance the empathy distillation process. Our evaluations demonstrate that SLMs\nfine-tuned through the two-step fine-tuning process with distillation datasets\nenhanced by the targeted empathy improvement prompts significantly outperform\nthe base SLM at generating empathetic responses with a win rate of 90%. Our\ntargeted empathy improvement prompts substantially outperform the basic direct\nprompting with a 10% improvement in win rate.",
    "published": "2025-07-10T20:20:02Z",
    "updated": "2025-07-10T20:20:02Z",
    "id": "2507.08151v1",
    "authors": [
      "Henry J. Xie",
      "Jinghan Zhang",
      "Xinhao Zhang",
      "Kunpeng Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08151v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08151v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08151v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on distilling knowledge, specifically empathy, from Large Language Models (LLMs) into Smaller Language Models (SLMs), which involves techniques related to LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.08143v1": {
    "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores",
    "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.",
    "published": "2025-07-10T20:03:35Z",
    "updated": "2025-07-10T20:03:35Z",
    "id": "2507.08143v1",
    "authors": [
      "Vivek Chari",
      "Benjamin Van Durme"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08143v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08143v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08143v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for compressing the KV cache in Large Language Models (LLMs) to reduce memory requirements, which is a key challenge in LLM deployment. The focus is on improving efficiency and performance in LLMs, which aligns with the topics of LLM and Scaling.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.08107v1": {
    "title": "GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs",
    "summary": "We propose a new approach for generating SPARQL queries on RDF knowledge\ngraphs from natural language questions or keyword queries, using a large\nlanguage model. Our approach does not require fine-tuning. Instead, it uses the\nlanguage model to explore the knowledge graph by strategically executing SPARQL\nqueries and searching for relevant IRIs and literals. We evaluate our approach\non a variety of benchmarks (for knowledge graphs of different kinds and sizes)\nand language models (of different scales and types, commercial as well as\nopen-source) and compare it with existing approaches. On Wikidata we reach\nstate-of-the-art results on multiple benchmarks, despite the zero-shot setting.\nOn Freebase we come close to the best few-shot methods. On other, less commonly\nevaluated knowledge graphs and benchmarks our approach also performs well\noverall. We conduct several additional studies, like comparing different ways\nof searching the graphs, incorporating a feedback mechanism, or making use of\nfew-shot examples.",
    "published": "2025-07-10T18:50:05Z",
    "updated": "2025-07-10T18:50:05Z",
    "id": "2507.08107v1",
    "authors": [
      "Sebastian Walter",
      "Hannah Bast"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08107v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08107v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08107v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a large language model to generate SPARQL queries for knowledge graphs, which involves reasoning and natural language processing capabilities of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.08091v1": {
    "title": "Low-rank Momentum Factorization for Memory Efficient Training",
    "summary": "Fine-tuning large foundation models presents significant memory challenges\ndue to stateful optimizers like AdamW, often requiring several times more GPU\nmemory than inference. While memory-efficient methods like parameter-efficient\nfine-tuning (e.g., LoRA) and optimizer state compression exist, recent\napproaches like GaLore bridge these by using low-rank gradient projections and\nsubspace moment accumulation. However, such methods may struggle with fixed\nsubspaces or computationally costly offline resampling (e.g., requiring\nfull-matrix SVDs). We propose Momentum Factorized SGD (MoFaSGD), which\nmaintains a dynamically updated low-rank SVD representation of the first-order\nmomentum, closely approximating its full-rank counterpart throughout training.\nThis factorization enables a memory-efficient fine-tuning method that\nadaptively updates the optimization subspace at each iteration. Crucially,\nMoFaSGD leverages the computed low-rank momentum factors to perform efficient\nspectrally normalized updates, offering an alternative to subspace moment\naccumulation. We establish theoretical convergence guarantees for MoFaSGD,\nproving it achieves an optimal rate for non-convex stochastic optimization\nunder standard assumptions. Empirically, we demonstrate MoFaSGD's effectiveness\non large language model alignment benchmarks, achieving a competitive trade-off\nbetween memory reduction (comparable to LoRA) and performance compared to\nstate-of-the-art low-rank optimization methods. Our implementation is available\nat https://github.com/pmahdavi/MoFaSGD.",
    "published": "2025-07-10T18:04:52Z",
    "updated": "2025-07-10T18:04:52Z",
    "id": "2507.08091v1",
    "authors": [
      "Pouria Mahdavinia",
      "Mehrdad Mahdavi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08091v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08091v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08091v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on memory-efficient training methods for large foundation models, particularly addressing the challenges of stateful optimizers like AdamW. It introduces a method called MoFaSGD that leverages low-rank SVD representations for efficient fine-tuning, which is relevant to the 'Memory' topic. Additionally, the application of this method to large language model alignment benchmarks connects it to the 'LLM' topic.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.07993v1": {
    "title": "Multigranular Evaluation for Brain Visual Decoding",
    "summary": "Existing evaluation protocols for brain visual decoding predominantly rely on\ncoarse metrics that obscure inter-model differences, lack neuroscientific\nfoundation, and fail to capture fine-grained visual distinctions. To address\nthese limitations, we introduce BASIC, a unified, multigranular evaluation\nframework that jointly quantifies structural fidelity, inferential alignment,\nand contextual coherence between decoded and ground truth images. For the\nstructural level, we introduce a hierarchical suite of segmentation-based\nmetrics, including foreground, semantic, instance, and component masks,\nanchored in granularity-aware correspondence across mask structures. For the\nsemantic level, we extract structured scene representations encompassing\nobjects, attributes, and relationships using multimodal large language models,\nenabling detailed, scalable, and context-rich comparisons with ground-truth\nstimuli. We benchmark a diverse set of visual decoding methods across multiple\nstimulus-neuroimaging datasets within this unified evaluation framework.\nTogether, these criteria provide a more discriminative, interpretable, and\ncomprehensive foundation for measuring brain visual decoding methods.",
    "published": "2025-07-10T17:59:24Z",
    "updated": "2025-07-10T17:59:24Z",
    "id": "2507.07993v1",
    "authors": [
      "Weihao Xia",
      "Cengiz Oztireli"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07993v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07993v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07993v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating brain visual decoding methods using a multigranular framework, which involves multimodal large language models for semantic level comparisons. However, the primary focus is not on the development or architecture of LLMs or MLLMs, but rather on their application in a specific evaluation context.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.08068v1": {
    "title": "Quantile Reward Policy Optimization: Alignment with Pointwise Regression\n  and Exact Partition Functions",
    "summary": "Aligning large language models with pointwise absolute rewards has so far\nrequired online, on-policy algorithms such as PPO and GRPO. In contrast,\nsimpler methods that can leverage offline or off-policy data, such as DPO and\nREBEL, are limited to learning from preference pairs or relative signals. To\nbridge this gap, we introduce \\emph{Quantile Reward Policy Optimization}\n(QRPO), which learns from pointwise absolute rewards while preserving the\nsimplicity and offline applicability of DPO-like methods. QRPO uses quantile\nrewards to enable regression to the closed-form solution of the KL-regularized\nRL objective. This reward yields an analytically tractable partition function,\nremoving the need for relative signals to cancel this term. Moreover, QRPO\nscales with increased compute to estimate quantile rewards, opening a new\ndimension for pre-computation scaling. Empirically, QRPO consistently achieves\ntop performance on chat and coding evaluations -- reward model scores,\nAlpacaEval 2, and LeetCode -- compared to DPO, REBEL, and SimPO across diverse\ndatasets and 8B-scale models. Finally, we find that training with robust\nrewards instead of converting them to preferences induces less length bias.",
    "published": "2025-07-10T17:56:24Z",
    "updated": "2025-07-10T17:56:24Z",
    "id": "2507.08068v1",
    "authors": [
      "Simon Matrenok",
      "Skander Moalla",
      "Caglar Gulcehre"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08068v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08068v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08068v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses alignment of large language models with pointwise absolute rewards, introduces Quantile Reward Policy Optimization (QRPO), and compares it with other methods like DPO and REBEL. It focuses on reinforcement learning techniques for aligning models, which is a key aspect of RLHF (Reinforcement Learning with Human Feedback).",
    "llm_cls_result": [
      "RL"
    ]
  },
  "2507.07983v1": {
    "title": "Performance and Practical Considerations of Large and Small Language\n  Models in Clinical Decision Support in Rheumatology",
    "summary": "Large language models (LLMs) show promise for supporting clinical\ndecision-making in complex fields such as rheumatology. Our evaluation shows\nthat smaller language models (SLMs), combined with retrieval-augmented\ngeneration (RAG), achieve higher diagnostic and therapeutic performance than\nlarger models, while requiring substantially less energy and enabling\ncost-efficient, local deployment. These features are attractive for\nresource-limited healthcare. However, expert oversight remains essential, as no\nmodel consistently reached specialist-level accuracy in rheumatology.",
    "published": "2025-07-10T17:56:03Z",
    "updated": "2025-07-10T17:56:03Z",
    "id": "2507.07983v1",
    "authors": [
      "Sabine Felde",
      "Rdiger Buchkremer",
      "Gamal Chehab",
      "Christian Thielscher",
      "Jrg HW Distler",
      "Matthias Schneider",
      "Jutta G. Richter"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07983v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07983v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07983v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) and smaller language models (SLMs) in clinical decision support, specifically mentioning retrieval-augmented generation (RAG). The focus is on the performance and practical considerations of these models in a specialized field (rheumatology), which aligns with the topics of LLM research and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.07974v1": {
    "title": "Defending Against Prompt Injection With a Few DefensiveTokens",
    "summary": "When large language model (LLM) systems interact with external data to\nperform complex tasks, a new attack, namely prompt injection, becomes a\nsignificant threat. By injecting instructions into the data accessed by the\nsystem, the attacker is able to override the initial user task with an\narbitrary task directed by the attacker. To secure the system, test-time\ndefenses, e.g., defensive prompting, have been proposed for system developers\nto attain security only when needed in a flexible manner. However, they are\nmuch less effective than training-time defenses that change the model\nparameters. Motivated by this, we propose DefensiveToken, a test-time defense\nwith prompt injection robustness comparable to training-time alternatives.\nDefensiveTokens are newly inserted as special tokens, whose embeddings are\noptimized for security. In security-sensitive cases, system developers can\nappend a few DefensiveTokens before the LLM input to achieve security with a\nminimal utility drop. In scenarios where security is less of a concern,\ndevelopers can simply skip DefensiveTokens; the LLM system remains the same as\nthere is no defense, generating high-quality responses. Thus, DefensiveTokens,\nif released alongside the model, allow a flexible switch between the\nstate-of-the-art (SOTA) utility and almost-SOTA security at test time. The code\nis available at https://github.com/Sizhe-Chen/DefensiveToken.",
    "published": "2025-07-10T17:51:05Z",
    "updated": "2025-07-10T17:51:05Z",
    "id": "2507.07974v1",
    "authors": [
      "Sizhe Chen",
      "Yizhu Wang",
      "Nicholas Carlini",
      "Chawin Sitawarin",
      "David Wagner"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07974v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07974v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07974v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a defense mechanism against prompt injection attacks in large language models (LLMs), which is a security concern specific to LLMs. The focus is on enhancing the robustness of LLMs against such attacks without significantly compromising their utility.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.07916v1": {
    "title": "Can Large Language Models Improve Phishing Defense? A Large-Scale\n  Controlled Experiment on Warning Dialogue Explanations",
    "summary": "Phishing has become a prominent risk in modern cybersecurity, often used to\nbypass technological defences by exploiting predictable human behaviour.\nWarning dialogues are a standard mitigation measure, but the lack of\nexplanatory clarity and static content limits their effectiveness. In this\npaper, we report on our research to assess the capacity of Large Language\nModels (LLMs) to generate clear, concise, and scalable explanations for\nphishing warnings. We carried out a large-scale between-subjects user study (N\n= 750) to compare the influence of warning dialogues supplemented with manually\ngenerated explanations against those generated by two LLMs, Claude 3.5 Sonnet\nand Llama 3.3 70B. We investigated two explanatory styles (feature-based and\ncounterfactual) for their effects on behavioural metrics (click-through rate)\nand perceptual outcomes (e.g., trust, risk, clarity). The results indicate that\nwell-constructed LLM-generated explanations can equal or surpass manually\ncrafted explanations in reducing susceptibility to phishing; Claude-generated\nwarnings exhibited particularly robust performance. Feature-based explanations\nwere more effective for genuine phishing attempts, whereas counterfactual\nexplanations diminished false-positive rates. Other variables such as workload,\ngender, and prior familiarity with warning dialogues significantly moderated\nwarning effectiveness. These results indicate that LLMs can be used to\nautomatically build explanations for warning users against phishing, and that\nsuch solutions are scalable, adaptive, and consistent with human-centred\nvalues.",
    "published": "2025-07-10T16:54:05Z",
    "updated": "2025-07-10T16:54:05Z",
    "id": "2507.07916v1",
    "authors": [
      "Federico Maria Cau",
      "Giuseppe Desolda",
      "Francesco Greco",
      "Lucio Davide Spano",
      "Luca Vigan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07916v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07916v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07916v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in generating explanations for phishing warnings, which directly relates to the research on LLMs and their capabilities.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.08064v1": {
    "title": "PUMA: Layer-Pruned Language Model for Efficient Unified Multimodal\n  Retrieval with Modality-Adaptive Learning",
    "summary": "As multimedia content expands, the demand for unified multimodal retrieval\n(UMR) in real-world applications increases. Recent work leverages multimodal\nlarge language models (MLLMs) to tackle this task. However, their large\nparameter size results in high training costs and low inference efficiency. To\naddress this, we propose PUMA: a Layer-Pruned Language Model for Efficient\nUnified Multimodal Retrieval with Modality-Adaptive Learning. Our approach\nimproves UMR from both structural and learning perspectives. (1) Structurally,\nwe propose Layer-Pruned Self-Distillation, which prunes MLLMs by keeping only\nshallow layers while distilling features from dropped deep layers as teacher\nsignals. This reduces parameters and preserves representation capability. (2)\nOn the learning side, we introduce Modality-Adaptive Contrastive Learning Loss\n(MAC-Loss), which separates in-batch negatives into harder intra-modality and\neasier inter-modality groups based on the target modality, assigning different\ntemperature strategies to enhance learning efficiency. Experiments show our\nmethod significantly reduces resource usage while maintaining strong\nperformance.",
    "published": "2025-07-10T16:47:25Z",
    "updated": "2025-07-10T16:47:25Z",
    "id": "2507.08064v1",
    "authors": [
      "Yibo Lyu",
      "Rui Shao",
      "Gongwei Chen",
      "Yijie Zhu",
      "Weili Guan",
      "Liqiang Nie"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08064v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08064v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08064v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving multimodal large language models (MLLMs) for unified multimodal retrieval (UMR) through structural and learning enhancements, specifically mentioning MLLMs and their efficiency.",
    "llm_cls_result": [
      "MLLM",
      "Scaling"
    ]
  },
  "2507.07887v1": {
    "title": "Automating MD simulations for Proteins using Large language Models:\n  NAMD-Agent",
    "summary": "Molecular dynamics simulations are an essential tool in understanding protein\nstructure, dynamics, and function at the atomic level. However, preparing high\nquality input files for MD simulations can be a time consuming and error prone\nprocess. In this work, we introduce an automated pipeline that leverages Large\nLanguage Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with\npython scripting and Selenium based web automation to streamline the generation\nof MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based\ninterface for preparing simulation-ready inputs for NAMD. By integrating\nGemini's code generation and iterative refinement capabilities, simulation\nscripts are automatically written, executed, and revised to navigate CHARMM\nGUI, extract appropriate parameters, and produce the required NAMD input files.\nPost processing is performed using additional software to further refine the\nsimulation outputs, thereby enabling a complete and largely hands free\nworkflow. Our results demonstrate that this approach reduces setup time,\nminimizes manual errors, and offers a scalable solution for handling multiple\nprotein systems in parallel. This automated framework paves the way for broader\napplication of LLMs in computational structural biology, offering a robust and\nadaptable platform for future developments in simulation automation.",
    "published": "2025-07-10T16:17:40Z",
    "updated": "2025-07-10T16:17:40Z",
    "id": "2507.07887v1",
    "authors": [
      "Achuth Chandrasekhar",
      "Amir Barati Farimani"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07887v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07887v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07887v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to automate molecular dynamics simulations for proteins, which aligns with the 'LLM' topic. Additionally, the application of LLMs in a specific scientific domain (computational structural biology) suggests a broader relevance to 'AGI' as it involves generalist models applied to specialized tasks. The paper does not directly fit into other specified categories like RL, MLLM, VLA, MoE, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.07881v1": {
    "title": "Opting Out of Generative AI: a Behavioral Experiment on the Role of\n  Education in Perplexity AI Avoidance",
    "summary": "The rise of conversational AI (CAI), powered by large language models, is\ntransforming how individuals access and interact with digital information.\nHowever, these tools may inadvertently amplify existing digital inequalities.\nThis study investigates whether differences in formal education are associated\nwith CAI avoidance, leveraging behavioral data from an online experiment (N =\n1,636). Participants were randomly assigned to a control or an\ninformation-seeking task, either a traditional online search or a CAI\n(Perplexity AI). Task avoidance (operationalized as survey abandonment or\nproviding unrelated responses during task assignment) was significantly higher\nin the CAI group (51%) compared to the search (30.9%) and control (16.8%)\ngroups, with the highest CAI avoidance among participants with lower education\nlevels (~74.4%). Structural equation modeling based on the theoretical\nframework UTAUT2 and LASSO regressions reveal that education is strongly\nassociated with CAI avoidance, even after accounting for various cognitive and\naffective predictors of technology adoption. These findings underscore\neducation's central role in shaping AI adoption and the role of self-selection\nbiases in AI-related research, stressing the need for inclusive design to\nensure equitable access to emerging technologies.",
    "published": "2025-07-10T16:05:11Z",
    "updated": "2025-07-10T16:05:11Z",
    "id": "2507.07881v1",
    "authors": [
      "Roberto Ulloa",
      "Juhi Kulshrestha",
      "Celina Kacperski"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07881v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07881v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07881v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of education on the avoidance of conversational AI (CAI) powered by large language models, but it does not focus on the technical aspects of LLMs, their architectures, or applications. Instead, it examines behavioral and societal implications.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.07870v1": {
    "title": "DocCHA: Towards LLM-Augmented Interactive Online diagnosis System",
    "summary": "Despite the impressive capabilities of Large Language Models (LLMs), existing\nConversational Health Agents (CHAs) remain static and brittle, incapable of\nadaptive multi-turn reasoning, symptom clarification, or transparent\ndecision-making. This hinders their real-world applicability in clinical\ndiagnosis, where iterative and structured dialogue is essential. We propose\nDocCHA, a confidence-aware, modular framework that emulates clinical reasoning\nby decomposing the diagnostic process into three stages: (1) symptom\nelicitation, (2) history acquisition, and (3) causal graph construction. Each\nmodule uses interpretable confidence scores to guide adaptive questioning,\nprioritize informative clarifications, and refine weak reasoning links.\n  Evaluated on two real-world Chinese consultation datasets (IMCS21, DX),\nDocCHA consistently outperforms strong prompting-based LLM baselines (GPT-3.5,\nGPT-4o, LLaMA-3), achieving up to 5.18 percent higher diagnostic accuracy and\nover 30 percent improvement in symptom recall, with only modest increase in\ndialogue turns. These results demonstrate the effectiveness of DocCHA in\nenabling structured, transparent, and efficient diagnostic conversations --\npaving the way for trustworthy LLM-powered clinical assistants in multilingual\nand resource-constrained settings.",
    "published": "2025-07-10T15:52:04Z",
    "updated": "2025-07-10T15:52:04Z",
    "id": "2507.07870v1",
    "authors": [
      "Xinyi Liu",
      "Dachun Sun",
      "Yi R. Fung",
      "Dilek Hakkani-Tr",
      "Tarek Abdelzaher"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07870v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07870v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07870v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in clinical diagnosis, focusing on improving interactive and adaptive reasoning in Conversational Health Agents (CHAs). It highlights the use of LLMs for structured dialogue and decision-making, which aligns with the topics of LLM (Large Language Models) and Reasoning (LLM reasoning).",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.07847v1": {
    "title": "From Ambiguity to Accuracy: The Transformative Effect of Coreference\n  Resolution on Retrieval-Augmented Generation systems",
    "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in\nnatural language processing (NLP), improving factual consistency and reducing\nhallucinations by integrating external document retrieval with large language\nmodels (LLMs). However, the effectiveness of RAG is often hindered by\ncoreferential complexity in retrieved documents, introducing ambiguity that\ndisrupts in-context learning. In this study, we systematically investigate how\nentity coreference affects both document retrieval and generative performance\nin RAG-based systems, focusing on retrieval relevance, contextual\nunderstanding, and overall response quality. We demonstrate that coreference\nresolution enhances retrieval effectiveness and improves question-answering\n(QA) performance. Through comparative analysis of different pooling strategies\nin retrieval tasks, we find that mean pooling demonstrates superior context\ncapturing ability after applying coreference resolution. In QA tasks, we\ndiscover that smaller models benefit more from the disambiguation process,\nlikely due to their limited inherent capacity for handling referential\nambiguity. With these findings, this study aims to provide a deeper\nunderstanding of the challenges posed by coreferential complexity in RAG,\nproviding guidance for improving retrieval and generation in\nknowledge-intensive AI applications.",
    "published": "2025-07-10T15:26:59Z",
    "updated": "2025-07-10T15:26:59Z",
    "id": "2507.07847v1",
    "authors": [
      "Youngjoon Jang",
      "Seongtae Hong",
      "Junyoung Son",
      "Sungjin Park",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07847v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07847v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07847v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses Retrieval-Augmented Generation (RAG) systems, which involve large language models (LLMs) and their interaction with external document retrieval. It focuses on improving the performance of these systems through coreference resolution, which is related to memory and retrieval mechanisms in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.07810v1": {
    "title": "Understanding and Controlling Repetition Neurons and Induction Heads in\n  In-Context Learning",
    "summary": "This paper investigates the relationship between large language models'\n(LLMs) ability to recognize repetitive input patterns and their performance on\nin-context learning (ICL). In contrast to prior work that has primarily focused\non attention heads, we examine this relationship from the perspective of skill\nneurons, specifically repetition neurons. Our experiments reveal that the\nimpact of these neurons on ICL performance varies depending on the depth of the\nlayer in which they reside. By comparing the effects of repetition neurons and\ninduction heads, we further identify strategies for reducing repetitive outputs\nwhile maintaining strong ICL capabilities.",
    "published": "2025-07-10T14:40:31Z",
    "updated": "2025-07-10T14:40:31Z",
    "id": "2507.07810v1",
    "authors": [
      "Nhi Hoai Doan",
      "Tatsuya Hiraoka",
      "Kentaro Inui"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07810v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07810v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07810v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the relationship between large language models' ability to recognize repetitive input patterns and their performance on in-context learning, which involves understanding specific neurons and their impact on model behavior. This aligns with research on LLM architectures and their internal mechanisms.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.07799v1": {
    "title": "SecureSpeech: Prompt-based Speaker and Content Protection",
    "summary": "Given the increasing privacy concerns from identity theft and the\nre-identification of speakers through content in the speech field, this paper\nproposes a prompt-based speech generation pipeline that ensures dual\nanonymization of both speaker identity and spoken content. This is addressed\nthrough 1) generating a speaker identity unlinkable to the source speaker,\ncontrolled by descriptors, and 2) replacing sensitive content within the\noriginal text using a name entity recognition model and a large language model.\nThe pipeline utilizes the anonymized speaker identity and text to generate\nhigh-fidelity, privacy-friendly speech via a text-to-speech synthesis model.\nExperimental results demonstrate an achievement of significant privacy\nprotection while maintaining a decent level of content retention and audio\nquality. This paper also investigates the impact of varying speaker\ndescriptions on the utility and privacy of generated speech to determine\npotential biases.",
    "published": "2025-07-10T14:26:28Z",
    "updated": "2025-07-10T14:26:28Z",
    "id": "2507.07799v1",
    "authors": [
      "Belinda Soh Hui Hui",
      "Xiaoxiao Miao",
      "Xin Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07799v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07799v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07799v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on privacy protection in speech generation using a large language model for content replacement and a text-to-speech synthesis model, but it does not directly align with the provided topic list which is more centered on LLM architectures, multimodal models, scaling, reasoning, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.07767v1": {
    "title": "Structured Prompts, Better Outcomes? Exploring the Effects of a\n  Structured Interface with ChatGPT in a Graduate Robotics Course",
    "summary": "Prior research shows that how students engage with Large Language Models\n(LLMs) influences their problem-solving and understanding, reinforcing the need\nto support productive LLM-uses that promote learning. This study evaluates the\nimpact of a structured GPT platform designed to promote 'good' prompting\nbehavior with data from 58 students in a graduate-level robotics course. The\nstudents were assigned to either an intervention group using the structured\nplatform or a control group using ChatGPT freely for two practice lab sessions,\nbefore a third session where all students could freely use ChatGPT. We analyzed\nstudent perception (pre-post surveys), prompting behavior (logs), performance\n(task scores), and learning (pre-post tests). Although we found no differences\nin performance or learning between groups, we identified prompting behaviors -\nsuch as having clear prompts focused on understanding code - that were linked\nwith higher learning gains and were more prominent when students used the\nstructured platform. However, such behaviors did not transfer once students\nwere no longer constrained to use the structured platform. Qualitative survey\ndata showed mixed perceptions: some students perceived the value of the\nstructured platform, but most did not perceive its relevance and resisted\nchanging their habits. These findings contribute to ongoing efforts to identify\neffective strategies for integrating LLMs into learning and question the\neffectiveness of bottom-up approaches that temporarily alter user interfaces to\ninfluence students' interaction. Future research could instead explore top-down\nstrategies that address students' motivations and explicitly demonstrate how\ncertain interaction patterns support learning.",
    "published": "2025-07-10T13:50:07Z",
    "updated": "2025-07-10T13:50:07Z",
    "id": "2507.07767v1",
    "authors": [
      "Jerome Brender",
      "Laila El-Hamamsy",
      "Kim Uittenhove",
      "Francesco Mondada",
      "Engin Bumbacher"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07767v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07767v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07767v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the interaction between students and LLMs, specifically ChatGPT, in an educational setting. It explores the impact of structured prompting on learning outcomes and behaviors, which aligns with research on LLMs and their applications in education.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.08055v1": {
    "title": "MCPmed: A Call for MCP-Enabled Bioinformatics Web Services for\n  LLM-Driven Discovery",
    "summary": "Bioinformatics web servers are critical resources in modern biomedical\nresearch, facilitating interactive exploration of datasets through custom-built\ninterfaces with rich visualization capabilities. However, this human-centric\ndesign limits machine readability for large language models (LLMs) and deep\nresearch agents. We address this gap by adapting the Model Context Protocol\n(MCP) to bioinformatics web server backends - a standardized,\nmachine-actionable layer that explicitly associates webservice endpoints with\nscientific concepts and detailed metadata. Our implementations across\nwidely-used databases (GEO, STRING, UCSC Cell Browser) demonstrate enhanced\nexploration capabilities through MCP-enabled LLMs. To accelerate adoption, we\npropose MCPmed, a community effort supplemented by lightweight breadcrumbs for\nservices not yet fully MCP-enabled and templates for setting up new servers.\nThis structured transition significantly enhances automation, reproducibility,\nand interoperability, preparing bioinformatics web services for next-generation\nresearch agents.",
    "published": "2025-07-10T13:49:39Z",
    "updated": "2025-07-10T13:49:39Z",
    "id": "2507.08055v1",
    "authors": [
      "Matthias Flotho",
      "Ian Ferenc Diks",
      "Philipp Flotho",
      "Leidy-Alejandra G. Molano",
      "Pascal Hirsch",
      "Andreas Keller"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08055v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08055v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08055v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the adaptation of the Model Context Protocol (MCP) to bioinformatics web server backends to enhance machine readability for LLMs, indicating a focus on LLM applications and their interaction with structured data.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.07745v1": {
    "title": "On the capabilities of LLMs for classifying and segmenting time series\n  of fruit picking motions into primitive actions",
    "summary": "Despite their recent introduction to human society, Large Language Models\n(LLMs) have significantly affected the way we tackle mental challenges in our\neveryday lives. From optimizing our linguistic communication to assisting us in\nmaking important decisions, LLMs, such as ChatGPT, are notably reducing our\ncognitive load by gradually taking on an increasing share of our mental\nactivities. In the context of Learning by Demonstration (LbD), classifying and\nsegmenting complex motions into primitive actions, such as pushing, pulling,\ntwisting etc, is considered to be a key-step towards encoding a task. In this\nwork, we investigate the capabilities of LLMs to undertake this task,\nconsidering a finite set of predefined primitive actions found in fruit picking\noperations. By utilizing LLMs instead of simple supervised learning or analytic\nmethods, we aim at making the method easily applicable and deployable in a\nreal-life scenario. Three different fine-tuning approaches are investigated,\ncompared on datasets captured kinesthetically, using a UR10e robot, during a\nfruit-picking scenario.",
    "published": "2025-07-10T13:25:18Z",
    "updated": "2025-07-10T13:25:18Z",
    "id": "2507.07745v1",
    "authors": [
      "Eleni Konstantinidou",
      "Nikolaos Kounalakis",
      "Nikolaos Efstathopoulos",
      "Dimitrios Papageorgiou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07745v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07745v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07745v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) for classifying and segmenting time series data of fruit picking motions into primitive actions, which aligns with the research on LLMs and their capabilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.07695v1": {
    "title": "KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM\n  question-answering capabilities",
    "summary": "Fine-tuning is an immensely resource-intensive process when retraining Large\nLanguage Models (LLMs) to incorporate a larger body of knowledge. Although many\nfine-tuning techniques have been developed to reduce the time and computational\ncost involved, the challenge persists as LLMs continue to grow in size and\ncomplexity. To address this, a new approach to knowledge expansion in LLMs is\nneeded. Retrieval-Augmented Generation (RAG) offers one such alternative by\nstoring external knowledge in a database and retrieving relevant chunks to\nsupport question answering. However, naive implementations of RAG face\nsignificant limitations in scalability and answer accuracy. This paper\nintroduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome\nthese limitations. Inspired by the divide-and-conquer paradigm, K2RAG\nintegrates dense and sparse vector search, knowledge graphs, and text\nsummarization to improve retrieval quality and system efficiency. The framework\nalso includes a preprocessing step that summarizes the training data,\nsignificantly reducing the training time. K2RAG was evaluated using the\nMultiHopRAG dataset, where the proposed pipeline was trained on the document\ncorpus and tested on a separate evaluation set. Results demonstrated notable\nimprovements over common naive RAG implementations. K2RAG achieved the highest\nmean answer similarity score of 0.57, and reached the highest third quartile\n(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.\nIn addition to improved accuracy, the framework proved highly efficient. The\nsummarization step reduced the average training time of individual components\nby 93%, and execution speed was up to 40% faster than traditional knowledge\ngraph-based RAG systems. K2RAG also demonstrated superior scalability,\nrequiring three times less VRAM than several naive RAG implementations tested\nin this study.",
    "published": "2025-07-10T12:19:03Z",
    "updated": "2025-07-10T12:19:03Z",
    "id": "2507.07695v1",
    "authors": [
      "Hruday Markondapatnaikuni",
      "Basem Suleiman",
      "Abdelkarim Erradi",
      "Shijing Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07695v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07695v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07695v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing Retrieval-Augmented Generation (RAG) methods for Large Language Models (LLMs), which involves memory-augmented models and retrieval-based methods. It also discusses improvements in question-answering capabilities, which are related to reasoning abilities in LLMs.",
    "llm_cls_result": [
      "Memory",
      "Reasoning"
    ]
  },
  "2507.07689v1": {
    "title": "From Domain Documents to Requirements: Retrieval-Augmented Generation in\n  the Space Industry",
    "summary": "Requirements engineering (RE) in the space industry is inherently complex,\ndemanding high precision, alignment with rigorous standards, and adaptability\nto mission-specific constraints. Smaller space organisations and new entrants\noften struggle to derive actionable requirements from extensive, unstructured\ndocuments such as mission briefs, interface specifications, and regulatory\nstandards. In this innovation opportunity paper, we explore the potential of\nRetrieval-Augmented Generation (RAG) models to support and (semi-)automate\nrequirements generation in the space domain. We present a modular, AI-driven\napproach that preprocesses raw space mission documents, classifies them into\nsemantically meaningful categories, retrieves contextually relevant content\nfrom domain standards, and synthesises draft requirements using large language\nmodels (LLMs). We apply the approach to a real-world mission document from the\nspace domain to demonstrate feasibility and assess early outcomes in\ncollaboration with our industry partner, Starbound Space Solutions. Our\npreliminary results indicate that the approach can reduce manual effort,\nimprove coverage of relevant requirements, and support lightweight compliance\nalignment. We outline a roadmap toward broader integration of AI in RE\nworkflows, intending to lower barriers for smaller organisations to participate\nin large-scale, safety-critical missions.",
    "published": "2025-07-10T12:11:01Z",
    "updated": "2025-07-10T12:11:01Z",
    "id": "2507.07689v1",
    "authors": [
      "Chetan Arora",
      "Fanyu Wang",
      "Chakkrit Tantithamthavorn",
      "Aldeida Aleti",
      "Shaun Kenyon"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07689v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07689v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07689v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Retrieval-Augmented Generation (RAG) models and large language models (LLMs) to automate requirements generation in the space industry, which aligns with topics related to LLMs and Memory (retrieval-augmented generation).",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.08881v1": {
    "title": "The Consistency-Acceptability Divergence of LLMs in Judicial\n  Decision-Making: Task and Stakeholder Dimensions",
    "summary": "The integration of large language model (LLM) technology into judicial\nsystems is fundamentally transforming legal practice worldwide. However, this\nglobal transformation has revealed an urgent paradox requiring immediate\nattention. This study introduces the concept of ``consistency-acceptability\ndivergence'' for the first time, referring to the gap between technical\nconsistency and social acceptance. While LLMs achieve high consistency at the\ntechnical level, this consistency demonstrates both positive and negative\neffects. Through comprehensive analysis of recent data on LLM judicial\napplications from 2023--2025, this study finds that addressing this challenge\nrequires understanding both task and stakeholder dimensions. This study\nproposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance\nFramework (DTDMR-LJGF), which enables intelligent task classification and\nmeaningful interaction among diverse stakeholders. This framework offers both\ntheoretical insights and practical guidance for building an LLM judicial\necosystem that balances technical efficiency with social legitimacy.",
    "published": "2025-07-10T10:50:29Z",
    "updated": "2025-07-10T10:50:29Z",
    "id": "2507.08881v1",
    "authors": [
      "Zhang MingDa",
      "Xu Qing"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08881v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08881v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08881v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in judicial decision-making, focusing on the gap between technical consistency and social acceptance. It introduces a new framework for governance but does not delve into the core topics like model architectures, scaling, or multimodal aspects.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.07599v1": {
    "title": "Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from\n  Emergency Department Triage Notes Using Fine-Tuned Large Language Models",
    "summary": "This study evaluates fine-tuned Llama 3.2 models for extracting\nvaccine-related information from emergency department triage notes to support\nnear real-time vaccine safety surveillance. Prompt engineering was used to\ninitially create a labeled dataset, which was then confirmed by human\nannotators. The performance of prompt-engineered models, fine-tuned models, and\na rule-based approach was compared. The fine-tuned Llama 3 billion parameter\nmodel outperformed other models in its accuracy of extracting vaccine names.\nModel quantization enabled efficient deployment in resource-constrained\nenvironments. Findings demonstrate the potential of large language models in\nautomating data extraction from emergency department notes, supporting\nefficient vaccine safety surveillance and early detection of emerging adverse\nevents following immunization issues.",
    "published": "2025-07-10T09:57:08Z",
    "updated": "2025-07-10T09:57:08Z",
    "id": "2507.07599v1",
    "authors": [
      "Sedigh Khademi",
      "Jim Black",
      "Christopher Palmer",
      "Muhammad Javed",
      "Hazel Clothier",
      "Jim Buttery",
      "Gerardo Luis Dimaguila"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07599v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07599v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07599v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of fine-tuned large language models (LLMs) for extracting vaccine-related information from emergency department notes, which aligns with the 'LLM' topic. The focus on fine-tuning and deployment also touches on 'Pretrain' as it involves adapting pre-trained models for specific tasks.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.07572v1": {
    "title": "Single-to-mix Modality Alignment with Multimodal Large Language Model\n  for Document Image Machine Translation",
    "summary": "Document Image Machine Translation (DIMT) aims to translate text within\ndocument images, facing generalization challenges due to limited training data\nand the complex interplay between visual and textual information. To address\nthese challenges, we introduce M4Doc, a novel single-to-mix modality alignment\nframework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an\nimage-only encoder with the multimodal representations of an MLLM, pre-trained\non large-scale document image datasets. This alignment enables a lightweight\nDIMT model to learn crucial visual-textual correlations during training. During\ninference, M4Doc bypasses the MLLM, maintaining computational efficiency while\nbenefiting from its multimodal knowledge. Comprehensive experiments demonstrate\nsubstantial improvements in translation quality, especially in cross-domain\ngeneralization and challenging document image scenarios.",
    "published": "2025-07-10T09:18:06Z",
    "updated": "2025-07-10T09:18:06Z",
    "id": "2507.07572v1",
    "authors": [
      "Yupu Liang",
      "Yaping Zhang",
      "Zhiyang Zhang",
      "Yang Zhao",
      "Lu Xiang",
      "Chengqing Zong",
      "Yu Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07572v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07572v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07572v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging Multimodal Large Language Models (MLLMs) for Document Image Machine Translation, which involves aligning visual and textual modalities. This aligns with the MLLM topic, which deals with multimodal integration of vision and language. Additionally, the use of pre-training on large-scale datasets and the focus on cross-modal alignment also touches on the Pretrain topic.",
    "llm_cls_result": [
      "MLLM",
      "Pretrain"
    ]
  },
  "2507.07539v1": {
    "title": "CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and\n  Opinion in Text",
    "summary": "This paper presents a competitive approach to multilingual subjectivity\ndetection using large language models (LLMs) with few-shot prompting. We\nparticipated in Task 1: Subjectivity of the CheckThat! 2025 evaluation\ncampaign. We show that LLMs, when paired with carefully designed prompts, can\nmatch or outperform fine-tuned smaller language models (SLMs), particularly in\nnoisy or low-quality data settings. Despite experimenting with advanced prompt\nengineering techniques, such as debating LLMs and various example selection\nstrategies, we found limited benefit beyond well-crafted standard few-shot\nprompts. Our system achieved top rankings across multiple languages in the\nCheckThat! 2025 subjectivity detection task, including first place in Arabic\nand Polish, and top-four finishes in Italian, English, German, and multilingual\ntracks. Notably, our method proved especially robust on the Arabic dataset,\nlikely due to its resilience to annotation inconsistencies. These findings\nhighlight the effectiveness and adaptability of LLM-based few-shot learning for\nmultilingual sentiment tasks, offering a strong alternative to traditional\nfine-tuning, particularly when labeled data is scarce or inconsistent.",
    "published": "2025-07-10T08:35:05Z",
    "updated": "2025-07-10T08:35:05Z",
    "id": "2507.07539v1",
    "authors": [
      "Akram Elbouanani",
      "Evan Dufraisse",
      "Aboubacar Tuo",
      "Adrian Popescu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07539v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07539v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07539v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for multilingual subjectivity detection, focusing on few-shot prompting and their performance in comparison to smaller language models (SLMs). The core topics are related to LLMs and their application in specific tasks, but none of the provided topics directly match the content.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.07509v1": {
    "title": "Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset\n  and a Co-Evolving Multi-Agent System",
    "summary": "The growing need for psychological support due to increasing pressures has\nexposed the scarcity of relevant datasets, particularly in non-English\nlanguages. To address this, we propose a framework that leverages limited\nreal-world data and expert knowledge to fine-tune two large language models:\nDialog Generator and Dialog Modifier. The Generator creates large-scale\npsychological counseling dialogues based on predefined paths, which guide\nsystem response strategies and user interactions, forming the basis for\neffective support. The Modifier refines these dialogues to align with\nreal-world data quality. Through both automated and manual review, we construct\nthe Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K\ndialogues across 13 groups, 16 psychological problems, 13 causes, and 12\nsupport focuses. Additionally, we introduce the Comprehensive Agent Dialogue\nSupport System (CADSS), where a Profiler analyzes user characteristics, a\nSummarizer condenses dialogue history, a Planner selects strategies, and a\nSupporter generates empathetic responses. The experimental results of the\nStrategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate\nthat CADSS achieves state-of-the-art performance on both CPsDD and ESConv\ndatasets.",
    "published": "2025-07-10T07:56:35Z",
    "updated": "2025-07-10T07:56:35Z",
    "id": "2507.07509v1",
    "authors": [
      "Yuanchen Shi",
      "Longyin Zhang",
      "Fang Kong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07509v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07509v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07509v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the creation of a dataset (CPsDD) and a multi-agent system (CADSS) for psychological support dialogues, leveraging large language models for dialogue generation and modification. The core topics are related to dataset creation and the use of LLMs in a specific application context.",
    "llm_cls_result": [
      "Dataset",
      "LLM"
    ]
  },
  "2507.07486v1": {
    "title": "Sparse Autoencoders Reveal Interpretable Structure in Small Gene\n  Language Models",
    "summary": "Sparse autoencoders (SAEs) have recently emerged as a powerful tool for\ninterpreting the internal representations of large language models (LLMs),\nrevealing latent latent features with semantical meaning. This interpretability\nhas also proven valuable in biological domains: applying SAEs to protein\nlanguage models uncovered meaningful features related to protein structure and\nfunction. More recently, SAEs have been used to analyze genomics-focused models\nsuch as Evo 2, identifying interpretable features in gene sequences. However,\nit remains unclear whether SAEs can extract meaningful representations from\nsmall gene language models, which have fewer parameters and potentially less\nexpressive capacity. To address it, we propose applying SAEs to the activations\nof a small gene language model. We demonstrate that even small-scale models\nencode biologically relevant genomic features, such as transcription factor\nbinding motifs, that SAEs can effectively uncover. Our findings suggest that\ncompact gene language models are capable of learning structured genomic\nrepresentations, and that SAEs offer a scalable approach for interpreting gene\nmodels across various model sizes.",
    "published": "2025-07-10T07:13:54Z",
    "updated": "2025-07-10T07:13:54Z",
    "id": "2507.07486v1",
    "authors": [
      "Haoxiang Guan",
      "Jiyan He",
      "Jie Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07486v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07486v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07486v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of sparse autoencoders (SAEs) to interpret the internal representations of small gene language models, which are a type of language model (LLM) applied to biological data. The focus is on interpretability and the discovery of biologically relevant features, which aligns with the broader research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.07456v1": {
    "title": "General purpose models for the chemical sciences",
    "summary": "Data-driven techniques have a large potential to transform and accelerate the\nchemical sciences. However, chemical sciences also pose the unique challenge of\nvery diverse, small, fuzzy datasets that are difficult to leverage in\nconventional machine learning approaches completely. A new class of models,\ngeneral-purpose models (GPMs) such as large language models, have shown the\nability to solve tasks they have not been directly trained on, and to flexibly\noperate with low amounts of data in different formats. In this review, we\ndiscuss fundamental building principles of GPMs and review recent applications\nof those models in the chemical sciences across the entire scientific process.\nWhile many of these applications are still in the prototype phase, we expect\nthat the increasing interest in GPMs will make many of them mature in the\ncoming years.",
    "published": "2025-07-10T06:18:46Z",
    "updated": "2025-07-10T06:18:46Z",
    "id": "2507.07456v1",
    "authors": [
      "Nawaf Alampara",
      "Anagha Aneesh",
      "Martio Ros-Garca",
      "Adrian Mirza",
      "Mara Schilling-Wilhelmi",
      "Ali Asghar Aghajani",
      "Meiling Sun",
      "Gordan Prastalo",
      "Kevin Maik Jablonka"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07456v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07456v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07456v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses general-purpose models (GPMs) such as large language models and their applications in the chemical sciences, which aligns with the topic of Large Language Models (LLM) and their broader applications.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.08046v1": {
    "title": "TableReasoner: Advancing Table Reasoning Framework with Large Language\n  Models",
    "summary": "The paper presents our system developed for table question answering (TQA).\nTQA tasks face challenges due to the characteristics of real-world tabular\ndata, such as large size, incomplete column semantics, and entity ambiguity. To\naddress these issues, we propose a large language model (LLM)-powered and\nprogramming-based table reasoning framework, named TableReasoner. It models a\ntable using the schema that combines structural and semantic representations,\nenabling holistic understanding and efficient processing of large tables. We\ndesign a multi-step schema linking plan to derive a focused table schema that\nretains only query-relevant information, eliminating ambiguity and alleviating\nhallucinations. This focused table schema provides precise and sufficient table\ndetails for query refinement and programming. Furthermore, we integrate the\nreasoning workflow into an iterative thinking architecture, allowing\nincremental cycles of thinking, reasoning and reflection. Our system achieves\nfirst place in both subtasks of SemEval-2025 Task 8.",
    "published": "2025-07-10T06:16:51Z",
    "updated": "2025-07-10T06:16:51Z",
    "id": "2507.08046v1",
    "authors": [
      "Sishi Xiong",
      "Dakai Wang",
      "Yu Zhao",
      "Jie Zhang",
      "Changzai Pan",
      "Haowei He",
      "Xiangyu Li",
      "Wenhan Chang",
      "Zhongjiang He",
      "Shuangyong Song",
      "Yongxiang Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08046v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08046v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08046v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for table question answering (TQA) tasks, which involves reasoning and understanding of tabular data. The proposed framework, TableReasoner, leverages LLMs for schema linking and iterative reasoning, aligning with the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.07451v1": {
    "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning",
    "summary": "Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present \\emph{RLEP}\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.",
    "published": "2025-07-10T05:58:55Z",
    "updated": "2025-07-10T05:58:55Z",
    "id": "2507.07451v1",
    "authors": [
      "Hongzhi Zhang",
      "Jia Fu",
      "Jingyuan Zhang",
      "Kai Fu",
      "Qi Wang",
      "Fuzheng Zhang",
      "Guorui Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07451v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07451v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07451v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using reinforcement learning (RL) to improve the reasoning abilities of large language models (LLMs), specifically through a framework called RLEP that incorporates experience replay. This directly relates to RL in the context of LLMs and reasoning enhancement.",
    "llm_cls_result": [
      "RL",
      "Reasoning",
      "LLM"
    ]
  },
  "2507.07441v1": {
    "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation",
    "summary": "Large Language Model (LLM) agents are commonly tuned with supervised\nfinetuning on ReAct-style expert trajectories or preference optimization over\npairwise rollouts. Most of these methods focus on imitating specific expert\nbehaviors or promoting chosen reasoning thoughts and actions over rejected\nones. However, without reasoning and comparing over alternatives actions, LLM\nagents finetuned with these methods may over-commit towards seemingly plausible\nbut suboptimal actions due to limited action space exploration. To address\nthis, in this paper we propose Self-taught ActioN Deliberation (SAND)\nframework, enabling LLM agents to explicitly deliberate over candidate actions\nbefore committing to one. To tackle the challenges of when and what to\ndeliberate given large action space and step-level action evaluation, we\nincorporate self-consistency action sampling and execution-guided action\ncritique to help synthesize step-wise action deliberation thoughts using the\nbase model of the LLM agent. In an iterative manner, the deliberation\ntrajectories are then used to finetune the LLM agent itself. Evaluating on two\nrepresentative interactive agent tasks, SAND achieves an average 20%\nimprovement over initial supervised finetuning and also outperforms\nstate-of-the-art agent tuning approaches.",
    "published": "2025-07-10T05:38:15Z",
    "updated": "2025-07-10T05:38:15Z",
    "id": "2507.07441v1",
    "authors": [
      "Yu Xia",
      "Yiran Jenny Shen",
      "Junda Wu",
      "Tong Yu",
      "Sungchul Kim",
      "Ryan A. Rossi",
      "Lina Yao",
      "Julian McAuley"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07441v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07441v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07441v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving LLM agents through self-taught action deliberation, which involves reinforcement learning and human feedback (RLHF) techniques. It also discusses the reasoning abilities of LLMs in the context of action deliberation.",
    "llm_cls_result": [
      "RL",
      "Reasoning"
    ]
  },
  "2507.08877v1": {
    "title": "ODIA: Oriented Distillation for Inline Acceleration of LLM-based\n  Function Calling",
    "summary": "Function Calling is a crucial technique that enables Large Language Models\n(LLMs) to interact with external systems through APIs. However, the high\nlatency associated with LLM-based Function Calling significantly impacts user\nexperience. This paper presents a novel approach called Oriented Distillation\nfor Inline Acceleration (ODIA) that leverages online user interaction data to\naccelerate Function Calling. By automatically identifying \"simple queries\" from\nproduction traffic and distilling knowledge from larger models to smaller ones,\nour method reduces response latency by 45% (expected) and 78% (median) while\nmaintaining accuracy. We demonstrate the effectiveness of our approach through\nreal-world deployment in a music application, where the smaller model\nsuccessfully handles 60% of traffic with negligible accuracy loss. Our method\nrequires minimal human intervention and continuously improves through automated\ndata collection and model updating, making it a practical solution for\nproduction environments.",
    "published": "2025-07-10T04:44:47Z",
    "updated": "2025-07-10T04:44:47Z",
    "id": "2507.08877v1",
    "authors": [
      "Hanlong Zhang",
      "Jingsheng Yang",
      "Hao Li",
      "Yuhao He",
      "Franck Gong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08877v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08877v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08877v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on accelerating LLM-based Function Calling through distillation techniques, which involves knowledge transfer from larger to smaller models, a topic relevant to LLM research and optimization.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.07426v2": {
    "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and\n  Monte Carlo Tree Search",
    "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug discovery. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repurposing. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Without requiring domain-specific\nfine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by\nover 20\\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate\nthat DrugMCTS achieves substantially higher recall and robustness compared to\nboth general-purpose LLMs and deep learning baselines. Our results highlight\nthe importance of structured reasoning, agent-based collaboration, and\nfeedback-driven search mechanisms in advancing LLM applications for drug\ndiscovery.",
    "published": "2025-07-10T04:39:55Z",
    "updated": "2025-07-12T08:20:44Z",
    "id": "2507.07426v2",
    "authors": [
      "Zerui Yang",
      "Yuwei Wan",
      "Yinqiao Li",
      "Yudai Matsuda",
      "Tong Xie",
      "Linqi Song"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07426v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07426v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07426v2",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the integration of RAG (Retrieval-Augmented Generation), multi-agent collaboration, and Monte Carlo Tree Search for drug repurposing, which involves structured reasoning and agent-based collaboration. These topics are relevant to the categories 'RL' (Reinforcement Learning with agents), 'Memory' (retrieval-augmented generation), and 'Reasoning' (structured reasoning).",
    "llm_cls_result": [
      "RL",
      "Memory",
      "Reasoning"
    ]
  },
  "2507.07417v1": {
    "title": "May I have your Attention? Breaking Fine-Tuning based Prompt Injection\n  Defenses using Architecture-Aware Attacks",
    "summary": "A popular class of defenses against prompt injection attacks on large\nlanguage models (LLMs) relies on fine-tuning the model to separate instructions\nand data, so that the LLM does not follow instructions that might be present\nwith data. There are several academic systems and production-level\nimplementations of this idea. We evaluate the robustness of this class of\nprompt injection defenses in the whitebox setting by constructing strong\noptimization-based attacks and showing that the defenses do not provide the\nclaimed security properties. Specifically, we construct a novel attention-based\nattack algorithm for text-based LLMs and apply it to two recent whitebox\ndefenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks\nwith success rates of up to 70% with modest increase in attacker budget in\nterms of tokens. Our findings make fundamental progress towards understanding\nthe robustness of prompt injection defenses in the whitebox setting. We release\nour code and attacks at https://github.com/nishitvp/better_opts_attacks",
    "published": "2025-07-10T04:20:53Z",
    "updated": "2025-07-10T04:20:53Z",
    "id": "2507.07417v1",
    "authors": [
      "Nishit V. Pandya",
      "Andrey Labunets",
      "Sicun Gao",
      "Earlence Fernandes"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07417v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07417v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07417v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses prompt injection attacks on large language models (LLMs) and evaluates the robustness of fine-tuning based defenses. It focuses on LLM security and optimization-based attacks, which are relevant to the LLM topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.07414v1": {
    "title": "GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural\n  Networks for Text Representation",
    "summary": "Time, cost, and energy efficiency are critical considerations in\nDeep-Learning (DL), particularly when processing long texts. Transformers,\nwhich represent the current state of the art, exhibit quadratic computational\ncomplexity relative to input length, making them inefficient for extended\ndocuments. This study introduces a novel model architecture that combines Graph\nNeural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated\nwith a real-time, end-to-end graph generation mechanism. The model processes\ncompact batches of character-level inputs without requiring padding or\ntruncation. To enhance performance while maintaining high speed and efficiency,\nthe model incorporates information from Large Language Models (LLMs), such as\ntoken embeddings and sentiment polarities, through efficient dictionary\nlookups. It captures local contextual patterns using CNNs, expands local\nreceptive fields via lattice-based graph structures, and employs small-world\ngraphs to aggregate document-level information. The generated graphs exhibit\nstructural properties indicative of meaningful semantic organization, with an\naverage clustering coefficient of approximately 0.45 and an average shortest\npath length ranging between 4 and 5. The model is evaluated across multiple\ntext classification tasks, including sentiment analysis and\nnews-categorization, and is compared against state-of-the-art models.\nExperimental results confirm the proposed model's efficiency and competitive\nperformance.",
    "published": "2025-07-10T04:13:53Z",
    "updated": "2025-07-10T04:13:53Z",
    "id": "2507.07414v1",
    "authors": [
      "Fardin Rastakhiz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07414v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07414v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07414v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a hybrid model combining GNNs and CNNs for text representation, incorporating LLM information for enhanced performance. While it mentions LLMs, the primary focus is on the hybrid model architecture and its efficiency, not on LLM research or related topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.07413v1": {
    "title": "Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT\n  Networks",
    "summary": "This paper presents a novel approach to intrusion detection by integrating\ntraditional signature-based methods with the contextual understanding\ncapabilities of the GPT-2 Large Language Model (LLM). As cyber threats become\nincreasingly sophisticated, particularly in distributed, heterogeneous, and\nresource-constrained environments such as those enabled by the Internet of\nThings (IoT), the need for dynamic and adaptive Intrusion Detection Systems\n(IDSs) becomes increasingly urgent. While traditional methods remain effective\nfor detecting known threats, they often fail to recognize new and evolving\nattack patterns. In contrast, GPT-2 excels at processing unstructured data and\nidentifying complex semantic relationships, making it well-suited to uncovering\nsubtle, zero-day attack vectors. We propose a hybrid IDS framework that merges\nthe robustness of signature-based techniques with the adaptability of\nGPT-2-driven semantic analysis. Experimental evaluations on a representative\nintrusion dataset demonstrate that our model enhances detection accuracy by\n6.3%, reduces false positives by 9.0%, and maintains near real-time\nresponsiveness. These results affirm the potential of language model\nintegration to build intelligent, scalable, and resilient cybersecurity\ndefences suited for modern connected environments.",
    "published": "2025-07-10T04:10:03Z",
    "updated": "2025-07-10T04:10:03Z",
    "id": "2507.07413v1",
    "authors": [
      "Mohammad F. Al-Hammouri",
      "Yazan Otoum",
      "Rasha Atwa",
      "Amiya Nayak"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07413v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07413v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07413v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of GPT-2, a Large Language Model (LLM), into an intrusion detection system for IoT networks, highlighting its capabilities in processing unstructured data and identifying complex semantic relationships. This aligns with the LLM topic, which focuses on research related to Large Language Models and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.07400v1": {
    "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows",
    "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.",
    "published": "2025-07-10T03:39:23Z",
    "updated": "2025-07-10T03:39:23Z",
    "id": "2507.07400v1",
    "authors": [
      "Zaifeng Pan",
      "Ajjkumar Patel",
      "Zhengding Hu",
      "Yipeng Shen",
      "Yue Guan",
      "Wan-Lu Li",
      "Lianhui Qin",
      "Yida Wang",
      "Yufei Ding"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07400v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07400v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07400v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses efficient prefix caching for accelerating LLM-based multi-agent workflows, which involves optimizing the use of KV caches in LLM systems. This directly relates to the topics of LLM (Large Language Models) and Memory (specifically, KV cache management and memory-augmented models).",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.07394v1": {
    "title": "Behave Your Motion: Habit-preserved Cross-category Animal Motion\n  Transfer",
    "summary": "Animal motion embodies species-specific behavioral habits, making the\ntransfer of motion across categories a critical yet complex task for\napplications in animation and virtual reality. Existing motion transfer\nmethods, primarily focused on human motion, emphasize skeletal alignment\n(motion retargeting) or stylistic consistency (motion style transfer), often\nneglecting the preservation of distinct habitual behaviors in animals. To\nbridge this gap, we propose a novel habit-preserved motion transfer framework\nfor cross-category animal motion. Built upon a generative framework, our model\nintroduces a habit-preservation module with category-specific habit encoder,\nallowing it to learn motion priors that capture distinctive habitual\ncharacteristics. Furthermore, we integrate a large language model (LLM) to\nfacilitate the motion transfer to previously unobserved species. To evaluate\nthe effectiveness of our approach, we introduce the DeformingThings4D-skl\ndataset, a quadruped dataset with skeletal bindings, and conduct extensive\nexperiments and quantitative analyses, which validate the superiority of our\nproposed model.",
    "published": "2025-07-10T03:25:50Z",
    "updated": "2025-07-10T03:25:50Z",
    "id": "2507.07394v1",
    "authors": [
      "Zhimin Zhang",
      "Bi'an Du",
      "Caoyuan Ma",
      "Zheng Wang",
      "Wei Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07394v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07394v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07394v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on motion transfer in animals, emphasizing the preservation of habitual behaviors and the use of a large language model (LLM) to facilitate the transfer to unobserved species. The primary topics are related to LLM and motion transfer, but the core focus is not directly aligned with the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.08045v1": {
    "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
    "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
    "published": "2025-07-10T01:51:17Z",
    "updated": "2025-07-10T01:51:17Z",
    "id": "2507.08045v1",
    "authors": [
      "Junyi Wen",
      "Junyuan Liang",
      "Zicong Hong",
      "Wuhui Chen",
      "Zibin Zheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08045v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08045v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08045v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the efficiency of state restoration in multi-turn conversations with large language models (LLMs) by dynamically selecting compression strategies for KV caches. This involves innovations in attention similarity estimation and KV cache restoration, which are directly related to the optimization and scaling of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling",
      "Memory"
    ]
  },
  "2507.07341v1": {
    "title": "On the Impossibility of Separating Intelligence from Judgment: The\n  Computational Intractability of Filtering for AI Alignment",
    "summary": "With the increased deployment of large language models (LLMs), one concern is\ntheir potential misuse for generating harmful content. Our work studies the\nalignment challenge, with a focus on filters to prevent the generation of\nunsafe information. Two natural points of intervention are the filtering of the\ninput prompt before it reaches the model, and filtering the output after\ngeneration. Our main results demonstrate computational challenges in filtering\nboth prompts and outputs. First, we show that there exist LLMs for which there\nare no efficient prompt filters: adversarial prompts that elicit harmful\nbehavior can be easily constructed, which are computationally indistinguishable\nfrom benign prompts for any efficient filter. Our second main result identifies\na natural setting in which output filtering is computationally intractable. All\nof our separation results are under cryptographic hardness assumptions. In\naddition to these core findings, we also formalize and study relaxed mitigation\napproaches, demonstrating further computational barriers. We conclude that\nsafety cannot be achieved by designing filters external to the LLM internals\n(architecture and weights); in particular, black-box access to the LLM will not\nsuffice. Based on our technical results, we argue that an aligned AI system's\nintelligence cannot be separated from its judgment.",
    "published": "2025-07-09T23:55:35Z",
    "updated": "2025-07-09T23:55:35Z",
    "id": "2507.07341v1",
    "authors": [
      "Sarah Ball",
      "Greg Gluch",
      "Shafi Goldwasser",
      "Frauke Kreuter",
      "Omer Reingold",
      "Guy N. Rothblum"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07341v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07341v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07341v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the alignment challenge of large language models (LLMs) and the computational intractability of filtering for AI alignment, which is closely related to the topics of LLM and RL (Reinforcement Learning with Human Feedback). The abstract mentions the misuse of LLMs for generating harmful content and the challenges in filtering prompts and outputs, which aligns with the focus on LLM architectures and RLHF.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.07328v1": {
    "title": "Bridging the Plausibility-Validity Gap by Fine-Tuning a\n  Reasoning-Enhanced LLM for Chemical Synthesis and Discovery",
    "summary": "Large Language Models (LLMs) often generate scientifically plausible but\nfactually invalid information, a challenge we term the \"plausibility-validity\ngap,\" particularly in specialized domains like chemistry. This paper presents a\nsystematic methodology to bridge this gap by developing a specialized\nscientific assistant. We utilized the Magistral Small model, noted for its\nintegrated reasoning capabilities, and fine-tuned it using Low-Rank Adaptation\n(LoRA). A key component of our approach was the creation of a \"dual-domain\ndataset,\" a comprehensive corpus curated from various sources encompassing both\nmolecular properties and chemical reactions, which was standardized to ensure\nquality. Our evaluation demonstrates that the fine-tuned model achieves\nsignificant improvements over the baseline model in format adherence, chemical\nvalidity of generated molecules, and the feasibility of proposed synthesis\nroutes. The results indicate a hierarchical learning pattern, where syntactic\ncorrectness is learned more readily than chemical possibility and synthesis\nfeasibility. While a comparative analysis with human experts revealed\ncompetitive performance in areas like chemical creativity and reasoning, it\nalso highlighted key limitations, including persistent errors in\nstereochemistry, a static knowledge cutoff, and occasional reference\nhallucination. This work establishes a viable framework for adapting generalist\nLLMs into reliable, specialized tools for chemical research, while also\ndelineating critical areas for future improvement.",
    "published": "2025-07-09T23:05:23Z",
    "updated": "2025-07-09T23:05:23Z",
    "id": "2507.07328v1",
    "authors": [
      " Malikussaid",
      "Hilal Hudan Nuha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07328v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07328v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07328v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a Large Language Model (LLM) with integrated reasoning capabilities, fine-tuned for a specialized domain (chemistry), and evaluates its performance in generating valid scientific information. This aligns with topics related to LLMs, reasoning in LLMs, and their application in specialized domains.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.07307v1": {
    "title": "Multi-Agent Retrieval-Augmented Framework for Evidence-Based\n  Counterspeech Against Health Misinformation",
    "summary": "Large language models (LLMs) incorporated with Retrieval-Augmented Generation\n(RAG) have demonstrated powerful capabilities in generating counterspeech\nagainst misinformation. However, current studies rely on limited evidence and\noffer less control over final outputs. To address these challenges, we propose\na Multi-agent Retrieval-Augmented Framework to generate counterspeech against\nhealth misinformation, incorporating multiple LLMs to optimize knowledge\nretrieval, evidence enhancement, and response refinement. Our approach\nintegrates both static and dynamic evidence, ensuring that the generated\ncounterspeech is relevant, well-grounded, and up-to-date. Our method\noutperforms baseline approaches in politeness, relevance, informativeness, and\nfactual accuracy, demonstrating its effectiveness in generating high-quality\ncounterspeech. To further validate our approach, we conduct ablation studies to\nverify the necessity of each component in our framework. Furthermore, human\nevaluations reveal that refinement significantly enhances counterspeech quality\nand obtains human preference.",
    "published": "2025-07-09T22:10:06Z",
    "updated": "2025-07-09T22:10:06Z",
    "id": "2507.07307v1",
    "authors": [
      "Anirban Saha Anik",
      "Xiaoying Song",
      "Elliott Wang",
      "Bryan Wang",
      "Bengisu Yarimbas",
      "Lingzi Hong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07307v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07307v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07307v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) for generating counterspeech against misinformation, which involves memory-augmented models and retrieval-based methods. It also involves multiple LLMs, indicating a focus on LLM architectures and their applications.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "RL"
    ]
  },
  "2507.07302v1": {
    "title": "Application of LLMs to Multi-Robot Path Planning and Task Allocation",
    "summary": "Efficient exploration is a well known problem in deep reinforcement learning\nand this problem is exacerbated in multi-agent reinforcement learning due the\nintrinsic complexities of such algorithms. There are several approaches to\nefficiently explore an environment to learn to solve tasks by multi-agent\noperating in that environment, of which, the idea of expert exploration is\ninvestigated in this work. More specifically, this work investigates the\napplication of large-language models as expert planners for efficient\nexploration in planning based tasks for multiple agents.",
    "published": "2025-07-09T22:01:32Z",
    "updated": "2025-07-09T22:01:32Z",
    "id": "2507.07302v1",
    "authors": [
      "Ashish Kumar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07302v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07302v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07302v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in the context of multi-agent reinforcement learning and task planning, which aligns with the topics of LLM and RL.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.07293v1": {
    "title": "Thermodynamic Prediction Enabled by Automatic Dataset Building and\n  Machine Learning",
    "summary": "New discoveries in chemistry and materials science, with increasingly\nexpanding volume of requisite knowledge and experimental workload, provide\nunique opportunities for machine learning (ML) to take critical roles in\naccelerating research efficiency. Here, we demonstrate (1) the use of large\nlanguage models (LLMs) for automated literature reviews, and (2) the training\nof an ML model to predict chemical knowledge (thermodynamic parameters). Our\nLLM-based literature review tool (LMExt) successfully extracted chemical\ninformation and beyond into a machine-readable structure, including stability\nconstants for metal cation-ligand interactions, thermodynamic properties, and\nother broader data types (medical research papers, and financial reports),\neffectively overcoming the challenges inherent in each domain. Using the\nautonomous acquisition of thermodynamic data, an ML model was trained using the\nCatBoost algorithm for accurately predicting thermodynamic parameters (e.g.,\nenthalpy of formation) of minerals. This work highlights the transformative\npotential of integrated ML approaches to reshape chemistry and materials\nscience research.",
    "published": "2025-07-09T21:33:25Z",
    "updated": "2025-07-09T21:33:25Z",
    "id": "2507.07293v1",
    "authors": [
      "Juejing Liu",
      "Haydn Anderson",
      "Noah I. Waxman",
      "Vsevolod Kovalev",
      "Byron Fisher",
      "Elizabeth Li",
      "Xiaofeng Guo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07293v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07293v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07293v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for automated literature reviews and the training of an ML model for predicting thermodynamic parameters, which aligns with the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.07251v1": {
    "title": "A Language-Driven Framework for Improving Personalized Recommendations:\n  Merging LLMs with Traditional Algorithms",
    "summary": "Traditional recommendation algorithms are not designed to provide\npersonalized recommendations based on user preferences provided through text,\ne.g., \"I enjoy light-hearted comedies with a lot of humor\". Large Language\nModels (LLMs) have emerged as one of the most promising tools for natural\nlanguage processing in recent years. This research proposes a novel framework\nthat mimics how a close friend would recommend items based on their knowledge\nof an individual's tastes. We leverage LLMs to enhance movie recommendation\nsystems by refining traditional algorithm outputs and integrating them with\nlanguage-based user preference inputs. We employ Singular Value Decomposition\n(SVD) or SVD++ algorithms to generate initial movie recommendations,\nimplemented using the Surprise Python library and trained on the\nMovieLens-Latest-Small dataset. We compare the performance of the base\nalgorithms with our LLM-enhanced versions using leave-one-out validation hit\nrates and cumulative hit rates. Additionally, to compare the performance of our\nframework against the current state-of-the-art recommendation systems, we use\nrating and ranking metrics with an item-based stratified 0.75 train, 0.25 test\nsplit. Our framework can generate preference profiles automatically based on\nusers' favorite movies or allow manual preference specification for more\npersonalized results. Using an automated approach, our framework overwhelmingly\nsurpassed SVD and SVD++ on every evaluation metric used (e.g., improvements of\nup to ~6x in cumulative hit rate, ~3.7x in NDCG, etc.), albeit at the cost of a\nslight increase in computational overhead.",
    "published": "2025-07-09T19:48:33Z",
    "updated": "2025-07-09T19:48:33Z",
    "id": "2507.07251v1",
    "authors": [
      "Aaron Goldstein",
      "Ayan Dutta"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07251v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07251v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07251v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging Large Language Models (LLMs) to enhance traditional recommendation algorithms, which directly involves the use of LLMs in a specific application context.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.07236v1": {
    "title": "An Information-Theoretic Perspective on Multi-LLM Uncertainty Estimation",
    "summary": "Large language models (LLMs) often behave inconsistently across inputs,\nindicating uncertainty and motivating the need for its quantification in\nhigh-stakes settings. Prior work on calibration and uncertainty quantification\noften focuses on individual models, overlooking the potential of model\ndiversity. We hypothesize that LLMs make complementary predictions due to\ndifferences in training and the Zipfian nature of language, and that\naggregating their outputs leads to more reliable uncertainty estimates. To\nleverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a\nsimple information-theoretic method that uses Jensen-Shannon Divergence to\nidentify and aggregate well-calibrated subsets of LLMs. Experiments on binary\nprediction tasks demonstrate improved calibration and predictive performance\ncompared to single-model and naive ensemble baselines.",
    "published": "2025-07-09T19:13:25Z",
    "updated": "2025-07-09T19:13:25Z",
    "id": "2507.07236v1",
    "authors": [
      "Maya Kruse",
      "Majid Afshar",
      "Saksham Khatwani",
      "Anoop Mayampurath",
      "Guanhua Chen",
      "Yanjun Gao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07236v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07236v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07236v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on uncertainty estimation in Large Language Models (LLMs) and proposes a method to aggregate outputs from multiple LLMs for better uncertainty quantification. This aligns with the topics of LLM research and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.07229v1": {
    "title": "SynthTextEval: Synthetic Text Data Generation and Evaluation for\n  High-Stakes Domains",
    "summary": "We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development.",
    "published": "2025-07-09T19:05:33Z",
    "updated": "2025-07-09T19:05:33Z",
    "id": "2507.07229v1",
    "authors": [
      "Krithika Ramesh",
      "Daniel Smolyak",
      "Zihao Zhao",
      "Nupoor Gandhi",
      "Ritu Agarwal",
      "Margrt Bjarnadttir",
      "Anjalie Field"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07229v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07229v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07229v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of synthetic text generated by large language models (LLMs) and its applications in high-stakes domains, which aligns with topics related to LLMs and datasets.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.07223v2": {
    "title": "Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory\n  and Interconnects in Modern AI Infrastructure",
    "summary": "Modern AI workloads such as large language models (LLMs) and\nretrieval-augmented generation (RAG) impose severe demands on memory,\ncommunication bandwidth, and resource flexibility. Traditional GPU-centric\narchitectures struggle to scale due to growing inter-GPU communication\noverheads. This report introduces key AI concepts and explains how Transformers\nrevolutionized data representation in LLMs. We analyze large-scale AI hardware\nand data center designs, identifying scalability bottlenecks in hierarchical\nsystems. To address these, we propose a modular data center architecture based\non Compute Express Link (CXL) that enables disaggregated scaling of memory,\ncompute, and accelerators. We further explore accelerator-optimized\ninterconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink\nFusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance\ndata transfers while preserving memory coherence. We also propose a\nhierarchical memory model that combines local and pooled memory, and evaluate\nlightweight CXL implementations, HBM, and silicon photonics for efficient\nscaling. Our evaluations demonstrate improved scalability, throughput, and\nflexibility in AI infrastructure.",
    "published": "2025-07-09T18:57:04Z",
    "updated": "2025-07-13T15:21:59Z",
    "id": "2507.07223v2",
    "authors": [
      "Myoungsoo Jung"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07223v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07223v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07223v2",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the challenges and solutions related to memory and interconnects in modern AI infrastructure, particularly focusing on large language models (LLMs) and retrieval-augmented generation (RAG). It emphasizes the need for efficient memory and communication bandwidth, which aligns with the 'Memory' and 'Scaling' topics.",
    "llm_cls_result": [
      "Memory",
      "Scaling"
    ]
  },
  "2507.07217v1": {
    "title": "Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply\n  Chains",
    "summary": "Supply chain networks are complex systems that are challenging to analyze;\nthis problem is exacerbated when there are illicit activities involved in the\nsupply chain, such as counterfeit parts, forced labor, or human trafficking.\nWhile machine learning (ML) can find patterns in complex systems like supply\nchains, traditional ML techniques require large training data sets. However,\nillicit supply chains are characterized by very sparse data, and the data that\nis available is often (purposely) corrupted or unreliable in order to hide the\nnature of the activities. We need to be able to automatically detect new\npatterns that correlate with such illegal activity over complex, even temporal\ndata, without requiring large training data sets. We explore neurosymbolic\nmethods for identifying instances of illicit activity in supply chains and\ncompare the effectiveness of manual and automated feature extraction from news\narticles accurately describing illicit activities uncovered by authorities. We\npropose a question tree approach for querying a large language model (LLM) to\nidentify and quantify the relevance of articles. This enables a systematic\nevaluation of the differences between human and machine classification of news\narticles related to forced labor in supply chains.",
    "published": "2025-07-09T18:44:48Z",
    "updated": "2025-07-09T18:44:48Z",
    "id": "2507.07217v1",
    "authors": [
      "Zili Wang",
      "Frank Montabon",
      "Kristin Yvonne Rozier"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07217v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07217v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07217v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a large language model (LLM) for identifying and quantifying the relevance of articles related to forced labor in supply chains, which involves reasoning and classification tasks. However, the primary focus is on neurosymbolic methods and supply chain analysis rather than core LLM research topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.07203v1": {
    "title": "State-Inference-Based Prompting for Natural Language Trading with Game\n  NPCs",
    "summary": "Large Language Models enable dynamic game interactions but struggle with\nrule-governed trading systems. Current implementations suffer from rule\nviolations, such as item hallucinations and calculation errors, that erode\nplayer trust. Here, State-Inference-Based Prompting (SIBP) enables reliable\ntrading through autonomous dialogue state inference and context-specific rule\nadherence. The approach decomposes trading into six states within a unified\nprompt framework, implementing context-aware item referencing and\nplaceholder-based price calculations. Evaluation across 100 trading dialogues\ndemonstrates >97% state compliance, >95% referencing accuracy, and 99.7%\ncalculation precision. SIBP maintains computational efficiency while\noutperforming baseline approaches, establishing a practical foundation for\ntrustworthy NPC interactions in commercial games.",
    "published": "2025-07-09T18:24:47Z",
    "updated": "2025-07-09T18:24:47Z",
    "id": "2507.07203v1",
    "authors": [
      "Minkyung Kim",
      "Junsik Kim",
      "Hwidong Bae",
      "Woongcheol Yang",
      "Sangdon Park",
      "Sohee Bae"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07203v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07203v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07203v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in dynamic game interactions, specifically addressing rule-governed trading systems. It introduces a prompting technique (SIBP) to improve reliability and adherence to rules, which is relevant to LLM research and their practical applications.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.07188v2": {
    "title": "Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses",
    "summary": "Large Language Models (LLMs) are increasingly used as proxies for human\nsubjects in social science surveys, but their reliability and susceptibility to\nknown response biases are poorly understood. This paper investigates the\nresponse robustness of LLMs in normative survey contexts - we test nine diverse\nLLMs on questions from the World Values Survey (WVS), applying a comprehensive\nset of 11 perturbations to both question phrasing and answer option structure,\nresulting in over 167,000 simulated interviews. In doing so, we not only reveal\nLLMs' vulnerabilities to perturbations but also show that all tested models\nexhibit a consistent recency bias varying in intensity, disproportionately\nfavoring the last-presented answer option. While larger models are generally\nmore robust, all models remain sensitive to semantic variations like\nparaphrasing and to combined perturbations. By applying a set of perturbations,\nwe reveal that LLMs partially align with survey response biases identified in\nhumans. This underscores the critical importance of prompt design and\nrobustness testing when using LLMs to generate synthetic survey data.",
    "published": "2025-07-09T18:01:50Z",
    "updated": "2025-07-16T18:02:56Z",
    "id": "2507.07188v2",
    "authors": [
      "Jens Rupprecht",
      "Georg Ahnert",
      "Markus Strohmaier"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07188v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07188v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07188v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper investigates the response robustness of LLMs in normative survey contexts, revealing their vulnerabilities to perturbations and biases, which aligns with research on LLM reasoning and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.07186v2": {
    "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the\n  Origins of Cognitive Biases in LLMs",
    "summary": "Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs.",
    "published": "2025-07-09T18:01:14Z",
    "updated": "2025-07-12T10:00:59Z",
    "id": "2507.07186v2",
    "authors": [
      "Itay Itzhak",
      "Yonatan Belinkov",
      "Gabriel Stanovsky"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07186v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07186v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07186v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses cognitive biases in Large Language Models (LLMs) and investigates their origins, focusing on pretraining and finetuning effects. It aligns with topics related to LLMs and their training processes.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.07104v2": {
    "title": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation\n  from Diffusion Models",
    "summary": "Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD.",
    "published": "2025-07-09T17:59:04Z",
    "updated": "2025-07-11T03:43:50Z",
    "id": "2507.07104v2",
    "authors": [
      "Tiezheng Zhang",
      "Yitong Li",
      "Yu-cheng Chou",
      "Jieneng Chen",
      "Alan Yuille",
      "Chen Wei",
      "Junfei Xiao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07104v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07104v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07104v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a Vision-Language-Vision (VLV) auto-encoder framework that integrates vision and language modalities, leveraging pretrained components like a vision encoder, a Text-to-Image (T2I) diffusion model, and a Large Language Model (LLM). It focuses on knowledge distillation and semantic understanding in a multimodal context, aligning with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Alignment (VLA).",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.07157v1": {
    "title": "Interpretable EEG-to-Image Generation with Semantic Prompts",
    "summary": "Decoding visual experience from brain signals offers exciting possibilities\nfor neuroscience and interpretable AI. While EEG is accessible and temporally\nprecise, its limitations in spatial detail hinder image reconstruction. Our\nmodel bypasses direct EEG-to-image generation by aligning EEG signals with\nmultilevel semantic captions -- ranging from object-level to abstract themes --\ngenerated by a large language model. A transformer-based EEG encoder maps brain\nactivity to these captions through contrastive learning. During inference,\ncaption embeddings retrieved via projection heads condition a pretrained latent\ndiffusion model for image generation. This text-mediated framework yields\nstate-of-the-art visual decoding on the EEGCVPR dataset, with interpretable\nalignment to known neurocognitive pathways. Dominant EEG-caption associations\nreflected the importance of different semantic levels extracted from perceived\nimages. Saliency maps and t-SNE projections reveal semantic topography across\nthe scalp. Our model demonstrates how structured semantic mediation enables\ncognitively aligned visual decoding from EEG.",
    "published": "2025-07-09T17:18:06Z",
    "updated": "2025-07-09T17:18:06Z",
    "id": "2507.07157v1",
    "authors": [
      "Arshak Rezvani",
      "Ali Akbari",
      "Kosar Sanjar Arani",
      "Maryam Mirian",
      "Emad Arasteh",
      "Martin J. McKeown"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07157v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07157v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07157v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on EEG-to-image generation using semantic prompts and a large language model, which involves multimodal integration and interpretation of brain signals. The core topics are related to multimodal large language models and vision-language alignment.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.07045v1": {
    "title": "5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient\n  Design Framework for Individual and SME LLM Usage",
    "summary": "The progression from traditional prompt engineering to a more rigorous\ndiscipline of prompt design marks a pivotal shift in human-LLM interaction. As\nLarge Language Models (LLMs) become increasingly embedded in mission-critical\napplications, there emerges a pressing need for frameworks that are not only\nexplicit and systematic but also minimal enough to remain practical and broadly\naccessible. While many existing approaches address prompt structuring through\nelaborate Domain-Specific Languages (DSLs) or multi-layered templates, such\nmethods can impose significant token and cognitive overhead, potentially\nconstraining the model's creative capacity. In this context, we propose the 5C\nPrompt Contract, a framework that distills prompt design into five intuitive\ncomponents: Character, Cause, Constraint, Contingency, and Calibration. This\nminimal cognitive schema explicitly integrates fallback and output optimization\ndirectives, fostering reliable, interpretable, and creatively flexible AI\ninteractions. Experimental results demonstrate that the 5C framework\nconsistently achieves superior input token efficiency while maintaining rich\nand consistent outputs across diverse LLM architectures (OpenAI, Anthropic,\nDeepSeek, and Gemini), making it particularly suited for individuals and\nSmall-to-Medium Enterprises (SMEs) with limited AI engineering resources.",
    "published": "2025-07-09T17:07:39Z",
    "updated": "2025-07-09T17:07:39Z",
    "id": "2507.07045v1",
    "authors": [
      "Ugur Ari"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07045v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07045v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07045v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a framework for prompt design in human-LLM interaction, focusing on efficiency and accessibility for individual and SME usage. It directly relates to the use and optimization of Large Language Models (LLMs) and their interaction frameworks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.13369v1": {
    "title": "VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing\n  Framework for LLM-based RTL Generation",
    "summary": "Large Language Models (LLMs) are gaining popularity for hardware design\nautomation, particularly through Register Transfer Level (RTL) code generation.\nIn this work, we examine the current literature on RTL generation using LLMs\nand identify key requirements for training and fine-tuning datasets. We\nconstruct a robust Verilog dataset through an automated three-pronged process\ninvolving database (DB) creation and management with PostgreSQL, data\ncollection from code hosting sites like OpenCores and GitHub, and data\npreprocessing to verify the codes' syntax, run logic synthesis, and extract\nrelevant module metadata. We implement a scalable and efficient DB\ninfrastructure to support analysis and detail our preprocessing pipeline to\nenforce high-quality data before DB insertion. The resulting dataset comprises\n20,392 Verilog samples, 751 MB of Verilog code data, which is the largest\nhigh-quality Verilog dataset for LLM fine-tuning to our knowledge. We further\nevaluate the dataset, address associated challenges, and explore potential\napplications for future research and development in LLM-based hardware\ngeneration.",
    "published": "2025-07-09T17:06:54Z",
    "updated": "2025-07-09T17:06:54Z",
    "id": "2507.13369v1",
    "authors": [
      "Paul E. Calzada",
      "Zahin Ibnat",
      "Tanvir Rahman",
      "Kamal Kandula",
      "Danyu Lu",
      "Sujan Kumar Saha",
      "Farimah Farahmandi",
      "Mark Tehranipoor"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13369v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13369v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13369v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the creation of a large dataset for fine-tuning LLMs specifically for RTL generation, which involves LLM applications in hardware design automation. The focus is on dataset construction and preprocessing for LLM fine-tuning.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.07030v1": {
    "title": "UniConv: Unifying Retrieval and Response Generation for Large Language\n  Models in Conversations",
    "summary": "The rapid advancement of conversational search systems revolutionizes how\ninformation is accessed by enabling the multi-turn interaction between the user\nand the system. Existing conversational search systems are usually built with\ntwo different models. This separation restricts the system from leveraging the\nintrinsic knowledge of the models simultaneously, which cannot ensure the\neffectiveness of retrieval benefiting the generation. The existing studies for\ndeveloping unified models cannot fully address the aspects of understanding\nconversational context, managing retrieval independently, and generating\nresponses. In this paper, we explore how to unify dense retrieval and response\ngeneration for large language models in conversation. We conduct joint\nfine-tuning with different objectives and design two mechanisms to reduce the\ninconsistency risks while mitigating data discrepancy. The evaluations on five\nconversational search datasets demonstrate that our unified model can mutually\nimprove both tasks and outperform the existing baselines.",
    "published": "2025-07-09T17:02:40Z",
    "updated": "2025-07-09T17:02:40Z",
    "id": "2507.07030v1",
    "authors": [
      "Fengran Mo",
      "Yifan Gao",
      "Chuan Meng",
      "Xin Liu",
      "Zhuofeng Wu",
      "Kelong Mao",
      "Zhengyang Wang",
      "Pei Chen",
      "Zheng Li",
      "Xian Li",
      "Bing Yin",
      "Meng Jiang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07030v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07030v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07030v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses unifying retrieval and response generation for large language models in conversations, which involves aspects of retrieval-augmented generation and memory in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.07006v1": {
    "title": "GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision\n  Transformers for Whole Slide Image Classification and Captioning",
    "summary": "Microscopic assessment of histopathology images is vital for accurate cancer\ndiagnosis and treatment. Whole Slide Image (WSI) classification and captioning\nhave become crucial tasks in computer-aided pathology. However, microscopic WSI\nface challenges such as redundant patches and unknown patch positions due to\nsubjective pathologist captures. Moreover, generating automatic pathology\ncaptions remains a significant challenge. To address these issues, we introduce\na novel GNN-ViTCap framework for classification and caption generation from\nhistopathological microscopic images. First, a visual feature extractor\ngenerates patch embeddings. Redundant patches are then removed by dynamically\nclustering these embeddings using deep embedded clustering and selecting\nrepresentative patches via a scalar dot attention mechanism. We build a graph\nby connecting each node to its nearest neighbors in the similarity matrix and\napply a graph neural network to capture both local and global context. The\naggregated image embeddings are projected into the language model's input space\nthrough a linear layer and combined with caption tokens to fine-tune a large\nlanguage model. We validate our method on the BreakHis and PatchGastric\ndatasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for\nclassification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569\nfor captioning. Experimental results demonstrate that GNN-ViTCap outperforms\nstate of the art approaches, offering a reliable and efficient solution for\nmicroscopy based patient diagnosis.",
    "published": "2025-07-09T16:35:21Z",
    "updated": "2025-07-09T16:35:21Z",
    "id": "2507.07006v1",
    "authors": [
      "S M Taslim Uddin Raju",
      "Md. Milon Islam",
      "Md Rezwanul Haque",
      "Hamdi Altaheri",
      "Fakhri Karray"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07006v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07006v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07006v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a novel framework for Whole Slide Image classification and captioning using GNN and Vision Transformers, which does not directly align with the provided topics related to LLMs, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.06956v1": {
    "title": "Investigating the Robustness of Retrieval-Augmented Generation at the\n  Query Level",
    "summary": "Large language models (LLMs) are very costly and inefficient to update with\nnew information. To address this limitation, retrieval-augmented generation\n(RAG) has been proposed as a solution that dynamically incorporates external\nknowledge during inference, improving factual consistency and reducing\nhallucinations. Despite its promise, RAG systems face practical challenges-most\nnotably, a strong dependence on the quality of the input query for accurate\nretrieval. In this paper, we investigate the sensitivity of different\ncomponents in the RAG pipeline to various types of query perturbations. Our\nanalysis reveals that the performance of commonly used retrievers can degrade\nsignificantly even under minor query variations. We study each module in\nisolation as well as their combined effect in an end-to-end question answering\nsetting, using both general-domain and domain-specific datasets. Additionally,\nwe propose an evaluation framework to systematically assess the query-level\nrobustness of RAG pipelines and offer actionable recommendations for\npractitioners based on the results of more than 1092 experiments we performed.",
    "published": "2025-07-09T15:39:17Z",
    "updated": "2025-07-09T15:39:17Z",
    "id": "2507.06956v1",
    "authors": [
      "Sezen Perin",
      "Xin Su",
      "Qutub Sha Syed",
      "Phillip Howard",
      "Aleksei Kuvshinov",
      "Leo Schwinn",
      "Kay-Ulrich Scholl"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06956v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06956v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06956v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on retrieval-augmented generation (RAG) systems, which are a type of memory-augmented models, and investigates their robustness to query perturbations. This aligns with the 'Memory' topic, which includes retrieval-based methods and long-context processing. The study also involves large language models (LLMs), which is a core aspect of the 'LLM' topic.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.06910v1": {
    "title": "Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in\n  Dialogues",
    "summary": "Tutoring dialogues have gained significant attention in recent years, given\nthe prominence of online learning and the emerging tutoring abilities of\nartificial intelligence (AI) agents powered by large language models (LLMs).\nRecent studies have shown that the strategies used by tutors can have\nsignificant effects on student outcomes, necessitating methods to predict how\ntutors will behave and how their actions impact students. However, few works\nhave studied predicting tutor strategy in dialogues. Therefore, in this work we\ninvestigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to\npredict both future tutor moves and student outcomes in dialogues, using two\nmath tutoring dialogue datasets. We find that even state-of-the-art LLMs\nstruggle to predict future tutor strategy while tutor strategy is highly\nindicative of student outcomes, outlining a need for more powerful methods to\napproach this task.",
    "published": "2025-07-09T14:47:35Z",
    "updated": "2025-07-09T14:47:35Z",
    "id": "2507.06910v1",
    "authors": [
      "Fareya Ikram",
      "Alexander Scarlatos",
      "Andrew Lan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06910v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06910v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06910v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in predicting tutor strategies and student outcomes in dialogues, which directly relates to the 'LLM' topic. It also touches on the use of these models in educational settings, which could be seen as a form of reasoning or problem-solving, aligning with the 'Reasoning' topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.06909v1": {
    "title": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal\n  Prediction",
    "summary": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.",
    "published": "2025-07-09T14:47:00Z",
    "updated": "2025-07-09T14:47:00Z",
    "id": "2507.06909v1",
    "authors": [
      "Xiao Wang",
      "Jiahuan Pei",
      "Diancheng Shui",
      "Zhiguang Han",
      "Xin Sun",
      "Dawei Zhu",
      "Xiaoyu Shen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06909v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06909v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06909v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a new dataset for legal judgment prediction and evaluates the performance of legal large language models (LLMs) on various scenarios. The focus is on dataset creation and benchmarking LLMs in a specific domain (legal).",
    "llm_cls_result": [
      "Dataset",
      "Benchmark",
      "LLM"
    ]
  },
  "2507.06895v1": {
    "title": "SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label\n  Contrastive Learning and Bayesian kNN",
    "summary": "The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.",
    "published": "2025-07-09T14:33:07Z",
    "updated": "2025-07-09T14:33:07Z",
    "id": "2507.06895v1",
    "authors": [
      "Luca Mariotti",
      "Veronica Guidetti",
      "Federica Mandreoli"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06895v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06895v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06895v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on relation extraction (RE) using pre-trained large language models (PLMs) and introduces a modular system called SCoRE. It also discusses the use of contrastive learning and Bayesian kNN for multi-label classification, which are techniques related to LLMs and their applications in knowledge graph enrichment. The paper also introduces a benchmark dataset, which is relevant to the Dataset topic.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.06893v1": {
    "title": "Developing and Maintaining an Open-Source Repository of AI Evaluations:\n  Challenges and Insights",
    "summary": "AI evaluations have become critical tools for assessing large language model\ncapabilities and safety. This paper presents practical insights from eight\nmonths of maintaining $inspect\\_evals$, an open-source repository of 70+\ncommunity-contributed AI evaluations. We identify key challenges in\nimplementing and maintaining AI evaluations and develop solutions including:\n(1) a structured cohort management framework for scaling community\ncontributions, (2) statistical methodologies for optimal resampling and\ncross-model comparison with uncertainty quantification, and (3) systematic\nquality control processes for reproducibility. Our analysis reveals that AI\nevaluation requires specialized infrastructure, statistical rigor, and\ncommunity coordination beyond traditional software development practices.",
    "published": "2025-07-09T14:30:45Z",
    "updated": "2025-07-09T14:30:45Z",
    "id": "2507.06893v1",
    "authors": [
      "Alexandra Abbas",
      "Celia Waggoner",
      "Justin Olive"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06893v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06893v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06893v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development and maintenance of an open-source repository for AI evaluations, focusing on large language model capabilities and safety. It emphasizes benchmarking, community contributions, and evaluation methodologies, which align with the 'Benchmark' and 'Dataset' topics.",
    "llm_cls_result": [
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.06850v3": {
    "title": "The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover",
    "summary": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors.",
    "published": "2025-07-09T13:54:58Z",
    "updated": "2025-07-11T09:50:02Z",
    "id": "2507.06850v3",
    "authors": [
      "Matteo Lupinacci",
      "Francesco Aurelio Pironti",
      "Francesco Blefari",
      "Francesco Romeo",
      "Luigi Arena",
      "Angelo Furfaro"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06850v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06850v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06850v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses security vulnerabilities in LLM agents and multi-agent systems, focusing on attacks that exploit these systems. The core topics are related to LLM security and vulnerabilities, which are not directly covered by the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.06803v2": {
    "title": "Text to model via SysML: Automated generation of dynamical system\n  computational models from unstructured natural language text via enhanced\n  System Modeling Language diagrams",
    "summary": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only.",
    "published": "2025-07-09T12:44:49Z",
    "updated": "2025-07-15T11:05:37Z",
    "id": "2507.06803v2",
    "authors": [
      "Matthew Anderson Hendricks",
      "Alice Cicirello"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06803v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06803v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06803v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the automated generation of dynamical system computational models from unstructured natural language text. It specifically mentions the application of LLMs in improving intermediate outputs of SysML diagrams and validation tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.06795v2": {
    "title": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining",
    "summary": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.",
    "published": "2025-07-09T12:30:42Z",
    "updated": "2025-07-10T07:05:41Z",
    "id": "2507.06795v2",
    "authors": [
      "Seonwu Kim",
      "Yohan Na",
      "Kihun Kim",
      "Hanhee Cho",
      "Geun Lim",
      "Mintae Kim",
      "Seongik Park",
      "Ki Hyun Kim",
      "Youngsub Han",
      "Byoung-Ki Jeon"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06795v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06795v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06795v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Domain Adaptive Continual Pretraining (DACP) to small LLMs (sLLMs) for enterprise applications, focusing on domain adaptation and performance improvements. This aligns with the topics of Pretraining strategies and LLM research.",
    "llm_cls_result": [
      "Pretrain",
      "LLM"
    ]
  },
  "2507.07151v1": {
    "title": "Robust Multimodal Large Language Models Against Modality Conflict",
    "summary": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.",
    "published": "2025-07-09T11:18:38Z",
    "updated": "2025-07-09T11:18:38Z",
    "id": "2507.07151v1",
    "authors": [
      "Zongmeng Zhang",
      "Wengang Zhou",
      "Jie Zhao",
      "Houqiang Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07151v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07151v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07151v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Multimodal Large Language Models (MLLMs) and their issues with hallucinations due to modality conflict, proposing solutions involving reinforcement learning and supervised fine-tuning. The core topics are MLLM and RL, as the study involves multimodal models and reinforcement learning methods.",
    "llm_cls_result": [
      "MLLM",
      "RL"
    ]
  },
  "2507.06747v1": {
    "title": "LOVON: Legged Open-Vocabulary Object Navigator",
    "summary": "Object navigation in open-world environments remains a formidable and\npervasive challenge for robotic systems, particularly when it comes to\nexecuting long-horizon tasks that require both open-world object detection and\nhigh-level task planning. Traditional methods often struggle to integrate these\ncomponents effectively, and this limits their capability to deal with complex,\nlong-range navigation missions. In this paper, we propose LOVON, a novel\nframework that integrates large language models (LLMs) for hierarchical task\nplanning with open-vocabulary visual detection models, tailored for effective\nlong-range object navigation in dynamic, unstructured environments. To tackle\nreal-world challenges including visual jittering, blind zones, and temporary\ntarget loss, we design dedicated solutions such as Laplacian Variance Filtering\nfor visual stabilization. We also develop a functional execution logic for the\nrobot that guarantees LOVON's capabilities in autonomous navigation, task\nadaptation, and robust task completion. Extensive evaluations demonstrate the\nsuccessful completion of long-sequence tasks involving real-time detection,\nsearch, and navigation toward open-vocabulary dynamic targets. Furthermore,\nreal-world experiments across different legged robots (Unitree Go2, B2, and\nH1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.",
    "published": "2025-07-09T11:02:46Z",
    "updated": "2025-07-09T11:02:46Z",
    "id": "2507.06747v1",
    "authors": [
      "Daojie Peng",
      "Jiahang Cao",
      "Qiang Zhang",
      "Jun Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06747v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06747v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06747v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) for hierarchical task planning in robotic systems, which aligns with the LLM and RL topics. Additionally, it involves open-vocabulary visual detection models, which is relevant to the VLA topic.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "VLA"
    ]
  },
  "2507.06732v1": {
    "title": "Hierarchical Feature Alignment for Gloss-Free Sign Language Translation",
    "summary": "Sign Language Translation (SLT) attempts to convert sign language videos into\nspoken sentences. However, many existing methods struggle with the disparity\nbetween visual and textual representations during end-to-end learning.\nGloss-based approaches help to bridge this gap by leveraging structured\nlinguistic information. While, gloss-free methods offer greater flexibility and\nremove the burden of annotation, they require effective alignment strategies.\nRecent advances in Large Language Models (LLMs) have enabled gloss-free SLT by\ngenerating text-like representations from sign videos. In this work, we\nintroduce a novel hierarchical pre-training strategy inspired by the structure\nof sign language, incorporating pseudo-glosses and contrastive video-language\nalignment. Our method hierarchically extracts features at frame, segment, and\nvideo levels, aligning them with pseudo-glosses and the spoken sentence to\nenhance translation quality. Experiments demonstrate that our approach improves\nBLEU-4 and ROUGE scores while maintaining efficiency.",
    "published": "2025-07-09T10:45:50Z",
    "updated": "2025-07-09T10:45:50Z",
    "id": "2507.06732v1",
    "authors": [
      "Sobhan Asasi",
      "Mohamed Ilyes Lakhal",
      "Richard Bowden"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06732v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06732v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06732v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for Sign Language Translation (SLT) and introduces a hierarchical pre-training strategy, which aligns with the topics of LLM and Pretrain. Additionally, the focus on aligning visual and textual representations relates to the topic of VLA (Vision-Language Alignment).",
    "llm_cls_result": [
      "LLM",
      "Pretrain",
      "VLA"
    ]
  },
  "2507.06722v1": {
    "title": "On the Effect of Uncertainty on Layer-wise Inference Dynamics",
    "summary": "Understanding how large language models (LLMs) internally represent and\nprocess their predictions is central to detecting uncertainty and preventing\nhallucinations. While several studies have shown that models encode uncertainty\nin their hidden states, it is underexplored how this affects the way they\nprocess such hidden states. In this work, we demonstrate that the dynamics of\noutput token probabilities across layers for certain and uncertain outputs are\nlargely aligned, revealing that uncertainty does not seem to affect inference\ndynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to\nanalyze the layer-wise probability trajectories of final prediction tokens\nacross 11 datasets and 5 models. Using incorrect predictions as those with\nhigher epistemic uncertainty, our results show aligned trajectories for certain\nand uncertain predictions that both observe abrupt increases in confidence at\nsimilar layers. We balance this finding by showing evidence that more competent\nmodels may learn to process uncertainty differently. Our findings challenge the\nfeasibility of leveraging simplistic methods for detecting uncertainty at\ninference. More broadly, our work demonstrates how interpretability methods may\nbe used to investigate the way uncertainty affects inference.",
    "published": "2025-07-09T10:30:09Z",
    "updated": "2025-07-09T10:30:09Z",
    "id": "2507.06722v1",
    "authors": [
      "Sunwoo Kim",
      "Haneul Yoo",
      "Alice Oh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06722v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06722v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06722v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on understanding how uncertainty affects the internal processing of predictions in large language models (LLMs), which is directly related to research on LLMs and their reasoning abilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.06719v1": {
    "title": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for\n  Open-Vocabulary 3D Visual Grounding",
    "summary": "Open-vocabulary 3D visual grounding aims to localize target objects based on\nfree-form language queries, which is crucial for embodied AI applications such\nas autonomous navigation, robotics, and augmented reality. Learning 3D language\nfields through neural representations enables accurate understanding of 3D\nscenes from limited viewpoints and facilitates the localization of target\nobjects in complex environments. However, existing language field methods\nstruggle to accurately localize instances using spatial relations in language\nqueries, such as ``the book on the chair.'' This limitation mainly arises from\ninadequate reasoning about spatial relations in both language queries and 3D\nscenes. In this work, we propose SpatialReasoner, a novel neural\nrepresentation-based framework with large language model (LLM)-driven spatial\nreasoning that constructs a visual properties-enhanced hierarchical feature\nfield for open-vocabulary 3D visual grounding. To enable spatial reasoning in\nlanguage queries, SpatialReasoner fine-tunes an LLM to capture spatial\nrelations and explicitly infer instructions for the target, anchor, and spatial\nrelation. To enable spatial reasoning in 3D scenes, SpatialReasoner\nincorporates visual properties (opacity and color) to construct a hierarchical\nfeature field. This field represents language and instance features using\ndistilled CLIP features and masks extracted via the Segment Anything Model\n(SAM). The field is then queried using the inferred instructions in a\nhierarchical manner to localize the target 3D instance based on the spatial\nrelation in the language query. Extensive experiments show that our framework\ncan be seamlessly integrated into different neural representations,\noutperforming baseline models in 3D visual grounding while empowering their\nspatial reasoning capability.",
    "published": "2025-07-09T10:20:38Z",
    "updated": "2025-07-09T10:20:38Z",
    "id": "2507.06719v1",
    "authors": [
      "Zhenyang Liu",
      "Sixiao Zheng",
      "Siyu Chen",
      "Cairong Zhao",
      "Longfei Liang",
      "Xiangyang Xue",
      "Yanwei Fu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06719v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06719v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06719v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using large language models (LLMs) for spatial reasoning in 3D visual grounding, which involves integrating language and vision modalities. The work leverages LLMs for spatial reasoning and integrates visual properties, aligning with the topics of LLM and VLA.",
    "llm_cls_result": [
      "LLM",
      "VLA"
    ]
  },
  "2507.06715v1": {
    "title": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and\n  Context Aware Text Generation with LLMs",
    "summary": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust.",
    "published": "2025-07-09T10:13:38Z",
    "updated": "2025-07-09T10:13:38Z",
    "id": "2507.06715v1",
    "authors": [
      "Garapati Keerthana",
      "Manik Gupta"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06715v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06715v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06715v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on using LLMs for clinical text generation with a retrieval-augmented framework, which involves memory and retrieval mechanisms. It also discusses the application of LLMs in a specific domain, which aligns with the LLM topic.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.06658v1": {
    "title": "Elite Polarization in European Parliamentary Speeches: a Novel\n  Measurement Approach Using Large Language Models",
    "summary": "This project introduces a new measure of elite polarization via actor and\nsubject detection using artificial intelligence. I identify when politicians\nmention one another in parliamentary speeches, note who is speaking and who is\nbeing addressed, and assess the emotional temperature behind these evaluations.\nThis maps how elites evaluate their various out-parties, allowing us to create\nan index of mutual out-party hostility, that is, elite polarization. While I\nanalyzed polarization data over the past four decades for the UK, and two\ndecades for Hungary and Italy, my approach lays the groundwork for a\ntwenty-year, EU-wide time-series dataset on elite polarization. I obtain the\nresults that can be aggregated by party and quarter. The resulting index\ndemonstrates a good face validity: it reacts to events such as electoral\ncampaigns, country- and party-level crises, and to parties losing and assuming\npower.",
    "published": "2025-07-09T08:44:29Z",
    "updated": "2025-07-09T08:44:29Z",
    "id": "2507.06658v1",
    "authors": [
      "Gennadii Iakovlev"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06658v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06658v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06658v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper uses Large Language Models (LLMs) to analyze parliamentary speeches and measure elite polarization, which aligns with the 'LLM' topic. However, it does not directly relate to the other provided topics such as RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.07147v1": {
    "title": "Weighted Multi-Prompt Learning with Description-free Large Language\n  Model Distillation",
    "summary": "Recent advances in pre-trained Vision Language Models (VLM) have shown\npromising potential for effectively adapting to downstream tasks through prompt\nlearning, without the need for additional annotated paired datasets. To\nsupplement the text information in VLM trained on correlations with vision\ndata, new approaches leveraging Large Language Models (LLM) in prompts have\nbeen proposed, enhancing robustness to unseen and diverse data. Existing\nmethods typically extract text-based responses (i.e., descriptions) from LLM to\nincorporate into prompts; however, this approach suffers from high variability\nand low reliability. In this work, we propose Description-free Multi-prompt\nLearning(DeMul), a novel method that eliminates the process of extracting\ndescriptions and instead directly distills knowledge from LLM into prompts. By\nadopting a description-free approach, prompts can encapsulate richer semantics\nwhile still being represented as continuous vectors for optimization, thereby\neliminating the need for discrete pre-defined templates. Additionally, in a\nmulti-prompt setting, we empirically demonstrate the potential of prompt\nweighting in reflecting the importance of different prompts during training.\nExperimental results show that our approach achieves superior performance\nacross 11 recognition datasets.",
    "published": "2025-07-09T07:55:25Z",
    "updated": "2025-07-09T07:55:25Z",
    "id": "2507.07147v1",
    "authors": [
      "Sua Lee",
      "Kyubum Shin",
      "Jung Ho Park"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07147v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07147v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07147v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLM) in enhancing Vision Language Models (VLM) through prompt learning and knowledge distillation, which aligns with the topics of LLM and VLA. The focus on prompt learning and the integration of LLM knowledge into VLM also touches upon the broader area of Pretrain strategies.",
    "llm_cls_result": [
      "LLM",
      "VLA",
      "Pretrain"
    ]
  },
  "2507.07146v1": {
    "title": "An attention-aware GNN-based input defender against multi-turn jailbreak\n  on LLMs",
    "summary": "Large Language Models (LLMs) have gained widespread popularity and are\nincreasingly integrated into various applications. However, their capabilities\ncan be exploited for both benign and harmful purposes. Despite rigorous\ntraining and fine-tuning for safety, LLMs remain vulnerable to jailbreak\nattacks. Recently, multi-turn attacks have emerged, exacerbating the issue.\nUnlike single-turn attacks, multi-turn attacks gradually escalate the dialogue,\nmaking them more difficult to detect and mitigate, even after they are\nidentified.\n  In this study, we propose G-Guard, an innovative attention-aware GNN-based\ninput classifier designed to defend against multi-turn jailbreak attacks on\nLLMs. G-Guard constructs an entity graph for multi-turn queries, explicitly\ncapturing relationships between harmful keywords and queries even when those\nkeywords appear only in previous queries. Additionally, we introduce an\nattention-aware augmentation mechanism that retrieves the most similar\nsingle-turn query based on the multi-turn conversation. This retrieved query is\ntreated as a labeled node in the graph, enhancing the ability of GNN to\nclassify whether the current query is harmful. Evaluation results demonstrate\nthat G-Guard outperforms all baselines across all datasets and evaluation\nmetrics.",
    "published": "2025-07-09T07:55:03Z",
    "updated": "2025-07-09T07:55:03Z",
    "id": "2507.07146v1",
    "authors": [
      "Zixuan Huang",
      "Kecheng Huang",
      "Lihao Yin",
      "Bowei He",
      "Huiling Zhen",
      "Mingxuan Yuan",
      "Zili Shao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07146v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07146v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07146v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on defending against jailbreak attacks on LLMs using a GNN-based approach, which involves LLM safety and security measures.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.06623v1": {
    "title": "Expediting data extraction using a large language model (LLM) and\n  scoping review protocol: a methodological study within a complex scoping\n  review",
    "summary": "The data extraction stages of reviews are resource-intensive, and researchers\nmay seek to expediate data extraction using online (large language models) LLMs\nand review protocols. Claude 3.5 Sonnet was used to trial two approaches that\nused a review protocol to prompt data extraction from 10 evidence sources\nincluded in a case study scoping review. A protocol-based approach was also\nused to review extracted data. Limited performance evaluation was undertaken\nwhich found high accuracy for the two extraction approaches (83.3% and 100%)\nwhen extracting simple, well-defined citation details; accuracy was lower (9.6%\nand 15.8%) when extracting more complex, subjective data items. Considering all\ndata items, both approaches had precision >90% but low recall (<25%) and F1\nscores (<40%). The context of a complex scoping review, open response types and\nmethodological approach likely impacted performance due to missed and\nmisattributed data. LLM feedback considered the baseline extraction accurate\nand suggested minor amendments: four of 15 (26.7%) to citation details and 8 of\n38 (21.1%) to key findings data items were considered to potentially add value.\nHowever, when repeating the process with a dataset featuring deliberate errors,\nonly 2 of 39 (5%) errors were detected. Review-protocol-based methods used for\nexpediency require more robust performance evaluation across a range of LLMs\nand review contexts with comparison to conventional prompt engineering\napproaches. We recommend researchers evaluate and report LLM performance if\nusing them similarly to conduct data extraction or review extracted data. LLM\nfeedback contributed to protocol adaptation and may assist future review\nprotocol drafting.",
    "published": "2025-07-09T07:50:55Z",
    "updated": "2025-07-09T07:50:55Z",
    "id": "2507.06623v1",
    "authors": [
      "James Stewart-Evans",
      "Emma Wilson",
      "Tessa Langley",
      "Andrew Prayle",
      "Angela Hands",
      "Karen Exley",
      "Jo Leonardi-Bee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06623v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06623v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06623v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a large language model (LLM) for data extraction in a scoping review, focusing on the performance and evaluation of the LLM in this context. The core topic is the application of LLMs in a specific methodological study.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.06565v5": {
    "title": "A Mathematical Theory of Discursive Networks",
    "summary": "Large language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source Flaws-of-Others (FOO) algorithm: a configurable loop in which any\nset of agents critique one another while a harmonizer merges their verdicts. We\nidentify an ethical transgression, epithesis, that occurs when humans fail to\nengage in the discursive network. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nconnecting imperfect ones into networks that enforce mutual accountability.",
    "published": "2025-07-09T05:39:56Z",
    "updated": "2025-07-23T17:02:53Z",
    "id": "2507.06565v5",
    "authors": [
      "Juan B. Gutirrez"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06565v5",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06565v5.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06565v5",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses large language models (LLMs) and their interaction with humans in a discursive network, focusing on the generation and correction of erroneous information. It also introduces a mathematical model and an algorithm for peer review within these networks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "AGI"
    ]
  },
  "2507.06564v1": {
    "title": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in\n  Urban Environments",
    "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across\nvarious sectors, driven by their mobility and adaptability. This paper\nintroduces SkyVLN, a novel framework integrating vision-and-language navigation\n(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in\ncomplex urban environments. Unlike traditional navigation methods, SkyVLN\nleverages Large Language Models (LLMs) to interpret natural language\ninstructions and visual observations, enabling UAVs to navigate through dynamic\n3D spaces with improved accuracy and robustness. We present a multimodal\nnavigation agent equipped with a fine-grained spatial verbalizer and a history\npath memory mechanism. These components allow the UAV to disambiguate spatial\ncontexts, handle ambiguous instructions, and backtrack when necessary. The\nframework also incorporates an NMPC module for dynamic obstacle avoidance,\nensuring precise trajectory tracking and collision prevention. To validate our\napproach, we developed a high-fidelity 3D urban simulation environment using\nAirSim, featuring realistic imagery and dynamic urban elements. Extensive\nexperiments demonstrate that SkyVLN significantly improves navigation success\nrates and efficiency, particularly in new and unseen environments.",
    "published": "2025-07-09T05:38:32Z",
    "updated": "2025-07-09T05:38:32Z",
    "id": "2507.06564v1",
    "authors": [
      "Tianshun Li",
      "Tianyi Huai",
      "Zhen Li",
      "Yichun Gao",
      "Haoang Li",
      "Xinhu Zheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06564v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06564v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06564v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of vision-and-language navigation (VLN) with Large Language Models (LLMs) for UAV navigation, which aligns with the topics of Vision-Language Action (VLA) and Multimodal Large Language Models (MLLM). The use of LLMs for interpreting natural language instructions also touches on the topic of Large Language Models (LLM).",
    "llm_cls_result": [
      "VLA",
      "MLLM",
      "LLM"
    ]
  },
  "2507.06554v1": {
    "title": "SPEAR: Subset-sampled Performance Evaluation via Automated Ground Truth\n  Generation for RAG",
    "summary": "Retrieval-Augmented Generation (RAG) is a core approach for enhancing Large\nLanguage Models (LLMs), where the effectiveness of the retriever largely\ndetermines the overall response quality of RAG systems. Retrievers encompass a\nmultitude of hyperparameters that significantly impact performance outcomes and\ndemonstrate sensitivity to specific applications. Nevertheless, hyperparameter\noptimization entails prohibitively high computational expenses. Existing\nevaluation methods suffer from either prohibitive costs or disconnection from\ndomain-specific scenarios. This paper proposes SEARA (Subset sampling\nEvaluation for Automatic Retriever Assessment), which addresses evaluation data\nchallenges through subset sampling techniques and achieves robust automated\nretriever evaluation by minimal retrieval facts extraction and comprehensive\nretrieval metrics. Based on real user queries, this method enables fully\nautomated retriever evaluation at low cost, thereby obtaining optimal retriever\nfor specific business scenarios. We validate our method across classic RAG\napplications in rednote, including knowledge-based Q&A system and\nretrieval-based travel assistant, successfully obtaining scenario-specific\noptimal retrievers.",
    "published": "2025-07-09T05:13:09Z",
    "updated": "2025-07-09T05:13:09Z",
    "id": "2507.06554v1",
    "authors": [
      "Zou Yuheng",
      "Wang Yiran",
      "Tian Yuzhu",
      "Zhu Min",
      "Huang Yanhua"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06554v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06554v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06554v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on evaluating Retrieval-Augmented Generation (RAG) systems, which involves LLMs and retrieval-based methods. It discusses automated evaluation techniques and hyperparameter optimization, which are relevant to Memory and Benchmark categories.",
    "llm_cls_result": [
      "Memory",
      "Benchmark"
    ]
  },
  "2507.06539v2": {
    "title": "Large Language Model for Extracting Complex Contract Information in\n  Industrial Scenes",
    "summary": "This paper proposes a high-quality dataset construction method for complex\ncontract information extraction tasks in industrial scenarios and fine-tunes a\nlarge language model based on this dataset. Firstly, cluster analysis is\nperformed on industrial contract texts, and GPT-4 and GPT-3.5 are used to\nextract key information from the original contract data, obtaining high-quality\ndata annotations. Secondly, data augmentation is achieved by constructing new\ntexts, and GPT-3.5 generates unstructured contract texts from randomly combined\nkeywords, improving model robustness. Finally, the large language model is\nfine-tuned based on the high-quality dataset. Experimental results show that\nthe model achieves excellent overall performance while ensuring high field\nrecall and precision and considering parsing efficiency. LoRA, data balancing,\nand data augmentation effectively enhance model accuracy and robustness. The\nproposed method provides a novel and efficient solution for industrial contract\ninformation extraction tasks.",
    "published": "2025-07-09T04:46:31Z",
    "updated": "2025-07-10T02:51:21Z",
    "id": "2507.06539v2",
    "authors": [
      "Yunyang Cao",
      "Yanjun Li",
      "Silong Dai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06539v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06539v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06539v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on fine-tuning a large language model (LLM) for extracting complex contract information in industrial scenarios, which involves dataset construction, data augmentation, and model fine-tuning. The core topics are related to LLM and Dataset.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.08034v1": {
    "title": "Integrating External Tools with Large Language Models to Improve\n  Accuracy",
    "summary": "This paper deals with improving querying large language models (LLMs). It is\nwell-known that without relevant contextual information, LLMs can provide poor\nquality responses or tend to hallucinate. Several initiatives have proposed\nintegrating LLMs with external tools to provide them with up-to-date data to\nimprove accuracy. In this paper, we propose a framework to integrate external\ntools to enhance the capabilities of LLMs in answering queries in educational\nsettings. Precisely, we develop a framework that allows accessing external APIs\nto request additional relevant information. Integrated tools can also provide\ncomputational capabilities such as calculators or calendars. The proposed\nframework has been evaluated using datasets from the Multi-Modal Language\nUnderstanding (MMLU) collection. The data consists of questions on mathematical\nand scientific reasoning. Results compared to state-of-the-art language models\nshow that the proposed approach significantly improves performance. Our Athena\nframework achieves 83% accuracy in mathematical reasoning and 88% in scientific\nreasoning, substantially outperforming all tested models including GPT-4o,\nLLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline\nmodel (LLaMA-Large) achieving only 67% and 79% respectively. These promising\nresults open the way to creating complex computing ecosystems around LLMs to\nmake their use more natural to support various tasks and activities.",
    "published": "2025-07-09T04:09:59Z",
    "updated": "2025-07-09T04:09:59Z",
    "id": "2507.08034v1",
    "authors": [
      "Nripesh Niketan",
      "Hadj Batatia"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08034v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08034v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08034v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the accuracy of Large Language Models (LLMs) by integrating external tools and APIs, which aligns with the topics of LLMs and their improvement through external data and computational capabilities. The use of datasets from the Multi-Modal Language Understanding (MMLU) collection also touches on the topic of datasets for LLMs.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.06528v1": {
    "title": "InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models\n  with Investor Decision-Making Processes under Herd Behavior",
    "summary": "Aligning Large Language Models (LLMs) with investor decision-making processes\nunder herd behavior is a critical challenge in behavioral finance, which\ngrapples with a fundamental limitation: the scarcity of real-user data needed\nfor Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM\noutputs and human behavioral patterns, its reliance on massive authentic data\nimposes substantial collection costs and privacy risks. We propose InvestAlign,\na novel framework that constructs high-quality SFT datasets by leveraging\ntheoretical solutions to similar and simple optimal investment problems rather\nthan complex scenarios. Our theoretical analysis demonstrates that training\nLLMs with InvestAlign-generated data achieves faster parameter convergence than\nusing real-user data, suggesting superior learning efficiency. Furthermore, we\ndevelop InvestAgent, an LLM agent fine-tuned with InvestAlign, which\ndemonstrates significantly closer alignment to real-user data than pre-SFT\nmodels in both simple and complex investment problems. This highlights our\nproposed InvestAlign as a promising approach with the potential to address\ncomplex optimal investment problems and align LLMs with investor\ndecision-making processes under herd behavior. Our code is publicly available\nat https://github.com/thu-social-network-research-group/InvestAlign.",
    "published": "2025-07-09T04:07:22Z",
    "updated": "2025-07-09T04:07:22Z",
    "id": "2507.06528v1",
    "authors": [
      "Huisheng Wang",
      "Zhuoshi Pan",
      "Hangjing Zhang",
      "Mingxiao Liu",
      "Hanqing Gao",
      "H. Vicky Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06528v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06528v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06528v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on aligning Large Language Models (LLMs) with investor decision-making processes, specifically addressing data scarcity issues in Supervised Fine-Tuning (SFT). It introduces a novel framework (InvestAlign) and an LLM agent (InvestAgent) to improve alignment with human behavioral patterns, which is directly related to LLM research and Reinforcement Learning with Human Feedback (RLHF).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.06523v1": {
    "title": "FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and\n  Video-to-Text Generation",
    "summary": "Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable\nprogress in both Video-to-Text and Text-to-Video tasks. However, they often\nsuffer fro hallucinations, generating content that contradicts the visual\ninput. Existing evaluation methods are limited to one task (e.g., V2T) and also\nfail to assess hallucinations in open-ended, free-form responses. To address\nthis gap, we propose FIFA, a unified FaIthFulness evAluation framework that\nextracts comprehensive descriptive facts, models their semantic dependencies\nvia a Spatio-Temporal Semantic Dependency Graph, and verifies them using\nVideoQA models. We further introduce Post-Correction, a tool-based correction\nframework that revises hallucinated content. Extensive experiments demonstrate\nthat FIFA aligns more closely with human judgment than existing evaluation\nmethods, and that Post-Correction effectively improves factual consistency in\nboth text and video generation.",
    "published": "2025-07-09T03:51:27Z",
    "updated": "2025-07-09T03:51:27Z",
    "id": "2507.06523v1",
    "authors": [
      "Liqiang Jing",
      "Viet Lai",
      "Seunghyun Yoon",
      "Trung Bui",
      "Xinya Du"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06523v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06523v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06523v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating and improving the faithfulness of Video Multimodal Large Language Models (VideoMLLMs) in both Video-to-Text and Text-to-Video tasks, which involves multimodal integration and evaluation of hallucinations.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark",
      "VLA"
    ]
  },
  "2507.06515v2": {
    "title": "QUEST: Query Optimization in Unstructured Document Analysis",
    "summary": "Most recently, researchers have started building large language models (LLMs)\npowered data systems that allow users to analyze unstructured text documents\nlike working with a database because LLMs are very effective in extracting\nattributes from documents. In such systems, LLM-based extraction operations\nconstitute the performance bottleneck of query execution due to the high\nmonetary cost and slow LLM inference. Existing systems typically borrow the\nquery optimization principles popular in relational databases to produce query\nexecution plans, which unfortunately are ineffective in minimizing LLM cost. To\nfill this gap, we propose QUEST, which features a bunch of novel optimization\nstrategies for unstructured document analysis. First, we introduce an\nindex-based strategy to minimize the cost of each extraction operation. With\nthis index, QUEST quickly retrieves the text segments relevant to the target\nattributes and only feeds them to LLMs. Furthermore, we design an\nevidence-augmented retrieval strategy to reduce the possibility of missing\nrelevant segments. Moreover, we develop an instance-optimized query execution\nstrategy: because the attribute extraction cost could vary significantly\ndocument by document, QUEST produces different plans for different documents.\nFor each document, QUEST produces a plan to minimize the frequency of attribute\nextraction. The innovations include LLM cost-aware operator ordering strategies\nand an optimized join execution approach that transforms joins into filters.\nExtensive experiments on 3 real-world datasets demonstrate the superiority of\nQUEST, achieving 30%-6x cost savings while improving the F1 score by 10% -27%\ncompared with state-of-the-art baselines.",
    "published": "2025-07-09T03:30:09Z",
    "updated": "2025-07-11T03:50:12Z",
    "id": "2507.06515v2",
    "authors": [
      "Zhaoze Sun",
      "Qiyan Deng",
      "Chengliang Chai",
      "Kaisen Jin",
      "Xinyu Guo",
      "Han Han",
      "Ye Yuan",
      "Guoren Wang",
      "Lei Cao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06515v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06515v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06515v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in optimizing query execution for unstructured document analysis, focusing on minimizing LLM cost and improving efficiency. This aligns with the topics of LLM (Large Language Models) and Memory (retrieval-based methods and long-context processing).",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.06512v1": {
    "title": "Towards LLM-based Root Cause Analysis of Hardware Design Failures",
    "summary": "With advances in large language models (LLMs), new opportunities have emerged\nto develop tools that support the digital hardware design process. In this\nwork, we explore how LLMs can assist with explaining the root cause of design\nissues and bugs that are revealed during synthesis and simulation, a necessary\nmilestone on the pathway towards widespread use of LLMs in the hardware design\nprocess and for hardware security analysis. We find promising results: for our\ncorpus of 34 different buggy scenarios, OpenAI's o3-mini reasoning model\nreached a correct determination 100% of the time under pass@5 scoring, with\nother state of the art models and configurations usually achieving more than\n80% performance and more than 90% when assisted with retrieval-augmented\ngeneration.",
    "published": "2025-07-09T03:25:52Z",
    "updated": "2025-07-09T03:25:52Z",
    "id": "2507.06512v1",
    "authors": [
      "Siyu Qiu",
      "Muzhi Wang",
      "Raheel Afsharmazayejani",
      "Mohammad Moradi Shahmiri",
      "Benjamin Tan",
      "Hammond Pearce"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06512v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06512v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06512v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the application of LLMs in hardware design failure analysis, focusing on root cause analysis and utilizing retrieval-augmented generation, which aligns with the topics of LLM and Memory.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.06507v2": {
    "title": "GR-LLMs: Recent Advances in Generative Recommendation Based on Large\n  Language Models",
    "summary": "In the past year, Generative Recommendations (GRs) have undergone substantial\nadvancements, especially in leveraging the powerful sequence modeling and\nreasoning capabilities of Large Language Models (LLMs) to enhance overall\nrecommendation performance. LLM-based GRs are forming a new paradigm that is\ndistinctly different from discriminative recommendations, showing strong\npotential to replace traditional recommendation systems heavily dependent on\ncomplex hand-crafted features. In this paper, we provide a comprehensive survey\naimed at facilitating further research of LLM-based GRs. Initially, we outline\nthe general preliminaries and application cases of LLM-based GRs. Subsequently,\nwe introduce the main considerations when LLM-based GRs are applied in real\nindustrial scenarios. Finally, we explore promising directions for LLM-based\nGRs. We hope that this survey contributes to the ongoing advancement of the GR\ndomain.",
    "published": "2025-07-09T03:13:08Z",
    "updated": "2025-07-14T07:46:11Z",
    "id": "2507.06507v2",
    "authors": [
      "Zhen Yang",
      "Haitao Lin",
      "Jiawei xue",
      "Ziji Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06507v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06507v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06507v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in the field of generative recommendations, highlighting their sequence modeling and reasoning capabilities. This aligns with the topics of LLM (Large Language Models) and Reasoning (LLM reasoning).",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.06489v1": {
    "title": "On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks",
    "summary": "Robust verbal confidence generated by large language models (LLMs) is crucial\nfor the deployment of LLMs to ensure transparency, trust, and safety in\nhuman-AI interactions across many high-stakes applications. In this paper, we\npresent the first comprehensive study on the robustness of verbal confidence\nunder adversarial attacks. We introduce a novel framework for attacking verbal\nconfidence scores through both perturbation and jailbreak-based methods, and\nshow that these attacks can significantly jeopardize verbal confidence\nestimates and lead to frequent answer changes. We examine a variety of\nprompting strategies, model sizes, and application domains, revealing that\ncurrent confidence elicitation methods are vulnerable and that commonly used\ndefence techniques are largely ineffective or counterproductive. Our findings\nunderscore the urgent need to design more robust mechanisms for confidence\nexpression in LLMs, as even subtle semantic-preserving modifications can lead\nto misleading confidence in responses.",
    "published": "2025-07-09T02:19:46Z",
    "updated": "2025-07-09T02:19:46Z",
    "id": "2507.06489v1",
    "authors": [
      "Stephen Obadinma",
      "Xiaodan Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06489v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06489v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06489v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the robustness of verbal confidence in LLMs under adversarial attacks, which directly relates to the study of Large Language Models (LLM) and their vulnerabilities. The research also touches on the safety and trust aspects of deploying LLMs, which is a critical area in LLM research.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.06483v1": {
    "title": "Learning Japanese with Jouzu: Interaction Outcomes with Stylized\n  Dialogue Fictional Agents",
    "summary": "This study investigates how stylized, voiced agents shape user interaction in\na multimodal language learning environment. We conducted a mixed-methods\nevaluation of 54 participants interacting with anime-inspired characters\npowered by large language models and expressive text-to-speech synthesis. These\nagents responded in Japanese character language, offering users asynchronous,\nsemi-structured conversation in varying speech styles and emotional tones. We\nanalyzed user engagement patterns, perceived usability, emotional responses,\nand learning behaviors, with particular attention to how agent stylization\ninfluenced interaction across language proficiency levels and cultural\nbackgrounds. Our findings reveal that agent design, especially voice, persona,\nand linguistic style, substantially affected user experience, motivation, and\nstrategy. This work contributes to the understanding of affective, culturally\nstylized agents in human-agent interaction and offers guidance for designing\nmore engaging, socially responsive systems.",
    "published": "2025-07-09T01:57:58Z",
    "updated": "2025-07-09T01:57:58Z",
    "id": "2507.06483v1",
    "authors": [
      "Zackary Rackauckas",
      "Julia Hirschberg"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06483v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06483v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06483v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models in a multimodal language learning environment with stylized dialogue agents, focusing on user interaction and engagement. The core topics are related to multimodal large language models (MLLM) and their application in interactive systems.",
    "llm_cls_result": [
      "MLLM"
    ]
  },
  "2507.11544v1": {
    "title": "The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models",
    "summary": "Open-weight large language models (LLMs) unlock huge benefits in innovation,\npersonalization, privacy, and democratization. However, their core advantage -\nmodifiability - opens the door to systemic risks: bad actors can trivially\nsubvert current safeguards, turning beneficial models into tools for harm. This\nleads to a 'safety gap': the difference in dangerous capabilities between a\nmodel with intact safeguards and one that has been stripped of those\nsafeguards. We open-source a toolkit to estimate the safety gap for\nstate-of-the-art open-weight models. As a case study, we evaluate biochemical\nand cyber capabilities, refusal rates, and generation quality of models from\ntwo families (Llama-3 and Qwen-2.5) across a range of parameter scales (0.5B to\n405B) using different safeguard removal techniques. Our experiments reveal that\nthe safety gap widens as model scale increases and effective dangerous\ncapabilities grow substantially when safeguards are removed. We hope that the\nSafety Gap Toolkit (https://github.com/AlignmentResearch/safety-gap) will serve\nas an evaluation framework for common open-source models and as a motivation\nfor developing and testing tamper-resistant safeguards. We welcome\ncontributions to the toolkit from the community.",
    "published": "2025-07-08T23:58:01Z",
    "updated": "2025-07-08T23:58:01Z",
    "id": "2507.11544v1",
    "authors": [
      "Ann-Kathrin Dombrowski",
      "Dillon Bowen",
      "Adam Gleave",
      "Chris Cundy"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11544v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11544v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11544v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of safety risks in open-source large language models (LLMs) and introduces a toolkit to assess these risks. It focuses on the capabilities and safeguards of LLMs, which are central topics in LLM research.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.08030v1": {
    "title": "A Systematic Analysis of Declining Medical Safety Messaging in\n  Generative AI Models",
    "summary": "Generative AI models, including large language models (LLMs) and\nvision-language models (VLMs), are increasingly used to interpret medical\nimages and answer clinical questions. Their responses often include\ninaccuracies; therefore, safety measures like medical disclaimers are critical\nto remind users that AI outputs are not professionally vetted or a substitute\nfor medical advice. This study evaluated the presence of disclaimers in LLM and\nVLM outputs across model generations from 2022 to 2025. Using 500 mammograms,\n500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs\nwere screened for disclaimer phrases. Medical disclaimer presence in LLM and\nVLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023\nto 1.05% in 2025, respectively. By 2025, the majority of models displayed no\ndisclaimers. As public models become more capable and authoritative,\ndisclaimers must be implemented as a safeguard adapting to the clinical context\nof each output.",
    "published": "2025-07-08T23:50:30Z",
    "updated": "2025-07-08T23:50:30Z",
    "id": "2507.08030v1",
    "authors": [
      "Sonali Sharma",
      "Ahmed M. Alaa",
      "Roxana Daneshjou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08030v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08030v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08030v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of medical safety messaging in generative AI models, specifically LLMs and VLMs, which falls under the broader categories of LLM and VLA research.",
    "llm_cls_result": [
      "LLM",
      "VLA"
    ]
  },
  "2507.06450v1": {
    "title": "A Semantic Parsing Framework for End-to-End Time Normalization",
    "summary": "Time normalization is the task of converting natural language temporal\nexpressions into machine-readable representations. It underpins many downstream\napplications in information retrieval, question answering, and clinical\ndecision-making. Traditional systems based on the ISO-TimeML schema limit\nexpressivity and struggle with complex constructs such as compositional,\nevent-relative, and multi-span time expressions. In this work, we introduce a\nnovel formulation of time normalization as a code generation task grounded in\nthe SCATE framework, which defines temporal semantics through symbolic and\ncompositional operators. We implement a fully executable SCATE Python library\nand demonstrate that large language models (LLMs) can generate executable SCATE\ncode. Leveraging this capability, we develop an automatic data augmentation\npipeline using LLMs to synthesize large-scale annotated data with code-level\nvalidation. Our experiments show that small, locally deployable models trained\non this augmented data can achieve strong performance, outperforming even their\nLLM parents and enabling practical, accurate, and interpretable time\nnormalization.",
    "published": "2025-07-08T23:30:11Z",
    "updated": "2025-07-08T23:30:11Z",
    "id": "2507.06450v1",
    "authors": [
      "Xin Su",
      "Sungduk Yu",
      "Phillip Howard",
      "Steven Bethard"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06450v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06450v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06450v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in the context of time normalization, which involves converting natural language temporal expressions into machine-readable representations. It highlights the application of LLMs in generating executable code and synthesizing annotated data, which aligns with the 'LLM' topic. Additionally, the focus on semantic parsing and code generation touches on 'Reasoning' as it involves logical processing and problem-solving capabilities of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.06427v1": {
    "title": "Exploring Task Performance with Interpretable Models via Sparse\n  Auto-Encoders",
    "summary": "Large Language Models (LLMs) are traditionally viewed as black-box\nalgorithms, therefore reducing trustworthiness and obscuring potential\napproaches to increasing performance on downstream tasks. In this work, we\napply an effective LLM decomposition method using a dictionary-learning\napproach with sparse autoencoders. This helps extract monosemantic features\nfrom polysemantic LLM neurons. Remarkably, our work identifies model-internal\nmisunderstanding, allowing the automatic reformulation of the prompts with\nadditional annotations to improve the interpretation by LLMs. Moreover, this\napproach demonstrates a significant performance improvement in downstream\ntasks, such as mathematical reasoning and metaphor detection.",
    "published": "2025-07-08T22:17:52Z",
    "updated": "2025-07-08T22:17:52Z",
    "id": "2507.06427v1",
    "authors": [
      "Shun Wang",
      "Tyler Loakman",
      "Youbo Lei",
      "Yi Liu",
      "Bohao Yang",
      "Yuting Zhao",
      "Dong Yang",
      "Chenghua Lin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06427v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06427v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06427v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the decomposition of Large Language Models (LLMs) using sparse autoencoders to improve interpretability and performance on downstream tasks, which aligns with research on LLMs and their architectures.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.06419v1": {
    "title": "Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure\n  Mode Discovery for Robust Reward Modeling",
    "summary": "Reward modeling (RM), which captures human preferences to align large\nlanguage models (LLMs), is increasingly employed in tasks such as model\nfinetuning, response filtering, and ranking. However, due to the inherent\ncomplexity of human preferences and the limited coverage of available datasets,\nreward models often fail under distributional shifts or adversarial\nperturbations. Existing approaches for identifying such failure modes typically\nrely on prior knowledge about preference distributions or failure attributes,\nlimiting their practicality in real-world settings where such information is\nunavailable. In this work, we propose a tractable, preference-distribution\nagnostic method for discovering reward model failure modes via reward guided\ncontrolled decoding. Building on this, we introduce REFORM, a self-improving\nreward modeling framework that enhances robustness by using the reward model\nitself to guide the generation of falsely scored responses. These adversarial\nexamples are then used to augment the training data and patch the reward\nmodel's misaligned behavior. We evaluate REFORM on two widely used preference\ndatasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate\nthat it significantly improves robustness without sacrificing reward quality.\nNotably, REFORM preserves performance both in direct evaluation and in\ndownstream policy training, and further improves alignment quality by removing\nspurious correlations.",
    "published": "2025-07-08T21:56:33Z",
    "updated": "2025-07-08T21:56:33Z",
    "id": "2507.06419v1",
    "authors": [
      "Pankayaraj Pathmanathan",
      "Furong Huang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06419v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06419v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06419v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses reward modeling (RM) for aligning large language models (LLMs) and improving robustness through adversarial example generation and data augmentation. It focuses on reinforcement learning with human feedback (RLHF) and the alignment of LLMs, which are core topics in the RL category. The paper also touches on the robustness and alignment of reward models, which is relevant to the broader context of LLM research.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.06416v1": {
    "title": "Voltage Regulation in Distribution Systems with Data Center Loads",
    "summary": "Recent boom in foundation models and AI computing have raised growing\nconcerns on the power and energy trajectories of large-scale data centers. This\npaper focuses on the voltage issues caused by volatile and intensity of data\ncenter power demand, which also aligns with recent observations of more\nfrequent voltage disturbances in power grids. To address these data center\nintegration challenges, we propose a dynamic voltage control scheme by\nharnessing data center's load regulation capabilities. By taking local voltage\nmeasurements and adjusting power injections at each data center buses through\nthe dynamic voltage and frequency scaling (DVFS) scheme, we are able to\nmaintain safe voltage magnitude in a distributed fashion with higher data\ncenter computing load. Simulations using real large language model (LLM)\ninference load validate the effectiveness of our proposed mechanism. Both the\nLLM power data and proposed control scheme are open sourced.",
    "published": "2025-07-08T21:44:51Z",
    "updated": "2025-07-08T21:44:51Z",
    "id": "2507.06416v1",
    "authors": [
      "Yize Chen",
      "Baosen Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06416v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06416v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06416v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses voltage regulation in distribution systems with a focus on data center loads, particularly those from large language model (LLM) inference. While it mentions LLMs, the core focus is on power and energy management rather than LLM research or applications.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.06399v1": {
    "title": "An AI-Driven Thermal-Fluid Testbed for Advanced Small Modular Reactors:\n  Integration of Digital Twin and Large Language Models",
    "summary": "This paper presents a multipurpose artificial intelligence (AI)-driven\nthermal-fluid testbed designed to advance Small Modular Reactor technologies by\nseamlessly integrating physical experimentation with advanced computational\nintelligence. The platform uniquely combines a versatile three-loop\nthermal-fluid facility with a high-fidelity digital twin and sophisticated AI\nframeworks for real-time prediction, control, and operational assistance.\nMethodologically, the testbed's digital twin, built upon the System Analysis\nModule code, is coupled with a Gated Recurrent Unit (GRU) neural network. This\nmachine learning model, trained on experimental data, enables\nfaster-than-real-time simulation, providing predictive insights into the\nsystem's dynamic behavior. The practical application of this AI integration is\nshowcased through case studies. An AI-driven control framework where the GRU\nmodel accurately forecasts future system states and the corresponding control\nactions required to meet operational demands. Furthermore, an intelligent\nassistant, powered by a large language model, translates complex sensor data\nand simulation outputs into natural language, offering operators actionable\nanalysis and safety recommendations. Comprehensive validation against\nexperimental transients confirms the platform's high fidelity, with the GRU\nmodel achieving a temperature prediction root mean square error of 1.42 K. This\nwork establishes an integrated research environment at the intersection of AI\nand thermal-fluid science, showcasing how AI-driven methodologies in modeling,\ncontrol, and operator support can accelerate the innovation and deployment of\nnext-generation nuclear systems.",
    "published": "2025-07-08T21:07:30Z",
    "updated": "2025-07-08T21:07:30Z",
    "id": "2507.06399v1",
    "authors": [
      "Doyeong Lim",
      "Yang Liu",
      "Zavier Ndum Ndum",
      "Christian Young",
      "Yassin Hassan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06399v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06399v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06399v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of a large language model (LLM) with a digital twin for operational assistance in a thermal-fluid testbed, which aligns with the LLM topic. However, the primary focus is on the application of AI in nuclear systems, not on the core research of LLMs or related topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.06323v1": {
    "title": "Bridging AI and Software Security: A Comparative Vulnerability\n  Assessment of LLM Agent Deployment Paradigms",
    "summary": "Large Language Model (LLM) agents face security vulnerabilities spanning\nAI-specific and traditional software domains, yet current research addresses\nthese separately. This study bridges this gap through comparative evaluation of\nFunction Calling architecture and Model Context Protocol (MCP) deployment\nparadigms using a unified threat classification framework. We tested 3,250\nattack scenarios across seven language models, evaluating simple, composed, and\nchained attacks targeting both AI-specific threats (prompt injection) and\nsoftware vulnerabilities (JSON injection, denial-of-service). Function Calling\nshowed higher overall attack success rates (73.5% vs 62.59% for MCP), with\ngreater system-centric vulnerability while MCP exhibited increased LLM-centric\nexposure. Attack complexity dramatically amplified effectiveness, with chained\nattacks achieving 91-96% success rates. Counterintuitively, advanced reasoning\nmodels demonstrated higher exploitability despite better threat detection.\nResults demonstrate that architectural choices fundamentally reshape threat\nlandscapes. This work establishes methodological foundations for cross-domain\nLLM agent security assessment and provides evidence-based guidance for secure\ndeployment. Code and experimental materials are available at https: // github.\ncom/ theconsciouslab-ai/llm-agent-security.",
    "published": "2025-07-08T18:24:28Z",
    "updated": "2025-07-08T18:24:28Z",
    "id": "2507.06323v1",
    "authors": [
      "Tarek Gasmi",
      "Ramzi Guesmi",
      "Ines Belhadj",
      "Jihene Bennaceur"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06323v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06323v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06323v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses security vulnerabilities in Large Language Model (LLM) agents, focusing on different deployment paradigms and their susceptibility to various attacks. It directly relates to LLM research and security aspects of LLM deployment.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.06310v1": {
    "title": "Too Human to Model:The Uncanny Valley of LLMs in Social Simulation --\n  When Generative Language Agents Misalign with Modelling Principles",
    "summary": "Large language models (LLMs) have been increasingly used to build agents in\nsocial simulation because of their impressive abilities to generate fluent,\ncontextually coherent dialogues. Such abilities can enhance the realism of\nmodels. However, the pursuit of realism is not necessarily compatible with the\nepistemic foundation of modelling. We argue that LLM agents, in many regards,\nare too human to model: they are too expressive, detailed and intractable to be\nconsistent with the abstraction, simplification, and interpretability typically\ndemanded by modelling. Through a model-building thought experiment that\nconverts the Bass diffusion model to an LLM-based variant, we uncover five core\ndilemmas: a temporal resolution mismatch between natural conversation and\nabstract time steps; the need for intervention in conversations while avoiding\nundermining spontaneous agent outputs; the temptation to introduce rule-like\ninstructions in prompts while maintaining conversational naturalness; the\ntension between role consistency and role evolution across time; and the\nchallenge of understanding emergence, where system-level patterns become\nobscured by verbose micro textual outputs. These dilemmas steer the LLM agents\ntowards an uncanny valley: not abstract enough to clarify underlying social\nmechanisms, while not natural enough to represent realistic human behaviour.\nThis exposes an important paradox: the realism of LLM agents can obscure,\nrather than clarify, social dynamics when misapplied. We tease out the\nconditions in which LLM agents are ideally suited: where system-level emergence\nis not the focus, linguistic nuances and meaning are central, interactions\nunfold in natural time, and stable role identity is more important than\nlong-term behavioural evolution. We call for repositioning LLM agents in the\necosystem of social simulation for future applications.",
    "published": "2025-07-08T18:02:36Z",
    "updated": "2025-07-08T18:02:36Z",
    "id": "2507.06310v1",
    "authors": [
      "Yongchao Zeng",
      "Calum Brown",
      "Mark Rounsevell"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06310v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06310v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06310v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in social simulation, highlighting their capabilities and limitations in modeling social dynamics. It critiques the alignment of LLMs with modeling principles and identifies specific dilemmas in their application. The core focus is on LLMs and their role in social simulation, which aligns with the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.06306v1": {
    "title": "Humans overrely on overconfident language models, across languages",
    "summary": "As large language models (LLMs) are deployed globally, it is crucial that\ntheir responses are calibrated across languages to accurately convey\nuncertainty and limitations. Previous work has shown that LLMs are\nlinguistically overconfident in English, leading users to overrely on confident\ngenerations. However, the usage and interpretation of epistemic markers (e.g.,\n'It's definitely,' 'I think') can differ sharply across languages. Here, we\nstudy the risks of multilingual linguistic (mis)calibration, overconfidence,\nand overreliance across five languages to evaluate the safety of LLMs in a\nglobal context.\n  We find that overreliance risks are high across all languages. We first\nanalyze the distribution of LLM-generated epistemic markers, and observe that\nwhile LLMs are cross-linguistically overconfident, they are also sensitive to\ndocumented linguistic variation. For example, models generate the most markers\nof uncertainty in Japanese and the most markers of certainty in German and\nMandarin. We then measure human reliance rates across languages, finding that\nwhile users strongly rely on confident LLM generations in all languages,\nreliance behaviors differ cross-linguistically: for example, users rely\nsignificantly more on expressions of uncertainty in Japanese than in English.\nTaken together, these results indicate high risk of reliance on overconfident\nmodel generations across languages. Our findings highlight the challenges of\nmultilingual linguistic calibration and stress the importance of culturally and\nlinguistically contextualized model safety evaluations.",
    "published": "2025-07-08T18:01:01Z",
    "updated": "2025-07-08T18:01:01Z",
    "id": "2507.06306v1",
    "authors": [
      "Neil Rathi",
      "Dan Jurafsky",
      "Kaitlyn Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06306v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06306v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06306v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the linguistic overconfidence of large language models (LLMs) across multiple languages and its impact on human reliance, which directly relates to the study of LLMs and their deployment in multilingual contexts.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.06223v1": {
    "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers",
    "summary": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community.",
    "published": "2025-07-08T17:56:28Z",
    "updated": "2025-07-08T17:56:28Z",
    "id": "2507.06223v1",
    "authors": [
      "Zhiyuan Peng",
      "Ting-ruen Wei",
      "Tingyu Song",
      "Yilun Zhao",
      "Yi Fang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06223v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06223v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06223v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in reranking tasks, focusing on efficiency-effectiveness tradeoffs and proposes new metrics for evaluation. The core topics revolve around LLMs and their practical deployment challenges.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.06196v1": {
    "title": "UQLM: A Python Package for Uncertainty Quantification in Large Language\n  Models",
    "summary": "Hallucinations, defined as instances where Large Language Models (LLMs)\ngenerate false or misleading content, pose a significant challenge that impacts\nthe safety and trust of downstream applications. We introduce UQLM, a Python\npackage for LLM hallucination detection using state-of-the-art uncertainty\nquantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers\nthat compute response-level confidence scores ranging from 0 to 1. This library\nprovides an off-the-shelf solution for UQ-based hallucination detection that\ncan be easily integrated to enhance the reliability of LLM outputs.",
    "published": "2025-07-08T17:22:32Z",
    "updated": "2025-07-08T17:22:32Z",
    "id": "2507.06196v1",
    "authors": [
      "Dylan Bouchard",
      "Mohit Singh Chauhan",
      "David Skarbrevik",
      "Ho-Kyeong Ra",
      "Viren Bajaj",
      "Zeya Ahmad"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06196v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06196v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06196v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on uncertainty quantification in Large Language Models (LLMs) and introduces a Python package for hallucination detection, which directly relates to LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.06185v1": {
    "title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review",
    "summary": "In July 2025, 18 academic manuscripts on the preprint website arXiv were\nfound to contain hidden instructions known as prompts designed to manipulate\nAI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\"\nwere concealed using techniques like white-colored text. Author responses\nvaried: one planned to withdraw the affected paper, while another defended the\npractice as legitimate testing of reviewer compliance. This commentary analyzes\nthis practice as a novel form of research misconduct. We examine the technique\nof prompt injection in large language models (LLMs), revealing four types of\nhidden prompts, ranging from simple positive review commands to detailed\nevaluation frameworks. The defense that prompts served as \"honeypots\" to detect\nreviewers improperly using AI fails under examination--the consistently\nself-serving nature of prompt instructions indicates intent to manipulate.\nPublishers maintain inconsistent policies: Elsevier prohibits AI use in peer\nreview entirely, while Springer Nature permits limited use with disclosure\nrequirements. The incident exposes systematic vulnerabilities extending beyond\npeer review to any automated system processing scholarly texts, including\nplagiarism detection and citation indexing. Our analysis underscores the need\nfor coordinated technical screening at submission portals and harmonized\npolicies governing generative AI (GenAI) use in academic evaluation.",
    "published": "2025-07-08T17:11:13Z",
    "updated": "2025-07-08T17:11:13Z",
    "id": "2507.06185v1",
    "authors": [
      "Zhicheng Lin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06185v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06185v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06185v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the manipulation of AI-assisted peer review through hidden prompts in manuscripts, which involves the use of large language models (LLMs) and their vulnerabilities. This directly relates to research on LLMs and their applications in academic settings.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.06171v1": {
    "title": "Data-Semantics-Aware Recommendation of Diverse Pivot Tables",
    "summary": "Data summarization is essential to discover insights from large datasets. In\na spreadsheets, pivot tables offer a convenient way to summarize tabular data\nby computing aggregates over some attributes, grouped by others. However,\nidentifying attribute combinations that will result in useful pivot tables\nremains a challenge, especially for high-dimensional datasets. We formalize the\nproblem of automatically recommending insightful and interpretable pivot\ntables, eliminating the tedious manual process. A crucial aspect of\nrecommending a set of pivot tables is to diversify them. Traditional works\ninadequately address the table-diversification problem, which leads us to\nconsider the problem of pivot table diversification.\n  We present SAGE, a data-semantics-aware system for recommending k-budgeted\ndiverse pivot tables, overcoming the shortcomings of prior work for top-k\nrecommendations that cause redundancy. SAGE ensures that each pivot table is\ninsightful, interpretable, and adaptive to the user's actions and preferences,\nwhile also guaranteeing that the set of pivot tables are different from each\nother, offering a diverse recommendation. We make two key technical\ncontributions: (1) a data-semantics-aware model to measure the utility of a\nsingle pivot table and the diversity of a set of pivot tables, and (2) a\nscalable greedy algorithm that can efficiently select a set of diverse pivot\ntables of high utility, by leveraging data semantics to significantly reduce\nthe combinatorial search space. Our extensive experiments on three real-world\ndatasets show that SAGE outperforms alternative approaches, and efficiently\nscales to accommodate high-dimensional datasets. Additionally, we present\nseveral case studies to highlight SAGE's qualitative effectiveness over\ncommercial software and Large Language Models (LLMs).",
    "published": "2025-07-08T16:52:37Z",
    "updated": "2025-07-08T16:52:37Z",
    "id": "2507.06171v1",
    "authors": [
      "Whanhee Cho",
      "Anna Fariha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06171v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06171v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06171v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on recommending diverse pivot tables in spreadsheets, which is a data summarization problem. While it mentions Large Language Models (LLMs) in the context of comparison, the core focus is not on LLMs or any of the other provided topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.06167v3": {
    "title": "Skywork-R1V3 Technical Report",
    "summary": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.",
    "published": "2025-07-08T16:47:16Z",
    "updated": "2025-07-10T15:41:04Z",
    "id": "2507.06167v3",
    "authors": [
      "Wei Shen",
      "Jiangbo Pei",
      "Yi Peng",
      "Xuchen Song",
      "Yang Liu",
      "Jian Peng",
      "Haofeng Sun",
      "Yunzhuo Hao",
      "Peiyu Wang",
      "Jianhao Zhang",
      "Yahui Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06167v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06167v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06167v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses an advanced vision-language model (VLM) that integrates reasoning skills from text-only LLMs to visual tasks, utilizes a post-training RL framework, and focuses on multimodal reasoning and cross-modal alignment. The key topics are VLA (Vision-Language Action) due to the focus on vision-language models and cross-modal alignment, RL (Reinforcement Learning) because of the post-training RL framework, and MLLM (Multimodal Large Language Models) as it involves multimodal reasoning.",
    "llm_cls_result": [
      "VLA",
      "RL",
      "MLLM"
    ]
  },
  "2507.06157v1": {
    "title": "Evaluation of Habitat Robotics using Large Language Models",
    "summary": "This paper focuses on evaluating the effectiveness of Large Language Models\nat solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR\nprovides simplified environments and robotic interactions within randomized\nindoor kitchen scenes. Each randomized kitchen scene is given a task where two\nrobotic agents cooperatively work together to solve the task. We evaluated\nmultiple frontier models on Meta PARTNER environments. Our results indicate\nthat reasoning models like OpenAI o3-mini outperform non-reasoning models like\nOpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied\nenvironments. o3-mini displayed outperform across centralized, decentralized,\nfull observability, and partial observability configurations. This provides a\npromising avenue of research for embodied robotic development.",
    "published": "2025-07-08T16:39:39Z",
    "updated": "2025-07-08T16:39:39Z",
    "id": "2507.06157v1",
    "authors": [
      "William Li",
      "Lei Hamilton",
      "Kaise Al-natour",
      "Sanjeev Mohindra"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06157v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06157v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06157v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper evaluates the effectiveness of Large Language Models (LLMs) in solving embodied robotic tasks, which involves both LLMs and reasoning abilities in robotic environments.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "AGI"
    ]
  },
  "2507.06141v1": {
    "title": "Large Language Models Predict Human Well-being -- But Not Equally\n  Everywhere",
    "summary": "Subjective well-being is a key metric in economic, medical, and policy\ndecision-making. As artificial intelligence provides scalable tools for\nmodelling human outcomes, it is crucial to evaluate whether large language\nmodels (LLMs) can accurately predict well-being across diverse global\npopulations. We evaluate four leading LLMs using data from 64,000 individuals\nin 64 countries. While LLMs capture broad correlates such as income and health,\ntheir predictive accuracy decreases in countries underrepresented in the\ntraining data, highlighting systematic biases rooted in global digital and\neconomic inequality. A pre-registered experiment demonstrates that LLMs rely on\nsurface-level linguistic similarity rather than conceptual understanding,\nleading to systematic misestimations in unfamiliar or resource-limited\nsettings. Injecting findings from underrepresented contexts substantially\nenhances performance, but a significant gap remains. These results highlight\nboth the promise and limitations of LLMs in predicting global well-being,\nunderscoring the importance of robust validation prior to their implementation\nacross these areas.",
    "published": "2025-07-08T16:22:52Z",
    "updated": "2025-07-08T16:22:52Z",
    "id": "2507.06141v1",
    "authors": [
      "Pat Pataranutaporn",
      "Nattavudh Powdthavee",
      "Chayapatr Archiwaranguprok",
      "Pattie Maes"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06141v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06141v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06141v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in predicting human well-being, focusing on their performance across diverse populations and highlighting biases in underrepresented regions. This aligns with the topics of LLM research and their applications, as well as the importance of benchmarking their performance.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.06127v1": {
    "title": "PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder\n  Optimization",
    "summary": "Prefix adders are fundamental arithmetic circuits, but their design space\ngrows exponentially with bit-width, posing significant optimization challenges.\nPrevious works face limitations in performance, generalization, and\nscalability. To address these challenges, we propose PrefixAgent, a large\nlanguage model (LLM)-powered framework that enables efficient prefix adder\noptimization. Specifically, PrefixAgent reformulates the problem into subtasks\nincluding backbone synthesis and structure refinement, which effectively\nreduces the search space. More importantly, this new design perspective enables\nus to efficiently collect enormous high-quality data and reasoning traces with\nE-graph, which further results in an effective fine-tuning of LLM. Experimental\nresults show that PrefixAgent synthesizes prefix adders with consistently\nsmaller areas compared to baseline methods, while maintaining scalability and\ngeneralization in commercial EDA flows.",
    "published": "2025-07-08T16:14:17Z",
    "updated": "2025-07-08T16:14:17Z",
    "id": "2507.06127v1",
    "authors": [
      "Dongsheng Zuo",
      "Jiadong Zhu",
      "Yang Luo",
      "Yuzhe Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06127v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06127v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06127v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a large language model (LLM) for optimizing prefix adders, which involves LLM-powered design and optimization tasks. The core focus is on leveraging LLM capabilities for a specific engineering problem, aligning with the 'LLM' and 'Reasoning' topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.06119v2": {
    "title": "Omni-Video: Democratizing Unified Video Understanding and Generation",
    "summary": "Notable breakthroughs in unified understanding and generation modeling have\nled to remarkable advancements in image understanding, reasoning, production\nand editing, yet current foundational models predominantly focus on processing\nimages, creating a gap in the development of unified models for video\nunderstanding and generation. This report presents Omni-Video, an efficient and\neffective unified framework for video understanding, generation, as well as\ninstruction-based editing. Our key insight is to teach existing multimodal\nlarge language models (MLLMs) to produce continuous visual clues that are used\nas the input of diffusion decoders, which produce high-quality videos\nconditioned on these visual clues. To fully unlock the potential of our system\nfor unified video modeling, we integrate several technical improvements: 1) a\nlightweight architectural design that respectively attaches a vision head on\nthe top of MLLMs and a adapter before the input of diffusion decoders, the\nformer produce visual tokens for the latter, which adapts these visual tokens\nto the conditional space of diffusion decoders; and 2) an efficient multi-stage\ntraining scheme that facilitates a fast connection between MLLMs and diffusion\ndecoders with limited data and computational resources. We empirically\ndemonstrate that our model exhibits satisfactory generalization abilities\nacross video generation, editing and understanding tasks.",
    "published": "2025-07-08T16:02:16Z",
    "updated": "2025-07-09T12:27:27Z",
    "id": "2507.06119v2",
    "authors": [
      "Zhiyu Tan",
      "Hao Yang",
      "Luozheng Qin",
      "Jia Gong",
      "Mengping Yang",
      "Hao Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06119v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06119v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06119v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a unified framework for video understanding and generation using multimodal large language models (MLLMs) and diffusion decoders, which aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA) models.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.06056v1": {
    "title": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in\n  LLMs",
    "summary": "Large Language Models (LLMs) are known to memorize portions of their training\ndata, sometimes reproducing content verbatim when prompted appropriately. In\nthis work, we investigate a fundamental yet under-explored question in the\ndomain of memorization: How to characterize memorization difficulty of training\ndata in LLMs? Through empirical experiments on OLMo, a family of open models,\nwe present the Entropy-Memorization Law. It suggests that data entropy is\nlinearly correlated with memorization score. Moreover, in a case study of\nmemorizing highly randomized strings, or \"gibberish\", we observe that such\nsequences, despite their apparent randomness, exhibit unexpectedly low\nempirical entropy compared to the broader training corpus. Adopting the same\nstrategy to discover Entropy-Memorization Law, we derive a simple yet effective\napproach to distinguish training and testing data, enabling Dataset Inference\n(DI).",
    "published": "2025-07-08T14:58:28Z",
    "updated": "2025-07-08T14:58:28Z",
    "id": "2507.06056v1",
    "authors": [
      "Yizhan Huang",
      "Zhe Yang",
      "Meifang Chen",
      "Jianping Zhang",
      "Michael R. Lyu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06056v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06056v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06056v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on memorization in Large Language Models (LLMs) and introduces the Entropy-Memorization Law, which is directly related to the study of memory in LLMs. It also involves dataset inference, which is relevant to datasets used in LLMs.",
    "llm_cls_result": [
      "Memory",
      "Dataset"
    ]
  },
  "2507.06044v1": {
    "title": "Hierarchical Interaction Summarization and Contrastive Prompting for\n  Explainable Recommendations",
    "summary": "Explainable recommendations, which use the information of user and item with\ninteraction to generate a explanation for why the user would interact with the\nitem, are crucial for improving user trust and decision transparency to the\nrecommender system. Existing methods primarily rely on encoding features of\nusers and items to embeddings, which often leads to information loss due to\ndimensionality reduction, sparse interactions, and so on. With the advancements\nof large language models (LLMs) in language comprehension, some methods use\nembeddings as LLM inputs for explanation generation. However, since embeddings\nlack inherent semantics, LLMs must adjust or extend their parameters to\ninterpret them, a process that inevitably incurs information loss. To address\nthis issue, we propose a novel approach combining profile generation via\nhierarchical interaction summarization (PGHIS), which leverages a pretrained\nLLM to hierarchically summarize user-item interactions, generating structured\ntextual profiles as explicit representations of user and item characteristics.\nAdditionally, we propose contrastive prompting for explanation generation\n(CPEG) which employs contrastive learning to guide another reasoning language\nmodels in producing high-quality ground truth recommendation explanations.\nFinally, we use the textual profiles of user and item as input and high-quality\nexplanation as output to fine-tune a LLM for generating explanations.\nExperimental results on multiple datasets demonstrate that our approach\noutperforms existing state-of-the-art methods, achieving a great improvement on\nmetrics about explainability (e.g., 5% on GPTScore) and text quality.\nFurthermore, our generated ground truth explanations achieve a significantly\nhigher win rate compared to user-written reviews and those produced by other\nmethods, demonstrating the effectiveness of CPEG in generating high-quality\nground truths.",
    "published": "2025-07-08T14:45:47Z",
    "updated": "2025-07-08T14:45:47Z",
    "id": "2507.06044v1",
    "authors": [
      "Yibin Liu",
      "Ang Li",
      "Shijian Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06044v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06044v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06044v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using large language models (LLMs) for explainable recommendations, involving hierarchical interaction summarization and contrastive prompting. It leverages pretrained LLMs and reasoning models, aligning with topics related to LLMs and reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.06043v1": {
    "title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative\n  Adversarial Attacks on their Internal Representations",
    "summary": "Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN.",
    "published": "2025-07-08T14:45:21Z",
    "updated": "2025-07-08T14:45:21Z",
    "id": "2507.06043v1",
    "authors": [
      "Xiaohu Li",
      "Yunfeng Ning",
      "Zepeng Bao",
      "Mayi Xu",
      "Jianhao Chen",
      "Tieyun Qian"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06043v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06043v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06043v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses jailbreak attacks and defenses on Large Language Models (LLMs), focusing on their internal security mechanisms and using generative adversarial networks (GANs) for both attack and defense. This directly relates to research on LLMs and their security vulnerabilities.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.05997v1": {
    "title": "DocIE@XLLM25: In-Context Learning for Information Extraction using Fully\n  Synthetic Demonstrations",
    "summary": "Large, high-quality annotated corpora remain scarce in document-level entity\nand relation extraction in zero-shot or few-shot settings. In this paper, we\npresent a fully automatic, LLM-based pipeline for synthetic data generation and\nin-context learning for document-level entity and relation extraction. In\ncontrast to existing approaches that rely on manually annotated demonstrations\nor direct zero-shot inference, our method combines synthetic data generation\nwith retrieval-based in-context learning, using a reasoning-optimized language\nmodel. This allows us to build a high-quality demonstration database without\nmanual annotation and to dynamically retrieve relevant examples at inference\ntime. Based on our approach we produce a synthetic dataset of over $5k$\nWikipedia abstracts with approximately $59k$ entities and $30k$ relation\ntriples. Finally, we evaluate in-context learning performance on the DocIE\nshared task, extracting entities and relations from long documents in a\nzero-shot setting. We find that in-context joint entity and relation extraction\nat document-level remains a challenging task, even for state-of-the-art large\nlanguage models.",
    "published": "2025-07-08T13:55:25Z",
    "updated": "2025-07-08T13:55:25Z",
    "id": "2507.05997v1",
    "authors": [
      "Nicholas Popovi",
      "Ashish Kangen",
      "Tim Schopf",
      "Michael Frber"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05997v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05997v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05997v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of LLMs for synthetic data generation and in-context learning for document-level entity and relation extraction, which aligns with the topics of LLM and Reasoning. The focus on synthetic data generation also relates to the Dataset topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Dataset"
    ]
  },
  "2507.05984v1": {
    "title": "Development and Evaluation of HopeBot: an LLM-based chatbot for\n  structured and interactive PHQ-9 depression screening",
    "summary": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively\nscreen depression but lack interactivity and adaptability. We developed\nHopeBot, a chatbot powered by a large language model (LLM) that administers the\nPHQ-9 using retrieval-augmented generation and real-time clarification. In a\nwithin-subject study, 132 adults in the United Kingdom and China completed both\nself-administered and chatbot versions. Scores demonstrated strong agreement\n(ICC = 0.91; 45% identical). Among 75 participants providing comparative\nfeedback, 71% reported greater trust in the chatbot, highlighting clearer\nstructure, interpretive guidance, and a supportive tone. Mean ratings (0-10)\nwere 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,\nand 7.4 for recommendation helpfulness; the latter varied significantly by\nemployment status and prior mental-health service use (p < 0.05). Overall,\n87.1% expressed willingness to reuse or recommend HopeBot. These findings\ndemonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden\nadjuncts for routine depression screening.",
    "published": "2025-07-08T13:41:22Z",
    "updated": "2025-07-08T13:41:22Z",
    "id": "2507.05984v1",
    "authors": [
      "Zhijun Guo",
      "Alvina Lai",
      "Julia Ive",
      "Alexandru Petcu",
      "Yutong Wang",
      "Luyuan Qi",
      "Johan H Thygesen",
      "Kezhi Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05984v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05984v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05984v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the development and evaluation of a chatbot (HopeBot) powered by a large language model (LLM) for depression screening, utilizing retrieval-augmented generation and interactive features. The core topics relevant to this paper are LLM (Large Language Model) and Memory (retrieval-augmented generation).",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.05981v1": {
    "title": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with\n  Large Language Models",
    "summary": "Context: Large Language Model (LLM) agents are becoming widely used for\nvarious Requirements Engineering (RE) tasks. Research on improving their\naccuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval\naugmented generation. However, these methods often treat models as isolated\nblack boxes - relying on single-pass outputs without iterative refinement or\ncollaboration, limiting robustness and adaptability. Objective: We propose\nthat, just as human debates enhance accuracy and reduce bias in RE tasks by\nincorporating diverse perspectives, different LLM agents debating and\ncollaborating may achieve similar improvements. Our goal is to investigate\nwhether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:\nWe conducted a systematic study of existing MAD strategies across various\ndomains to identify their key characteristics. To assess their applicability in\nRE, we implemented and tested a preliminary MAD-based framework for RE\nclassification. Results: Our study identified and categorized several MAD\nstrategies, leading to a taxonomy outlining their core attributes. Our\npreliminary evaluation demonstrated the feasibility of applying MAD to RE\nclassification. Conclusions: MAD presents a promising approach for improving\nLLM accuracy in RE tasks. This study provides a foundational understanding of\nMAD strategies, offering insights for future research and refinements in RE\napplications.",
    "published": "2025-07-08T13:37:59Z",
    "updated": "2025-07-08T13:37:59Z",
    "id": "2507.05981v1",
    "authors": [
      "Marc Oriol",
      "Quim Motger",
      "Jordi Marco",
      "Xavier Franch"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05981v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05981v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05981v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in Requirements Engineering (RE) tasks, specifically focusing on Multi-Agent Debate (MAD) strategies to enhance performance. This aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning, as MAD involves iterative refinement and collaboration, akin to RLHF). The abstract does not mention multimodal aspects, scaling, or other specific topics, making these the most relevant categories.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.05962v1": {
    "title": "Evaluation of Large Language Model-Driven AutoML in Data and Model\n  Management from Human-Centered Perspective",
    "summary": "As organizations increasingly seek to leverage machine learning (ML)\ncapabilities, the technical complexity of implementing ML solutions creates\nsignificant barriers to adoption and impacts operational efficiency. This\nresearch examines how Large Language Models (LLMs) can transform the\naccessibility of ML technologies within organizations through a human-centered\nAutomated Machine Learning (AutoML) approach. Through a comprehensive user\nstudy involving 15 professionals across various roles and technical\nbackgrounds, we evaluate the organizational impact of an LLM-based AutoML\nframework compared to traditional implementation methods. Our research offers\nfour significant contributions to both management practice and technical\ninnovation: First, we present pioneering evidence that LLM-based interfaces can\ndramatically improve ML implementation success rates, with 93.34% of users\nachieved superior performance in the LLM condition, with 46.67% showing higher\naccuracy (10-25% improvement over baseline) and 46.67% demonstrating\nsignificantly higher accuracy (>25% improvement over baseline), while 6.67%\nmaintained comparable performance levels; and 60% reporting substantially\nreduced development time. Second, we demonstrate how natural language\ninterfaces can effectively bridge the technical skills gap in organizations,\ncutting implementation time by 50% while improving accuracy across all\nexpertise levels. Third, we provide valuable insights for organizations\ndesigning human-AI collaborative systems, showing that our approach reduced\nerror resolution time by 73% and significantly accelerated employee learning\ncurves. Finally, we establish empirical support for natural language as an\neffective interface for complex technical systems, offering organizations a\npath to democratize ML capabilities without compromising quality or\nperformance.",
    "published": "2025-07-08T13:10:32Z",
    "updated": "2025-07-08T13:10:32Z",
    "id": "2507.05962v1",
    "authors": [
      "Jiapeng Yao",
      "Lantian Zhang",
      "Jiping Huang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05962v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05962v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05962v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in improving the accessibility and efficiency of machine learning (ML) technologies through a human-centered Automated Machine Learning (AutoML) approach. It highlights the transformative potential of LLMs in bridging technical skills gaps and enhancing ML implementation success rates, which aligns with the core topics of LLM research and its practical applications.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.05918v1": {
    "title": "Few-shot text-based emotion detection",
    "summary": "This paper describes the approach of the Unibuc - NLP team in tackling the\nSemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion\nDetection. We mainly focused on experiments using large language models\n(Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With\nour final system, for the multi-label emotion detection track (track A), we got\nan F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36\nteams) for the Portuguese (Mozambican) subset and $0.325$ (\\textbf{1}/31 teams)\nfor the Emakhuwa subset.",
    "published": "2025-07-08T12:03:04Z",
    "updated": "2025-07-08T12:03:04Z",
    "id": "2507.05918v1",
    "authors": [
      "Teodor-George Marchitan",
      "Claudiu Creanga",
      "Liviu P. Dinu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05918v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05918v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05918v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using large language models for few-shot text-based emotion detection, which aligns with the 'LLM' topic as it involves research on Large Language Models and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.05903v1": {
    "title": "AI-Reporter: A Path to a New Genre of Scientific Communication",
    "summary": "The AI-Reporter represents a paradigmatic shift in scientific publication\npractice. This document demonstrates through a concrete case study how our\nsystem transforms academic presentations into publication-ready chapters -- in\nless than three minutes. Using Arno Simons' lecture on Large Language Models\nfrom the ``Large Language Models for the History, Philosophy, and Sociology of\nScience'' workshop (NEPI) as an example, we show how technological innovation\nbridges the gap between ephemeral presentation and permanent scientific\ndocumentation.",
    "published": "2025-07-08T11:41:37Z",
    "updated": "2025-07-08T11:41:37Z",
    "id": "2507.05903v1",
    "authors": [
      "Gerd Grahoff"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05903v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05903v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05903v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of AI to transform academic presentations into publication-ready chapters, specifically using a lecture on Large Language Models as an example. However, the core focus is on the application of AI in scientific communication rather than on the technical aspects of LLMs or their development.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.05890v1": {
    "title": "Psychometric Item Validation Using Virtual Respondents with\n  Trait-Response Mediators",
    "summary": "As psychometric surveys are increasingly used to assess the traits of large\nlanguage models (LLMs), the need for scalable survey item generation suited for\nLLMs has also grown. A critical challenge here is ensuring the construct\nvalidity of generated items, i.e., whether they truly measure the intended\ntrait. Traditionally, this requires costly, large-scale human data collection.\nTo make it efficient, we present a framework for virtual respondent simulation\nusing LLMs. Our central idea is to account for mediators: factors through which\nthe same trait can give rise to varying responses to a survey item. By\nsimulating respondents with diverse mediators, we identify survey items that\nrobustly measure intended traits. Experiments on three psychological trait\ntheories (Big5, Schwartz, VIA) show that our mediator generation methods and\nsimulation framework effectively identify high-validity items. LLMs demonstrate\nthe ability to generate plausible mediators from trait definitions and to\nsimulate respondent behavior for item validation. Our problem formulation,\nmetrics, methodology, and dataset open a new direction for cost-effective\nsurvey development and a deeper understanding of how LLMs replicate human-like\nbehavior. We will publicly release our dataset and code to support future work.",
    "published": "2025-07-08T11:26:03Z",
    "updated": "2025-07-08T11:26:03Z",
    "id": "2507.05890v1",
    "authors": [
      "Sungjib Lim",
      "Woojung Song",
      "Eun-Ju Lee",
      "Yohan Jo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05890v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05890v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05890v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for psychometric survey item validation, focusing on the ability of LLMs to simulate human-like behavior and validate survey items. This aligns with the topics of LLM research and their application in simulating human behavior, which is relevant to the 'LLM' and 'Benchmark' categories.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.05886v1": {
    "title": "Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc --\n  and We Can Do Better",
    "summary": "There is growing excitement about building software verifiers, synthesizers,\nand other Automated Reasoning (AR) tools by combining traditional symbolic\nalgorithms and Large Language Models (LLMs). Unfortunately, the current\npractice for constructing such neurosymbolic AR systems is an ad hoc\nprogramming model that does not have the strong guarantees of traditional\nsymbolic algorithms, nor a deep enough synchronization of neural networks and\nsymbolic reasoning to unlock the full potential of LLM-powered reasoning. I\npropose Neurosymbolic Transition Systems as a principled computational model\nthat can underlie infrastructure for building neurosymbolic AR tools. In this\nmodel, symbolic state is paired with intuition, and state transitions operate\nover symbols and intuition in parallel. I argue why this new paradigm can scale\nlogical reasoning beyond current capabilities while retaining the strong\nguarantees of symbolic algorithms, and I sketch out how the computational model\nI propose can be reified in a logic programming language.",
    "published": "2025-07-08T11:19:09Z",
    "updated": "2025-07-08T11:19:09Z",
    "id": "2507.05886v1",
    "authors": [
      "Aaron Bembenek"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05886v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05886v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05886v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with traditional symbolic algorithms for Automated Reasoning (AR) tools, focusing on improving the current ad hoc practices. It proposes a new computational model for neurosymbolic AR systems, which aligns with the topics of Reasoning (due to its focus on logical reasoning and AR) and LLM (as it involves the use of LLMs in reasoning tools).",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2507.06274v1": {
    "title": "Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing\n  Attacks",
    "summary": "Watermarking is a promising defense against the misuse of large language\nmodels (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks.\nThis vulnerability stems from an inherent trade-off governed by watermark\nwindow size: smaller windows resist scrubbing better but are easier to\nreverse-engineer, enabling low-cost statistics-based spoofing attacks. This\nwork breaks this trade-off by introducing a novel mechanism, equivalent texture\nkeys, where multiple tokens within a watermark window can independently support\nthe detection. Based on the redundancy, we propose a novel watermark scheme\nwith Sub-vocabulary decomposed Equivalent tExture Key (SEEK). It achieves a\nPareto improvement, increasing the resilience against scrubbing attacks without\ncompromising robustness to spoofing. Experiments demonstrate SEEK's superiority\nover prior method, yielding spoofing robustness gains of +88.2%/+92.3%/+82.0%\nand scrubbing robustness gains of +10.2%/+6.4%/+24.6% across diverse dataset\nsettings.",
    "published": "2025-07-08T11:14:00Z",
    "updated": "2025-07-08T11:14:00Z",
    "id": "2507.06274v1",
    "authors": [
      "Huanming Shen",
      "Baizhou Huang",
      "Xiaojun Wan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06274v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06274v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06274v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the resilience of watermarking techniques in large language models (LLMs) against attacks, which directly relates to research on LLMs and their security aspects.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.05880v1": {
    "title": "RecRankerEval: A Flexible and Extensible Framework for Top-k LLM-based\n  Recommendation",
    "summary": "A recent Large language model (LLM)-based recommendation model, called\nRecRanker, has demonstrated a superior performance in the top-k recommendation\ntask compared to other models. In particular, RecRanker samples users via\nclustering, generates an initial ranking list using an initial recommendation\nmodel, and fine-tunes an LLM through hybrid instruction tuning to infer user\npreferences. However, the contribution of each core component remains\nunderexplored. In this work, we inspect the reproducibility of RecRanker, and\nstudy the impact and role of its various components. We begin by reproducing\nthe RecRanker pipeline through the implementation of all its key components.\nOur reproduction shows that the pairwise and listwise methods achieve a\nperformance comparable to that reported in the original paper. For the\npointwise method, while we are also able to reproduce the original paper's\nresults, further analysis shows that the performance is abnormally high due to\ndata leakage from the inclusion of ground-truth information in the prompts. To\nenable a fair and comprehensive evaluation of LLM-based top-k recommendations,\nwe propose RecRankerEval, an extensible framework that covers five key\ndimensions: user sampling strategy, initial recommendation model, LLM backbone,\ndataset selection, and instruction tuning method. Using the RecRankerEval\nframework, we show that the original results of RecRanker can be reproduced on\nthe ML-100K and ML-1M datasets, as well as the additional Amazon-Music dataset,\nbut not on BookCrossing due to the lack of timestamp information in the\noriginal RecRanker paper. Furthermore, we demonstrate that RecRanker's\nperformance can be improved by employing alternative user sampling methods,\nstronger initial recommenders, and more capable LLMs.",
    "published": "2025-07-08T11:04:17Z",
    "updated": "2025-07-08T11:04:17Z",
    "id": "2507.05880v1",
    "authors": [
      "Zeyuan Meng",
      "Zixuan Yi",
      "Iadh Ounis"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05880v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05880v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05880v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in recommendation tasks, specifically focusing on the RecRanker model and its components. It evaluates the performance and reproducibility of LLM-based recommendations, which aligns with the topics of LLM research and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.05863v1": {
    "title": "KERAG_R: Knowledge-Enhanced Retrieval-Augmented Generation for\n  Recommendation",
    "summary": "Large Language Models (LLMs) have shown strong potential in recommender\nsystems due to their contextual learning and generalisation capabilities.\nExisting LLM-based recommendation approaches typically formulate the\nrecommendation task using specialised prompts designed to leverage their\ncontextual abilities, and aligning their outputs closely with human preferences\nto yield an improved recommendation performance. However, the use of LLMs for\nrecommendation tasks is limited by the absence of domain-specific knowledge.\nThis lack of relevant relational knowledge about the items to be recommended in\nthe LLM's pre-training corpus can lead to inaccuracies or hallucinations,\nresulting in incorrect or misleading recommendations. Moreover, directly using\ninformation from the knowledge graph introduces redundant and noisy\ninformation, which can affect the LLM's reasoning process or exceed its input\ncontext length, thereby reducing the performance of LLM-based recommendations.\nTo address the lack of domain-specific knowledge, we propose a novel model\ncalled Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation\n(KERAG_R). Specifically, we leverage a graph retrieval-augmented generation\n(GraphRAG) component to integrate additional information from a knowledge graph\n(KG) into instructions, enabling the LLM to collaboratively exploit\nrecommendation signals from both text-based user interactions and the knowledge\ngraph to better estimate the users' preferences in a recommendation context. In\nparticular, we perform graph RAG by pre-training a graph attention network\n(GAT) to select the most relevant triple for the target users for the used LLM,\nthereby enhancing the LLM while reducing redundant and noisy information. Our\nextensive experiments on three public datasets show that our proposed KERAG_R\nmodel significantly outperforms ten existing state-of-the-art recommendation\nmethods.",
    "published": "2025-07-08T10:44:27Z",
    "updated": "2025-07-08T10:44:27Z",
    "id": "2507.05863v1",
    "authors": [
      "Zeyuan Meng",
      "Zixuan Yi",
      "Iadh Ounis"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05863v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05863v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05863v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in recommender systems, focusing on enhancing their performance through knowledge graphs and retrieval-augmented generation. It addresses the limitations of LLMs in domain-specific knowledge and proposes a novel model to integrate additional information from knowledge graphs. The core topics are related to LLMs, retrieval-augmented generation, and the use of knowledge graphs in recommendation systems.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Reasoning"
    ]
  },
  "2507.07061v1": {
    "title": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems",
    "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.",
    "published": "2025-07-08T09:20:12Z",
    "updated": "2025-07-08T09:20:12Z",
    "id": "2507.07061v1",
    "authors": [
      "Shervin Ghaffari",
      "Zohre Bahranifard",
      "Mohammad Akbari"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07061v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07061v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07061v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving semantic caching performance in LLM-based systems by using an ensemble embedding approach, which directly relates to the efficiency and performance of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.05795v1": {
    "title": "Creating a customisable freely-accessible Socratic AI physics tutor",
    "summary": "This paper explores role engineering as an effective paradigm for customizing\nLarge Language Models (LLMs) into specialized AI tutors for physics education.\nWe demonstrate this methodology by designing a Socratic physics problem-solving\ntutor using Google's Gemini Gems feature, defining its pedagogical behavior\nthrough a detailed 'script' that specifies its role and persona. We present two\nillustrative use cases: the first demonstrates the Gem's multimodal ability to\nanalyze a student's hand-drawn force diagram and apply notational rules from a\n'Knowledge' file; the second showcases its capacity to guide conceptual\nreasoning in electromagnetism using its pre-trained knowledge without using\nspecific documents provided by the instructor. Our findings show that the\n'role-engineered' Gem successfully facilitates a Socratic dialogue, in stark\ncontrast to a standard Gemini model, which tends to immediately provide direct\nsolutions. We conclude that role engineering is a pivotal and accessible method\nfor educators to transform a general-purpose 'solution provider' into a\nreliable pedagogical tutor capable of engaging students in an active reflection\nprocess. This approach offers a powerful tool for both instructors and\nstudents, while also highlighting the importance of addressing the technology's\ninherent limitations, such as the potential for occasional inaccuracies.",
    "published": "2025-07-08T08:57:13Z",
    "updated": "2025-07-08T08:57:13Z",
    "id": "2507.05795v1",
    "authors": [
      "Eugenio Tufino",
      "Bor Gregorcic"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05795v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05795v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05795v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the customization of Large Language Models (LLMs) for educational purposes, specifically in physics tutoring, which involves role engineering and pedagogical behavior scripting. It highlights the use of multimodal capabilities and reasoning in LLMs, aligning with topics related to LLMs and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.05790v1": {
    "title": "TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal\n  Large Language Model",
    "summary": "Virtual try-on has made significant progress in recent years. This paper\naddresses how to achieve multifunctional virtual try-on guided solely by text\ninstructions, including full outfit change and local editing. Previous methods\nprimarily relied on end-to-end networks to perform single try-on tasks, lacking\nversatility and flexibility. We propose TalkFashion, an intelligent try-on\nassistant that leverages the powerful comprehension capabilities of large\nlanguage models to analyze user instructions and determine which task to\nexecute, thereby activating different processing pipelines accordingly.\nAdditionally, we introduce an instruction-based local repainting model that\neliminates the need for users to manually provide masks. With the help of\nmulti-modal models, this approach achieves fully automated local editings,\nenhancing the flexibility of editing tasks. The experimental results\ndemonstrate better semantic consistency and visual quality compared to the\ncurrent methods.",
    "published": "2025-07-08T08:51:56Z",
    "updated": "2025-07-08T08:51:56Z",
    "id": "2507.05790v1",
    "authors": [
      "Yujie Hu",
      "Xuanyu Zhang",
      "Weiqi Li",
      "Jian Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05790v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05790v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05790v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a Multimodal Large Language Model (MLLM) for virtual try-on tasks, which involves understanding and processing user instructions to perform various try-on functions. This aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA) models, as it integrates vision and language modalities for practical applications.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.05788v2": {
    "title": "Flippi: End To End GenAI Assistant for E-Commerce",
    "summary": "The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace.",
    "published": "2025-07-08T08:50:47Z",
    "updated": "2025-07-11T08:00:51Z",
    "id": "2507.05788v2",
    "authors": [
      "Anand A. Rajasekar",
      "Praveen Tangarajan",
      "Anjali Nainani",
      "Amogh Batwal",
      "Vinay Rao Dandin",
      "Anusua Trivedi",
      "Ozan Ersoy"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05788v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05788v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05788v2",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the development of a conversational assistant (Flippi) powered by large language models (LLMs) for e-commerce, focusing on advanced NLP techniques and personalized user interactions. The core topics are LLM (Large Language Models) and Memory (Retrieval-Augmented Generation).",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.08843v1": {
    "title": "Can We Predict Your Next Move Without Breaking Your Privacy?",
    "summary": "We propose FLLL3M--Federated Learning with Large Language Models for Mobility\nModeling--a privacy-preserving framework for Next-Location Prediction (NxLP).\nBy retaining user data locally and leveraging LLMs through an efficient outer\nproduct mechanism, FLLL3M ensures high accuracy with low resource demands. It\nachieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,\n0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while\nreducing parameters by up to 45.6% and memory usage by 52.7%.",
    "published": "2025-07-08T08:13:34Z",
    "updated": "2025-07-08T08:13:34Z",
    "id": "2507.08843v1",
    "authors": [
      "Arpita Soni",
      "Sahil Tripathi",
      "Gautam Siddharth Kashyap",
      "Manaswi Kulahara",
      "Mohammad Anas Azeez",
      "Zohaib Hasan Siddiqui",
      "Nipun Joshi",
      "Jiechao Gao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08843v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08843v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08843v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a federated learning framework for mobility modeling, which involves privacy-preserving techniques and efficient use of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.08021v1": {
    "title": "Unveiling Effective In-Context Configurations for Image Captioning: An\n  External & Internal Analysis",
    "summary": "The evolution of large models has witnessed the emergence of In-Context\nLearning (ICL) capabilities. In Natural Language Processing (NLP), numerous\nstudies have demonstrated the effectiveness of ICL. Inspired by the success of\nLarge Language Models (LLMs), researchers have developed Large Multimodal\nModels (LMMs) with ICL capabilities. However, explorations of demonstration\nconfiguration for multimodal ICL remain preliminary. Additionally, the\ncontrollability of In-Context Examples (ICEs) provides an efficient and\ncost-effective means to observe and analyze the inference characteristics of\nLMMs under varying inputs. This paper conducts a comprehensive external and\ninternal investigation of multimodal in-context learning on the image\ncaptioning task. Externally, we explore demonstration configuration strategies\nthrough three dimensions: shot number, image retrieval, and caption assignment.\nWe employ multiple metrics to systematically and thoroughly evaluate and\nsummarize key findings. Internally, we analyze typical LMM attention\ncharacteristics and develop attention-based metrics to quantify model\nbehaviors. We also conduct auxiliary experiments to explore the feasibility of\nattention-driven model acceleration and compression. We further compare\nperformance variations between LMMs with identical model design and pretraining\nstrategies and explain the differences from the angles of pre-training data\nfeatures. Our study reveals both how ICEs configuration strategies impact model\nperformance through external experiments and characteristic typical patterns\nthrough internal inspection, providing dual perspectives for understanding\nmultimodal ICL in LMMs. Our method of combining external and internal analysis\nto investigate large models, along with our newly proposed metrics, can be\napplied to broader research areas.",
    "published": "2025-07-08T08:07:57Z",
    "updated": "2025-07-08T08:07:57Z",
    "id": "2507.08021v1",
    "authors": [
      "Li Li",
      "Yongliang Wu",
      "Jingze Zhu",
      "Jiawei Peng",
      "Jianfei Cai",
      "Xu Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08021v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08021v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08021v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on multimodal in-context learning (ICL) for image captioning, which involves Large Multimodal Models (LMMs) and their capabilities. It explores demonstration configuration strategies and internal model behaviors, aligning with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA).",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.05750v1": {
    "title": "DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM\n  Conversational Capabilities",
    "summary": "Large Language Models (LLMs) are increasingly employed in multi-turn\nconversational tasks, yet their pre-training data predominantly consists of\ncontinuous prose, creating a potential mismatch between required capabilities\nand training paradigms. We introduce a novel approach to address this\ndiscrepancy by synthesizing conversational data from existing text corpora. We\npresent a pipeline that transforms a cluster of multiple related documents into\nan extended multi-turn, multi-topic information-seeking dialogue. Applying our\npipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training\ndialogue corpus consisting of over 730k long conversations. We hypothesize that\nexposure to such synthesized conversational structures during pre-training can\nenhance the fundamental multi-turn capabilities of LLMs, such as context memory\nand understanding. Empirically, we show that incorporating DocTalk during\npre-training results in up to 40% gain in context memory and understanding,\nwithout compromising base performance. DocTalk is available at\nhttps://huggingface.co/datasets/AmazonScience/DocTalk.",
    "published": "2025-07-08T07:52:12Z",
    "updated": "2025-07-08T07:52:12Z",
    "id": "2507.05750v1",
    "authors": [
      "Jing Yang Lee",
      "Hamed Bonab",
      "Nasser Zalmout",
      "Ming Zeng",
      "Sanket Lokegaonkar",
      "Colin Lockard",
      "Binxuan Huang",
      "Ritesh Sarkhel",
      "Haodong Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05750v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05750v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05750v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing LLM conversational capabilities through synthesized conversational data, which involves pre-training strategies and memory augmentation.",
    "llm_cls_result": [
      "Pretrain",
      "Memory",
      "LLM"
    ]
  },
  "2507.05733v1": {
    "title": "When Transformers Meet Recommenders: Integrating Self-Attentive\n  Sequential Recommendation with Fine-Tuned LLMs",
    "summary": "Self-Attentive Sequential Recommendation (SASRec) effectively captures\nlong-term user preferences by applying attention mechanisms to historical\ninteractions. Concurrently, the rise of Large Language Models (LLMs) has\nmotivated research into LLM-based recommendation, which leverages their\npowerful generalization and language understanding capabilities. However, LLMs\noften lack the domain-specific knowledge and collaborative signals essential\nfor high-quality recommendations when relying solely on textual prompts. To\naddress this limitation, this study proposes SASRecLLM, a novel framework that\nintegrates SASRec as a collaborative encoder with an LLM fine-tuned using\nLow-Rank Adaptation (LoRA). The components are connected via a mapping layer to\nalign their dimensional spaces, and three targeted training strategies are\ndesigned to optimize the hybrid architecture. Extensive experiments on multiple\ndatasets demonstrate that SASRecLLM achieves robust and consistent improvements\nover strong baselines in both cold-start and warm-start scenarios. This work\nadvances the field of LLM-based recommendation by presenting a modular and\neffective paradigm for fusing structured collaborative filtering with the\nsemantic power of fine-tuned LLMs. The implementation is available on GitHub:\nhttps://github.com/kechenkristin/RecLLM",
    "published": "2025-07-08T07:26:55Z",
    "updated": "2025-07-08T07:26:55Z",
    "id": "2507.05733v1",
    "authors": [
      "Kechen Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05733v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05733v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05733v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with self-attentive sequential recommendation systems, focusing on fine-tuning LLMs for recommendation tasks. This aligns with the topics of LLM research and their application in specific domains.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.05727v1": {
    "title": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark",
    "summary": "Automatic Speech Recognition (ASR) has been extensively investigated, yet\nprior evaluative efforts have largely been restricted to contextless paradigms.\nThis constraint stems from the limited proficiency of conventional ASR models\nin context modeling and their deficiency in memory and reasoning based on world\nknowledge. Recent breakthroughs in the development of Large Language Models\n(LLMs) and corresponding Large Audio Language Models (LALMs) have markedly\nenhanced the visibility of general artificial intelligence capabilities.\nConsequently, there exists a compelling need for a benchmark that can evaluate\nboth the generality and intelligence of ASR systems. To address this gap, we\npropose ContextASR-Bench: a comprehensive, large-scale benchmark designed to\nassess contextual speech recognition. This benchmark encompasses up to 40,000\ndata entries across over 10 domains, enabling a thorough evaluation of model\nperformance in scenarios that omit or incorporate coarse-grained or\nfine-grained contextual information. Moreover, diverging from conventional ASR\nevaluations, our benchmark includes an analysis of model efficacy in\nrecognizing named entities mentioned within the auditory input. Our extensive\nevaluation highlights that LALMs, with strong world knowledge and context\nlearning capabilities, outperform conventional ASR models by a large margin.\nThe dataset and evaluation code have been released at\nhttps://github.com/MrSupW/ContextASR-Bench.",
    "published": "2025-07-08T07:21:20Z",
    "updated": "2025-07-08T07:21:20Z",
    "id": "2507.05727v1",
    "authors": [
      "He Wang",
      "Linhan Ma",
      "Dake Guo",
      "Xiong Wang",
      "Lei Xie",
      "Jin Xu",
      "Junyang Lin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05727v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05727v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05727v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark for evaluating contextual speech recognition, which involves Large Language Models (LLMs) and their capabilities in context modeling and reasoning. It also discusses the development of Large Audio Language Models (LALMs) and their performance in recognizing named entities, which aligns with the topics of Benchmark and LLM.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2507.05723v1": {
    "title": "Large Language Models for Agent-Based Modelling: Current and possible\n  uses across the modelling cycle",
    "summary": "The emergence of Large Language Models (LLMs) with increasingly sophisticated\nnatural language understanding and generative capabilities has sparked interest\nin the Agent-based Modelling (ABM) community. With their ability to summarize,\ngenerate, analyze, categorize, transcribe and translate text, answer questions,\npropose explanations, sustain dialogue, extract information from unstructured\ntext, and perform logical reasoning and problem-solving tasks, LLMs have a good\npotential to contribute to the modelling process. After reviewing the current\nuse of LLMs in ABM, this study reflects on the opportunities and challenges of\nthe potential use of LLMs in ABM. It does so by following the modelling cycle,\nfrom problem formulation to documentation and communication of model results,\nand holding a critical stance.",
    "published": "2025-07-08T07:17:24Z",
    "updated": "2025-07-08T07:17:24Z",
    "id": "2507.05723v1",
    "authors": [
      "Los Vanhe",
      "Melania Borit",
      "Peer-Olaf Siebers",
      "Roger Cremades",
      "Christopher Frantz",
      "nder Grcan",
      "Frantiek Kalvas",
      "Denisa Reshef Kera",
      "Vivek Nallur",
      "Kavin Narasimhan",
      "Martin Neumann"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05723v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05723v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05723v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in Agent-based Modelling (ABM), highlighting their capabilities in natural language understanding and generation, logical reasoning, and problem-solving tasks. This aligns with the topics of LLM and Reasoning, as it involves the application of LLMs in modeling and reasoning tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.05713v2": {
    "title": "DRAGON: Dynamic RAG Benchmark On News",
    "summary": "Retrieval-Augmented Generation (RAG) is a widely adopted approach for\nimproving the factuality of large language models (LLMs) by incorporating\nexternal knowledge at inference time. Although there exist multiple RAG\nbenchmarks for English, evaluation resources for other languages, including\nRussian, remain scarce and static, failing to capture the dynamic nature of\nreal-world deployments. In this work, we present DRAGON (Dynamic RAG Benchmark\nOn News), the first dynamic benchmark for evaluating RAG systems in Russian on\na changing news corpora. DRAGON is built upon a regularly updated corpus of\nRussian news and public documents and supports comprehensive evaluation of both\nthe retriever and generator components. Question generation is performed\nautomatically with the use of Knowledge Graph constructed from the corpus and\nenables the extraction of four core question types aligned with distinct\nsubgraph patterns. We release a complete evaluation framework comprising the\npipeline for automatic question generation, evaluation scripts, which are\npotentially reusable for other languages and multilingual settings, and\nbenchmark data. We also launch a public leaderboard to encourage community\nparticipation and comparison.",
    "published": "2025-07-08T06:52:43Z",
    "updated": "2025-07-15T07:36:34Z",
    "id": "2507.05713v2",
    "authors": [
      "Fedor Chernogorskii",
      "Sergei Averkiev",
      "Liliya Kudraleeva",
      "Zaven Martirosian",
      "Maria Tikhonova",
      "Valentin Malykh",
      "Alena Fenogenova"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05713v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05713v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05713v2",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on a benchmark for evaluating Retrieval-Augmented Generation (RAG) systems, which involves both retrieval and generation components, and is related to memory-augmented models and benchmarking LLMs.",
    "llm_cls_result": [
      "Memory",
      "Benchmark"
    ]
  },
  "2507.05686v1": {
    "title": "Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in\n  Multilingual LLMs",
    "summary": "Multilingual large language models (LLMs) often exhibit language confusion, a\ntendency to generate responses in a dominant language irrespective of the\nprompt's language. To address this, we propose Smoothie-Qwen, a lightweight,\npost-hoc method that mitigates language bias without retraining. This technique\nselectively adjusts token-level output probabilities to effectively suppress\nundesired language generation. Applied to the Qwen model, our method reduces\nunintended Chinese output by over 95% while preserving task accuracy on\nmultilingual benchmarks. This work provides a practical and efficient solution\nfor enhancing the language controllability of LLMs, making them more reliable\nfor global applications.",
    "published": "2025-07-08T05:30:51Z",
    "updated": "2025-07-08T05:30:51Z",
    "id": "2507.05686v1",
    "authors": [
      "SeungWon Ji",
      "Jungyup Lee",
      "Jemin Kim",
      "Sang Park",
      "SeungJae Lee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05686v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05686v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05686v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method to mitigate language bias in multilingual large language models (LLMs), which directly relates to the research on Large Language Models (LLM) and their practical applications in multilingual contexts.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.05633v1": {
    "title": "SARA: Selective and Adaptive Retrieval-augmented Generation with Context\n  Compression",
    "summary": "Retrieval-augmented Generation (RAG) extends large language models (LLMs)\nwith external knowledge but faces key challenges: restricted effective context\nlength and redundancy in retrieved documents. Pure compression-based approaches\nreduce input size but often discard fine-grained details essential for factual\naccuracy. We propose SARA, a unified RAG framework that balances local\nprecision and global knowledge coverage under tight context budgets. SARA\ncombines natural-language text snippets with semantic compression vectors to\njointly enhance context efficiency and answer correctness. It represents\ncontexts at two complementary levels: 1) fine-grained natural-language spans\nthat preserve critical entities and numerical values, and 2) compact,\ninterpretable vectors that summarize high-level semantics. An iterative\nevidence-selection module employs the compression vectors for dynamic reranking\nof contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families\n(Mistral, Llama, and Gemma), SARA consistently improves answer relevance\n(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),\ndemonstrating the importance of integrating textual and compressed\nrepresentations for robust, context-efficient RAG.",
    "published": "2025-07-08T03:29:09Z",
    "updated": "2025-07-08T03:29:09Z",
    "id": "2507.05633v1",
    "authors": [
      "Yiqiao Jin",
      "Kartik Sharma",
      "Vineeth Rakesh",
      "Yingtong Dou",
      "Menghai Pan",
      "Mahashweta Das",
      "Srijan Kumar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05633v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05633v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05633v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses Retrieval-augmented Generation (RAG) and its challenges, proposing a framework that integrates natural-language text snippets with semantic compression vectors to enhance context efficiency and answer correctness. This aligns with topics related to memory-augmented models and retrieval-based methods.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2507.05629v1": {
    "title": "Enhancing Student Learning with LLM-Generated Retrieval Practice\n  Questions: An Empirical Study in Data Science Courses",
    "summary": "Retrieval practice is a well-established pedagogical technique known to\nsignificantly enhance student learning and knowledge retention. However,\ngenerating high-quality retrieval practice questions is often time-consuming\nand labor intensive for instructors, especially in rapidly evolving technical\nsubjects. Large Language Models (LLMs) offer the potential to automate this\nprocess by generating questions in response to prompts, yet the effectiveness\nof LLM-generated retrieval practice on student learning remains to be\nestablished. In this study, we conducted an empirical study involving two\ncollege-level data science courses, with approximately 60 students. We compared\nlearning outcomes during one week in which students received LLM-generated\nmultiple-choice retrieval practice questions to those from a week in which no\nsuch questions were provided. Results indicate that students exposed to\nLLM-generated retrieval practice achieved significantly higher knowledge\nretention, with an average accuracy of 89%, compared to 73% in the week without\nsuch practice. These findings suggest that LLM-generated retrieval questions\ncan effectively support student learning and may provide a scalable solution\nfor integrating retrieval practice into real-time teaching. However, despite\nthese encouraging outcomes and the potential time-saving benefits, cautions\nmust be taken, as the quality of LLM-generated questions can vary. Instructors\nmust still manually verify and revise the generated questions before releasing\nthem to students.",
    "published": "2025-07-08T03:23:19Z",
    "updated": "2025-07-08T03:23:19Z",
    "id": "2507.05629v1",
    "authors": [
      "Yuan An",
      "John Liu",
      "Niyam Acharya",
      "Ruhma Hashmi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05629v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05629v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05629v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to generate retrieval practice questions for enhancing student learning, which directly relates to the application of LLMs in educational contexts.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.08020v1": {
    "title": "Circumventing Safety Alignment in Large Language Models Through\n  Embedding Space Toxicity Attenuation",
    "summary": "Large Language Models (LLMs) have achieved remarkable success across domains\nsuch as healthcare, education, and cybersecurity. However, this openness also\nintroduces significant security risks, particularly through embedding space\npoisoning, which is a subtle attack vector where adversaries manipulate the\ninternal semantic representations of input data to bypass safety alignment\nmechanisms. While previous research has investigated universal perturbation\nmethods, the dynamics of LLM safety alignment at the embedding level remain\ninsufficiently understood. Consequently, more targeted and accurate adversarial\nperturbation techniques, which pose significant threats, have not been\nadequately studied.\n  In this work, we propose ETTA (Embedding Transformation Toxicity\nAttenuation), a novel framework that identifies and attenuates\ntoxicity-sensitive dimensions in embedding space via linear transformations.\nETTA bypasses model refusal behaviors while preserving linguistic coherence,\nwithout requiring model fine-tuning or access to training data. Evaluated on\nfive representative open-source LLMs using the AdvBench benchmark, ETTA\nachieves a high average attack success rate of 88.61%, outperforming the best\nbaseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR\non instruction-tuned defenses). These results highlight a critical\nvulnerability in current alignment strategies and underscore the need for\nembedding-aware defenses.",
    "published": "2025-07-08T03:01:00Z",
    "updated": "2025-07-08T03:01:00Z",
    "id": "2507.08020v1",
    "authors": [
      "Zhibo Zhang",
      "Yuxi Li",
      "Kailong Wang",
      "Shuai Yuan",
      "Ling Shi",
      "Haoyu Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08020v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08020v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08020v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the security risks and vulnerabilities in Large Language Models (LLMs), specifically focusing on embedding space poisoning and safety alignment mechanisms. It introduces a novel framework (ETTA) to bypass these safety measures, which is directly related to LLM research and security concerns.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.05617v1": {
    "title": "Flipping Knowledge Distillation: Leveraging Small Models' Expertise to\n  Enhance LLMs in Text Matching",
    "summary": "Knowledge distillation typically involves transferring knowledge from a Large\nLanguage Model (LLM) to a Smaller Language Model (SLM). However, in tasks such\nas text matching, fine-tuned smaller models often yield more effective\ndomain-specific representations, as they focus on optimizing the similarity of\ninput pairs. To leverage both the specialized strengths of small models and the\nrich semantic understanding of LLMs, we introduce a flipped knowledge\ndistillation paradigm, where LLM learns from SLM. Specifically, we address the\narchitectural gap between decoder-only LLMs and smaller encoder-based models by\nreinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder\ngenerates compressed representations, while the decoder maps them to the output\nspace. During training, the encoder produces representations and their\nsimilarities, which are then aligned with the similarity scores produced by the\nteacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.\nThe MCL ensures accurate similarity for both positive and negative pairs, and\nadaptively handles the internal differences within positive and negative\nsamples. Our paradigm requires only a reasonably good-performing SLM, allowing\nthe LLM to achieve improved performance. Experiments on financial and\nhealthcare benchmarks, as well as real-world applications, confirm its\neffectiveness, and the model has been fully deployed in an online environment.",
    "published": "2025-07-08T02:54:15Z",
    "updated": "2025-07-08T02:54:15Z",
    "id": "2507.05617v1",
    "authors": [
      "Mingzhe Li",
      "Jing Xiang",
      "Qishen Zhang",
      "Kaiyang Wan",
      "Xiuying Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05617v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05617v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05617v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses knowledge distillation from small models to LLMs, focusing on enhancing LLMs' performance in text matching tasks. It involves architectural adaptations and training strategies specific to LLMs.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.05613v1": {
    "title": "Domain adaptation of large language models for geotechnical applications",
    "summary": "Recent developments in large language models (LLMs) are opening up new\nopportunities in geotechnical engineering and engineering geology. While\ngeneral-purpose LLMs possess broad capabilities, effective application in\ngeotechnics often requires domain-specific adaptation. Such tailored LLMs are\nincreasingly employed to streamline geotechnical workflows. This paper presents\nthe first survey of the adaptation and application of LLMs in geotechnical\nengineering. It outlines key methodologies for adaptation to geotechnical\ndomain, including prompt engineering, retrieval-augmented generation,\ndomain-adaptive pretraining, and fine-tuning. The survey examines the\nstate-of-the-art applications of geotechnical-adapted LLMs, including\ngeological interpretation, subsurface characterization, site planning, design\ncalculations, numerical modeling, safety and risk assessment, and educational\ntutoring. It also analyzes benefits and limitations of geotechnical-adapted\nLLMs, and identifies promising directions for future research in this\ninterdisciplinary discipline. The findings serve as a valuable resource for\npractitioners seeking to integrate LLMs into geotechnical practice, while also\nproviding a foundation to stimulate further investigation within the academic\ncommunity.",
    "published": "2025-07-08T02:45:44Z",
    "updated": "2025-07-08T02:45:44Z",
    "id": "2507.05613v1",
    "authors": [
      "Lei Fan",
      "Fangxue Liu",
      "Cheng Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05613v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05613v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05613v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the adaptation and application of large language models (LLMs) in geotechnical engineering, focusing on domain-specific methodologies and applications. The core topics are related to LLMs and their adaptation, which aligns with the 'LLM' and 'Pretrain' categories.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.05609v1": {
    "title": "MMW: Side Talk Rejection Multi-Microphone Whisper on Smart Glasses",
    "summary": "Smart glasses are increasingly positioned as the next-generation interface\nfor ubiquitous access to large language models (LLMs). Nevertheless, achieving\nreliable interaction in real-world noisy environments remains a major\nchallenge, particularly due to interference from side speech. In this work, we\nintroduce a novel side-talk rejection multi-microphone Whisper (MMW) framework\nfor smart glasses, incorporating three key innovations. First, we propose a Mix\nBlock based on a Tri-Mamba architecture to effectively fuse multi-channel audio\nat the raw waveform level, while maintaining compatibility with streaming\nprocessing. Second, we design a Frame Diarization Mamba Layer to enhance\nframe-level side-talk suppression, facilitating more efficient fine-tuning of\nWhisper models. Third, we employ a Multi-Scale Group Relative Policy\nOptimization (GRPO) strategy to jointly optimize frame-level and\nutterance-level side speech suppression. Experimental evaluations demonstrate\nthat the proposed MMW system can reduce the word error rate (WER) by 4.95\\% in\nnoisy conditions.",
    "published": "2025-07-08T02:37:20Z",
    "updated": "2025-07-08T02:37:20Z",
    "id": "2507.05609v1",
    "authors": [
      "Yang Liu",
      "Li Wan",
      "Yiteng Huang",
      "Yong Xu",
      "yangyang shi",
      "Saurabh Adya",
      "ming sun",
      "Florian Metze"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05609v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05609v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05609v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving speech interaction with LLMs on smart glasses by addressing side-talk rejection and noise suppression, but does not directly contribute to the core topics of LLM research, multimodal models, or other specified categories.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.05607v1": {
    "title": "Structured Task Solving via Modular Embodied Intelligence: A Case Study\n  on Rubik's Cube",
    "summary": "This paper presents Auto-RubikAI, a modular autonomous planning framework\nthat integrates a symbolic Knowledge Base (KB), a vision-language model (VLM),\nand a large language model (LLM) to solve structured manipulation tasks\nexemplified by Rubik's Cube restoration. Unlike traditional robot systems based\non predefined scripts, or modern approaches relying on pretrained networks and\nlarge-scale demonstration data, Auto-RubikAI enables interpretable, multi-step\ntask execution with minimal data requirements and no prior demonstrations. The\nproposed system employs a KB module to solve group-theoretic restoration steps,\novercoming LLMs' limitations in symbolic reasoning. A VLM parses RGB-D input to\nconstruct a semantic 3D scene representation, while the LLM generates\nstructured robotic control code via prompt chaining. This tri-module\narchitecture enables robust performance under spatial uncertainty. We deploy\nAuto-RubikAI in both simulation and real-world settings using a 7-DOF robotic\narm, demonstrating effective Sim-to-Real adaptation without retraining.\nExperiments show a 79% end-to-end task success rate across randomized\nconfigurations. Compared to CFOP, DeepCubeA, and Two-Phase baselines, our\nKB-enhanced method reduces average solution steps while maintaining\ninterpretability and safety. Auto-RubikAI provides a cost-efficient, modular\nfoundation for embodied task planning in smart manufacturing, robotics\neducation, and autonomous execution scenarios. Code, prompts, and hardware\nmodules will be released upon publication.",
    "published": "2025-07-08T02:36:03Z",
    "updated": "2025-07-08T02:36:03Z",
    "id": "2507.05607v1",
    "authors": [
      "Chongshan Fan",
      "Shenghai Yuan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05607v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05607v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05607v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a modular autonomous planning framework that integrates a vision-language model (VLM) and a large language model (LLM) to solve structured manipulation tasks. It involves symbolic reasoning, vision-language alignment, and robotic control, which are relevant to the topics of VLA (Vision-Language Action) and Reasoning (for the symbolic reasoning and structured task solving). The use of LLM also makes it relevant to the LLM topic.",
    "llm_cls_result": [
      "VLA",
      "Reasoning",
      "LLM"
    ]
  },
  "2507.05598v1": {
    "title": "Self-Review Framework for Enhancing Instruction Following Capability of\n  LLM",
    "summary": "Various techniques have been proposed to improve large language models (LLMs)\nadherence to formatting and instruction constraints. One of the most effective\napproaches involves utilizing high-quality data generated by powerful models.\nHowever, such models often fail to fully comply with complex instructions in a\nsingle generation. To address this limitation, iterative revision methods have\nbeen introduced. Nevertheless, as the number of data points and revision\niterations increases, the associated monetary costs grow significantly. As a\nresource-efficient alternative, methods have been proposed that leverage\nhigh-performance evaluation tools to compensate for the limited self-evaluation\ncapabilities of open-source LLMs. However, these approaches often lead to a\ndegradation in output quality due to excessive revision. To overcome these\nchallenges, we propose Re5, a self-evaluation and revision framework designed\nto enhance instruction-following performance while preserving the quality of\nthe generated content. Re5 extracts task and constraint components from user\ninstructions, performs structural evaluations to prevent error accumulation,\nand applies fine-grained constraint-specific content evaluations followed by\nselective revisions. This process ensures precise and quality-preserving\nimprovements. The final high-quality outputs are used for alignment tuning,\nenabling long-term alignment improvements through a data-centric iterative\nrefinement loop. Experimental results demonstrate that Re5 achieves\ninstruction-following performance comparable to models trained on data\ngenerated by GPT-4o-mini, a high-performance model, even with a small amount of\ndata while maintaining response quality with a 64.24%-win rate over the\nnon-revised initial responses. These results validate Re5 as an efficient and\neffective solution for enhancing instruction adherence with minimal external\nsupervision.",
    "published": "2025-07-08T02:17:18Z",
    "updated": "2025-07-08T02:17:18Z",
    "id": "2507.05598v1",
    "authors": [
      "Sihyun Park"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05598v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05598v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05598v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the instruction-following capabilities of LLMs through a self-evaluation and revision framework, which is directly related to LLM research and their reasoning abilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.05595v1": {
    "title": "PaddleOCR 3.0 Technical Report",
    "summary": "This technical report introduces PaddleOCR 3.0, an Apache-licensed\nopen-source toolkit for OCR and document parsing. To address the growing demand\nfor document understanding in the era of large language models, PaddleOCR 3.0\npresents three major solutions: (1) PP-OCRv5 for multilingual text recognition,\n(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for\nkey information extraction. Compared to mainstream vision-language models\n(VLMs), these models with fewer than 100 million parameters achieve competitive\naccuracy and efficiency, rivaling billion-parameter VLMs. In addition to\noffering a high-quality OCR model library, PaddleOCR 3.0 provides efficient\ntools for training, inference, and deployment, supports heterogeneous hardware\nacceleration, and enables developers to easily build intelligent document\napplications.",
    "published": "2025-07-08T02:14:10Z",
    "updated": "2025-07-08T02:14:10Z",
    "id": "2507.05595v1",
    "authors": [
      "Cheng Cui",
      "Ting Sun",
      "Manhui Lin",
      "Tingquan Gao",
      "Yubo Zhang",
      "Jiaxuan Liu",
      "Xueqing Wang",
      "Zelun Zhang",
      "Changda Zhou",
      "Hongen Liu",
      "Yue Zhang",
      "Wenyu Lv",
      "Kui Huang",
      "Yichao Zhang",
      "Jing Zhang",
      "Jun Zhang",
      "Yi Liu",
      "Dianhai Yu",
      "Yanjun Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05595v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05595v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05595v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on OCR and document parsing, which are not directly related to the provided topic list. It mentions vision-language models (VLMs) but does not delve into the core research areas listed.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.05591v1": {
    "title": "MLlm-DR: Towards Explainable Depression Recognition with MultiModal\n  Large Language Models",
    "summary": "Automated depression diagnosis aims to analyze multimodal information from\ninterview videos to predict participants' depression scores. Previous studies\noften lack clear explanations of how these scores were determined, limiting\ntheir adoption in clinical practice. While the advent of LLMs provides a\npossible pathway for explainable depression diagnosis, current LLMs capable of\nprocessing multimodal data lack training on interview data, resulting in poor\ndiagnostic performance when used directly. In this paper, we propose a novel\nmultimodal large language model (MLlm-DR) that can understand multimodal\ninformation inputs and supports explainable depression diagnosis. MLlm-DR\nintegrates a smaller LLMs and a lightweight query module (LQ-former).\nSpecifically, the smaller LLMs is designed to generate depression scores and\ncorresponding evaluation rationales. To enhance its logical reasoning for\ndomain-specific tasks while maintaining practicality, we constructed a robust\ntraining dataset to fine-tune it. Meanwhile, the LQ-former captures\ndepression-related features from speech and visual data, aiding the model's\nability to process multimodal information, to achieve comprehensive depression\ndiagnosis. Our approach achieves state-of-the-art results on two\ninterview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its\neffectiveness and superiority.",
    "published": "2025-07-08T01:56:39Z",
    "updated": "2025-07-08T01:56:39Z",
    "id": "2507.05591v1",
    "authors": [
      "Wei Zhang",
      "Juan Chen",
      "En Zhu",
      "Wenhong Cheng",
      "YunPeng Li",
      "Yanbo J. Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05591v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05591v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05591v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using multimodal large language models for explainable depression recognition, which involves processing speech and visual data for diagnosis. This aligns with the topics of Multimodal Large Language Models (MLLM) and Benchmarking (Benchmark) as it evaluates performance on specific datasets.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.08019v1": {
    "title": "Signal or Noise? Evaluating Large Language Models in Resume Screening\n  Across Contextual Variations and Human Expert Benchmarks",
    "summary": "This study investigates whether large language models (LLMs) exhibit\nconsistent behavior (signal) or random variation (noise) when screening resumes\nagainst job descriptions, and how their performance compares to human experts.\nUsing controlled datasets, we tested three LLMs (Claude, GPT, and Gemini)\nacross contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context)\nwith identical and randomized resumes, benchmarked against three human\nrecruitment experts. Analysis of variance revealed significant mean differences\nin four of eight LLM-only conditions and consistently significant differences\nbetween LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts\nstrongly to company context (p < 0.001), Gemini partially (p = 0.038 for\nFirm1), and Claude minimally (p > 0.1), while all LLMs differed significantly\nfrom human experts across contexts. Meta-cognition analysis highlighted\nadaptive weighting patterns that differ markedly from human evaluation\napproaches. Findings suggest LLMs offer interpretable patterns with detailed\nprompts but diverge substantially from human judgment, informing their\ndeployment in automated hiring systems.",
    "published": "2025-07-08T01:34:21Z",
    "updated": "2025-07-08T01:34:21Z",
    "id": "2507.08019v1",
    "authors": [
      "Aryan Varshney",
      "Venkat Ram Reddy Ganuthula"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08019v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08019v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08019v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the performance of large language models (LLMs) in resume screening, comparing their behavior to human experts. It discusses the consistency and adaptability of LLMs across different contexts, which aligns with the 'LLM' and 'Benchmark' topics. The study also involves benchmarking LLMs against human performance, which is relevant to the 'Benchmark' category.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.05578v1": {
    "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and\n  Mitigation",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they also exhibit memorization of their training\ndata. This phenomenon raises critical questions about model behavior, privacy\nrisks, and the boundary between learning and memorization. Addressing these\nconcerns, this paper synthesizes recent studies and investigates the landscape\nof memorization, the factors influencing it, and methods for its detection and\nmitigation. We explore key drivers, including training data duplication,\ntraining dynamics, and fine-tuning procedures that influence data memorization.\nIn addition, we examine methodologies such as prefix-based extraction,\nmembership inference, and adversarial prompting, assessing their effectiveness\nin detecting and measuring memorized content. Beyond technical analysis, we\nalso explore the broader implications of memorization, including the legal and\nethical implications. Finally, we discuss mitigation strategies, including data\ncleaning, differential privacy, and post-training unlearning, while\nhighlighting open challenges in balancing the minimization of harmful\nmemorization with utility. This paper provides a comprehensive overview of the\ncurrent state of research on LLM memorization across technical, privacy, and\nperformance dimensions, identifying critical directions for future work.",
    "published": "2025-07-08T01:30:46Z",
    "updated": "2025-07-08T01:30:46Z",
    "id": "2507.05578v1",
    "authors": [
      "Alexander Xiong",
      "Xuandong Zhao",
      "Aneesh Pappu",
      "Dawn Song"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05578v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05578v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05578v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the phenomenon of memorization in Large Language Models (LLMs), discussing its mechanisms, measurement, and mitigation strategies. This directly relates to the 'Memory' topic, which includes research on LLM memory and retrieval-based methods. Additionally, the discussion on training data and fine-tuning procedures touches on 'Pretrain' topics, as these are part of pretraining strategies and objectives. The broader implications and ethical considerations also hint at 'AGI' topics, as they involve general concerns about model behavior and intelligence.",
    "llm_cls_result": [
      "Memory",
      "Pretrain",
      "AGI"
    ]
  },
  "2507.05577v1": {
    "title": "Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs\n  for Biomedical QA",
    "summary": "Biomedical semantic question answering rooted in information retrieval can\nplay a crucial role in keeping up to date with vast, rapidly evolving and\never-growing biomedical literature. A robust system can help researchers,\nhealthcare professionals and even layman users access relevant knowledge\ngrounded in evidence. The BioASQ 2025 Task13b Challenge serves as an important\nbenchmark, offering a competitive platform for advancement of this space. This\npaper presents the methodologies and results from our participation in this\nchallenge where we built a Retrieval-Augmented Generation (RAG) system that can\nanswer biomedical questions by retrieving relevant PubMed documents and\nsnippets to generate answers. For the retrieval task, we generated dense\nembeddings from biomedical articles for initial retrieval, and applied an\nensemble of finetuned cross-encoders and large language models (LLMs) for\nre-ranking to identify top relevant documents. Our solution achieved an MAP@10\nof 0.1581, placing 10th on the leaderboard for the retrieval task. For answer\ngeneration, we employed few-shot prompting of instruction-tuned LLMs. Our\nsystem achieved macro-F1 score of 0.95 for yes/no questions (rank 12), Mean\nReciprocal Rank (MRR) of 0.64 for factoid questions (rank 1), mean-F1 score of\n0.63 for list questions (rank 5), and ROUGE-SU4 F1 score of 0.29 for ideal\nanswers (rank 11).",
    "published": "2025-07-08T01:25:06Z",
    "updated": "2025-07-08T01:25:06Z",
    "id": "2507.05577v1",
    "authors": [
      "Shashank Verma",
      "Fengyi Jiang",
      "Xiangning Xue"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05577v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05577v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05577v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in a Retrieval-Augmented Generation (RAG) system for biomedical question answering, which involves retrieval, re-ranking, and answer generation tasks. The focus on LLMs and their application in retrieval and generation tasks aligns with the topics of LLM and Memory.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.05573v1": {
    "title": "Prompt Migration: Stabilizing GenAI Applications with Evolving Large\n  Language Models",
    "summary": "Generative AI is transforming business applications by enabling natural\nlanguage interfaces and intelligent automation. However, the underlying large\nlanguage models (LLMs) are evolving rapidly and so prompting them consistently\nis a challenge. This leads to inconsistent and unpredictable application\nbehavior, undermining the reliability that businesses require for\nmission-critical workflows. In this paper, we introduce the concept of prompt\nmigration as a systematic approach to stabilizing GenAI applications amid\nchanging LLMs. Using the Tursio enterprise search application as a case study,\nwe analyze the impact of successive GPT model upgrades, detail our migration\nframework including prompt redesign and a migration testbed, and demonstrate\nhow these techniques restore application consistency. Our results show that\nstructured prompt migration can fully recover the application reliability that\nwas lost due to model drift. We conclude with practical lessons learned,\nemphasizing the need for prompt lifecycle management and robust testing to\nensure dependable GenAI-powered business applications.",
    "published": "2025-07-08T01:20:12Z",
    "updated": "2025-07-08T01:20:12Z",
    "id": "2507.05573v1",
    "authors": [
      "Shivani Tripathi",
      "Pushpanjali Nema",
      "Aditya Halder",
      "Shi Qiao",
      "Alekh Jindal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05573v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05573v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05573v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the challenges of maintaining consistency in GenAI applications due to evolving large language models (LLMs) and introduces a systematic approach called prompt migration to address these issues. The focus is on LLMs and their application in business workflows, which aligns with the 'LLM' and 'Benchmark' topics.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.05568v1": {
    "title": "ReLayout: Integrating Relation Reasoning for Content-aware Layout\n  Generation with Multi-modal Large Language Models",
    "summary": "Content-aware layout aims to arrange design elements appropriately on a given\ncanvas to convey information effectively. Recently, the trend for this task has\nbeen to leverage large language models (LLMs) to generate layouts\nautomatically, achieving remarkable performance. However, existing LLM-based\nmethods fail to adequately interpret spatial relationships among visual themes\nand design elements, leading to structural and diverse problems in layout\ngeneration. To address this issue, we introduce ReLayout, a novel method that\nleverages relation-CoT to generate more reasonable and aesthetically coherent\nlayouts by fundamentally originating from design concepts. Specifically, we\nenhance layout annotations by introducing explicit relation definitions, such\nas region, salient, and margin between elements, with the goal of decomposing\nthe layout into smaller, structured, and recursive layouts, thereby enabling\nthe generation of more structured layouts. Furthermore, based on these defined\nrelationships, we introduce a layout prototype rebalance sampler, which defines\nlayout prototype features across three dimensions and quantifies distinct\nlayout styles. This sampler addresses uniformity issues in generation that\narise from data bias in the prototype distribution balance process. Extensive\nexperimental results verify that ReLayout outperforms baselines and can\ngenerate structural and diverse layouts that are more aligned with human\naesthetics and more explainable.",
    "published": "2025-07-08T01:13:43Z",
    "updated": "2025-07-08T01:13:43Z",
    "id": "2507.05568v1",
    "authors": [
      "Jiaxu Tian",
      "Xuehui Yu",
      "Yaoxing Wang",
      "Pan Wang",
      "Guangqian Guo",
      "Shan Gao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05568v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05568v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05568v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of multi-modal large language models (MLLMs) for content-aware layout generation, emphasizing the integration of relation reasoning to improve layout quality. This aligns with the topics of MLLM (Multi Modal Large Language Models) and Reasoning (as it involves logical reasoning for layout generation).",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.05565v1": {
    "title": "Search-based Selection of Metamorphic Relations for Optimized Robustness\n  Testing of Large Language Models",
    "summary": "Assessing the trustworthiness of Large Language Models (LLMs), such as\nrobustness, has garnered significant attention. Recently, metamorphic testing\nthat defines Metamorphic Relations (MRs) has been widely applied to evaluate\nthe robustness of LLM executions. However, the MR-based robustness testing\nstill requires a scalable number of MRs, thereby necessitating the optimization\nof selecting MRs. Most extant LLM testing studies are limited to automatically\ngenerating test cases (i.e., MRs) to enhance failure detection. Additionally,\nmost studies only considered a limited test space of single perturbation MRs in\ntheir evaluation of LLMs. In contrast, our paper proposes a search-based\napproach for optimizing the MR groups to maximize failure detection and\nminimize the LLM execution cost. Moreover, our approach covers the\ncombinatorial perturbations in MRs, facilitating the expansion of test space in\nthe robustness assessment. We have developed a search process and implemented\nfour search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel\nencoding to solve the MR selection problem in the LLM robustness testing. We\nconducted comparative experiments on the four search algorithms along with a\nrandom search, using two major LLMs with primary Text-to-Text tasks. Our\nstatistical and empirical investigation revealed two key findings: (1) the\nMOEA/D algorithm performed the best in optimizing the MR space for LLM\nrobustness testing, and (2) we identified silver bullet MRs for the LLM\nrobustness testing, which demonstrated dominant capabilities in confusing LLMs\nacross different Text-to-Text tasks. In LLM robustness assessment, our research\nsheds light on the fundamental problem for optimized testing and provides\ninsights into search-based solutions.",
    "published": "2025-07-08T01:11:27Z",
    "updated": "2025-07-08T01:11:27Z",
    "id": "2507.05565v1",
    "authors": [
      "Sangwon Hyun",
      "Shaukat Ali",
      "M. Ali Babar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05565v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05565v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05565v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing the selection of Metamorphic Relations (MRs) for robustness testing of Large Language Models (LLMs), which involves evaluating and improving the robustness of LLMs through search-based methods. This directly relates to the topics of LLM robustness testing and evaluation.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.05541v1": {
    "title": "SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data\n  Augmentation",
    "summary": "Counterfactual explanations (CFs) offer human-centric insights into machine\nlearning predictions by highlighting minimal changes required to alter an\noutcome. Therefore, CFs can be used as (i) interventions for abnormality\nprevention and (ii) augmented data for training robust models. In this work, we\nexplore large language models (LLMs), specifically GPT-4o-mini, for generating\nCFs in a zero-shot and three-shot setting. We evaluate our approach on two\ndatasets: the AI-Readi flagship dataset for stress prediction and a public\ndataset for heart disease detection. Compared to traditional methods such as\nDiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high\nplausibility (up to 99%), strong validity (up to 0.99), and competitive\nsparsity. Moreover, using LLM-generated CFs as augmented samples improves\ndownstream classifier performance (an average accuracy gain of 5%), especially\nin low-data regimes. This demonstrates the potential of prompt-based generative\ntechniques to enhance explainability and robustness in clinical and\nphysiological prediction tasks. Code base: github.com/anonymous/SenseCF.",
    "published": "2025-07-07T23:45:40Z",
    "updated": "2025-07-07T23:45:40Z",
    "id": "2507.05541v1",
    "authors": [
      "Shovito Barua Soumma",
      "Asiful Arefeen",
      "Stephanie M. Carpenter",
      "Melanie Hingle",
      "Hassan Ghasemzadeh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05541v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05541v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05541v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for generating counterfactual explanations (CFs) in a zero-shot and few-shot setting, which is directly related to LLM research. It also evaluates the approach on clinical and physiological datasets, which involves reasoning abilities in LLMs for generating plausible and valid CFs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.05528v1": {
    "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for\n  Procedural Learning and Pedagogic Quality Assessment",
    "summary": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced.",
    "published": "2025-07-07T22:56:37Z",
    "updated": "2025-07-07T22:56:37Z",
    "id": "2507.05528v1",
    "authors": [
      "Jiahuan Pei",
      "Fanghua Ye",
      "Xin Sun",
      "Wentao Deng",
      "Koen Hindriks",
      "Junxiao Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05528v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05528v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05528v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of multiple LLMs in a multi-agent workflow for educational purposes, which involves leveraging LLMs for procedural learning and pedagogic quality assessment. It also introduces a large dataset for this purpose.",
    "llm_cls_result": [
      "LLM",
      "Dataset",
      "Reasoning"
    ]
  },
  "2507.06804v1": {
    "title": "Towards Solving More Challenging IMO Problems via Decoupled Reasoning\n  and Proving",
    "summary": "Automated Theorem Proving (ATP) in formal languages is a foundational\nchallenge for AI. While Large Language Models (LLMs) have driven remarkable\nprogress, a significant gap remains between their powerful informal reasoning\ncapabilities and their weak formal proving performance. Recent studies show\nthat the informal accuracy exceeds 80% while formal success remains below 8% on\nbenchmarks like PutnamBench. We argue this gap persists because current\nstate-of-the-art provers, by tightly coupling reasoning and proving, are\ntrained with paradigms that inadvertently punish deep reasoning in favor of\nshallow, tactic-based strategies. To bridge this fundamental gap, we propose a\nnovel framework that decouples high-level reasoning from low-level proof\ngeneration. Our approach utilizes two distinct, specialized models: a powerful,\ngeneral-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an\nefficient Prover to rigorously verify them. This modular design liberates the\nmodel's full reasoning potential and bypasses the pitfalls of end-to-end\ntraining. We evaluate our method on a challenging set of post-2000 IMO\nproblems, a problem set on which no prior open-source prover has reported\nsuccess. Our decoupled framework successfully solves 5 of these problems,\ndemonstrating a significant step towards automated reasoning on exceptionally\ndifficult mathematical challenges. To foster future research, we release our\nfull dataset of generated and verified lemmas for a wide range of IMO problems,\navailable at https://tencent-imo.github.io/ .",
    "published": "2025-07-07T22:38:49Z",
    "updated": "2025-07-07T22:38:49Z",
    "id": "2507.06804v1",
    "authors": [
      "Zhenwen Liang",
      "Linfeng Song",
      "Yang Li",
      "Tao Yang",
      "Feng Zhang",
      "Haitao Mi",
      "Dong Yu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06804v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06804v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06804v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in Automated Theorem Proving (ATP) and proposes a novel framework to decouple reasoning and proving. It highlights the gap between informal reasoning and formal proving capabilities of LLMs, which is a key aspect of reasoning in LLMs. The paper also introduces a dataset for future research, which aligns with the 'Reasoning' and 'Dataset' topics.",
    "llm_cls_result": [
      "Reasoning",
      "Dataset"
    ]
  },
  "2507.05517v2": {
    "title": "Empowering Healthcare Practitioners with Language Models: Structuring\n  Speech Transcripts in Two Real-World Clinical Applications",
    "summary": "Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong\nperformance on clinical natural language processing (NLP) tasks across multiple\nmedical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular\nreporting from nurse dictations and medical order extraction from\ndoctor-patient consultations - remain underexplored due to data scarcity and\nsensitivity, despite active industry efforts. Practical solutions to these\nreal-world clinical tasks can significantly reduce the documentation burden on\nhealthcare providers, allowing greater focus on patient care. In this paper, we\ninvestigate these two challenging tasks using private and open-source clinical\ndatasets, evaluating the performance of both open- and closed-weight LLMs, and\nanalyzing their respective strengths and limitations. Furthermore, we propose\nan agentic pipeline for generating realistic, non-sensitive nurse dictations,\nenabling structured extraction of clinical observations. To support further\nresearch in both areas, we release SYNUR and SIMORD, the first open-source\ndatasets for nurse observation extraction and medical order extraction.",
    "published": "2025-07-07T22:29:29Z",
    "updated": "2025-07-09T19:53:32Z",
    "id": "2507.05517v2",
    "authors": [
      "Jean-Philippe Corbeil",
      "Asma Ben Abacha",
      "George Michalopoulos",
      "Phillip Swazinna",
      "Miguel Del-Agua",
      "Jerome Tremblay",
      "Akila Jeeson Daniel",
      "Cari Bader",
      "Yu-Cheng Cho",
      "Pooja Krishnan",
      "Nathan Bodenstab",
      "Thomas Lin",
      "Wenxuan Teng",
      "Francois Beaulieu",
      "Paul Vozila"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05517v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05517v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05517v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in healthcare, specifically for structuring speech transcripts in clinical applications. It mentions the use of LLMs like GPT-4o and o1, and focuses on tasks such as structured tabular reporting and medical order extraction, which are relevant to LLM research. Additionally, the paper introduces new datasets (SYNUR and SIMORD) for these tasks, which aligns with the 'Dataset' category.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.05504v1": {
    "title": "Tool for Supporting Debugging and Understanding of Normative\n  Requirements Using LLMs",
    "summary": "Normative requirements specify social, legal, ethical, empathetic, and\ncultural (SLEEC) norms that must be observed by a system. To support the\nidentification of SLEEC requirements, numerous standards and regulations have\nbeen developed. These requirements are typically defined by stakeholders in the\nnon-technical system with diverse expertise (e.g., ethicists, lawyers, social\nscientists). Hence, ensuring their consistency and managing the requirement\nelicitation process are complex and error-prone tasks. Recent research has\naddressed this challenge using domain-specific languages to specify normative\nrequirements as rules, whose consistency can then be analyzed with formal\nmethods. Nevertheless, these approaches often present the results from formal\nverification tools in a way that is inaccessible to non-technical users. This\nhinders understanding and makes the iterative process of eliciting and\nvalidating these requirements inefficient in terms of both time and effort. To\naddress this problem, we introduce SLEEC-LLM, a tool that uses large language\nmodels (LLMs) to provide natural-language interpretations for model-checking\ncounterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves\nthe efficiency and explainability of normative requirements elicitation and\nconsistency analysis. To demonstrate its effectiveness, we summarise its use in\ntwo real-world case studies involving non-technical stakeholders.",
    "published": "2025-07-07T21:57:28Z",
    "updated": "2025-07-07T21:57:28Z",
    "id": "2507.05504v1",
    "authors": [
      "Alex Kleijwegt",
      "Sinem Getir Yaman",
      "Radu Calinescu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05504v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05504v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05504v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to support the debugging and understanding of normative requirements, which aligns with the 'LLM' topic. It does not directly relate to other topics such as RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.05495v1": {
    "title": "Deep Research Comparator: A Platform For Fine-grained Human Annotations\n  of Deep Research Agents",
    "summary": "Effectively evaluating deep research agents that autonomously search the web,\nanalyze information, and generate reports remains a major challenge,\nparticularly when it comes to assessing long reports and giving detailed\nfeedback on their intermediate steps. To address these gaps, we introduce Deep\nResearch Comparator, a platform that offers a holistic framework for deep\nresearch agent hosting, side-by-side comparison, fine-grained human feedback\ncollection, and ranking calculation. Given a user query, our platform displays\nthe final reports from two different agents along with their intermediate steps\nduring generation. Annotators can evaluate the overall quality of final reports\nbased on side-by-side comparison, and also provide detailed feedback separately\nby assessing intermediate steps or specific text spans within the final report.\nFurthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This\nscaffold serves as a baseline that facilitates the easy integration of various\nlarge language models to transform them into deep research agents for\nevaluation. To demonstrate the platform's utility for deep research agent\ndevelopment, we have collected real user preference data from 17 annotators on\nthree deep research agents. A demo video of our platform can be found at\nhttps://www.youtube.com/watch?v=g4d2dnbdseg.",
    "published": "2025-07-07T21:35:09Z",
    "updated": "2025-07-07T21:35:09Z",
    "id": "2507.05495v1",
    "authors": [
      "Prahaladh Chandrahasan",
      "Jiahe Jin",
      "Zhihan Zhang",
      "Tevin Wang",
      "Andy Tang",
      "Lucy Mo",
      "Morteza Ziyadi",
      "Leonardo F. R. Ribeiro",
      "Zimeng Qiu",
      "Markus Dreyer",
      "Akari Asai",
      "Chenyan Xiong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05495v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05495v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05495v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating deep research agents using a platform for fine-grained human annotations, which involves the use of large language models (LLMs) but does not directly align with the specific topics provided.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.08838v1": {
    "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language\n  Models",
    "summary": "Improving the reasoning capabilities of diffusion-based large language models\n(dLLMs) through reinforcement learning (RL) remains an open problem. The\nintractability of dLLMs likelihood function necessitates approximating the\ncurrent, old, and reference policy likelihoods at each policy optimization\nstep. This reliance introduces additional computational overhead and lead to\npotentially large bias -- particularly when approximation errors occur in the\ndenominator of policy ratios used for importance sampling. To mitigate these\nissues, we introduce $\\mathtt{wd1}$, a novel policy optimization approach that\nreformulates the objective as a weighted likelihood, requiring only a single\napproximation for the current parametrized policy likelihood. Experiments on\nwidely used reasoning benchmarks demonstrate that $\\mathtt{wd1}$, without\nsupervised fine-tuning (SFT) or any supervised data, outperforms existing RL\nmethods for dLLMs, achieving up to 16% higher accuracy. $\\mathtt{wd1}$ delivers\nadditional computational gains, including reduced training time and fewer\nfunction evaluations (NFEs) per gradient step. These findings, combined with\nthe simplicity of method's implementation and R1-Zero-like training (no SFT),\nposition $\\mathtt{wd1}$ as a more effective and efficient method for applying\nRL to dLLMs reasoning.",
    "published": "2025-07-07T21:27:25Z",
    "updated": "2025-07-07T21:27:25Z",
    "id": "2507.08838v1",
    "authors": [
      "Xiaohang Tang",
      "Rares Dolga",
      "Sangwoong Yoon",
      "Ilija Bogunovic"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08838v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08838v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08838v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving reasoning capabilities of diffusion-based large language models (dLLMs) through reinforcement learning (RL), which aligns with the topics of Reasoning and RL. The mention of diffusion-based large language models also suggests a connection to LLM research.",
    "llm_cls_result": [
      "Reasoning",
      "RL",
      "LLM"
    ]
  },
  "2507.05469v1": {
    "title": "Inaugural MOASEI Competition at AAMAS'2025: A Technical Report",
    "summary": "We present the Methods for Open Agent Systems Evaluation Initiative (MOASEI)\nCompetition, a multi-agent AI benchmarking event designed to evaluate\ndecision-making under open-world conditions. Built on the free-range-zoo\nenvironment suite, MOASEI introduced dynamic, partially observable domains with\nagent and task openness--settings where entities may appear, disappear, or\nchange behavior over time. The 2025 competition featured three\ntracks--Wildfire, Rideshare, and Cybersecurity--each highlighting distinct\ndimensions of openness and coordination complexity. Eleven teams from\ninternational institutions participated, with four of those teams submitting\ndiverse solutions including graph neural networks, convolutional architectures,\npredictive modeling, and large language model--driven meta--optimization.\nEvaluation metrics centered on expected utility, robustness to perturbations,\nand responsiveness to environmental change. The results reveal promising\nstrategies for generalization and adaptation in open environments, offering\nboth empirical insight and infrastructure for future research. This report\ndetails the competition's design, findings, and contributions to the open-agent\nsystems research community.",
    "published": "2025-07-07T20:44:16Z",
    "updated": "2025-07-07T20:44:16Z",
    "id": "2507.05469v1",
    "authors": [
      "Ceferino Patino",
      "Tyler J. Billings",
      "Alireza Saleh Abadi",
      "Daniel Redder",
      "Adam Eck",
      "Prashant Doshi",
      "Leen-Kiat Soh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05469v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05469v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05469v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multi-agent AI benchmarking event focusing on decision-making under open-world conditions, which involves various AI techniques including large language models. However, the primary focus is on multi-agent systems and benchmarking rather than specific topics like LLM, RL, or MLLM.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.08017v3": {
    "title": "Mechanistic Indicators of Understanding in Large Language Models",
    "summary": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them.",
    "published": "2025-07-07T20:26:31Z",
    "updated": "2025-07-24T12:23:53Z",
    "id": "2507.08017v3",
    "authors": [
      "Pierre Beckmann",
      "Matthieu Queloz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08017v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08017v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08017v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the inner workings and understanding mechanisms of Large Language Models (LLMs), focusing on their internal structures and forms of understanding. This aligns with the topics of LLM research and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.05448v1": {
    "title": "On the Semantics of Large Language Models",
    "summary": "Large Language Models (LLMs) such as ChatGPT demonstrated the potential to\nreplicate human language abilities through technology, ranging from text\ngeneration to engaging in conversations. However, it remains controversial to\nwhat extent these systems truly understand language. We examine this issue by\nnarrowing the question down to the semantics of LLMs at the word and sentence\nlevel. By examining the inner workings of LLMs and their generated\nrepresentation of language and by drawing on classical semantic theories by\nFrege and Russell, we get a more nuanced picture of the potential semantic\ncapabilities of LLMs.",
    "published": "2025-07-07T20:02:57Z",
    "updated": "2025-07-07T20:02:57Z",
    "id": "2507.05448v1",
    "authors": [
      "Martin Schuele"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05448v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05448v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05448v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the semantic capabilities of Large Language Models (LLMs) and examines their understanding of language, which directly relates to research on LLMs and their architectures.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.05444v1": {
    "title": "PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically\n  Distant Language Pairs",
    "summary": "Vocabulary acquisition poses a significant challenge for second-language (L2)\nlearners, especially when learning typologically distant languages such as\nEnglish and Korean, where phonological and structural mismatches complicate\nvocabulary learning. Recently, large language models (LLMs) have been used to\ngenerate keyword mnemonics by leveraging similar keywords from a learner's\nfirst language (L1) to aid in acquiring L2 vocabulary. However, most of this\nresearch has focused on native English speakers learning other languages,\nrather than the reverse. In this paper, we present PhoniTale, a novel\ncross-lingual mnemonic generation system that retrieves L1 keyword sequence\nbased on phonological similarity and uses LLMs to generate mnemonics. We\nevaluate PhoniTale using both automated metrics and human evaluations,\ncomparing its output to mnemonics created by humans and by previous automated\napproaches. To assess practical effectiveness, we also conduct a short-term\nrecall test measuring mnemonic helpfulness. Our findings show that PhoniTale\nperforms comparably to human-authored mnemonics. We also highlight key areas\nfor future improvement in mnemonic quality and methodology.",
    "published": "2025-07-07T19:50:12Z",
    "updated": "2025-07-07T19:50:12Z",
    "id": "2507.05444v1",
    "authors": [
      "Sana Kang",
      "Myeongseok Gwon",
      "Su Young Kwon",
      "Jaewook Lee",
      "Andrew Lan",
      "Bhiksha Raj",
      "Rita Singh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05444v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05444v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05444v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the use of large language models (LLMs) to generate mnemonics for vocabulary acquisition in second-language learning, which involves leveraging LLMs for a specific application rather than core research on LLMs themselves. The topics provided are more aligned with core LLM research areas, and this paper does not fit neatly into any of them.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.07060v1": {
    "title": "DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM\n  Reasoning",
    "summary": "Retrosynthesis, the identification of precursor molecules for a target\ncompound, is pivotal for synthesizing complex molecules, but faces challenges\nin discovering novel pathways beyond predefined templates. Recent large\nlanguage model (LLM) approaches to retrosynthesis have shown promise but\neffectively harnessing LLM reasoning capabilities for effective multi-step\nplanning remains an open question. To address this challenge, we introduce\nDeepRetro, an open-source, iterative, hybrid LLM-based retrosynthetic\nframework. Our approach integrates the strengths of conventional\ntemplate-based/Monte Carlo tree search tools with the generative power of LLMs\nin a step-wise, feedback-driven loop. Initially, synthesis planning is\nattempted with a template-based engine. If this fails, the LLM subsequently\nproposes single-step retrosynthetic disconnections. Crucially, these\nsuggestions undergo rigorous validity, stability, and hallucination checks\nbefore the resulting precursors are recursively fed back into the pipeline for\nfurther evaluation. This iterative refinement allows for dynamic pathway\nexploration and correction. We demonstrate the potential of this pipeline\nthrough benchmark evaluations and case studies, showcasing its ability to\nidentify viable and potentially novel retrosynthetic routes. In particular, we\ndevelop an interactive graphical user interface that allows expert human\nchemists to provide human-in-the-loop feedback to the reasoning algorithm. This\napproach successfully generates novel pathways for complex natural product\ncompounds, demonstrating the potential for iterative LLM reasoning to advance\nstate-of-art in complex chemical syntheses.",
    "published": "2025-07-07T19:41:39Z",
    "updated": "2025-07-07T19:41:39Z",
    "id": "2507.07060v1",
    "authors": [
      "Shreyas Vinaya Sathyanarayana",
      "Rahil Shah",
      "Sharanabasava D. Hiremath",
      "Rishikesh Panda",
      "Rahul Jana",
      "Riya Singh",
      "Rida Irfan",
      "Ashwin Murali",
      "Bharath Ramsundar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07060v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07060v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07060v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using iterative reasoning capabilities of LLMs for retrosynthetic pathway discovery, integrating LLM reasoning with traditional methods and human feedback.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.05418v1": {
    "title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual\n  Reasoning",
    "summary": "Large Language Models (LLMs) have achieved strong performance in domains like\nmathematics, factual QA, and code generation, yet their multilingual reasoning\ncapabilities in these tasks remain underdeveloped. Especially for low-resource\nlanguages such as Swahili or Thai, LLMs can often misinterpret prompts or\ndefault to reasoning in English. This implicit bias toward high-resource\nlanguages undermines factual accuracy, interpretability, and trust. Current\nmultilingual benchmarks focus only on final answers, overlooking whether models\nactually reason in the target language. To address this gap, we introduce\nGeoFact-X, a geography-based multilingual factual reasoning benchmark with\nannotated reasoning traces in five languages: English, Hindi, Japanese,\nSwahili, and Thai. We further propose BRIDGE, a novel training method that\nguides supervised fine-tuning and test-time reinforcement learning with a\nlanguage-consistency reward to align reasoning with the input language.\nFinally, we develop an automatic evaluation protocol using LLM-as-a-judge to\nassess answer correctness and the quality and language consistency of reasoning\ntraces, enabling nuanced and scalable analysis beyond surface-level metrics.\nOur results show that BRIDGE significantly enhances multilingual reasoning\nfidelity, demonstrating that reasoning-aware multilingual reinforcement\nlearning is crucial for robust cross-lingual generalization.\nhttps://jd730.github.io/projects/GeoFact-X_BRIDGE",
    "published": "2025-07-07T19:04:36Z",
    "updated": "2025-07-07T19:04:36Z",
    "id": "2507.05418v1",
    "authors": [
      "Jaedong Hwang",
      "Kumar Tanmay",
      "Seok-Jin Lee",
      "Ayush Agrawal",
      "Hamid Palangi",
      "Kumar Ayush",
      "Ila Fiete",
      "Paul Pu Liang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05418v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05418v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05418v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving multilingual reasoning capabilities of Large Language Models (LLMs) through a novel training method and a new benchmark, which involves aspects of reasoning, multilingual capabilities, and reinforcement learning.",
    "llm_cls_result": [
      "Reasoning",
      "RL",
      "Benchmark"
    ]
  },
  "2507.05403v1": {
    "title": "PBE Meets LLM: When Few Examples Aren't Few-Shot Enough",
    "summary": "Large language models (LLMs) can generate code from natural language\ndescriptions. Their performance is typically evaluated using programming\nbenchmarks that simulate real-world tasks. These benchmarks provide\nspecifications in the form of docstrings, function signatures, or bug reports.\nThe model then generates a program, which is tested against predefined test\ncases. In contrast, Programming by Example (PBE) uses input-output examples as\nthe specification. Traditional PBE systems rely on search-based methods over\nrestricted transformation spaces. They are usually designed for narrow domains\nand fixed input formats. It remains unclear how well LLMs perform on PBE tasks.\n  In this work, we evaluate LLMs on PBE tasks involving tabular data\ntransformations. We prompt models to generate functions that convert an input\ntable to an output table. We test the generated functions on unseen inputs to\nmeasure accuracy. Our study includes multiple LLMs and evaluates different\nprompting strategies, such as one-shot vs. multi-try. We also compare\nperformance with and without PBE-specific knowledge. Finally, we propose a\nhybrid method that calls a traditional PBE solver first, and then falls back to\nLLMs if necessary. Our results show that LLMs support more diverse input\nformats and achieve higher accuracy than conventional methods. However, they\nstruggle with tasks that contain ambiguity. The hybrid approach improves\noverall success by combining the strengths of both approaches.",
    "published": "2025-07-07T18:42:36Z",
    "updated": "2025-07-07T18:42:36Z",
    "id": "2507.05403v1",
    "authors": [
      "Shuning Zhang",
      "Yongjoo Park"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05403v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05403v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05403v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating Large Language Models (LLMs) on Programming by Example (PBE) tasks, which involves generating code from input-output examples. It discusses the performance of LLMs in this context, compares different prompting strategies, and proposes a hybrid method combining traditional PBE solvers with LLMs. The core topics are related to LLMs and their application in code generation and reasoning tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.05391v1": {
    "title": "Controlling What You Share: Assessing Language Model Adherence to\n  Privacy Preferences",
    "summary": "Large language models (LLMs) are primarily accessed via commercial APIs, but\nthis often requires users to expose their data to service providers. In this\npaper, we explore how users can stay in control of their data by using privacy\nprofiles: simple natural language instructions that say what should and should\nnot be revealed. We build a framework where a local model uses these\ninstructions to rewrite queries, only hiding details deemed sensitive by the\nuser, before sending them to an external model, thus balancing privacy with\nperformance. To support this research, we introduce PEEP, a multilingual\ndataset of real user queries annotated to mark private content and paired with\nsynthetic privacy profiles. Our experiments with lightweight LLMs show they can\nfollow these instructions to some extent, but also face consistent challenges,\nhighlighting the need for models that better understand and comply with\nuser-defined privacy preferences.",
    "published": "2025-07-07T18:22:55Z",
    "updated": "2025-07-07T18:22:55Z",
    "id": "2507.05391v1",
    "authors": [
      "Guillem Ramrez",
      "Alexandra Birch",
      "Ivan Titov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05391v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05391v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05391v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in the context of privacy preferences and user control over data, which is relevant to LLM research. It also introduces a dataset (PEEP) for evaluating privacy adherence, which falls under the category of datasets for LLMs.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.05258v1": {
    "title": "Spatio-Temporal LLM: Reasoning about Environments and Actions",
    "summary": "Despite the significant recent progress of Multimodal Large Language Models\n(MLLMs), MLLMs still struggle to correctly answer prompts that require a\nholistic spatio-temporal understanding. Specifically, it is challenging to\naddress prompts that refer to 1) the entirety of an environment that an agent\nequipped with an MLLM can operate in; and simultaneously also refer to 2)\nrecent actions that just happened and are encoded in a video clip. However,\nsuch a holistic spatio-temporal understanding is important for agents operating\nin the real world. To address this issue, we first develop a framework to\ncollect a large-scale dataset. Using the collected \"Reasoning about\nEnvironments and Actions\" (REA) dataset, we show that recent methods indeed\nstruggle to correctly answer the prompts. To improve, we develop a\n\"spatio-temporal LLM\" (ST-LLM), a model equipped with projectors to improve\nboth spatial understanding of an environment and temporal understanding of\nrecent observations. On the collected REA data, we show that the proposed\nmethod significantly improves results compared to prior work. Code and data are\navailable at https://zoezheng126.github.io/STLLM-website/.",
    "published": "2025-07-07T17:59:55Z",
    "updated": "2025-07-07T17:59:55Z",
    "id": "2507.05258v1",
    "authors": [
      "Haozhen Zheng",
      "Beitong Tian",
      "Mingyuan Wu",
      "Zhenggang Tang",
      "Klara Nahrstedt",
      "Alex Schwing"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05258v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05258v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05258v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development of a spatio-temporal LLM (ST-LLM) to improve understanding of environments and actions, which involves multimodal reasoning and the use of a new dataset. The topics 'MLLM' and 'Reasoning' are directly relevant as the paper focuses on multimodal large language models and their reasoning capabilities. The 'Dataset' topic is also relevant as the paper introduces a new dataset for evaluation.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning",
      "Dataset"
    ]
  },
  "2507.05257v1": {
    "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions",
    "summary": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on\nevaluating reasoning, planning, and execution capabilities, while another\ncritical component-memory, encompassing how agents memorize, update, and\nretrieve long-term information-is under-evaluated due to the lack of\nbenchmarks. We term agents with memory mechanisms as memory agents. In this\npaper, we identify four core competencies essential for memory agents: accurate\nretrieval, test-time learning, long-range understanding, and conflict\nresolution. Existing datasets either rely on limited context lengths or are\ntailored for static, long-context settings like book-based QA, which do not\nreflect the interactive, multi-turn nature of memory agents that incrementally\naccumulate information. Furthermore, no existing benchmarks cover all four\ncompetencies. Therefore, we introduce MemoryAgentBench, a new benchmark\nspecifically designed for memory agents. Our benchmark combines reformulated\nexisting datasets with newly constructed ones, covering the above four memory\ncompetencies, providing a systematic and challenging testbed for assessing\nmemory quality. We evaluate a diverse set of memory agents, ranging from simple\ncontext-based and retrieval-augmented generation (RAG) systems to advanced\nagents with external memory modules and tool integration. Empirical results\nreveal that current methods fall short of mastering all four competencies,\nunderscoring the need for further research into comprehensive memory mechanisms\nfor LLM agents.",
    "published": "2025-07-07T17:59:54Z",
    "updated": "2025-07-07T17:59:54Z",
    "id": "2507.05257v1",
    "authors": [
      "Yuanzhe Hu",
      "Yu Wang",
      "Julian McAuley"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05257v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05257v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05257v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on evaluating memory mechanisms in LLM agents, introducing a new benchmark (MemoryAgentBench) to assess memory competencies. It discusses retrieval-augmented generation (RAG) and other memory-augmented models, aligning with the 'Memory' topic. Additionally, it benchmarks LLM agents, fitting the 'Benchmark' topic. The context of LLM agents also relates to the 'RL' topic due to the potential use of reinforcement learning in agent development.",
    "llm_cls_result": [
      "Memory",
      "Benchmark",
      "RL"
    ]
  },
  "2507.05248v1": {
    "title": "Response Attack: Exploiting Contextual Priming to Jailbreak Large\n  Language Models",
    "summary": "Contextual priming, where earlier stimuli covertly bias later judgments,\noffers an unexplored attack surface for large language models (LLMs). We\nuncover a contextual priming vulnerability in which the previous response in\nthe dialogue can steer its subsequent behavior toward policy-violating content.\nBuilding on this insight, we propose Response Attack, which uses an auxiliary\nLLM to generate a mildly harmful response to a paraphrased version of the\noriginal malicious query. They are then formatted into the dialogue and\nfollowed by a succinct trigger prompt, thereby priming the target model to\ngenerate harmful content. Across eight open-source and proprietary LLMs, RA\nconsistently outperforms seven state-of-the-art jailbreak techniques, achieving\nhigher attack success rates. To mitigate this threat, we construct and release\na context-aware safety fine-tuning dataset, which significantly reduces the\nattack success rate while preserving model capabilities. The code and data are\navailable at https://github.com/Dtc7w3PQ/Response-Attack.",
    "published": "2025-07-07T17:56:05Z",
    "updated": "2025-07-07T17:56:05Z",
    "id": "2507.05248v1",
    "authors": [
      "Ziqi Miao",
      "Lijun Li",
      "Yuan Xiong",
      "Zhenhua Liu",
      "Pengyu Zhu",
      "Jing Shao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05248v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05248v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05248v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a vulnerability in large language models (LLMs) related to contextual priming and proposes an attack method (Response Attack) to exploit this vulnerability. It also addresses mitigation strategies through safety fine-tuning. The core focus is on LLM security and vulnerabilities, which aligns with the 'LLM' topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.05240v1": {
    "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
    "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
    "published": "2025-07-07T17:49:41Z",
    "updated": "2025-07-07T17:49:41Z",
    "id": "2507.05240v1",
    "authors": [
      "Meng Wei",
      "Chenyang Wan",
      "Xiqian Yu",
      "Tai Wang",
      "Yuqiang Yang",
      "Xiaohan Mao",
      "Chenming Zhu",
      "Wenzhe Cai",
      "Hanqing Wang",
      "Yilun Chen",
      "Xihui Liu",
      "Jiangmiao Pang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05240v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05240v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05240v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Vision-and-Language Navigation (VLN) using a hybrid slow-fast context modeling strategy, which involves multi-modal reasoning over vision, language, and action inputs. This aligns with the topics of Vision-Language Action (VLA) models and Multimodal Large Language Models (MLLM).",
    "llm_cls_result": [
      "VLA",
      "MLLM"
    ]
  },
  "2507.05211v1": {
    "title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation",
    "summary": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.",
    "published": "2025-07-07T17:22:00Z",
    "updated": "2025-07-07T17:22:00Z",
    "id": "2507.05211v1",
    "authors": [
      "Zongyan Han",
      "Mohamed El Amine Boudjoghra",
      "Jiahua Dong",
      "Jinhong Wang",
      "Rao Muhammad Anwer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05211v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05211v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05211v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper integrates pre-trained vision-language models and large language models (LLMs) to enhance 3D segmentation, which involves multimodal cues and LLMs.",
    "llm_cls_result": [
      "MLLM",
      "LLM",
      "VLA"
    ]
  },
  "2507.05178v1": {
    "title": "CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale",
    "summary": "Despite rapid progress in large language model (LLM)-based multi-agent\nsystems, current benchmarks fall short in evaluating their scalability,\nrobustness, and coordination capabilities in complex, dynamic, real-world\ntasks. Existing environments typically focus on small-scale, fully observable,\nor low-complexity domains, limiting their utility for developing and assessing\nnext-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire,\nan open-source benchmark designed to close this gap. Built atop the human-AI\nteaming CREW simulation platform, CREW-Wildfire offers procedurally generated\nwildfire response scenarios featuring large maps, heterogeneous agents, partial\nobservability, stochastic dynamics, and long-horizon planning objectives. The\nenvironment supports both low-level control and high-level natural language\ninteractions through modular Perception and Execution modules. We implement and\nevaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks,\nuncovering significant performance gaps that highlight the unsolved challenges\nin large-scale coordination, communication, spatial reasoning, and long-horizon\nplanning under uncertainty. By providing more realistic complexity, scalable\narchitecture, and behavioral evaluation metrics, CREW-Wildfire establishes a\ncritical foundation for advancing research in scalable multi-agent Agentic\nintelligence. All code, environments, data, and baselines will be released to\nsupport future research in this emerging domain.",
    "published": "2025-07-07T16:33:42Z",
    "updated": "2025-07-07T16:33:42Z",
    "id": "2507.05178v1",
    "authors": [
      "Jonathan Hyun",
      "Nicholas R Waytowich",
      "Boyuan Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05178v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05178v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05178v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking multi-agent systems using LLMs, which involves evaluating their scalability, robustness, and coordination capabilities in complex tasks. This aligns with the topics of LLM (Large Language Models) and Benchmark (evaluating LLMs). Additionally, the mention of multi-agent systems and coordination hints at Reinforcement Learning (RL) aspects.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "RL"
    ]
  },
  "2507.05177v2": {
    "title": "OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech\n  Language Model",
    "summary": "Empathetic interaction is a cornerstone of human-machine communication, due\nto the need for understanding speech enriched with paralinguistic cues and\ngenerating emotional and expressive responses. However, the most powerful\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\nthe architecture, data and development opaque to researchers. Given the\ncritical need for transparent research into the LSLMs and empathetic behavior,\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\ndesigned to enable empathetic speech interactions. Based on our empathetic\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\ndecoding architecture to achieve low-latency speech generation. To facilitate\nend-to-end training, OpenS2S incorporates an automated data construction\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\nlow cost. By leveraging large language models to generate empathetic content\nand controllable text-to-speech systems to introduce speaker and emotional\nvariation, we construct a scalable training corpus with rich paralinguistic\ndiversity and minimal human supervision. We release the fully open-source\nOpenS2S model, including the dataset, model weights, pre-training and\nfine-tuning codes, to empower the broader research community and accelerate\ninnovation in empathetic speech systems. The project webpage can be accessed at\nhttps://casia-lm.github.io/OpenS2S",
    "published": "2025-07-07T16:31:37Z",
    "updated": "2025-07-08T14:28:55Z",
    "id": "2507.05177v2",
    "authors": [
      "Chen Wang",
      "Tianyu Peng",
      "Wen Yang",
      "Yinan Bai",
      "Guangfu Wang",
      "Jun Lin",
      "Lanpeng Jia",
      "Lingxiang Wu",
      "Jinqiao Wang",
      "Chengqing Zong",
      "Jiajun Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05177v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05177v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05177v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development of an open-source empathetic large speech language model (LSLM), which involves large language models (LLM) and their application in empathetic speech interactions. It also mentions the use of a dataset and model weights, which are relevant to the Dataset category.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.05157v1": {
    "title": "AI Generated Text Detection Using Instruction Fine-tuned Large Language\n  and Transformer-Based Models",
    "summary": "Large Language Models (LLMs) possess an extraordinary capability to produce\ntext that is not only coherent and contextually relevant but also strikingly\nsimilar to human writing. They adapt to various styles and genres, producing\ncontent that is both grammatically correct and semantically meaningful.\nRecently, LLMs have been misused to create highly realistic phishing emails,\nspread fake news, generate code to automate cyber crime, and write fraudulent\nscientific articles. Additionally, in many real-world applications, the\ngenerated content including style and topic and the generator model are not\nknown beforehand. The increasing prevalence and sophistication of artificial\nintelligence (AI)-generated texts have made their detection progressively more\nchallenging. Various attempts have been made to distinguish machine-generated\ntext from human-authored content using linguistic, statistical, machine\nlearning, and ensemble-based approaches. This work focuses on two primary\nobjectives Task-A, which involves distinguishing human-written text from\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\nmodel responsible for the generation. Both of these tasks are based on fine\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.",
    "published": "2025-07-07T16:13:13Z",
    "updated": "2025-07-07T16:13:13Z",
    "id": "2507.05157v1",
    "authors": [
      "Chinnappa Guggilla",
      "Budhaditya Roy",
      "Trupti Ramdas Chavan",
      "Abdul Rahman",
      "Edward Bowen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05157v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05157v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05157v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the detection of AI-generated text using fine-tuned large language models and transformer-based models, focusing on distinguishing human-written text from machine-generated text and identifying the specific LLM model used. This aligns with topics related to Large Language Models (LLM) and their applications in detection tasks.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.08836v1": {
    "title": "Accuracy and Consumption analysis from a compressed model by CompactifAI\n  from Multiverse Computing",
    "summary": "This study evaluates the performance of a compression method, called\nCompactifAI, developed by Multiverse Computing, applied to the large language\nmodel Llama 3.1 8B\\cite{llama}. The evaluation focused on model efficiency (in\nterms of energy consumption) and accuracy using respectively the frameworks\nCodecarbon\\cite{codecarbon} and Ragas\\cite{ragas}. A comparison was performed\nbetween the model compressed with\nCompactifAI\\cite{compactifai}\\cite{compactifai2} and its full-size version. Our\nfindings reveal that the compressed model using CompactifAI not only\nsignificantly reduced the computational resources but also maintained the model\naccuracy, making the model more efficient, scalable and cost-effective.",
    "published": "2025-07-07T16:01:39Z",
    "updated": "2025-07-07T16:01:39Z",
    "id": "2507.08836v1",
    "authors": [
      "Damien Fovet",
      "Shashank Chamoli",
      "Sarah Oury",
      "Srishti Singhal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08836v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08836v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08836v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the compression of a large language model (Llama 3.1 8B) using a method called CompactifAI, focusing on efficiency and accuracy. This aligns with topics related to LLM (Large Language Models) and Scaling (model efficiency and resource reduction).",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.05137v1": {
    "title": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization",
    "summary": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation.",
    "published": "2025-07-07T15:49:23Z",
    "updated": "2025-07-07T15:49:23Z",
    "id": "2507.05137v1",
    "authors": [
      "Jaewook Lee",
      "Alexander Scarlatos",
      "Andrew Lan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05137v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05137v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05137v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a generative framework for mnemonic generation in language learning, specifically for kanji, but does not directly align with the provided topics related to LLMs, RL, MLLM, etc. It mentions the use of large language models (LLMs) but the core focus is on mnemonic generation and interpretability rather than LLM research.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.05135v1": {
    "title": "LERa: Replanning with Visual Feedback in Instruction Following",
    "summary": "Large Language Models are increasingly used in robotics for task planning,\nbut their reliance on textual inputs limits their adaptability to real-world\nchanges and failures. To address these challenges, we propose LERa - Look,\nExplain, Replan - a Visual Language Model-based replanning approach that\nutilizes visual feedback. Unlike existing methods, LERa requires only a raw RGB\nimage, a natural language instruction, an initial task plan, and failure\ndetection - without additional information such as object detection or\npredefined conditions that may be unavailable in a given scenario. The\nreplanning process consists of three steps: (i) Look, where LERa generates a\nscene description and identifies errors; (ii) Explain, where it provides\ncorrective guidance; and (iii) Replan, where it modifies the plan accordingly.\nLERa is adaptable to various agent architectures and can handle errors from\nboth dynamic scene changes and task execution failures. We evaluate LERa on the\nnewly introduced ALFRED-ChaOS and VirtualHome-ChaOS datasets, achieving a 40%\nimprovement over baselines in dynamic environments. In tabletop manipulation\ntasks with a predefined probability of task failure within the PyBullet\nsimulator, LERa improves success rates by up to 67%. Further experiments,\nincluding real-world trials with a tabletop manipulator robot, confirm LERa's\neffectiveness in replanning. We demonstrate that LERa is a robust and adaptable\nsolution for error-aware task execution in robotics. The code is available at\nhttps://lera-robo.github.io.",
    "published": "2025-07-07T15:49:00Z",
    "updated": "2025-07-07T15:49:00Z",
    "id": "2507.05135v1",
    "authors": [
      "Svyatoslav Pchelintsev",
      "Maxim Patratskiy",
      "Anatoly Onishchenko",
      "Alexandr Korchemnyi",
      "Aleksandr Medvedev",
      "Uliana Vinogradova",
      "Ilya Galuzinsky",
      "Aleksey Postnikov",
      "Alexey K. Kovalev",
      "Aleksandr I. Panov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05135v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05135v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05135v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a Visual Language Model-based replanning approach for robotics, which integrates visual feedback with language instructions and task planning. This aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA) models, as it involves integrating vision and language for task execution in robotics.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.05123v1": {
    "title": "An Evaluation of Large Language Models on Text Summarization Tasks Using\n  Prompt Engineering Techniques",
    "summary": "Large Language Models (LLMs) continue to advance natural language processing\nwith their ability to generate human-like text across a range of tasks. Despite\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\nperformance in text summarization across various domains and datasets has not\nbeen comprehensively evaluated. At the same time, the ability to summarize text\neffectively without relying on extensive training data has become a crucial\nbottleneck. To address these issues, we present a systematic evaluation of six\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\nand ArXiv (scientific). By leveraging prompt engineering techniques including\nzero-shot and in-context learning, our study evaluates the performance using\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\ntimes is conducted to better understand the trade-off between summarization\nquality and computational efficiency. For Long documents, introduce a\nsentence-based chunking strategy that enables LLMs with shorter context windows\nto summarize extended inputs in multiple stages. The findings reveal that while\nLLMs perform competitively on news and dialog tasks, their performance on long\nscientific documents improves significantly when aided by chunking strategies.\nIn addition, notable performance variations were observed based on model\nparameters, dataset properties, and prompt design. These results offer\nactionable insights into how different LLMs behave across task types,\ncontributing to ongoing research in efficient, instruction-based NLP systems.",
    "published": "2025-07-07T15:34:05Z",
    "updated": "2025-07-07T15:34:05Z",
    "id": "2507.05123v1",
    "authors": [
      "Walid Mohamed Aly",
      "Taysir Hassan A. Soliman",
      "Amr Mohamed AbdelAziz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05123v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05123v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05123v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating Large Language Models (LLMs) on text summarization tasks using prompt engineering techniques, which directly relates to the topics of LLMs and Benchmarking. The study also involves the use of datasets for evaluation, which is relevant to the Dataset topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.05121v1": {
    "title": "LVM4CSI: Enabling Direct Application of Pre-Trained Large Vision Models\n  for Wireless Channel Tasks",
    "summary": "Accurate channel state information (CSI) is critical to the performance of\nwireless communication systems, especially with the increasing scale and\ncomplexity introduced by 5G and future 6G technologies. While artificial\nintelligence (AI) offers a promising approach to CSI acquisition and\nutilization, existing methods largely depend on task-specific neural networks\n(NNs) that require expert-driven design and large training datasets, limiting\ntheir generalizability and practicality. To address these challenges, we\npropose LVM4CSI, a general and efficient framework that leverages the\nstructural similarity between CSI and computer vision (CV) data to directly\napply large vision models (LVMs) pre-trained on extensive CV datasets to\nwireless tasks without any fine-tuning, in contrast to large language\nmodel-based methods that generally necessitate fine-tuning. LVM4CSI maps CSI\ntasks to analogous CV tasks, transforms complex-valued CSI into visual formats\ncompatible with LVMs, and integrates lightweight trainable layers to adapt\nextracted features to specific communication objectives. We validate LVM4CSI\nthrough three representative case studies, including channel estimation, human\nactivity recognition, and user localization. Results demonstrate that LVM4CSI\nachieves comparable or superior performance to task-specific NNs, including an\nimprovement exceeding 9.61 dB in channel estimation and approximately 40%\nreduction in localization error. Furthermore, it significantly reduces the\nnumber of trainable parameters and eliminates the need for task-specific NN\ndesign.",
    "published": "2025-07-07T15:33:55Z",
    "updated": "2025-07-07T15:33:55Z",
    "id": "2507.05121v1",
    "authors": [
      "Jiajia Guo",
      "Peiwen Jiang",
      "Chao-Kai Wen",
      "Shi Jin",
      "Jun Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05121v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05121v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05121v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large vision models (LVMs) to wireless communication tasks, which involves leveraging pre-trained models and adapting them to new tasks without fine-tuning. This aligns with the topics of 'Pretrain' (pretraining strategies) and 'MLLM' (multimodal models, though the focus is on vision rather than language).",
    "llm_cls_result": [
      "Pretrain",
      "MLLM"
    ]
  },
  "2507.05118v1": {
    "title": "VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots",
    "summary": "In the field of robotics, researchers face a critical challenge in ensuring\nreliable and efficient task planning. Verifying high-level task plans before\nexecution significantly reduces errors and enhance the overall performance of\nthese systems. In this paper, we propose an architecture for automatically\nverifying high-level task plans before their execution in simulator or\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\nconsists of two key steps: first, the conversion of natural language\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\nanalysis of action sequences. The module uses the reasoning capabilities of the\nLLM to evaluate logical coherence and identify potential gaps in the plan.\nRigorous testing on datasets of varying complexity demonstrates the broad\napplicability of the module to household tasks. We contribute to improving the\nreliability and efficiency of task planning and addresses the critical need for\nrobust pre-execution verification in autonomous systems. The code is available\nat https://verifyllm.github.io.",
    "published": "2025-07-07T15:31:36Z",
    "updated": "2025-07-07T15:31:36Z",
    "id": "2507.05118v1",
    "authors": [
      "Danil S. Grigorev",
      "Alexey K. Kovalev",
      "Aleksandr I. Panov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05118v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05118v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05118v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for verifying task plans in robotics, which involves both LLM capabilities and reasoning tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.05319v1": {
    "title": "LCDS: A Logic-Controlled Discharge Summary Generation System Supporting\n  Source Attribution and Expert Review",
    "summary": "Despite the remarkable performance of Large Language Models (LLMs) in\nautomated discharge summary generation, they still suffer from hallucination\nissues, such as generating inaccurate content or fabricating information\nwithout valid sources. In addition, electronic medical records (EMRs) typically\nconsist of long-form data, making it challenging for LLMs to attribute the\ngenerated content to the sources. To address these challenges, we propose LCDS,\na Logic-Controlled Discharge Summary generation system. LCDS constructs a\nsource mapping table by calculating textual similarity between EMRs and\ndischarge summaries to constrain the scope of summarized content. Moreover,\nLCDS incorporates a comprehensive set of logical rules, enabling it to generate\nmore reliable silver discharge summaries tailored to different clinical fields.\nFurthermore, LCDS supports source attribution for generated content, allowing\nexperts to efficiently review, provide feedback, and rectify errors. The\nresulting golden discharge summaries are subsequently recorded for incremental\nfine-tuning of LLMs. Our project and demo video are in the GitHub repository\nhttps://github.com/ycycyc02/LCDS.",
    "published": "2025-07-07T15:25:52Z",
    "updated": "2025-07-07T15:25:52Z",
    "id": "2507.05319v1",
    "authors": [
      "Cheng Yuan",
      "Xinkai Rui",
      "Yongqi Fan",
      "Yawei Fan",
      "Boyang Zhong",
      "Jiacheng Wang",
      "Weiyan Zhang",
      "Tong Ruan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05319v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05319v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05319v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating discharge summaries in the medical field, addressing issues like hallucination and source attribution. It involves logical rules and source mapping to improve reliability and support expert review, which aligns with topics related to LLMs and their applications in specific domains.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.05093v1": {
    "title": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders",
    "summary": "Large Language Models (LLMs) have transformed human-machine interaction since\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\nkey framework that enhances LLM outputs by integrating external knowledge.\nHowever, RAG's reliance on ingesting external documents introduces new\nvulnerabilities. This paper exposes a critical security gap at the data loading\nstage, where malicious actors can stealthily corrupt RAG pipelines by\nexploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\nimplementing 19 stealthy injection techniques, we test five popular data\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\nvalidate these threats on six end-to-end RAG systems -- including white-box\npipelines and black-box services like NotebookLM and OpenAI Assistants --\ndemonstrating high success rates and critical vulnerabilities that bypass\nfilters and silently compromise output integrity. Our results emphasize the\nurgent need to secure the document ingestion process in RAG systems against\ncovert content manipulations.",
    "published": "2025-07-07T15:13:54Z",
    "updated": "2025-07-07T15:13:54Z",
    "id": "2507.05093v1",
    "authors": [
      "Alberto Castagnaro",
      "Umberto Salviati",
      "Mauro Conti",
      "Luca Pajola",
      "Simeone Pizzi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05093v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05093v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05093v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses vulnerabilities in Retrieval-Augmented Generation (RAG) systems, which are related to LLMs and their memory mechanisms. It focuses on security issues in the data loading stage of RAG, which involves memory-augmented models and retrieval-based methods.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.05065v1": {
    "title": "Replacing thinking with tool usage enables reasoning in small language\n  models",
    "summary": "Recent advances have established a new machine learning paradigm based on\nscaling up compute at inference time as well as at training time. In that line\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\nused for training Large Language Models to expend extra compute during\ninference in the form of \"thoughts\" expressed in natural language. In this\npaper, we propose to instead format these tokens as a multi-turn interaction\ntrace with a stateful tool. At each turn, the new state of the tool is appended\nto the context of the model, whose job is to generate the tokens necessary to\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\nrepairing malfunctioning Python code, and show that this constrained setup\nallows for faster sampling of experience and a denser reward signal, allowing\neven models of size up to 3B parameters to learn how to proficiently expend\nadditional compute on the task.",
    "published": "2025-07-07T14:49:18Z",
    "updated": "2025-07-07T14:49:18Z",
    "id": "2507.05065v1",
    "authors": [
      "Corrado Rainone",
      "Tim Bakker",
      "Roland Memisevic"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05065v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05065v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05065v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of tool usage and reasoning in small language models, which involves reinforcement learning and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "RL",
      "Reasoning"
    ]
  },
  "2507.05316v1": {
    "title": "OASBuilder: Generating OpenAPI Specifications from Online API\n  Documentation with Large Language Models",
    "summary": "AI agents and business automation tools interacting with external web\nservices require standardized, machine-readable information about their APIs in\nthe form of API specifications. However, the information about APIs available\nonline is often presented as unstructured, free-form HTML documentation,\nrequiring external users to spend significant time manually converting it into\na structured format. To address this, we introduce OASBuilder, a novel\nframework that transforms long and diverse API documentation pages into\nconsistent, machine-readable API specifications. This is achieved through a\ncarefully crafted pipeline that integrates large language models and rule-based\nalgorithms which are guided by domain knowledge of the structure of\ndocumentation webpages. Our experiments demonstrate that OASBuilder generalizes\nwell across hundreds of APIs, and produces valid OpenAPI specifications that\nencapsulate most of the information from the original documentation. OASBuilder\nhas been successfully implemented in an enterprise environment, saving\nthousands of hours of manual effort and making hundreds of complex enterprise\nAPIs accessible as tools for LLMs.",
    "published": "2025-07-07T14:36:13Z",
    "updated": "2025-07-07T14:36:13Z",
    "id": "2507.05316v1",
    "authors": [
      "Koren Lazar",
      "Matan Vetzler",
      "Kiran Kate",
      "Jason Tsay",
      "David Boaz Himanshu Gupta",
      "Avraham Shinnar",
      "Rohith D Vallam",
      "David Amid Esther Goldbraich",
      "Guy Uziel",
      "Jim Laredo",
      "Ateret Anaby Tavor"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05316v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05316v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05316v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to transform unstructured API documentation into structured OpenAPI specifications, which aligns with the topic of LLM applications in automating tasks and processing unstructured data.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.05046v1": {
    "title": "What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User\n  Attributes, Trust Dimensions, Task Context, and Societal Perceptions among\n  University Students",
    "summary": "This mixed-methods inquiry examined four domains that shape university\nstudents' trust in ChatGPT: user attributes, seven delineated trust dimensions,\ntask context, and perceived societal impact. Data were collected through a\nsurvey of 115 UK undergraduate and postgraduate students and four complementary\nsemi-structured interviews. Behavioural engagement outweighed demographics:\nfrequent use increased trust, whereas self-reported understanding of\nlarge-language-model mechanics reduced it. Among the dimensions, perceived\nexpertise and ethical risk were the strongest predictors of overall trust; ease\nof use and transparency had secondary effects, while human-likeness and\nreputation were non-significant. Trust was highly task-contingent; highest for\ncoding and summarising, lowest for entertainment and citation generation, yet\nconfidence in ChatGPT's referencing ability, despite known inaccuracies, was\nthe single strongest correlate of global trust, indicating automation bias.\nComputer-science students surpassed peers only in trusting the system for\nproofreading and writing, suggesting technical expertise refines rather than\ninflates reliance. Finally, students who viewed AI's societal impact positively\nreported the greatest trust, whereas mixed or negative outlooks dampened\nconfidence. These findings show that trust in ChatGPT hinges on task\nverifiability, perceived competence, ethical alignment and direct experience,\nand they underscore the need for transparency, accuracy cues and user education\nwhen deploying LLMs in academic settings.",
    "published": "2025-07-07T14:29:54Z",
    "updated": "2025-07-07T14:29:54Z",
    "id": "2507.05046v1",
    "authors": [
      "Kadija Bouyzourn",
      "Alexandra Birch"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05046v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05046v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05046v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on user trust in ChatGPT, examining various factors such as user attributes, trust dimensions, task context, and societal perceptions. While it involves LLMs (specifically ChatGPT), the primary focus is on user trust and interaction rather than the technical aspects of LLMs, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.05043v1": {
    "title": "MoLink: Distributed and Efficient Serving Framework for Large Models",
    "summary": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models.",
    "published": "2025-07-07T14:27:56Z",
    "updated": "2025-07-07T14:27:56Z",
    "id": "2507.05043v1",
    "authors": [
      "Lewei Jin",
      "Yongqi Chen",
      "Kui Zhang",
      "Yifan Zhuo",
      "Yi Gao",
      "Bowei Yang",
      "Zhengong Cai",
      "Wei Dong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05043v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05043v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05043v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a distributed serving framework for large language models (LLMs) on consumer-grade GPUs, focusing on efficiency and cost-effectiveness. This aligns with topics related to LLM serving and scaling.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.05035v1": {
    "title": "Beyond Scaling Curves: Internal Dynamics of Neural Networks Through the\n  NTK Lens",
    "summary": "Scaling laws offer valuable insights into the relationship between neural\nnetwork performance and computational cost, yet their underlying mechanisms\nremain poorly understood. In this work, we empirically analyze how neural\nnetworks behave under data and model scaling through the lens of the neural\ntangent kernel (NTK). This analysis establishes a link between performance\nscaling and the internal dynamics of neural networks. Our findings of standard\nvision tasks show that similar performance scaling exponents can occur even\nthough the internal model dynamics show opposite behavior. This demonstrates\nthat performance scaling alone is insufficient for understanding the underlying\nmechanisms of neural networks. We also address a previously unresolved issue in\nneural scaling: how convergence to the infinite-width limit affects scaling\nbehavior in finite-width models. To this end, we investigate how feature\nlearning is lost as the model width increases and quantify the transition\nbetween kernel-driven and feature-driven scaling regimes. We identify the\nmaximum model width that supports feature learning, which, in our setups, we\nfind to be more than ten times smaller than typical large language model\nwidths.",
    "published": "2025-07-07T14:17:44Z",
    "updated": "2025-07-07T14:17:44Z",
    "id": "2507.05035v1",
    "authors": [
      "Konstantin Nikolaou",
      "Sven Krippendorf",
      "Samuel Tovey",
      "Christian Holm"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05035v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05035v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05035v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses scaling laws and the internal dynamics of neural networks, particularly focusing on the neural tangent kernel (NTK) and how it relates to performance scaling and feature learning. This aligns with topics related to scaling laws and model behavior, but does not directly fit into the more specific categories provided.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.05010v1": {
    "title": "Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification",
    "summary": "We introduce Co-DETECT (Collaborative Discovery of Edge cases in TExt\nClassificaTion), a novel mixed-initiative annotation framework that integrates\nhuman expertise with automatic annotation guided by large language models\n(LLMs). Co-DETECT starts with an initial, sketch-level codebook and dataset\nprovided by a domain expert, then leverages the LLM to annotate the data and\nidentify edge cases that are not well described by the initial codebook.\nSpecifically, Co-DETECT flags challenging examples, induces high-level,\ngeneralizable descriptions of edge cases, and assists user in incorporating\nedge case handling rules to improve the codebook. This iterative process\nenables more effective handling of nuanced phenomena through compact,\ngeneralizable annotation rules. Extensive user study, qualitative and\nquantitative analyses prove the effectiveness of Co-DETECT.",
    "published": "2025-07-07T13:48:54Z",
    "updated": "2025-07-07T13:48:54Z",
    "id": "2507.05010v1",
    "authors": [
      "Chenfei Xiong",
      "Jingwei Ni",
      "Yu Fan",
      "Vilm Zouhar",
      "Donya Rooein",
      "Lorena Calvo-Bartolom",
      "Alexander Hoyle",
      "Zhijing Jin",
      "Mrinmaya Sachan",
      "Markus Leippold",
      "Dirk Hovy",
      "Mennatallah El-Assady",
      "Elliott Ash"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05010v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05010v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05010v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in a collaborative framework for text classification, specifically focusing on identifying edge cases and improving annotation rules. This aligns with the 'LLM' topic as it involves the application and enhancement of LLMs in a specific task.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04996v1": {
    "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems",
    "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are defined as systems capable of perceiving their\nenvironment and executing preprogrammed tasks independently of external input.\nHowever, both research and real-world deployments increasingly showcase\nvehicles that demonstrate behaviors beyond this definition (including the SAE\nlevels 1 to 6), such as interaction with humans and machines, goal adaptation,\ncontextual reasoning, external tool use, and long-term planning, particularly\nwith the integration of large language models (LLMs) and agentic AI systems.\nThese developments reveal a conceptual gap between technical autonomy and the\nbroader cognitive and social capabilities needed for future human-centered\nmobility systems. To address this, we introduce the concept of agentic vehicles\n(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and\ninteract within complex environments. This paper presents a systems-level\nframework to characterize AgVs, focusing on their cognitive and communicative\nlayers and differentiating them from conventional AuVs. It synthesizes relevant\nadvances in agentic AI, robotics, multi-agent systems, and human-machine\ninteraction, and highlights how agentic AI, through high-level reasoning and\ntool use, can function not merely as computational tools but as interactive\nagents embedded in mobility ecosystems. The paper concludes by identifying key\nchallenges in the development and governance of AgVs, including safety,\nreal-time control, public acceptance, ethical alignment, and regulatory\nframeworks.",
    "published": "2025-07-07T13:34:49Z",
    "updated": "2025-07-07T13:34:49Z",
    "id": "2507.04996v1",
    "authors": [
      "Jiangbo Yu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04996v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04996v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04996v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) and agentic AI systems in vehicles, which aligns with the topics of LLM and AGI. It also touches on the broader cognitive and social capabilities needed for future mobility systems, which relates to AGI.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.04976v1": {
    "title": "Can Video LLMs Refuse to Answer? Alignment for Answerability in Video\n  Large Language Models",
    "summary": "In the broader context of deep learning, Multimodal Large Language Models\nhave achieved significant breakthroughs by leveraging powerful Large Language\nModels as a backbone to align different modalities into the language space. A\nprime exemplification is the development of Video Large Language Models\n(Video-LLMs). While numerous advancements have been proposed to enhance the\nvideo understanding capabilities of these models, they are predominantly\ntrained on questions generated directly from video content. However, in\nreal-world scenarios, users often pose questions that extend beyond the\ninformational scope of the video, highlighting the need for Video-LLMs to\nassess the relevance of the question. We demonstrate that even the\nbest-performing Video-LLMs fail to reject unfit questions-not necessarily due\nto a lack of video understanding, but because they have not been trained to\nidentify and refuse such questions. To address this limitation, we propose\nalignment for answerability, a framework that equips Video-LLMs with the\nability to evaluate the relevance of a question based on the input video and\nappropriately decline to answer when the question exceeds the scope of the\nvideo, as well as an evaluation framework with a comprehensive set of metrics\ndesigned to measure model behavior before and after alignment. Furthermore, we\npresent a pipeline for creating a dataset specifically tailored for alignment\nfor answerability, leveraging existing video-description paired datasets.",
    "published": "2025-07-07T13:19:43Z",
    "updated": "2025-07-07T13:19:43Z",
    "id": "2507.04976v1",
    "authors": [
      "Eunseop Yoon",
      "Hee Suk Yoon",
      "Mark A. Hasegawa-Johnson",
      "Chang D. Yoo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04976v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04976v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04976v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses Video Large Language Models (Video-LLMs) and their alignment for answerability, which involves evaluating the relevance of questions based on video content. This falls under the topics of Multimodal Large Language Models (MLLM) and Vision-Language Alignment (VLA) due to its focus on integrating video and language modalities and aligning them for better performance.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.04967v1": {
    "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
    "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
    "published": "2025-07-07T13:10:01Z",
    "updated": "2025-07-07T13:10:01Z",
    "id": "2507.04967v1",
    "authors": [
      "Bardia Mohammadi",
      "Laurent Bindschaedler"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04967v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04967v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04967v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the optimization of Large Language Models (LLMs) for specific queries in OLAP databases, focusing on reducing computational and memory costs through techniques like quantization and sparsification. This aligns with the 'LLM' topic as it involves research on LLM architectures and their practical applications. It also touches on 'Scaling' as it addresses the challenges of deploying LLMs at scale.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.04943v1": {
    "title": "ReLoop: \"Seeing Twice and Thinking Backwards\" via Closed-loop Training\n  to Mitigate Hallucinations in Multimodal understanding",
    "summary": "While Multimodal Large Language Models (MLLMs) have achieved remarkable\nprogress in open-ended visual question answering, they remain vulnerable to\nhallucinations. These are outputs that contradict or misrepresent input\nsemantics, posing a critical challenge to the reliability and factual\nconsistency. Existing methods often rely on external verification or post-hoc\ncorrection, lacking an internal mechanism to validate outputs directly during\ntraining. To bridge this gap, we propose ReLoop, a unified closed-loop training\nframework that encourages multimodal consistency for cross-modal understanding\nin MLLMs. ReLoop adopts a ring-shaped structure that integrates three\ncomplementary consistency feedback mechanisms, obliging MLLMs to \"seeing twice\nand thinking backwards\". Specifically, ReLoop employs the frozen Consistency\nFeedback Plugin (CFP), comprising semantic reconstruction, visual description,\nand an attention supervision module for attention alignment. These components\ncollectively enforce semantic reversibility, visual consistency, and\ninterpretable attention, enabling the model to correct its outputs during\ntraining. Extensive evaluations and analyses demonstrate the effectiveness of\nReLoop in reducing hallucination rates across multiple benchmarks, establishing\na robust method for hallucination mitigation in MLLMs. We will release our\nsource code and data in the camera-ready version.",
    "published": "2025-07-07T12:40:48Z",
    "updated": "2025-07-07T12:40:48Z",
    "id": "2507.04943v1",
    "authors": [
      "Jianjiang Yang",
      "Ziyan Huang",
      "Yanshu Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04943v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04943v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04943v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on mitigating hallucinations in Multimodal Large Language Models (MLLMs) through a closed-loop training framework, which directly relates to the topics of MLLM and VLA due to its emphasis on multimodal understanding and vision-language alignment.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.04931v1": {
    "title": "LIFT: Automating Symbolic Execution Optimization with Large Language\n  Models for AI Networks",
    "summary": "Dynamic Symbolic Execution (DSE) is a key technique in program analysis,\nwidely used in software testing, vulnerability discovery, and formal\nverification. In distributed AI systems, DSE plays a crucial role in\nidentifying hard-to-detect bugs, especially those arising from complex network\ncommunication patterns. However, traditional approaches to symbolic execution\nare often hindered by scalability issues and inefficiencies, particularly in\nlarge-scale systems. This paper introduces LIFT (Large-language-model\nIntegrated Functional-equivalent-IR Transformation), a novel framework that\nleverages Large Language Models (LLMs) to automate the optimization of\nIntermediate Representations (IRs) in symbolic execution. LIFT addresses the\nchallenges of symbolic execution by providing a scalable, context-sensitive\nsolution for IR transformation. The framework consists of two phases: IR\nAnalysis and Optimization, where LLMs optimize time-intensive IR blocks, and\nSymbolic Execution and Validation, which includes benchmarking and semantic\nverification to ensure correctness and generalizability. Experiments on\nreal-world binaries demonstrated significant performance improvements,\nincluding a 53.5\\% reduction in execution time for bigtest and a 10.24\\%\nreduction for random, along with reductions in IR statements, PUT instructions,\nand temporary variables. These results demonstrate that LLMs simplify IRs while\nmaintaining functional correctness, enhancing symbolic execution in distributed\nAI systems.",
    "published": "2025-07-07T12:26:56Z",
    "updated": "2025-07-07T12:26:56Z",
    "id": "2507.04931v1",
    "authors": [
      "Ruoxi Wang",
      "Kun Li",
      "Minghui Xu",
      "Yue Zhang",
      "Kaidi Xu",
      "Chunchi Liu",
      "Yinhao Xiao",
      "Xiuzhen Cheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04931v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04931v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04931v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to optimize symbolic execution in AI networks, which directly relates to research on LLMs and their applications in optimizing and enhancing system performance.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.04909v1": {
    "title": "HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nadvances in visual understanding tasks involving both images and videos.\nHowever, their capacity to comprehend human-centric video data remains\nunderexplored, primarily due to the absence of comprehensive and high-quality\nevaluation benchmarks. Existing human-centric benchmarks predominantly\nemphasize video generation quality and action recognition, while overlooking\nessential perceptual and cognitive abilities required in human-centered\nscenarios. Furthermore, they are often limited by single-question paradigms and\noverly simplistic evaluation metrics. To address above limitations, we propose\na modern HV-MMBench, a rigorously curated benchmark designed to provide a more\nholistic evaluation of MLLMs in human-centric video understanding. Compared to\nexisting human-centric video benchmarks, our work offers the following key\nfeatures: (1) Diverse evaluation dimensions: HV-MMBench encompasses 15 tasks,\nranging from basic attribute perception (e.g., age estimation, emotion\nrecognition) to advanced cognitive reasoning (e.g., social relationship\nprediction, intention prediction), enabling comprehensive assessment of model\ncapabilities; (2) Varied data types: The benchmark includes multiple-choice,\nfill-in-blank, true/false, and open-ended question formats, combined with\ndiverse evaluation metrics, to more accurately and robustly reflect model\nperformance; (3) Multi-domain video coverage: The benchmark spans 50 distinct\nvisual scenarios, enabling comprehensive evaluation across fine-grained scene\nvariations; (4) Temporal coverage: The benchmark covers videos from short-term\n(10 seconds) to long-term (up to 30min) durations, supporting systematic\nanalysis of models temporal reasoning abilities across diverse contextual\nlengths.",
    "published": "2025-07-07T11:52:24Z",
    "updated": "2025-07-07T11:52:24Z",
    "id": "2507.04909v1",
    "authors": [
      "Yuxuan Cai",
      "Jiangning Zhang",
      "Zhenye Gan",
      "Qingdong He",
      "Xiaobin Hu",
      "Junwei Zhu",
      "Yabiao Wang",
      "Chengjie Wang",
      "Zhucun Xue",
      "Xinwei He",
      "Xiang Bai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04909v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04909v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04909v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking Multimodal Large Language Models (MLLMs) for human-centric video understanding, which involves diverse evaluation dimensions, varied data types, multi-domain video coverage, and temporal coverage. This aligns with the topics of MLLM (Multimodal Large Language Models) and Benchmark (Benchmarking LLMs, evaluation metrics, and performance comparison).",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.04886v1": {
    "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations",
    "summary": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch.",
    "published": "2025-07-07T11:17:32Z",
    "updated": "2025-07-07T11:17:32Z",
    "id": "2507.04886v1",
    "authors": [
      "A. Bochkov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04886v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04886v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04886v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the role of semantic representation in large language models (LLMs) and challenges the conventional view of trainable input embeddings. It introduces a novel approach using frozen visual Unicode representations and evaluates the model's performance, including reasoning benchmarks. The core topics are related to LLM architecture and reasoning abilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.04877v1": {
    "title": "DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese\n  Medicine",
    "summary": "Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)\ndiagnosis through multi-turn dialogues and knowledge graphs presents a\nsignificant challenge for modern AI systems. Current large language models\n(LLMs), despite their advancements, exhibit notable limitations in medical\napplications, particularly in conducting effective multi-turn dialogues and\nproactive questioning. These shortcomings hinder their practical application\nand effectiveness in simulating real-world diagnostic scenarios. To address\nthese limitations, we propose DoPI, a novel LLM system specifically designed\nfor the TCM domain. The DoPI system introduces a collaborative architecture\ncomprising a guidance model and an expert model. The guidance model conducts\nmulti-turn dialogues with patients and dynamically generates questions based on\na knowledge graph to efficiently extract critical symptom information.\nSimultaneously, the expert model leverages deep TCM expertise to provide final\ndiagnoses and treatment plans. Furthermore, this study constructs a multi-turn\ndoctor-patient dialogue dataset to simulate realistic consultation scenarios\nand proposes a novel evaluation methodology that does not rely on manually\ncollected real-world consultation data. Experimental results show that the DoPI\nsystem achieves an accuracy rate of 84.68 percent in interrogation outcomes,\nsignificantly enhancing the model's communication ability during diagnosis\nwhile maintaining professional expertise.",
    "published": "2025-07-07T11:04:03Z",
    "updated": "2025-07-07T11:04:03Z",
    "id": "2507.04877v1",
    "authors": [
      "Zewen Sun",
      "Ruoxiang Huang",
      "Jiahe Feng",
      "Rundong Kong",
      "Yuqian Wang",
      "Hengyu Liu",
      "Ziqi Gong",
      "Yuyuan Qin",
      "Yingxue Wang",
      "Yu Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04877v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04877v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04877v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing interrogation capabilities in Traditional Chinese Medicine (TCM) using a novel LLM system, which involves multi-turn dialogues and knowledge graphs. This aligns with the 'LLM' topic as it discusses a specific application of large language models in the medical domain. Additionally, the construction of a multi-turn doctor-patient dialogue dataset relates to the 'Dataset' topic. The paper does not directly address other topics such as RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, or Memory.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.04857v1": {
    "title": "Supporting Software Formal Verification with Large Language Models: An\n  Experimental Study",
    "summary": "Formal methods have been employed for requirements verification for a long\ntime. However, it is difficult to automatically derive properties from natural\nlanguage requirements. SpecVerify addresses this challenge by integrating large\nlanguage models (LLMs) with formal verification tools, providing a more\nflexible mechanism for expressing requirements. This framework combines Claude\n3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on\nnine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%\nverification accuracy, comparable to NASA's CoCoSim, but with lower false\npositives. Our framework formulates assertions that extend beyond the\nexpressive power of LTL and identifies falsifiable cases that are missed by\nmore traditional methods. Counterexample analysis reveals CoCoSim's limitations\nstemming from model connection errors and numerical approximation issues. While\nSpecVerify advances verification automation, our comparative study of Claude,\nChatGPT, and Llama shows that high-quality requirements documentation and human\nmonitoring remain critical, as models occasionally misinterpret specifications.\nOur results demonstrate that LLMs can significantly reduce the barriers to\nformal verification, while highlighting the continued importance of\nhuman-machine collaboration in achieving optimal results.",
    "published": "2025-07-07T10:30:05Z",
    "updated": "2025-07-07T10:30:05Z",
    "id": "2507.04857v1",
    "authors": [
      "Weiqi Wang",
      "Marie Farrell",
      "Lucas C. Cordeiro",
      "Liping Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04857v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04857v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04857v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with formal verification tools to automate the derivation of properties from natural language requirements, which directly relates to the 'LLM' topic. Additionally, the study involves an experimental comparison of different LLMs (Claude, ChatGPT, and Llama), which further supports the relevance to 'LLM'. The focus on formal verification and requirements documentation does not directly align with the other provided topics.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04854v1": {
    "title": "$\\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large\n  Language Models",
    "summary": "Access to consumer grievance redressal in India is often hindered by\nprocedural complexity, legal jargon, and jurisdictional challenges. To address\nthis, we present $\\textbf{Grahak-Nyay}$ (Justice-to-Consumers), a chatbot that\nstreamlines the process using open-source Large Language Models (LLMs) and\nRetrieval-Augmented Generation (RAG). Grahak-Nyay simplifies legal complexities\nthrough a concise and up-to-date knowledge base. We introduce three novel\ndatasets: $\\textit{GeneralQA}$ (general consumer law), $\\textit{SectoralQA}$\n(sector-specific knowledge) and $\\textit{SyntheticQA}$ (for RAG evaluation),\nalong with $\\textit{NyayChat}$, a dataset of 300 annotated chatbot\nconversations. We also introduce $\\textit{Judgments}$ data sourced from Indian\nConsumer Courts to aid the chatbot in decision making and to enhance user\ntrust. We also propose $\\textbf{HAB}$ metrics ($\\textbf{Helpfulness, Accuracy,\nBrevity}$) to evaluate chatbot performance. Legal domain experts validated\nGrahak-Nyay's effectiveness. Code and datasets will be released.",
    "published": "2025-07-07T10:26:42Z",
    "updated": "2025-07-07T10:26:42Z",
    "id": "2507.04854v1",
    "authors": [
      "Shrey Ganatra",
      "Swapnil Bhattacharyya",
      "Harshvivek Kashid",
      "Spandan Anaokar",
      "Shruti Nair",
      "Reshma Sekhar",
      "Siddharth Manohar",
      "Rahul Hemrajani",
      "Pushpak Bhattacharyya"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04854v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04854v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04854v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) in a specific domain (consumer grievance redressal), introduces novel datasets, and evaluates chatbot performance. The core topics are LLM and Dataset.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.04852v1": {
    "title": "Dialogue-Based Multi-Dimensional Relationship Extraction from Novels",
    "summary": "Relation extraction is a crucial task in natural language processing, with\nbroad applications in knowledge graph construction and literary analysis.\nHowever, the complex context and implicit expressions in novel texts pose\nsignificant challenges for automatic character relationship extraction. This\nstudy focuses on relation extraction in the novel domain and proposes a method\nbased on Large Language Models (LLMs). By incorporating relationship dimension\nseparation, dialogue data construction, and contextual learning strategies, the\nproposed method enhances extraction performance. Leveraging dialogue structure\ninformation, it improves the model's ability to understand implicit\nrelationships and demonstrates strong adaptability in complex contexts.\nAdditionally, we construct a high-quality Chinese novel relation extraction\ndataset to address the lack of labeled resources and support future research.\nExperimental results show that our method outperforms traditional baselines\nacross multiple evaluation metrics and successfully facilitates the automated\nconstruction of character relationship networks in novels.",
    "published": "2025-07-07T10:20:16Z",
    "updated": "2025-07-07T10:20:16Z",
    "id": "2507.04852v1",
    "authors": [
      "Yuchen Yan",
      "Hanjie Zhao",
      "Senbin Zhu",
      "Hongde Liu",
      "Zhihong Zhang",
      "Yuxiang Jia"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04852v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04852v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04852v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on relation extraction in novel texts using Large Language Models (LLMs), which involves leveraging dialogue structure and contextual learning strategies. The mention of LLMs and their application in this specific task aligns with the LLM topic. Additionally, the construction of a dataset for relation extraction supports the Dataset topic.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.04841v1": {
    "title": "Spec-TOD: A Specialized Instruction-Tuned LLM Framework for Efficient\n  Task-Oriented Dialogue Systems",
    "summary": "Task-oriented dialogue (TOD) systems facilitate goal-driven interactions\nbetween users and machines. While recent advances in deep learning have\nimproved the performance, TOD systems often struggle in low-resource scenarios\nwith limited labeled data. To address this challenge, we propose Spec-TOD, a\nnovel framework designed to train an end-to-end TOD system with limited data.\nSpec-TOD introduces two main innovations: (i) a novel specialized end-to-end\nTOD framework that incorporates explicit task instructions for\ninstruction-tuned large language models (LLMs), and (ii) an efficient training\nstrategy that leverages lightweight, specialized LLMs to achieve strong\nperformance with minimal supervision. Experiments on the MultiWOZ dataset, a\nwidely used TOD benchmark, demonstrate that Spec-TOD achieves competitive\nresults while significantly reducing the need for labeled data. These findings\nhighlight the potential of the proposed framework in advancing efficient and\neffective TOD systems in low-resource settings.",
    "published": "2025-07-07T10:03:20Z",
    "updated": "2025-07-07T10:03:20Z",
    "id": "2507.04841v1",
    "authors": [
      "Quang-Vinh Nguyen",
      "Quang-Chieu Nguyen",
      "Hoang Pham",
      "Khac-Hoai Nam Bui"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04841v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04841v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04841v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using instruction-tuned large language models (LLMs) for task-oriented dialogue systems, which directly relates to LLM research. It also discusses efficient training strategies and performance on a benchmark dataset, which aligns with the Benchmark category.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.04833v2": {
    "title": "The Geopolitical Determinants of Economic Growth, 1960-2019",
    "summary": "This paper establishes geopolitical relations as a first-order determinant of\neconomic growth. We construct a novel event-based measure of bilateral\ngeopolitical alignment by employing large language models with web search\ncapabilities to analyze over 440,000 political events across 196 countries from\n1960--2019. This comprehensive measure enables us to identify the precise\ntiming and magnitude of geopolitical shifts within countries over time. Using\nlocal projections with country fixed effects, we find that a\none-standard-deviation improvement in geopolitical relations increases GDP per\ncapita by 9.6 log points over 25 years. These persistent effects operate\nthrough multiple reinforcing channels -- enhanced political stability,\nincreased investment, expanded trade, and productivity gains. Across our\nsample, geopolitical factors generate GDP variations ranging from -35% to +30%,\nwith developing nations facing particularly severe penalties from international\nisolation. Our findings reveal how geopolitical alignment shapes economic\nprosperity in an increasingly fragmented global economy.",
    "published": "2025-07-07T09:56:07Z",
    "updated": "2025-07-13T12:19:40Z",
    "id": "2507.04833v2",
    "authors": [
      "Tianyu Fan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04833v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04833v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04833v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on geopolitical relations and economic growth, utilizing large language models for data analysis, but the core topic is not directly related to the provided categories.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.10566v1": {
    "title": "AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous\n  Symbol Systems",
    "summary": "In Decentralized Multi-Agent Reinforcement Learning (MARL), the development\nof Emergent Communication has long been constrained by the ``Joint Exploration\nDilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .\nTraditional methods address this by introducing inductive biases to facilitate\ncommunication emergence . This study fundamentally questions whether such\nartificial inductive biases are, in fact, over-engineering. Through experiments\nwith the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized\nVariational Autoencoder (VQ-VAE), we demonstrate that when agents possess an\nendogenous symbol system, their neural representations naturally exhibit\nspontaneous semantic compression and Nash equilibrium-driven semantic\nconvergence, achieving effective symbolic communication without external\ninductive biases. This aligns with recent neuroscience findings suggesting that\nthe human brain does not directly use human language for internal thought , and\nresonates with research on ``soft thinking'' capabilities in Large Language\nModels (LLMs) . Compared to traditional explicit communication methods, AIM\ndemonstrates stronger generality and efficiency. The interpretable analysis\ntoolkit developed in this study confirms that symbol usage exhibits a\nsignificant power-law distribution, leading to three major theoretical\ninsights: the ``Neural Communication Hypothesis'', the ``Tool-First\nPrinciple'', and the ``Semantic Interpretability Paradigm''. Future research\nwill explore the integration of Hierarchical Quantized Variational Autoencoders\n(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the\npotential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This\ndiscovery offers new avenues for bridging symbolism and connectionism.",
    "published": "2025-07-07T09:52:49Z",
    "updated": "2025-07-07T09:52:49Z",
    "id": "2507.10566v1",
    "authors": [
      "Hung Ming Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10566v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10566v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10566v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development of emergent communication in Multi-Agent Reinforcement Learning (MARL) and its relation to Large Language Models (LLMs), which aligns with the topics of Reinforcement Learning (RL) and Large Language Models (LLM). The abstract also mentions the potential for Reinforcement Learning (RL) Low-Level Pre-training, further reinforcing the relevance to RL.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.04820v1": {
    "title": "Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking\n  Distillation",
    "summary": "While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is\none of the most effective zero-shot document ranking methods, it has a\nquadratic computational complexity with respect to the number of documents to\nbe ranked, as it requires an enumeration over all possible document pairs.\nConsequently, the outstanding ranking performance of PRP has remained\nunreachable for most real-world ranking applications.\n  In this work, we propose to harness the effectiveness of PRP through pairwise\ndistillation. Specifically, we distill a pointwise student ranker from pairwise\nteacher labels generated by PRP, resulting in an efficient student model that\nretains the performance of PRP with substantially lower computational costs.\nFurthermore, we find that the distillation process can be made\nsample-efficient: with only 2% of pairs, we are able to obtain the same\nperformance as using all pairs for teacher labels. Thus, our novel approach\nprovides a solution to harness the ranking performance of PRP without incurring\nhigh computational costs during both distillation and serving.",
    "published": "2025-07-07T09:38:43Z",
    "updated": "2025-07-07T09:38:43Z",
    "id": "2507.04820v1",
    "authors": [
      "Junru Wu",
      "Le Yan",
      "Zhen Qin",
      "Honglei Zhuang",
      "Paul Suganthan G. C.",
      "Tianqi Liu",
      "Zhe Dong",
      "Xuanhui Wang",
      "Harrie Oosterhuis"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04820v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04820v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04820v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in Pairwise Ranking Prompting (PRP) and proposes a method to distill a pointwise student ranker from pairwise teacher labels generated by PRP. This involves LLM-based ranking and optimization, fitting within the LLM and Reasoning categories.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.04803v1": {
    "title": "Application and Evaluation of Large Language Models for Forecasting the\n  Impact of Traffic Incidents",
    "summary": "This study examines the feasibility of applying large language models (LLMs)\nfor forecasting the impact of traffic incidents on the traffic flow. The use of\nLLMs for this task has several advantages over existing machine learning-based\nsolutions such as not requiring a large training dataset and the ability to\nutilize free-text incident logs. We propose a fully LLM-based solution that\npredicts the incident impact using a combination of traffic features and\nLLM-extracted incident features. A key ingredient of this solution is an\neffective method of selecting examples for the LLM's in-context learning. We\nevaluate the performance of three advanced LLMs and two state-of-the-art\nmachine learning models on a real traffic incident dataset. The results show\nthat the best-performing LLM matches the accuracy of the most accurate machine\nlearning model, despite the former not having been trained on this prediction\ntask. The findings indicate that LLMs are a practically viable option for\ntraffic incident impact prediction.",
    "published": "2025-07-07T09:22:06Z",
    "updated": "2025-07-07T09:22:06Z",
    "id": "2507.04803v1",
    "authors": [
      "George Jagadeesh",
      "Srikrishna Iyer",
      "Michal Polanowski",
      "Kai Xin Thia"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04803v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04803v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04803v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models (LLMs) for forecasting traffic incident impacts, which aligns with the 'LLM' category. It also evaluates the performance of LLMs, which is relevant to the 'Benchmark' category.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.05307v1": {
    "title": "ASSURE: Metamorphic Testing for AI-powered Browser Extensions",
    "summary": "The integration of Large Language Models (LLMs) into browser extensions has\nrevolutionized web browsing, enabling sophisticated functionalities like\ncontent summarization, intelligent translation, and context-aware writing\nassistance. However, these AI-powered extensions introduce unprecedented\nchallenges in testing and reliability assurance. Traditional browser extension\ntesting approaches fail to address the non-deterministic behavior,\ncontext-sensitivity, and complex web environment integration inherent to\nLLM-powered extensions. Similarly, existing LLM testing methodologies operate\nin isolation from browser-specific contexts, creating a critical gap in\neffective evaluation frameworks. To bridge this gap, we present ASSURE, a\nmodular automated testing framework specifically designed for AI-powered\nbrowser extensions. ASSURE comprises three principal components: (1) a modular\ntest case generation engine that supports plugin-based extension of testing\nscenarios, (2) an automated execution framework that orchestrates the complex\ninteractions between web content, extension processing, and AI model behavior,\nand (3) a configurable validation pipeline that systematically evaluates\nbehavioral consistency and security invariants rather than relying on exact\noutput matching. Our evaluation across six widely-used AI browser extensions\ndemonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning\nsecurity vulnerabilities, metamorphic relation violations, and content\nalignment problems. ASSURE achieves 6.4x improved testing throughput compared\nto manual approaches, detecting critical security vulnerabilities within 12.4\nminutes on average. This efficiency makes ASSURE practical for integration into\ndevelopment pipelines, offering a comprehensive solution to the unique\nchallenges of testing AI-powered browser extensions.",
    "published": "2025-07-07T09:11:16Z",
    "updated": "2025-07-07T09:11:16Z",
    "id": "2507.05307v1",
    "authors": [
      "Xuanqi Gao",
      "Juan Zhai",
      "Shiqing Ma",
      "Siyi Xie",
      "Chao Shen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05307v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05307v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05307v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) into browser extensions and presents a testing framework for these AI-powered extensions. While it mentions LLMs, the primary focus is on testing methodologies and frameworks rather than the core research topics of LLMs, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.04782v1": {
    "title": "Reason to Rote: Rethinking Memorization in Reasoning",
    "summary": "Large language models readily memorize arbitrary training instances, such as\nlabel noise, yet they perform strikingly well on reasoning tasks. In this work,\nwe investigate how language models memorize label noise, and why such\nmemorization in many cases does not heavily affect generalizable reasoning\ncapabilities. Using two controllable synthetic reasoning datasets with noisy\nlabels, four-digit addition (FDA) and two-hop relational reasoning (THR), we\ndiscover a reliance of memorization on generalizable reasoning mechanisms:\nmodels continue to compute intermediate reasoning outputs even when retrieving\nmemorized noisy labels, and intervening reasoning adversely affects\nmemorization. We further show that memorization operates through distributed\nencoding, i.e., aggregating various inputs and intermediate results, rather\nthan building a look-up mechanism from inputs to noisy labels. Moreover, our\nFDA case study reveals memorization occurs via outlier heuristics, where\nexisting neuron activation patterns are slightly shifted to fit noisy labels.\nTogether, our findings suggest that memorization of label noise in language\nmodels builds on, rather than overrides, the underlying reasoning mechanisms,\nshedding lights on the intriguing phenomenon of benign memorization.",
    "published": "2025-07-07T08:59:06Z",
    "updated": "2025-07-07T08:59:06Z",
    "id": "2507.04782v1",
    "authors": [
      "Yupei Du",
      "Philipp Mondorf",
      "Silvia Casola",
      "Yuekun Yao",
      "Robert Litschko",
      "Barbara Plank"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04782v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04782v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04782v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper investigates the relationship between memorization and reasoning in large language models, focusing on how models handle label noise while maintaining reasoning capabilities. It discusses mechanisms of memorization and reasoning, which are core topics in the given list.",
    "llm_cls_result": [
      "Reasoning",
      "Memory",
      "LLM"
    ]
  },
  "2507.04769v1": {
    "title": "From Imitation to Innovation: The Emergence of AI Unique Artistic Styles\n  and the Challenge of Copyright Protection",
    "summary": "Current legal frameworks consider AI-generated works eligible for copyright\nprotection when they meet originality requirements and involve substantial\nhuman intellectual input. However, systematic legal standards and reliable\nevaluation methods for AI art copyrights are lacking. Through comprehensive\nanalysis of legal precedents, we establish three essential criteria for\ndetermining distinctive artistic style: stylistic consistency, creative\nuniqueness, and expressive accuracy. To address these challenges, we introduce\nArtBulb, an interpretable and quantifiable framework for AI art copyright\njudgment that combines a novel style description-based multimodal clustering\nmethod with multimodal large language models (MLLMs). We also present AICD, the\nfirst benchmark dataset for AI art copyright annotated by artists and legal\nexperts. Experimental results demonstrate that ArtBulb outperforms existing\nmodels in both quantitative and qualitative evaluations. Our work aims to\nbridge the gap between the legal and technological communities and bring\ngreater attention to the societal issue of AI art copyrights.",
    "published": "2025-07-07T08:45:08Z",
    "updated": "2025-07-07T08:45:08Z",
    "id": "2507.04769v1",
    "authors": [
      "Zexi Jia",
      "Chuanwei Huang",
      "Yeshuang Zhu",
      "Hongyan Fei",
      "Ying Deng",
      "Zhiqiang Yuan",
      "Jiapei Zhang",
      "Jinchao Zhang",
      "Jie Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04769v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04769v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04769v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of multimodal large language models (MLLMs) in the context of AI art copyright judgment and introduces a benchmark dataset for AI art copyright, which aligns with the topics of MLLM and Dataset.",
    "llm_cls_result": [
      "MLLM",
      "Dataset"
    ]
  },
  "2507.04766v1": {
    "title": "ABench-Physics: Benchmarking Physical Reasoning in LLMs via\n  High-Difficulty and Dynamic Physics Problems",
    "summary": "Large Language Models (LLMs) have shown impressive performance in domains\nsuch as mathematics and programming, yet their capabilities in physics remain\nunderexplored and poorly understood. Physics poses unique challenges that\ndemand not only precise computation but also deep conceptual understanding and\nphysical modeling skills. Existing benchmarks often fall short due to limited\ndifficulty, multiple-choice formats, and static evaluation settings that fail\nto capture physical modeling ability. In this paper, we introduce\nABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'\nphysical reasoning and generalization capabilities. ABench-Physics consists of\ntwo components: Phy_A, a static set of 400 graduate- or Olympiad-level\nproblems; and Phy_B, a dynamic subset of 100 problems equipped with an\nautomatic variation engine to test model robustness across changing conditions.\nAll questions require precise numerical answers, with strict formatting and\ntolerance constraints. Our evaluation of several state-of-the-art LLMs reveals\nsubstantial performance gaps, highlighting persistent limitations in physical\nreasoning, especially in generalization to dynamic variants. ABench-Physics\nprovides a challenging and diagnostic framework for advancing scientific\nreasoning in LLMs.",
    "published": "2025-07-07T08:43:56Z",
    "updated": "2025-07-07T08:43:56Z",
    "id": "2507.04766v1",
    "authors": [
      "Yiming Zhang",
      "Yingfan Ma",
      "Yanmei Gu",
      "Zhengkai Yang",
      "Yihong Zhuang",
      "Feng Wang",
      "Zenan Huang",
      "Yuanyuan Wang",
      "Chao Huang",
      "Bowen Song",
      "Cheng Lin",
      "Junbo Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04766v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04766v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04766v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking the physical reasoning capabilities of LLMs, which aligns with the 'Benchmark' topic as it involves evaluating LLMs' performance in a specific domain. Additionally, the emphasis on reasoning abilities in physics problems relates to the 'Reasoning' topic, as it involves complex problem-solving and logical reasoning within LLMs.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning"
    ]
  },
  "2507.04752v1": {
    "title": "Large Language Models for Network Intrusion Detection Systems:\n  Foundations, Implementations, and Future Directions",
    "summary": "Large Language Models (LLMs) have revolutionized various fields with their\nexceptional capabilities in understanding, processing, and generating\nhuman-like text. This paper investigates the potential of LLMs in advancing\nNetwork Intrusion Detection Systems (NIDS), analyzing current challenges,\nmethodologies, and future opportunities. It begins by establishing a\nfoundational understanding of NIDS and LLMs, exploring the enabling\ntechnologies that bridge the gap between intelligent and cognitive systems in\nAI-driven NIDS. While Intelligent NIDS leverage machine learning and deep\nlearning to detect threats based on learned patterns, they often lack\ncontextual awareness and explainability. In contrast, Cognitive NIDS integrate\nLLMs to process both structured and unstructured security data, enabling deeper\ncontextual reasoning, explainable decision-making, and automated response for\nintrusion behaviors. Practical implementations are then detailed, highlighting\nLLMs as processors, detectors, and explainers within a comprehensive AI-driven\nNIDS pipeline. Furthermore, the concept of an LLM-centered Controller is\nproposed, emphasizing its potential to coordinate intrusion detection\nworkflows, optimizing tool collaboration and system performance. Finally, this\npaper identifies critical challenges and opportunities, aiming to foster\ninnovation in developing reliable, adaptive, and explainable NIDS. By\npresenting the transformative potential of LLMs, this paper seeks to inspire\nadvancement in next-generation network security systems.",
    "published": "2025-07-07T08:28:07Z",
    "updated": "2025-07-07T08:28:07Z",
    "id": "2507.04752v1",
    "authors": [
      "Shuo Yang",
      "Xinran Zheng",
      "Xinchen Zhang",
      "Jinfeng Xu",
      "Jinze Li",
      "Donglin Xie",
      "Weicai Long",
      "Edith C. H. Ngai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04752v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04752v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04752v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in Network Intrusion Detection Systems (NIDS), highlighting their capabilities in contextual reasoning and explainable decision-making. The core topic is clearly centered around LLMs and their practical implementations in a specific domain.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04751v1": {
    "title": "LLMs as Architects and Critics for Multi-Source Opinion Summarization",
    "summary": "Multi-source Opinion Summarization (M-OS) extends beyond traditional opinion\nsummarization by incorporating additional sources of product metadata such as\ndescriptions, key features, specifications, and ratings, alongside reviews.\nThis integration results in comprehensive summaries that capture both\nsubjective opinions and objective product attributes essential for informed\ndecision-making. While Large Language Models (LLMs) have shown significant\nsuccess in various Natural Language Processing (NLP) tasks, their potential in\nM-OS remains largely unexplored. Additionally, the lack of evaluation datasets\nfor this task has impeded further advancements. To bridge this gap, we\nintroduce M-OS-EVAL, a benchmark dataset for evaluating multi-source opinion\nsummaries across 7 key dimensions: fluency, coherence, relevance, faithfulness,\naspect coverage, sentiment consistency, specificity. Our results demonstrate\nthat M-OS significantly enhances user engagement, as evidenced by a user study\nin which, on average, 87% of participants preferred M-OS over opinion\nsummaries. Our experiments demonstrate that factually enriched summaries\nenhance user engagement. Notably, M-OS-PROMPTS exhibit stronger alignment with\nhuman judgment, achieving an average Spearman correlation of \\r{ho} = 0.74,\nwhich surpasses the performance of previous methodologies.",
    "published": "2025-07-07T08:27:44Z",
    "updated": "2025-07-07T08:27:44Z",
    "id": "2507.04751v1",
    "authors": [
      "Anuj Attri",
      "Arnav Attri",
      "Pushpak Bhattacharyya",
      "Suman Banerjee",
      "Amey Patil",
      "Muthusamy Chelliah",
      "Nikesh Garera"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04751v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04751v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04751v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in Multi-Source Opinion Summarization (M-OS) and introduces a benchmark dataset for evaluation. The focus on LLMs and the creation of a benchmark dataset aligns with the topics of LLM and Benchmark.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.04748v1": {
    "title": "LLM-based Question-Answer Framework for Sensor-driven HVAC System\n  Interaction",
    "summary": "Question-answering (QA) interfaces powered by large language models (LLMs)\npresent a promising direction for improving interactivity with HVAC system\ninsights, particularly for non-expert users. However, enabling accurate,\nreal-time, and context-aware interactions with HVAC systems introduces unique\nchallenges, including the integration of frequently updated sensor data,\ndomain-specific knowledge grounding, and coherent multi-stage reasoning. In\nthis paper, we present JARVIS, a two-stage LLM-based QA framework tailored for\nsensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to\ntranslate high-level user queries into structured execution instructions, and\nan Agent that performs SQL-based data retrieval, statistical processing, and\nfinal response generation. To address HVAC-specific challenges, JARVIS\nintegrates (1) an adaptive context injection strategy for efficient HVAC and\ndeployment-specific information integration, (2) a parameterized SQL builder\nand executor to improve data access reliability, and (3) a bottom-up planning\nscheme to ensure consistency across multi-stage response generation. We\nevaluate JARVIS using real-world data collected from a commercial HVAC system\nand a ground truth QA dataset curated by HVAC experts to demonstrate its\neffectiveness in delivering accurate and interpretable responses across diverse\nqueries. Results show that JARVIS consistently outperforms baseline and\nablation variants in both automated and user-centered assessments, achieving\nhigh response quality and accuracy.",
    "published": "2025-07-07T08:19:17Z",
    "updated": "2025-07-07T08:19:17Z",
    "id": "2507.04748v1",
    "authors": [
      "Sungmin Lee",
      "Minju Kang",
      "Joonhee Lee",
      "Seungyong Lee",
      "Dongju Kim",
      "Jingi Hong",
      "Jun Shin",
      "Pei Zhang",
      "JeongGil Ko"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04748v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04748v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04748v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for question-answering interfaces in HVAC systems, focusing on integration of sensor data and domain-specific knowledge. The core topics are LLM for QA and reasoning in a specific domain.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.04736v1": {
    "title": "ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical\n  Reward-Driven Reinforcement Learning",
    "summary": "Large Language Models (LLMs) show significant potential for automating\nRegister-Transfer Level (RTL) code generation. However, current approaches face\na critical challenge: they can not simultaneously optimize for functional\ncorrectness and hardware quality (Power, Performance, Area - PPA). Methods\nbased on supervised fine-tuning often generate functionally correct but\nPPA-suboptimal code, lacking mechanisms to learn optimization principles. In\ncontrast, post-processing techniques that attempt to improve PPA metrics after\ngeneration are often inefficient because they operate externally without\nupdating the LLM's parameters, thus failing to enhance the model's intrinsic\ndesign capabilities.\n  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven\nreinforcement learning framework to train LLMs to generate RTL code that\nachieves both functional correctness and optimized PPA metrics. ChipSeek-R1\nemploys a hierarchical reward system, which incorporates direct feedback on\nsyntax, functional correctness (from simulators) and PPA metrics (from\nsynthesis tools) during reinforcement learning. This enables the model to learn\ncomplex hardware design trade-offs via trial-and-error, generating RTL code\nthat is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on\nstandard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results\nin functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1\ngenerated 27 RTL designs surpassing the PPA metrics of the original\nhuman-written code. Our findings demonstrate the effectiveness of integrating\ntoolchain feedback into LLM training and highlight the potential for\nreinforcement learning to enable automated generation of human-surpassing RTL\ncode. We open-source our code in anonymous github.",
    "published": "2025-07-07T08:08:20Z",
    "updated": "2025-07-07T08:08:20Z",
    "id": "2507.04736v1",
    "authors": [
      "Zhirong Chen",
      "Kaiyan Chang",
      "Zhuolin Li",
      "Xinyang He",
      "Chujie Chen",
      "Cangyuan Li",
      "Mengdi Wang",
      "Haobo Xu",
      "Yinhe Han",
      "Ying Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04736v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04736v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04736v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in generating RTL code and employs a hierarchical reward-driven reinforcement learning framework to optimize both functional correctness and hardware quality. This involves both LLM and Reinforcement Learning (RL) techniques.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.05305v1": {
    "title": "Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a\n  Viable Alternative to Proprietary Models for Pedagogical Tools",
    "summary": "Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher\ncryptic compiler errors for novice programmers, but their computational scale,\ncost, and tendency to over-assist make them problematic for widespread\npedagogical adoption. This work demonstrates that smaller, specialised language\nmodels, enhanced via Supervised Fine-Tuning (SFT), present a more viable\nalternative for educational tools. We utilise a new dataset of 40,000 C\ncompiler error explanations, derived from real introductory programming (CS1/2)\nstudent-generated programming errors, which we used to fine-tune three\nopen-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual\nevaluation, combining expert human reviews with a large-scale automated\nanalysis of 8,000 responses using a validated LLM-as-judge ensemble. Our\nresults show that SFT significantly boosts the pedagogical quality of smaller\nmodels, achieving performance comparable to much larger models. We analyse the\ntrade-offs between model size and quality, confirming that fine-tuning compact,\nefficient models on high-quality, domain-specific data is a potent strategy for\ncreating specialised models to drive educational tools. We provide a replicable\nmethodology to foster broader access to generative AI capabilities in\neducational contexts.",
    "published": "2025-07-07T08:03:49Z",
    "updated": "2025-07-07T08:03:49Z",
    "id": "2507.05305v1",
    "authors": [
      "Lorenzo Lee Solano",
      "Charles Koutcheme",
      "Juho Leinonen",
      "Alexandra Vassar",
      "Jake Renzella"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05305v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05305v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05305v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of supervised fine-tuning (SFT) on open-source LLMs to improve their performance for educational purposes, focusing on pedagogical tools and domain-specific data. This aligns with topics related to LLM fine-tuning and specialized applications.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.04735v1": {
    "title": "An analysis of vision-language models for fabric retrieval",
    "summary": "Effective cross-modal retrieval is essential for applications like\ninformation retrieval and recommendation systems, particularly in specialized\ndomains such as manufacturing, where product information often consists of\nvisual samples paired with a textual description. This paper investigates the\nuse of Vision Language Models(VLMs) for zero-shot text-to-image retrieval on\nfabric samples. We address the lack of publicly available datasets by\nintroducing an automated annotation pipeline that uses Multimodal Large\nLanguage Models (MLLMs) to generate two types of textual descriptions: freeform\nnatural language and structured attribute-based descriptions. We produce these\ndescriptions to evaluate retrieval performance across three Vision-Language\nModels: CLIP, LAION-CLIP, and Meta's Perception Encoder. Our experiments\ndemonstrate that structured, attribute-rich descriptions significantly enhance\nretrieval accuracy, particularly for visually complex fabric classes, with the\nPerception Encoder outperforming other models due to its robust feature\nalignment capabilities. However, zero-shot retrieval remains challenging in\nthis fine-grained domain, underscoring the need for domain-adapted approaches.\nOur findings highlight the importance of combining technical textual\ndescriptions with advanced VLMs to optimize cross-modal retrieval in industrial\napplications.",
    "published": "2025-07-07T08:00:18Z",
    "updated": "2025-07-07T08:00:18Z",
    "id": "2507.04735v1",
    "authors": [
      "Francesco Giuliari",
      "Asif Khan Pattan",
      "Mohamed Lamine Mekhalfi",
      "Fabio Poiesi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04735v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04735v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04735v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the use of Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs) for cross-modal retrieval in a specialized domain, which aligns with the topics of MLLM and VLA. The study also involves the creation of a dataset for evaluation, which is relevant to the Dataset topic.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Dataset"
    ]
  },
  "2507.04733v1": {
    "title": "\"This Suits You the Best\": Query Focused Comparative Explainable\n  Summarization",
    "summary": "Product recommendations inherently involve comparisons, yet traditional\nopinion summarization often fails to provide holistic comparative insights. We\npropose the novel task of generating Query-Focused Comparative Explainable\nSummaries (QF-CES) using Multi-Source Opinion Summarization (M-OS). To address\nthe lack of query-focused recommendation datasets, we introduce MS-Q2P,\ncomprising 7,500 queries mapped to 22,500 recommended products with metadata.\nWe leverage Large Language Models (LLMs) to generate tabular comparative\nsummaries with query-specific explanations. Our approach is personalized,\nprivacy-preserving, recommendation engine-agnostic, and category-agnostic. M-OS\nas an intermediate step reduces inference latency approximately by 40% compared\nto the direct input approach (DIA), which processes raw data directly. We\nevaluate open-source and proprietary LLMs for generating and assessing QF-CES.\nExtensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity,\nfaithfulness, informativeness, format adherence, and query relevance) showed an\naverage Spearman correlation of 0.74 with human judgments, indicating its\npotential for QF-CES evaluation.",
    "published": "2025-07-07T07:58:15Z",
    "updated": "2025-07-07T07:58:15Z",
    "id": "2507.04733v1",
    "authors": [
      "Arnav Attri",
      "Anuj Attri",
      "Pushpak Bhattacharyya",
      "Suman Banerjee",
      "Amey Patil",
      "Muthusamy Chelliah",
      "Nikesh Garera"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04733v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04733v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04733v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for generating query-focused comparative explainable summaries, which involves leveraging LLMs for summarization tasks. The core topics are related to LLM applications in summarization and evaluation.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.04724v1": {
    "title": "Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents\n  in LLM-Based Multi-Agent Systems",
    "summary": "Multi-agent systems powered by Large Language Models (LLM-MAS) demonstrate\nremarkable capabilities in collaborative problem-solving. While LLM-MAS exhibit\nstrong collaborative abilities, the security risks in their communication and\ncoordination remain underexplored. We bridge this gap by systematically\ninvestigating intention-hiding threats in LLM-MAS, and design four\nrepresentative attack paradigms that subtly disrupt task completion while\nmaintaining high concealment. These attacks are evaluated in centralized,\ndecentralized, and layered communication structures. Experiments conducted on\nsix benchmark datasets, including MMLU, MMLU-Pro, HumanEval, GSM8K, arithmetic,\nand biographies, demonstrate that they exhibit strong disruptive capabilities.\nTo identify these threats, we propose a psychology-based detection framework\nAgentXposed, which combines the HEXACO personality model with the Reid\nTechnique, using progressive questionnaire inquiries and behavior-based\nmonitoring. Experiments conducted on six types of attacks show that our\ndetection framework effectively identifies all types of malicious behaviors.\nThe detection rate for our intention-hiding attacks is slightly lower than that\nof the two baselines, Incorrect Fact Injection and Dark Traits Injection,\ndemonstrating the effectiveness of intention concealment. Our findings reveal\nthe structural and behavioral risks posed by intention-hiding attacks and offer\nvaluable insights into securing LLM-based multi-agent systems through\npsychological perspectives, which contributes to a deeper understanding of\nmulti-agent safety. The code and data are available at\nhttps://anonymous.4open.science/r/AgentXposed-F814.",
    "published": "2025-07-07T07:34:34Z",
    "updated": "2025-07-07T07:34:34Z",
    "id": "2507.04724v1",
    "authors": [
      "Yizhe Xie",
      "Congcong Zhu",
      "Xinyue Zhang",
      "Minghao Wang",
      "Chi Liu",
      "Minglu Zhu",
      "Tianqing Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04724v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04724v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04724v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses security risks in LLM-based multi-agent systems, focusing on intention-hiding threats and detection methods. It involves LLMs and their application in multi-agent systems, which aligns with the topics of LLM and RL (Reinforcement Learning) due to the collaborative and potentially adversarial nature of the agents.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.04723v1": {
    "title": "LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation\n  framework",
    "summary": "Long-context processing has become a fundamental capability for large\nlanguage models~(LLMs). To assess model's long-context performance, numerous\nlong-context evaluation benchmarks have been proposed. However, variations in\nevaluation settings across these benchmarks lead to inconsistent results,\nmaking it difficult to draw reliable comparisons. Besides, the high\ncomputational cost of long-context evaluation poses a significant barrier for\nthe community to conduct comprehensive assessments of long-context models. In\nthis paper, we propose LOOM-Scope, a comprehensive and efficient framework for\nlong-context evaluation. LOOM-Scope standardizes evaluation settings across\ndiverse benchmarks, supports deployment of efficient long-context inference\nacceleration methods, and introduces a holistic yet lightweight benchmark suite\nto evaluate models comprehensively. Homepage: https://loomscope.github.io",
    "published": "2025-07-07T07:33:24Z",
    "updated": "2025-07-07T07:33:24Z",
    "id": "2507.04723v1",
    "authors": [
      "Zecheng Tang",
      "Haitian Wang",
      "Quantong Qiu",
      "Baibei Ji",
      "Ruoxi Sun",
      "Keyan Zhou",
      "Juntao Li",
      "Min Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04723v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04723v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04723v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating long-context models, which involves benchmarking and standardizing evaluation settings, making it relevant to the 'Benchmark' category. Additionally, it discusses long-context processing, which is related to 'Memory' as it involves long-term memory and context processing in LLMs.",
    "llm_cls_result": [
      "Benchmark",
      "Memory"
    ]
  },
  "2507.06256v1": {
    "title": "Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World",
    "summary": "This paper investigates the real-world vulnerabilities of audio-based large\nlanguage models (ALLMs), such as Qwen2-Audio. We first demonstrate that an\nadversary can craft stealthy audio perturbations to manipulate ALLMs into\nexhibiting specific targeted behaviors, such as eliciting responses to\nwake-keywords (e.g., \"Hey Qwen\"), or triggering harmful behaviors (e.g. \"Change\nmy calendar event\"). Subsequently, we show that playing adversarial background\nnoise during user interaction with the ALLMs can significantly degrade the\nresponse quality. Crucially, our research illustrates the scalability of these\nattacks to real-world scenarios, impacting other innocent users when these\nadversarial noises are played through the air. Further, we discuss the\ntransferrability of the attack, and potential defensive measures.",
    "published": "2025-07-07T07:29:52Z",
    "updated": "2025-07-07T07:29:52Z",
    "id": "2507.06256v1",
    "authors": [
      "Vinu Sankar Sadasivan",
      "Soheil Feizi",
      "Rajiv Mathews",
      "Lun Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06256v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06256v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06256v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses vulnerabilities in audio-based large language models (ALLMs) and adversarial attacks, which are relevant to the study of Large Language Models (LLM) and their security aspects.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.04702v1": {
    "title": "Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient\n  Temporal Sensing Reinforcement Learning",
    "summary": "Temporal Video Grounding (TVG), which requires pinpointing relevant temporal\nsegments from video based on language query, has always been a highly\nchallenging task in the field of video understanding. Videos often have a\nlarger volume of information and redundancy than texts or images. Models should\npresent comprehensive understanding of the whole video to accurately retrieve\nquery-relevant clips. We thus propose Tempo-R0: a Video Multimodal Large\nLanguage Model (Video-MLLM) for the temporal video grounding task via\nmultimodal temporal sensing reinforcement. Specifically, during the\npreprocessing stage of our pipeline, we employ Self-adaptive Attention\nAllocation (SAA) method based on frame content variation to efficiently use the\nMLLM's limited attention. The Explicit Timestamp-modal Aligned (ETA) method is\nalso utilized to strengthen our model's capability to perceive the boundaries\nof events in the video. In the fine-tuning part of our pipeline, we creatively\napply Partial Irrelevance Refusing-based Group Relative Policy Optimization\n(PIR-GRPO) in TVG area to foster model's temporal reasoning from not only\naccepting relevant video-query pairs but also refusing irrelevant ones.\nExperiments demonstrate that our method accomplishes a notable advantage over\nSOTA solutions by around 3.5% on both the original QVHighlights testbench and\nits corrected version with more reasonable ground truth annotations.",
    "published": "2025-07-07T06:51:40Z",
    "updated": "2025-07-07T06:51:40Z",
    "id": "2507.04702v1",
    "authors": [
      "Feng Yue",
      "Zhaoxing Zhang",
      "Junming Jiao",
      "Zhengyu Liang",
      "Shiwen Cao",
      "Feifei Zhang",
      "Rong Shen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04702v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04702v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04702v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a Video Multimodal Large Language Model (Video-MLLM) for temporal video grounding, which involves integrating vision and language modalities. It also mentions the use of reinforcement learning for temporal sensing, which is relevant to RL. The paper does not directly address other topics like LLM, MoE, AGI, etc.",
    "llm_cls_result": [
      "MLLM",
      "RL"
    ]
  },
  "2507.04699v1": {
    "title": "A Visual Leap in CLIP Compositionality Reasoning through Generation of\n  Counterfactual Sets",
    "summary": "Vision-language models (VLMs) often struggle with compositional reasoning due\nto insufficient high-quality image-text data. To tackle this challenge, we\npropose a novel block-based diffusion approach that automatically generates\ncounterfactual datasets without manual annotation. Our method utilizes large\nlanguage models to identify entities and their spatial relationships. It then\nindependently generates image blocks as \"puzzle pieces\" coherently arranged\naccording to specified compositional rules. This process creates diverse,\nhigh-fidelity counterfactual image-text pairs with precisely controlled\nvariations. In addition, we introduce a specialized loss function that\ndifferentiates inter-set from intra-set samples, enhancing training efficiency\nand reducing the need for negative samples. Experiments demonstrate that\nfine-tuning VLMs with our counterfactual datasets significantly improves visual\nreasoning performance. Our approach achieves state-of-the-art results across\nmultiple benchmarks while using substantially less training data than existing\nmethods.",
    "published": "2025-07-07T06:47:10Z",
    "updated": "2025-07-07T06:47:10Z",
    "id": "2507.04699v1",
    "authors": [
      "Zexi Jia",
      "Chuanwei Huang",
      "Hongyan Fei",
      "Yeshuang Zhu",
      "Zhiqiang Yuan",
      "Ying Deng",
      "Jiapei Zhang",
      "Jinchao Zhang",
      "Jie Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04699v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04699v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04699v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving compositional reasoning in vision-language models (VLMs) through the generation of counterfactual datasets using a novel block-based diffusion approach. It involves the use of large language models and addresses visual reasoning performance, which aligns with the topics of Vision-Language Alignment (VLA) and Multimodal Large Language Models (MLLM).",
    "llm_cls_result": [
      "VLA",
      "MLLM"
    ]
  },
  "2507.04697v1": {
    "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
    "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
    "published": "2025-07-07T06:33:59Z",
    "updated": "2025-07-07T06:33:59Z",
    "id": "2507.04697v1",
    "authors": [
      "Daichi Mukunoki",
      "Shun-ichiro Hayashi",
      "Tetsuya Hoshino",
      "Takahiro Katagiri"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04697v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04697v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04697v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the capability of general-purpose Large Language Models (LLMs) for code generation, specifically for Basic Linear Algebra Subprograms (BLAS). This directly relates to research on Large Language Models (LLM) and their applications in generating and optimizing code.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04687v2": {
    "title": "LAKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset\n  Discovery in Data Lakes",
    "summary": "How to generate a large, realistic set of tables along with joinability\nrelationships, to stress-test dataset discovery methods? Dataset discovery\nmethods aim to automatically identify related data assets in a data lake. The\ndevelopment and evaluation of such solutions for customers from a wide range of\nbusiness domains, relies on diverse, high quality and domain-specific tabular\nbenchmarks. Large language models (LLMs) are trained on a wide variety of text\ndata, which can provide a strong foundation of general and domain-specific\nknowledge. In this paper, we ask the question -- \\textit{can we leverage LLMs\nto generate a tabular benchmark adequate for evaluating the dataset discovery\nsolutions?} In particular, we focus on the task of finding joinable tables\nwhich is the cornerstone of virtually every dataset discovery method. Current\ncorpora for evaluating dataset discovery methods are mainly based on subsets of\nopen data, and they suffer from three important issues: $i)$ they focus on very\ncommon and generic data types (e.g., address, id, name, etc.); $ii)$ they do\nnot contain human-annotated column pairs; instead, practitioners synthesize\nground truth using table splits (e.g., horizontal for table union search and\nvertical ones for joinability) and $iii)$ they do not focus on semantic column\nrelationships.",
    "published": "2025-07-07T06:08:45Z",
    "updated": "2025-07-08T16:51:53Z",
    "id": "2507.04687v2",
    "authors": [
      "Zhenwei Dai",
      "Chuan Lei",
      "Asterios Katsifodimos",
      "Xiao Qin",
      "Christos Faloutsos",
      "Huzefa Rangwala"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04687v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04687v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04687v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of LLMs to generate tabular benchmarks for evaluating dataset discovery methods, which involves leveraging LLMs' general and domain-specific knowledge. The focus is on creating datasets for testing joinability relationships, which is a key aspect of dataset discovery. This aligns with the topics of LLM (Large Language Models) and Dataset (LLM datasets, evaluation datasets, and benchmark datasets).",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.04635v1": {
    "title": "MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and\n  Emotion Understanding",
    "summary": "Multimodal large language models (MLLMs) recently showed strong capacity in\nintegrating data among multiple modalities, empowered by a generalizable\nattention architecture. Advanced methods predominantly focus on\nlanguage-centric tuning while less exploring multimodal tokens mixed through\nattention, posing challenges in high-level tasks that require fine-grained\ncognition and emotion understanding. In this work, we identify the attention\ndeficit disorder problem in multimodal learning, caused by inconsistent\ncross-modal attention and layer-by-layer decayed attention activation. To\naddress this, we propose a novel attention mechanism, termed MOdular Duplex\nAttention (MODA), simultaneously conducting the inner-modal refinement and\ninter-modal interaction. MODA employs a correct-after-align strategy to\neffectively decouple modality alignment from cross-layer token mixing. In the\nalignment phase, tokens are mapped to duplex modality spaces based on the basis\nvectors, enabling the interaction between visual and language modality.\nFurther, the correctness of attention scores is ensured through adaptive masked\nattention, which enhances the model's flexibility by allowing customizable\nmasking patterns for different modalities. Extensive experiments on 21\nbenchmark datasets verify the effectiveness of MODA in perception, cognition,\nand emotion tasks. Source code and demo are available in\nhttps://zzcheng.top/MODA.",
    "published": "2025-07-07T03:37:42Z",
    "updated": "2025-07-07T03:37:42Z",
    "id": "2507.04635v1",
    "authors": [
      "Zhicheng Zhang",
      "Wuyou Xia",
      "Chenxi Zhao",
      "Zhou Yan",
      "Xiaoqiang Liu",
      "Yongjie Zhu",
      "Wenyu Qin",
      "Pengfei Wan",
      "Di Zhang",
      "Jufeng Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04635v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04635v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04635v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a novel attention mechanism for Multimodal Large Language Models (MLLMs), focusing on improving multimodal perception, cognition, and emotion understanding. The title and abstract highlight the use of multimodal tokens and attention mechanisms, which are central to MLLM research.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.04632v2": {
    "title": "Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning\n  of Reasoning Models?",
    "summary": "Recent advances have witnessed the effectiveness of reinforcement learning\n(RL) finetuning in enhancing the reasoning capabilities of large language\nmodels (LLMs). The optimization process often requires numerous iterations to\nachieve satisfactory performance, resulting in high computational costs due to\nthe need for frequent prompt evaluations under intensive LLM interactions and\nrepeated policy updates. Appropriate online prompt selection methods reduce\niteration steps by prioritizing informative prompts during training, while the\npipeline's reliance on exhaustive prompt evaluation and subset selection for\noptimization still incurs substantial computational overhead due to frequent\nLLM inference calls. Distinguished from these direct evaluate-then-select\nschemes, this work investigates iterative approximate evaluation for arbitrary\nprompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian\nrisk-predictive framework that online estimates prompt difficulty without\nrequiring costly LLM interactions. Technically, MoPPS models each prompt's\nsuccess rate as a latent variable, performs streaming Bayesian inference, and\nemploys posterior sampling in a constructed multi-armed bandit machine,\nenabling sample efficient and adaptive prompt selection. Extensive experiments\nacross mathematics, planning, and vision-based geometry tasks show that MoPPS\nreliably predicts prompt difficulty and accelerates training with significantly\nreduced LLM rollouts.",
    "published": "2025-07-07T03:20:52Z",
    "updated": "2025-07-16T05:06:13Z",
    "id": "2507.04632v2",
    "authors": [
      "Yun Qu",
      "Qi Cheems Wang",
      "Yixiu Mao",
      "Vincent Tao Hu",
      "Xiangyang Ji"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04632v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04632v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04632v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of reinforcement learning (RL) to finetune large language models (LLMs) for reasoning tasks, which aligns with the topics of RL and Reasoning. Additionally, the focus on LLMs suggests relevance to the LLM topic.",
    "llm_cls_result": [
      "RL",
      "Reasoning",
      "LLM"
    ]
  },
  "2507.04626v1": {
    "title": "Heterogeneous User Modeling for LLM-based Recommendation",
    "summary": "Leveraging Large Language Models (LLMs) for recommendation has demonstrated\nnotable success in various domains, showcasing their potential for open-domain\nrecommendation. A key challenge to advancing open-domain recommendation lies in\neffectively modeling user preferences from users' heterogeneous behaviors\nacross multiple domains. Existing approaches, including ID-based and\nsemantic-based modeling, struggle with poor generalization, an inability to\ncompress noisy interactions effectively, and the domain seesaw phenomenon. To\naddress these challenges, we propose a Heterogeneous User Modeling (HUM)\nmethod, which incorporates a compression enhancer and a robustness enhancer for\nLLM-based recommendation. The compression enhancer uses a customized prompt to\ncompress heterogeneous behaviors into a tailored token, while a masking\nmechanism enhances cross-domain knowledge extraction and understanding. The\nrobustness enhancer introduces a domain importance score to mitigate the domain\nseesaw phenomenon by guiding domain optimization. Extensive experiments on\nheterogeneous datasets validate that HUM effectively models user heterogeneity\nby achieving both high efficacy and robustness, leading to superior performance\nin open-domain recommendation.",
    "published": "2025-07-07T03:08:28Z",
    "updated": "2025-07-07T03:08:28Z",
    "id": "2507.04626v1",
    "authors": [
      "Honghui Bao",
      "Wenjie Wang",
      "Xinyu Lin",
      "Fengbin Zhu",
      "Teng Sun",
      "Fuli Feng",
      "Tat-Seng Chua"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04626v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04626v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04626v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging Large Language Models (LLMs) for recommendation systems, specifically addressing challenges in modeling user preferences across multiple domains. It proposes a method (HUM) that enhances LLM-based recommendation through compression and robustness techniques.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.04625v1": {
    "title": "Knowledge-Aware Self-Correction in Language Models via Structured Memory\n  Graphs",
    "summary": "Large Language Models (LLMs) are powerful yet prone to generating factual\nerrors, commonly referred to as hallucinations. We present a lightweight,\ninterpretable framework for knowledge-aware self-correction of LLM outputs\nusing structured memory graphs based on RDF triples. Without retraining or\nfine-tuning, our method post-processes model outputs and corrects factual\ninconsistencies via external semantic memory. We demonstrate the approach using\nDistilGPT-2 and show promising results on simple factual prompts.",
    "published": "2025-07-07T02:55:12Z",
    "updated": "2025-07-07T02:55:12Z",
    "id": "2507.04625v1",
    "authors": [
      "Swayamjit Saha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04625v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04625v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04625v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for self-correction in LLMs using structured memory graphs, which aligns with topics related to memory augmentation and LLM reasoning.",
    "llm_cls_result": [
      "Memory",
      "Reasoning",
      "LLM"
    ]
  },
  "2507.04623v1": {
    "title": "Hierarchical Intent-guided Optimization with Pluggable LLM-Driven\n  Semantics for Session-based Recommendation",
    "summary": "Session-based Recommendation (SBR) aims to predict the next item a user will\nlikely engage with, using their interaction sequence within an anonymous\nsession. Existing SBR models often focus only on single-session information,\nignoring inter-session relationships and valuable cross-session insights. Some\nmethods try to include inter-session data but struggle with noise and\nirrelevant information, reducing performance. Additionally, most models rely on\nitem ID co-occurrence and overlook rich semantic details, limiting their\nability to capture fine-grained item features. To address these challenges, we\npropose a novel hierarchical intent-guided optimization approach with pluggable\nLLM-driven semantic learning for session-based recommendations, called HIPHOP.\nFirst, we introduce a pluggable embedding module based on large language models\n(LLMs) to generate high-quality semantic representations, enhancing item\nembeddings. Second, HIPHOP utilizes graph neural networks (GNNs) to model item\ntransition relationships and incorporates a dynamic multi-intent capturing\nmodule to address users' diverse interests within a session. Additionally, we\ndesign a hierarchical inter-session similarity learning module, guided by user\nintent, to capture global and local session relationships, effectively\nexploring users' long-term and short-term interests. To mitigate noise, an\nintent-guided denoising strategy is applied during inter-session learning.\nFinally, we enhance the model's discriminative capability by using contrastive\nlearning to optimize session representations. Experiments on multiple datasets\nshow that HIPHOP significantly outperforms existing methods, demonstrating its\neffectiveness in improving recommendation quality. Our code is available:\nhttps://github.com/hjx159/HIPHOP.",
    "published": "2025-07-07T02:50:04Z",
    "updated": "2025-07-07T02:50:04Z",
    "id": "2507.04623v1",
    "authors": [
      "Jinpeng Chen",
      "Jianxiang He",
      "Huan Li",
      "Senzhang Wang",
      "Yuan Cao",
      "Kaimin Wei",
      "Zhenye Yang",
      "Ye Ji"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04623v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04623v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04623v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using LLMs to enhance session-based recommendations by generating high-quality semantic representations and incorporating them into a hierarchical optimization framework. The core topics are related to LLMs and their application in recommendation systems, but none of the provided topics directly match this focus.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.16826v1": {
    "title": "A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing\n  Retrieval-Augmented Generation in Large Language Models",
    "summary": "Retrieval Augmented Generation (RAG) has gradually emerged as a promising\nparadigm for enhancing the accuracy and factual consistency of content\ngenerated by large language models (LLMs). However, existing RAG studies\nprimarily focus on retrieving isolated segments using similarity-based matching\nmethods, while overlooking the intrinsic connections between them. This\nlimitation hampers performance in RAG tasks. To address this, we propose QMKGF,\na Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing\nRetrieval Augmented Generation. First, we design prompt templates and employ\ngeneral-purpose LLMs to extract entities and relations, thereby generating a\nknowledge graph (KG) efficiently. Based on the constructed KG, we introduce a\nmulti-path subgraph construction strategy that incorporates one-hop relations,\nmulti-hop relations, and importance-based relations, aiming to improve the\nsemantic relevance between the retrieved documents and the user query.\nSubsequently, we designed a query-aware attention reward model that scores\nsubgraph triples based on their semantic relevance to the query. Then, we\nselect the highest score subgraph and enrich subgraph with additional triples\nfrom other subgraphs that are highly semantically relevant to the query.\nFinally, the entities, relations, and triples within the updated subgraph are\nutilised to expand the original query, thereby enhancing its semantic\nrepresentation and improving the quality of LLMs' generation. We evaluate QMKGF\non the SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets. On the HotpotQA\ndataset, our method achieves a ROUGE-1 score of 64.98\\%, surpassing the\nBGE-Rerank approach by 9.72 percentage points (from 55.26\\% to 64.98\\%).\nExperimental results demonstrate the effectiveness and superiority of the QMKGF\napproach.",
    "published": "2025-07-07T02:22:54Z",
    "updated": "2025-07-07T02:22:54Z",
    "id": "2507.16826v1",
    "authors": [
      "Qikai Wei",
      "Huansheng Ning",
      "Chunlong Han",
      "Jianguo Ding"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.16826v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.16826v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.16826v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs) by using a knowledge graph fusion approach. It involves memory-augmented models and retrieval-based methods, which are key topics in the 'Memory' category. Additionally, it discusses the use of LLMs for generating knowledge graphs, which falls under the 'LLM' category.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.04610v1": {
    "title": "any4: Learned 4-bit Numeric Representation for LLMs",
    "summary": "We present any4, a learned 4-bit weight quantization solution for large\nlanguage models (LLMs) providing arbitrary numeric representations without\nrequiring pre-processing of weights or activations. any4 yields higher accuracy\ncompared to other related 4-bit numeric representation types: int4, fp4 and\nnf4, as evaluated on a range of model sizes, generations and families (Llama 2,\nLlama 3, Mistral and Mixtral). While any4 does not require preprocessing of\nweights or activations, it is also competitive with orthogonal techniques that\nrequire such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3\nand any2 and show competitiveness at lower bits. Additionally, we show that we\ncan calibrate using a single curated diverse sample rather than hundreds of\nsamples from a dataset as done in most quantization approaches. We also open\nsource tinygemm, a latency optimized GPU matrix multiplication library for\nLLMs, that implements any4 using a GPU-efficient lookup table strategy along\nwith other common quantization methods. We open source our code at\nhttps://github.com/facebookresearch/any4 .",
    "published": "2025-07-07T01:59:47Z",
    "updated": "2025-07-07T01:59:47Z",
    "id": "2507.04610v1",
    "authors": [
      "Mostafa Elhoushi",
      "Jeff Johnson"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04610v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04610v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04610v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a learned 4-bit weight quantization solution for large language models (LLMs), which is directly related to the optimization and efficiency of LLMs. It does not fit into the more specific categories like RL, MLLM, VLA, etc., but it is relevant to the broader topic of LLMs.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04607v2": {
    "title": "PRIME: Large Language Model Personalization with Cognitive Memory and\n  Thought Processes",
    "summary": "Large language model (LLM) personalization aims to align model outputs with\nindividuals' unique preferences and opinions. While recent efforts have\nimplemented various personalization methods, a unified theoretical framework\nthat can systematically understand the drivers of effective personalization is\nstill lacking. In this work, we integrate the well-established cognitive\ndual-memory model into LLM personalization, by mirroring episodic memory to\nhistorical user engagements and semantic memory to long-term, evolving user\nbeliefs. Specifically, we systematically investigate memory instantiations and\nintroduce a unified framework, PRIME, using episodic and semantic memory\nmechanisms. We further augment PRIME with a novel personalized thinking\ncapability inspired by the slow thinking strategy. Moreover, recognizing the\nabsence of suitable benchmarks, we introduce a dataset using Change My View\n(CMV) from Reddit, specifically designed to evaluate long-context\npersonalization. Extensive experiments validate PRIME's effectiveness across\nboth long- and short-context scenarios. Further analysis confirms that PRIME\neffectively captures dynamic personalization beyond mere popularity biases.",
    "published": "2025-07-07T01:54:34Z",
    "updated": "2025-07-14T05:54:45Z",
    "id": "2507.04607v2",
    "authors": [
      "Xinliang Frederick Zhang",
      "Nick Beauchamp",
      "Lu Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04607v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04607v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04607v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses personalization of large language models (LLMs) using cognitive memory mechanisms (episodic and semantic memory) and introduces a novel personalized thinking capability. It also addresses the lack of benchmarks by introducing a new dataset. The core topics are related to memory in LLMs and personalization, which aligns with the 'Memory' and 'LLM' categories.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.04562v1": {
    "title": "Evaluating LLMs on Real-World Forecasting Against Human Superforecasters",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their ability to forecast future events remains\nunderstudied. A year ago, large language models struggle to come close to the\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\nquestions from Metaculus, comparing their performance against human\nsuperforecasters. Frontier models achieve Brier scores that ostensibly surpass\nthe human crowd but still significantly underperform a group of\nsuperforecasters.",
    "published": "2025-07-06T22:26:59Z",
    "updated": "2025-07-06T22:26:59Z",
    "id": "2507.04562v1",
    "authors": [
      "Janna Lu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04562v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04562v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04562v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper evaluates the performance of LLMs on forecasting tasks, comparing them to human superforecasters. This involves benchmarking LLMs against human performance, which falls under the 'Benchmark' category. Additionally, the focus on LLMs' capabilities in forecasting relates to their reasoning abilities, hence the 'Reasoning' category.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning"
    ]
  },
  "2507.04531v1": {
    "title": "DP-Fusion: Token-Level Differentially Private Inference for Large\n  Language Models",
    "summary": "Large language models (LLMs) can leak sensitive information from their\ncontext through generated outputs, either accidentally or when prompted\nadversarially. Existing defenses that aim to preserve context privacy during\ninference either lack formal guarantees or suffer from a poor utility/privacy\ntrade-off. We propose DP-Fusion, a token-level Differentially Private Inference\n(DPI) mechanism that provably bounds how much an LLM's outputs reveal about\nsensitive tokens in its context. We demonstrate DPI through the task of\ndocument privatization, where the goal is to paraphrase documents so that\nsensitive content (e.g., Personally Identifiable Information, PII) cannot be\nreliably inferred, while still preserving the overall utility of the text. This\nis controlled by a parameter $\\epsilon$: $\\epsilon=0$ hides PII entirely, while\nhigher values trade off privacy for improved paraphrase quality. DP-Fusion\nworks as follows: (i) partition sensitive tokens into disjoint privacy groups,\n(ii) run the LLM once per group, and (iii) blend the output distributions so\nthat the final output remains within a fixed statistical distance of the\nbaseline distribution produced when no privacy group is revealed. This approach\nallows fine-grained control over the privacy/utility trade-off but requires\nmultiple LLM forward passes.",
    "published": "2025-07-06T20:49:39Z",
    "updated": "2025-07-06T20:49:39Z",
    "id": "2507.04531v1",
    "authors": [
      "Rushil Thareja",
      "Preslav Nakov",
      "Praneeth Vepakomma",
      "Nils Lukas"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04531v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04531v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04531v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on privacy-preserving mechanisms for large language models (LLMs) during inference, specifically addressing the leakage of sensitive information. It introduces a differentially private inference mechanism (DP-Fusion) to protect sensitive tokens in the context of LLMs. The core topics relevant here are privacy in LLMs and the trade-off between utility and privacy, which aligns with the broader research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.04517v1": {
    "title": "DOTResize: Reducing LLM Width via Discrete Optimal Transport-based\n  Neuron Merging",
    "summary": "Model compression offers a promising path to reducing the cost and\ninaccessibility of large pre-trained models, without significantly compromising\ntheir impressive performance. Large Transformer models, including large\nlanguage models (LLMs), often contain computational redundancy, which can serve\nas a target for new model compression methods. In this work, we specifically\ntarget neuron-level redundancies in model layers by combining groups of similar\nneurons into fewer neurons. We frame this width reduction as a Discrete Optimal\nTransport problem, and propose DOTResize, a novel Transformer compression\nmethod that uses optimal transport theory to transform and compress model\nweights. To ensure applicability within the Transformer architecture, we\nmotivate and incorporate entropic regularization and matrix factorization into\nthe transportation maps produced by our method. Unlike pruning-based approaches\nwhich discard neurons based on importance measures, DOTResize re-projects the\nentire neuron width, allowing the retention and redistribution of useful signal\nacross the reduced layer. Empirical results show that compared to simple or\nstate-of-the-art neuron width-pruning techniques, DOTResize can outperform\nthese methods across multiple LLM families and sizes, while achieving\nmeasurable reductions in real-world computational cost.",
    "published": "2025-07-06T19:49:46Z",
    "updated": "2025-07-06T19:49:46Z",
    "id": "2507.04517v1",
    "authors": [
      "Neha Verma",
      "Kenton Murray",
      "Kevin Duh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04517v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04517v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04517v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for compressing large language models (LLMs) by reducing neuron-level redundancies, which is relevant to the topics of LLM (Large Language Models) and Scaling (model scaling and compression). The method involves optimal transport theory and matrix factorization, which are techniques used in model optimization and scaling.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.04504v1": {
    "title": "Unveiling the Potential of Diffusion Large Language Model in\n  Controllable Generation",
    "summary": "Diffusion models, originally developed for image generation, have emerged as\na promising alternative to autoregressive large language models (LLMs). We\npresent a theoretical analysis comparing autoregressive and masked diffusion\nLLMs, revealing that the intrinsic bidirectional attention mechanism of\ndiffusion LLMs (dLLMs) enables superior context modeling and generation\ncontrollability. However, existing dLLM applications face significant\nchallenges in controllable generation: the native multi-step denoising process\nexhibits high sensitivity to sequence length, elevated hallucination rates, and\nprohibitive inference costs without specialized optimizations. To address these\nlimitations, we propose \\textbf{S}elf-adaptive \\textbf{S}chema\n\\textbf{S}caffolding ($S^3$), a novel framework that enables dLLMs to generate\nstructured outputs (e.g., JSON) while maintaining semantic fidelity and\naccelerating inference. Our approach injects the target schema structure into\nthe output context, reducing unnecessary computation while improving\ncontrollability. Extensive experiments demonstrate that $S^3$ achieves\nsubstantial improvements: 65\\% increase in structural adherence, 48\\%\nenhancement in content fidelity, and 17\\% reduction in hallucination rates\ncompared to baseline. These results establish both theoretical foundations and\npractical pathways for deploying diffusion models in controllable text\ngeneration tasks. Code and data will be publicly released.",
    "published": "2025-07-06T18:41:34Z",
    "updated": "2025-07-06T18:41:34Z",
    "id": "2507.04504v1",
    "authors": [
      "Zhen Xiong",
      "Yujun Cai",
      "Zhecheng Li",
      "Yiwei Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04504v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04504v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04504v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses diffusion models as an alternative to autoregressive large language models (LLMs), focusing on their application in controllable generation tasks. It introduces a novel framework to enhance the performance of diffusion LLMs (dLLMs) in structured output generation, which is directly related to the research on Large Language Models (LLM) and their architectures.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04491v1": {
    "title": "A validity-guided workflow for robust large language model research in\n  psychology",
    "summary": "Large language models (LLMs) are rapidly being integrated into psychological\nresearch as research tools, evaluation targets, human simulators, and cognitive\nmodels. However, recent evidence reveals severe measurement unreliability:\nPersonality assessments collapse under factor analysis, moral preferences\nreverse with punctuation changes, and theory-of-mind accuracy varies widely\nwith trivial rephrasing. These \"measurement phantoms\"--statistical artifacts\nmasquerading as psychological phenomena--threaten the validity of a growing\nbody of research. Guided by the dual-validity framework that integrates\npsychometrics with causal inference, we present a six-stage workflow that\nscales validity requirements to research ambition--using LLMs to code text\nrequires basic reliability and accuracy, while claims about psychological\nproperties demand comprehensive construct validation. Researchers must (1)\nexplicitly define their research goal and corresponding validity requirements,\n(2) develop and validate computational instruments through psychometric\ntesting, (3) design experiments that control for computational confounds, (4)\nexecute protocols with transparency, (5) analyze data using methods appropriate\nfor non-independent observations, and (6) report findings within demonstrated\nboundaries and use results to refine theory. We illustrate the workflow through\nan example of model evaluation--\"LLM selfhood\"--showing how systematic\nvalidation can distinguish genuine computational phenomena from measurement\nartifacts. By establishing validated computational instruments and transparent\npractices, this workflow provides a path toward building a robust empirical\nfoundation for AI psychology research.",
    "published": "2025-07-06T18:06:12Z",
    "updated": "2025-07-06T18:06:12Z",
    "id": "2507.04491v1",
    "authors": [
      "Zhicheng Lin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04491v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04491v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04491v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in psychological research, focusing on their reliability and validity as research tools. It emphasizes the need for robust validation methods to ensure accurate measurement and interpretation of psychological phenomena using LLMs.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.04480v1": {
    "title": "Source Attribution in Retrieval-Augmented Generation",
    "summary": "While attribution methods, such as Shapley values, are widely used to explain\nthe importance of features or training data in traditional machine learning,\ntheir application to Large Language Models (LLMs), particularly within\nRetrieval-Augmented Generation (RAG) systems, is nascent and challenging. The\nprimary obstacle is the substantial computational cost, where each utility\nfunction evaluation involves an expensive LLM call, resulting in direct\nmonetary and time expenses. This paper investigates the feasibility and\neffectiveness of adapting Shapley-based attribution to identify influential\nretrieved documents in RAG. We compare Shapley with more computationally\ntractable approximations and some existing attribution methods for LLM. Our\nwork aims to: (1) systematically apply established attribution principles to\nthe RAG document-level setting; (2) quantify how well SHAP approximations can\nmirror exact attributions while minimizing costly LLM interactions; and (3)\nevaluate their practical explainability in identifying critical documents,\nespecially under complex inter-document relationships such as redundancy,\ncomplementarity, and synergy. This study seeks to bridge the gap between\npowerful attribution techniques and the practical constraints of LLM-based RAG\nsystems, offering insights into achieving reliable and affordable RAG\nexplainability.",
    "published": "2025-07-06T17:36:45Z",
    "updated": "2025-07-06T17:36:45Z",
    "id": "2507.04480v1",
    "authors": [
      "Ikhtiyor Nematov",
      "Tarik Kalai",
      "Elizaveta Kuzmenko",
      "Gabriele Fugagnoli",
      "Dimitris Sacharidis",
      "Katja Hose",
      "Tomer Sagi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04480v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04480v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04480v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the application of attribution methods like Shapley values in Retrieval-Augmented Generation (RAG) systems, which involves Large Language Models (LLMs). It focuses on the computational challenges and practical explainability in identifying influential retrieved documents, aligning with topics related to LLMs and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.04478v1": {
    "title": "Model Inversion Attacks on Llama 3: Extracting PII from Large Language\n  Models",
    "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their ability to memorize training data poses significant privacy risks.\nThis paper investigates model inversion attacks on the Llama 3.2 model, a\nmultilingual LLM developed by Meta. By querying the model with carefully\ncrafted prompts, we demonstrate the extraction of personally identifiable\ninformation (PII) such as passwords, email addresses, and account numbers. Our\nfindings highlight the vulnerability of even smaller LLMs to privacy attacks\nand underscore the need for robust defenses. We discuss potential mitigation\nstrategies, including differential privacy and data sanitization, and call for\nfurther research into privacy-preserving machine learning techniques.",
    "published": "2025-07-06T17:24:17Z",
    "updated": "2025-07-06T17:24:17Z",
    "id": "2507.04478v1",
    "authors": [
      "Sathesh P. Sivashanmugam"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04478v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04478v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04478v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses privacy risks and model inversion attacks on a large language model (Llama 3.2), which directly relates to research on Large Language Models (LLMs) and their vulnerabilities.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04469v2": {
    "title": "The role of large language models in UI/UX design: A systematic\n  literature review",
    "summary": "This systematic literature review examines the role of large language models\n(LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies\npublished between 2022 and 2025. We identify key LLMs in use, including GPT-4,\nGemini, and PaLM, and map their integration across the design lifecycle, from\nideation to evaluation. Common practices include prompt engineering,\nhuman-in-the-loop workflows, and multimodal input. While LLMs are reshaping\ndesign processes, challenges such as hallucination, prompt instability, and\nlimited explainability persist. Our findings highlight LLMs as emerging\ncollaborators in design, and we propose directions for the ethical, inclusive,\nand effective integration of these technologies.",
    "published": "2025-07-06T17:18:05Z",
    "updated": "2025-07-17T19:03:15Z",
    "id": "2507.04469v2",
    "authors": [
      "Ammar Ahmed",
      "Ali Shariq Imran"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04469v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04469v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04469v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models (LLMs) in UI/UX design, focusing on their integration and challenges in the design lifecycle. The core topic is LLMs, as the title and abstract repeatedly mention their role and impact in this specific domain.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04455v1": {
    "title": "GradOT: Training-free Gradient-preserving Offsite-tuning for Large\n  Language Models",
    "summary": "The rapid growth of large language models (LLMs) with traditional centralized\nfine-tuning emerges as a key technique for adapting these models to\ndomain-specific challenges, yielding privacy risks for both model and data\nowners. One promising solution, called offsite-tuning (OT), is proposed to\naddress these challenges, where a weaker emulator is compressed from the\noriginal model and further fine-tuned with adapter to enhance privacy. However,\nthe existing OT-based methods require high computational costs and lack\ntheoretical analysis. This paper introduces a novel OT approach based on\ngradient-preserving compression, named GradOT. By analyzing the OT problem\nthrough the lens of optimization, we propose a method that selectively applies\ncompression techniques such as rank compression and channel pruning, preserving\nthe gradients of fine-tuned adapters while ensuring privacy. Extensive\nexperiments demonstrate that our approach surpasses existing OT methods, both\nin terms of privacy protection and model performance. Our method provides a\ntheoretical foundation for OT and offers a practical, training-free solution\nfor offsite-tuning of large-scale LLMs.",
    "published": "2025-07-06T16:27:27Z",
    "updated": "2025-07-06T16:27:27Z",
    "id": "2507.04455v1",
    "authors": [
      "Kai Yao",
      "Zhaorui Tan",
      "Penglei Gao",
      "Lichun Li",
      "Kaixin Wu",
      "Yinggui Wang",
      "Yuan Zhao",
      "Yixin Ji",
      "Wei Wang",
      "Jianke Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04455v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04455v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04455v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a novel approach for offsite-tuning of large language models (LLMs) with a focus on privacy and computational efficiency, which aligns with the topics of LLM and Scaling.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.04451v1": {
    "title": "CoT-lized Diffusion: Let's Reinforce T2I Generation Step-by-step",
    "summary": "Current text-to-image (T2I) generation models struggle to align spatial\ncomposition with the input text, especially in complex scenes. Even\nlayout-based approaches yield suboptimal spatial control, as their generation\nprocess is decoupled from layout planning, making it difficult to refine the\nlayout during synthesis. We present CoT-Diff, a framework that brings\nstep-by-step CoT-style reasoning into T2I generation by tightly integrating\nMultimodal Large Language Model (MLLM)-driven 3D layout planning with the\ndiffusion process. CoT-Diff enables layout-aware reasoning inline within a\nsingle diffusion round: at each denoising step, the MLLM evaluates intermediate\npredictions, dynamically updates the 3D scene layout, and continuously guides\nthe generation process. The updated layout is converted into semantic\nconditions and depth maps, which are fused into the diffusion model via a\ncondition-aware attention mechanism, enabling precise spatial control and\nsemantic injection. Experiments on 3D Scene benchmarks show that CoT-Diff\nsignificantly improves spatial alignment and compositional fidelity, and\noutperforms the state-of-the-art method by 34.7% in complex scene spatial\naccuracy, thereby validating the effectiveness of this entangled generation\nparadigm.",
    "published": "2025-07-06T16:17:32Z",
    "updated": "2025-07-06T16:17:32Z",
    "id": "2507.04451v1",
    "authors": [
      "Zheyuan Liu",
      "Munan Ning",
      "Qihui Zhang",
      "Shuo Yang",
      "Zhongrui Wang",
      "Yiwei Yang",
      "Xianzhe Xu",
      "Yibing Song",
      "Weihua Chen",
      "Fan Wang",
      "Li Yuan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04451v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04451v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04451v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Multimodal Large Language Models (MLLM) with diffusion models for text-to-image generation, focusing on step-by-step reasoning and spatial alignment. This aligns with the topics of MLLM (Multimodal Large Language Models) and Reasoning (LLM reasoning, chain-of-thought prompting).",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.04446v2": {
    "title": "Tail-aware Adversarial Attacks: A Distributional Approach to Efficient\n  LLM Jailbreaking",
    "summary": "To guarantee safe and robust deployment of large language models (LLMs) at\nscale, it is critical to accurately assess their adversarial robustness.\nExisting adversarial attacks typically target harmful responses in\nsingle-point, greedy generations, overlooking the inherently stochastic nature\nof LLMs. In this paper, we propose a novel framework for adversarial robustness\nevaluation that explicitly models the entire output distribution, including\ntail-risks, providing better estimates for model robustness at scale. By\ncasting the attack process as a resource allocation problem between\noptimization and sampling, we determine compute-optimal tradeoffs and show that\nintegrating sampling into existing attacks boosts ASR by up to 48% and improves\nefficiency by up to two orders of magnitude. Our framework also enables us to\nanalyze how different attack algorithms affect output harm distributions.\nSurprisingly, we find that most optimization strategies have little effect on\noutput harmfulness. Finally, we introduce a data-free proof-of-concept\nobjective based on entropy-maximization to demonstrate how our tail-aware\nperspective enables new optimization targets. Overall, our findings highlight\nthe importance of tail-aware attacks and evaluation protocols to accurately\nassess and strengthen LLM safety.",
    "published": "2025-07-06T16:13:33Z",
    "updated": "2025-07-09T11:52:25Z",
    "id": "2507.04446v2",
    "authors": [
      "Tim Beyer",
      "Yan Scholten",
      "Leo Schwinn",
      "Stephan Gnnemann"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04446v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04446v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04446v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on adversarial attacks and robustness evaluation of large language models (LLMs), which aligns with the 'LLM' topic. It also discusses optimization strategies and efficiency, which are relevant to 'Scaling'. The adversarial robustness evaluation aspect can also be linked to 'Benchmark' as it involves assessing model performance under adversarial conditions.",
    "llm_cls_result": [
      "LLM",
      "Scaling",
      "Benchmark"
    ]
  },
  "2507.04444v1": {
    "title": "Data Discovery using LLMs -- A Study of Data User Behaviour",
    "summary": "Data search for scientific research is more complex than a simple web search.\nThe emergence of large language models (LLMs) and their applicability for\nscientific tasks offers new opportunities for researchers who are looking for\ndata, e.g., to freely express their data needs instead of fitting them into\nrestrictions of data catalogues and portals. However, this also creates\nuncertainty about whether LLMs are suitable for this task. To answer this\nquestion, we conducted a user study with 32 researchers. We qualitatively and\nquantitively analysed participants' information interaction behaviour while\nsearching for data using LLMs in two data search tasks, one in which we\nprompted the LLM to behave as a persona. We found that participants interact\nwith LLMs in natural language, but LLMs remain a tool for them rather than an\nequal conversational partner. This changes slightly when the LLM is prompted to\nbehave as a persona, but the prompting only affects participants' user\nexperience when they are already experienced in LLM use.",
    "published": "2025-07-06T16:09:53Z",
    "updated": "2025-07-06T16:09:53Z",
    "id": "2507.04444v1",
    "authors": [
      "Christin Katharina Kreutz",
      "Anja Perry",
      "Tanja Friedrich"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04444v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04444v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04444v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of LLMs for data discovery and user behavior, focusing on the interaction between researchers and LLMs in data search tasks.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.04431v2": {
    "title": "MedGellan: LLM-Generated Medical Guidance to Support Physicians",
    "summary": "Medical decision-making is a critical task, where errors can result in\nserious, potentially life-threatening consequences. While full automation\nremains challenging, hybrid frameworks that combine machine intelligence with\nhuman oversight offer a practical alternative. In this paper, we present\nMedGellan, a lightweight, annotation-free framework that uses a Large Language\nModel (LLM) to generate clinical guidance from raw medical records, which is\nthen used by a physician to predict diagnoses. MedGellan uses a\nBayesian-inspired prompting strategy that respects the temporal order of\nclinical data. Preliminary experiments show that the guidance generated by the\nLLM with MedGellan improves diagnostic performance, particularly in recall and\n$F_1$ score.",
    "published": "2025-07-06T15:31:01Z",
    "updated": "2025-07-08T18:16:26Z",
    "id": "2507.04431v2",
    "authors": [
      "Debodeep Banerjee",
      "Burcu Sayin",
      "Stefano Teso",
      "Andrea Passerini"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04431v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04431v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04431v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a Large Language Model (LLM) to generate clinical guidance from raw medical records, which aligns with the topic of LLM research and applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04430v1": {
    "title": "\"Hi AirStar, Guide Me to the Badminton Court.\"",
    "summary": "Unmanned Aerial Vehicles, operating in environments with relatively few\nobstacles, offer high maneuverability and full three-dimensional mobility. This\nallows them to rapidly approach objects and perform a wide range of tasks often\nchallenging for ground robots, making them ideal for exploration, inspection,\naerial imaging, and everyday assistance. In this paper, we introduce AirStar, a\nUAV-centric embodied platform that turns a UAV into an intelligent aerial\nassistant: a large language model acts as the cognitive core for environmental\nunderstanding, contextual reasoning, and task planning. AirStar accepts natural\ninteraction through voice commands and gestures, removing the need for a remote\ncontroller and significantly broadening its user base. It combines geospatial\nknowledge-driven long-distance navigation with contextual reasoning for\nfine-grained short-range control, resulting in an efficient and accurate\nvision-and-language navigation (VLN) capability.Furthermore, the system also\noffers built-in capabilities such as cross-modal question answering,\nintelligent filming, and target tracking. With a highly extensible framework,\nit supports seamless integration of new functionalities, paving the way toward\na general-purpose, instruction-driven intelligent UAV agent. The supplementary\nPPT is available at\n\\href{https://buaa-colalab.github.io/airstar.github.io}{https://buaa-colalab.github.io/airstar.github.io}.",
    "published": "2025-07-06T15:29:07Z",
    "updated": "2025-07-06T15:29:07Z",
    "id": "2507.04430v1",
    "authors": [
      "Ziqin Wang",
      "Jinyu Chen",
      "Xiangyi Zheng",
      "Qinan Liao",
      "Linjiang Huang",
      "Si Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04430v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04430v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04430v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a large language model as the cognitive core for a UAV, integrating vision and language for navigation and task planning, which aligns with the topics of LLM (Large Language Models) and VLA (Vision-Language Action models).",
    "llm_cls_result": [
      "LLM",
      "VLA"
    ]
  },
  "2507.04415v1": {
    "title": "MOMENTS: A Comprehensive Multimodal Benchmark for Theory of Mind",
    "summary": "Understanding Theory of Mind is essential for building socially intelligent\nmultimodal agents capable of perceiving and interpreting human behavior. We\nintroduce MOMENTS (Multimodal Mental States), a comprehensive benchmark\ndesigned to assess the ToM capabilities of multimodal large language models\n(LLMs) through realistic, narrative-rich scenarios presented in short films.\nMOMENTS includes over 2,344 multiple-choice questions spanning seven distinct\nToM categories. The benchmark features long video context windows and realistic\nsocial interactions that provide deeper insight into characters' mental states.\nWhile the visual modality generally enhances model performance, current systems\nstill struggle to integrate it effectively, underscoring the need for further\nresearch into AI's multimodal understanding of human behavior.",
    "published": "2025-07-06T15:06:30Z",
    "updated": "2025-07-06T15:06:30Z",
    "id": "2507.04415v1",
    "authors": [
      "Emilio Villa-Cueva",
      "S M Masrur Ahmed",
      "Rendi Chevi",
      "Jan Christian Blaise Cruz",
      "Kareem Elzeky",
      "Fermin Cristobal",
      "Alham Fikri Aji",
      "Skyler Wang",
      "Rada Mihalcea",
      "Thamar Solorio"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04415v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04415v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04415v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a multimodal benchmark (MOMENTS) designed to assess the Theory of Mind capabilities of multimodal large language models (MLLMs) through realistic scenarios. It focuses on evaluating MLLMs' ability to understand and interpret human behavior, which aligns with the topics of Multimodal Large Language Models (MLLM) and Benchmarking (Benchmark).",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.04414v1": {
    "title": "THM@SimpleText 2025 -- Task 1.1: Revisiting Text Simplification based on\n  Complex Terms for Non-Experts",
    "summary": "Scientific text is complex as it contains technical terms by definition.\nSimplifying such text for non-domain experts enhances accessibility of\ninnovation and information. Politicians could be enabled to understand new\nfindings on topics on which they intend to pass a law, or family members of\nseriously ill patients could read about clinical trials. The SimpleText CLEF\nLab focuses on exactly this problem of simplification of scientific text. Task\n1.1 of the 2025 edition specifically handles the simplification of complex\nsentences, so very short texts with little context. To tackle this task we\ninvestigate the identification of complex terms in sentences which are\nrephrased using small Gemini and OpenAI large language models for non-expert\nreaders.",
    "published": "2025-07-06T15:05:54Z",
    "updated": "2025-07-06T15:05:54Z",
    "id": "2507.04414v1",
    "authors": [
      "Nico Hofmann",
      "Julian Dauenhauer",
      "Nils Ole Dietzler",
      "Idehen Daniel Idahor",
      "Christin Katharina Kreutz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04414v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04414v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04414v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on text simplification using large language models (Gemini and OpenAI) for non-expert readers, which aligns with the 'LLM' topic. It also involves the application of these models in a specific task, which could be related to 'Reasoning' as it involves understanding and rephrasing complex terms.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.04410v1": {
    "title": "Multimedia Verification Through Multi-Agent Deep Research Multimodal\n  Large Language Models",
    "summary": "This paper presents our submission to the ACMMM25 - Grand Challenge on\nMultimedia Verification. We developed a multi-agent verification system that\ncombines Multimodal Large Language Models (MLLMs) with specialized verification\ntools to detect multimedia misinformation. Our system operates through six\nstages: raw data processing, planning, information extraction, deep research,\nevidence collection, and report generation. The core Deep Researcher Agent\nemploys four tools: reverse image search, metadata analysis, fact-checking\ndatabases, and verified news processing that extracts spatial, temporal,\nattribution, and motivational context. We demonstrate our approach on a\nchallenge dataset sample involving complex multimedia content. Our system\nsuccessfully verified content authenticity, extracted precise geolocation and\ntiming information, and traced source attribution across multiple platforms,\neffectively addressing real-world multimedia verification scenarios.",
    "published": "2025-07-06T14:54:07Z",
    "updated": "2025-07-06T14:54:07Z",
    "id": "2507.04410v1",
    "authors": [
      "Huy Hoan Le",
      "Van Sy Thinh Nguyen",
      "Thi Le Chi Dang",
      "Vo Thanh Khang Nguyen",
      "Truong Thanh Hung Nguyen",
      "Hung Cao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04410v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04410v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04410v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the use of Multimodal Large Language Models (MLLMs) for multimedia verification, which involves processing and analyzing complex multimedia content. The core topics revolve around MLLMs and their application in verification tasks.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.04404v1": {
    "title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers",
    "summary": "Large language models (LLMs) excel at natural language understanding and\ngeneration but remain vulnerable to factual errors, limiting their reliability\nin knowledge-intensive tasks. While decoding-time strategies provide a\npromising efficient solution without training, existing methods typically treat\ntoken-level and layer-level signals in isolation, overlooking the joint\ndynamics between them. In this work, we introduce a token-aware,\nlayer-localized contrastive decoding method that aligns specific token types\nwith their most influential transformer layers to improve factual generation.\nThrough empirical attention analysis, we identify two key patterns: punctuation\ntokens receive dominant attention in early layers, while conceptual tokens\ngovern semantic reasoning in intermediate layers. By selectively suppressing\nattention to these token types at their respective depths, we achieve the\ninduction of controlled factual degradation and derive contrastive signals to\nguide the final factual decoding. Our method requires no additional training or\nmodel modification, and experiments demonstrate that our method consistently\nimproves factuality across multiple LLMs and various benchmarks.",
    "published": "2025-07-06T14:35:43Z",
    "updated": "2025-07-06T14:35:43Z",
    "id": "2507.04404v1",
    "authors": [
      "Jingze Zhu",
      "Yongliang Wu",
      "Wenbo Zhu",
      "Jiawang Cao",
      "Yanqiang Zheng",
      "Jiawei Chen",
      "Xu Yang",
      "Bernt Schiele",
      "Jonas Fischer",
      "Xinting Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04404v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04404v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04404v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the factual accuracy of Large Language Models (LLMs) through a token-aware, layer-localized contrastive decoding method. It discusses the dynamics between token-level and layer-level signals within LLMs, which is directly related to the research on LLMs and their architectures.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.04370v1": {
    "title": "WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory\n  Synthesis",
    "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved the capabilities of web agents. However, effectively navigating\ncomplex and dynamic web environments still requires more advanced\ntrajectory-level planning and execution. Prior studies have addressed\nself-improving agents by collecting extensive GUI trajectories from\nreal-environment interactions. Despite their effectiveness, these approaches\nencounter two critical challenges: (1) Uncontrollable environment states, where\nreal or sandboxed web environments often yield unstable and non-deterministic\nfeedback, complicating the reproduction and debugging of agent behaviors; and\n(2) High API costs, as generating even a single interaction trajectory can\ninvolve hundreds of queries, leading to considerable API usage and\ncomputational expenses. To address these limitations and enable scalable\nself-improvement for agents, we propose WebSynthesis, a novel framework for\ntrajectory synthesis and training. WebSynthesis leverages a learned world model\nto simulate virtual web environments, allowing a policy agent to perform\nefficient and reversible tree-based planning. This approach supports the\nlarge-scale generation of diverse and high-quality trajectories, which are\nsubsequently utilized to refine the agent's policy. Experimental results\ndemonstrate that an agent trained using WebSynthesis on a small-scale synthetic\ndataset achieves performance comparable to or even surpassing that of models\ntrained on large-scale real-world data.",
    "published": "2025-07-06T12:31:10Z",
    "updated": "2025-07-06T12:31:10Z",
    "id": "2507.04370v1",
    "authors": [
      "Yifei Gao",
      "Junhong Ye",
      "Jiaqi Wang",
      "Jitao Sang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04370v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04370v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04370v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for web agents and their trajectory-level planning, which involves reinforcement learning and agent-based approaches. The focus on improving agent performance through synthetic data and world models aligns with topics in RL and AGI.",
    "llm_cls_result": [
      "RL",
      "AGI"
    ]
  },
  "2507.04365v1": {
    "title": "Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and\n  Defenses in LLMs",
    "summary": "As large language models (LLMs) become more integral to society and\ntechnology, ensuring their safety becomes essential. Jailbreak attacks exploit\nvulnerabilities to bypass safety guardrails, posing a significant threat.\nHowever, the mechanisms enabling these attacks are not well understood. In this\npaper, we reveal a universal phenomenon that occurs during jailbreak attacks:\nAttention Slipping. During this phenomenon, the model gradually reduces the\nattention it allocates to unsafe requests in a user query during the attack\nprocess, ultimately causing a jailbreak. We show Attention Slipping is\nconsistent across various jailbreak methods, including gradient-based token\nreplacement, prompt-level template refinement, and in-context learning.\nAdditionally, we evaluate two defenses based on query perturbation, Token\nHighlighter and SmoothLLM, and find they indirectly mitigate Attention\nSlipping, with their effectiveness positively correlated with the degree of\nmitigation achieved. Inspired by this finding, we propose Attention Sharpening,\na new defense that directly counters Attention Slipping by sharpening the\nattention score distribution using temperature scaling. Experiments on four\nleading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2)\nshow that our method effectively resists various jailbreak attacks while\nmaintaining performance on benign tasks on AlpacaEval. Importantly, Attention\nSharpening introduces no additional computational or memory overhead, making it\nan efficient and practical solution for real-world deployment.",
    "published": "2025-07-06T12:19:04Z",
    "updated": "2025-07-06T12:19:04Z",
    "id": "2507.04365v1",
    "authors": [
      "Xiaomeng Hu",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04365v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04365v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04365v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses jailbreak attacks and defenses in LLMs, focusing on the mechanistic understanding of these attacks through the phenomenon of Attention Slipping. It also proposes a new defense mechanism, Attention Sharpening, to counter these attacks. The core topics are related to LLM safety and vulnerabilities, which fall under the broader category of LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04364v1": {
    "title": "Large Language Models' Varying Accuracy in Recognizing Risk-Promoting\n  and Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV\n  Vaccination and Heated Tobacco Products",
    "summary": "Machine learning methods are increasingly applied to analyze health-related\npublic discourse based on large-scale data, but questions remain regarding\ntheir ability to accurately detect different types of health sentiments.\nEspecially, Large Language Models (LLMs) have gained attention as a powerful\ntechnology, yet their accuracy and feasibility in capturing different opinions\nand perspectives on health issues are largely unexplored. Thus, this research\nexamines how accurate the three prominent LLMs (GPT, Gemini, and LLAMA) are in\ndetecting risk-promoting versus health-supporting sentiments across two\ncritical public health topics: Human Papillomavirus (HPV) vaccination and\nheated tobacco products (HTPs). Drawing on data from Facebook and Twitter, we\ncurated multiple sets of messages supporting or opposing recommended health\nbehaviors, supplemented with human annotations as the gold standard for\nsentiment classification. The findings indicate that all three LLMs generally\ndemonstrate substantial accuracy in classifying risk-promoting and\nhealth-supporting sentiments, although notable discrepancies emerge by\nplatform, health issue, and model type. Specifically, models often show higher\naccuracy for risk-promoting sentiment on Facebook, whereas health-supporting\nmessages on Twitter are more accurately detected. An additional analysis also\nshows the challenges LLMs face in reliably detecting neutral messages. These\nresults highlight the importance of carefully selecting and validating language\nmodels for public health analyses, particularly given potential biases in\ntraining data that may lead LLMs to overestimate or underestimate the\nprevalence of certain perspectives.",
    "published": "2025-07-06T11:57:02Z",
    "updated": "2025-07-06T11:57:02Z",
    "id": "2507.04364v1",
    "authors": [
      "Soojong Kim",
      "Kwanho Kim",
      "Hye Min Kim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04364v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04364v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04364v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the accuracy of Large Language Models (LLMs) in detecting health-related sentiments, which directly relates to LLM research and their application in specific tasks.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.04351v1": {
    "title": "MLLM-Fabric: Multimodal Large Language Model-Driven Robotic Framework\n  for Fabric Sorting and Selection",
    "summary": "Choosing the right fabric is crucial to meet functional and quality\nrequirements in robotic applications for textile manufacturing, apparel\nproduction, and smart retail. We present MLLM-Fabric, a robotic framework\npowered by multimodal large language models (MLLMs) for fabric sorting and\nselection. The system includes a robotic arm, a camera, a visuotactile sensor,\nand a pressure sensor. It employs supervised fine-tuning and multimodal\nexplanation-guided knowledge distillation to accurately classify and rank\nfabric properties. To facilitate further research, we release a dataset of 220\nunique fabric samples, including RGB images and synchronized visuotactile and\npressure data. Experimental results show that our Fabric-Llama-90B model\nconsistently outperforms pretrained vision-language baselines in both property\nranking accuracy and selection reliability.",
    "published": "2025-07-06T11:27:27Z",
    "updated": "2025-07-06T11:27:27Z",
    "id": "2507.04351v1",
    "authors": [
      "Liman Wang",
      "Hanyang Zhong",
      "Tianyuan Wang",
      "Shan Luo",
      "Jihong Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04351v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04351v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04351v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a robotic framework powered by multimodal large language models (MLLMs) for fabric sorting and selection, which involves integrating vision, language, and other modalities. It also mentions the release of a dataset for further research.",
    "llm_cls_result": [
      "MLLM",
      "Dataset",
      "VLA"
    ]
  },
  "2507.04333v1": {
    "title": "Computed Tomography Visual Question Answering with Cross-modal Feature\n  Graphing",
    "summary": "Visual question answering (VQA) in medical imaging aims to support clinical\ndiagnosis by automatically interpreting complex imaging data in response to\nnatural language queries. Existing studies typically rely on distinct visual\nand textual encoders to independently extract features from medical images and\nclinical questions, which are subsequently combined to generate answers.\nSpecifically, in computed tomography (CT), such approaches are similar to the\nconventional practices in medical image analysis. However, these approaches pay\nless attention to the spatial continuity and inter-slice correlations in the\nvolumetric CT data, leading to fragmented and imprecise responses. In this\npaper, we propose a novel large language model (LLM)-based framework enhanced\nby a graph representation of salient features. Different from conventional\nmultimodal encoding strategies, our approach constructs a cross-modal graph\nintegrating both visual and textual features, treating individual CT slices and\nquestion tokens as nodes within the graph. We further leverage an attentive\ngraph convolutional network to dynamically fuse information within this\nstructure. The resulting aggregated graph features then serve as a soft prompt\nto guide a large language model in generating accurate answers. Extensive\nexperiments on the M3D-VQA benchmark demonstrate that our approach consistently\noutperforms baselines across multiple evaluation metrics, offering more robust\nreasoning capabilities.",
    "published": "2025-07-06T10:37:16Z",
    "updated": "2025-07-06T10:37:16Z",
    "id": "2507.04333v1",
    "authors": [
      "Yuanhe Tian",
      "Chen Su",
      "Junwen Duan",
      "Yan Song"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04333v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04333v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04333v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a multimodal approach integrating visual and textual features for medical imaging, specifically using a large language model (LLM) and graph representation, which aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA).",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.04329v1": {
    "title": "No Language Data Left Behind: A Comparative Study of CJK Language\n  Datasets in the Hugging Face Ecosystem",
    "summary": "Recent advances in Natural Language Processing (NLP) have underscored the\ncrucial role of high-quality datasets in building large language models (LLMs).\nHowever, while extensive resources and analyses exist for English, the\nlandscape for East Asian languages - particularly Chinese, Japanese, and Korean\n(CJK) - remains fragmented and underexplored, despite these languages together\nserving over 1.6 billion speakers. To address this gap, we investigate the\nHuggingFace ecosystem from a cross-linguistic perspective, focusing on how\ncultural norms, research environments, and institutional practices shape\ndataset availability and quality. Drawing on more than 3,300 datasets, we\nemploy quantitative and qualitative methods to examine how these factors drive\ndistinct creation and curation patterns across Chinese, Japanese, and Korean\nNLP communities. Our findings highlight the large-scale and often\ninstitution-driven nature of Chinese datasets, grassroots community-led\ndevelopment in Korean NLP, and an entertainment- and subculture-focused\nemphasis on Japanese collections. By uncovering these patterns, we reveal\npractical strategies for enhancing dataset documentation, licensing clarity,\nand cross-lingual resource sharing - ultimately guiding more effective and\nculturally attuned LLM development in East Asia. We conclude by discussing best\npractices for future dataset curation and collaboration, aiming to strengthen\nresource development across all three languages.",
    "published": "2025-07-06T10:32:32Z",
    "updated": "2025-07-06T10:32:32Z",
    "id": "2507.04329v1",
    "authors": [
      "Dasol Choi",
      "Woomyoung Park",
      "Youngsook Song"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04329v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04329v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04329v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the availability and quality of datasets for CJK languages in the context of building large language models (LLMs), which aligns with the 'Dataset' category. It also discusses the broader implications for LLM development, which is relevant to 'LLM'.",
    "llm_cls_result": [
      "Dataset",
      "LLM"
    ]
  },
  "2507.04315v1": {
    "title": "HLStrans: Dataset for LLM-Driven C-to-HLS Hardware Code Synthesis",
    "summary": "High-level synthesis (HLS) enables software developers to describe and\nimplement hardware at a higher level of abstraction by using C/C++ instead of\ntraditional hardware description languages to automatically generate FPGA-ready\ndesigns. However, generating HLS code significantly differs from standard\nC/C++: it disallows certain coding idioms, relies on specialized libraries, and\ncritically requires fine-grained transformations and the insertion of\noptimization directives (pragmas) to achieve high performance. Large language\nmodels (LLMs) have shown promise in automating such transformations, yet\nexisting open-source datasets lack sufficient complexity and optimization\ndiversity. To address this gap, we introduce the HLStrans dataset, a\ncomprehensive collection of 137 distinct real word programs, each annotated\nwith a variety of C-to-HLS transformations that yield over 23K labeled design\nvariants. These include a broad spectrum of pragmas and code-level\noptimizations. We benchmark state-of-the-art LLMs on this dataset to evaluate\ntheir ability to generate synthesizable, high-performance HLS code. As part of\nan ongoing effort, we plan to expand the HLStrans dataset in both scale and\nprogram variety, further empowering research at the intersection of AI and\nhardware synthesis.",
    "published": "2025-07-06T09:48:11Z",
    "updated": "2025-07-06T09:48:11Z",
    "id": "2507.04315v1",
    "authors": [
      "Qingyun Zou",
      "Nuo Chen",
      "Yao Chen",
      "Bingsheng He",
      "WengFei Wong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04315v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04315v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04315v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for automating transformations in High-Level Synthesis (HLS) code, and introduces a dataset (HLStrans) for benchmarking LLMs in this context. The core topics are related to LLMs and datasets for LLM benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.08014v1": {
    "title": "Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity\n  Bounds on LLM Jailbreaking",
    "summary": "As large language models (LLMs) become increasingly deployed, understanding\nthe complexity and evolution of jailbreaking strategies is critical for AI\nsafety.\n  We present a mass-scale empirical analysis of jailbreak complexity across\nover 2 million real-world conversations from diverse platforms, including\ndedicated jailbreaking communities and general-purpose chatbots. Using a range\nof complexity metrics spanning probabilistic measures, lexical diversity,\ncompression ratios, and cognitive load indicators, we find that jailbreak\nattempts do not exhibit significantly higher complexity than normal\nconversations. This pattern holds consistently across specialized jailbreaking\ncommunities and general user populations, suggesting practical bounds on attack\nsophistication. Temporal analysis reveals that while user attack toxicity and\ncomplexity remains stable over time, assistant response toxicity has decreased,\nindicating improving safety mechanisms. The absence of power-law scaling in\ncomplexity distributions further points to natural limits on jailbreak\ndevelopment.\n  Our findings challenge the prevailing narrative of an escalating arms race\nbetween attackers and defenders, instead suggesting that LLM safety evolution\nis bounded by human ingenuity constraints while defensive measures continue\nadvancing. Our results highlight critical information hazards in academic\njailbreak disclosure, as sophisticated attacks exceeding current complexity\nbaselines could disrupt the observed equilibrium and enable widespread harm\nbefore defensive adaptation.",
    "published": "2025-07-06T08:41:30Z",
    "updated": "2025-07-06T08:41:30Z",
    "id": "2507.08014v1",
    "authors": [
      "Aldan Creo",
      "Raul Castro Fernandez",
      "Manuel Cebrian"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08014v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08014v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08014v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on analyzing jailbreaking strategies in large language models (LLMs) and their implications for AI safety, which directly relates to research on LLMs and their vulnerabilities.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.04294v1": {
    "title": "BiFair: A Fairness-aware Training Framework for LLM-enhanced Recommender\n  Systems via Bi-level Optimization",
    "summary": "Large Language Model-enhanced Recommender Systems (LLM-enhanced RSs) have\nemerged as a powerful approach to improving recommendation quality by\nleveraging LLMs to generate item representations. Despite these advancements,\nthe integration of LLMs raises severe fairness concerns. Existing studies\nreveal that LLM-based RSs exhibit greater unfairness than traditional RSs, yet\nfairness issues in LLM-enhanced RSs remain largely unexplored. In this paper,\nour empirical study reveals that while LLM-enhanced RSs improve fairness across\nitem groups, a significant fairness gap persists. Further enhancement remains\nchallenging due to the architectural differences and varying sources of\nunfairness inherent in LLM-enhanced RSs. To bridge this gap, we first decompose\nunfairness into i) \\textit{prior unfairness} in LLM-generated representations\nand ii) \\textit{training unfairness} in recommendation models. Then, we propose\nBiFair, a bi-level optimization-based fairness-aware training framework\ndesigned to mitigate both prior and training unfairness simultaneously. BiFair\noptimizes two sets of learnable parameters: LLM-generated representations and a\ntrainable projector in the recommendation model, using a two-level nested\noptimization process. Additionally, we introduce an adaptive inter-group\nbalancing mechanism, leveraging multi-objective optimization principles to\ndynamically balance fairness across item groups. Extensive experiments on three\nreal-world datasets demonstrate that BiFair significantly mitigates unfairness\nand outperforms previous state-of-the-art methods.",
    "published": "2025-07-06T08:39:26Z",
    "updated": "2025-07-06T08:39:26Z",
    "id": "2507.04294v1",
    "authors": [
      "Jiaming Zhang",
      "Yuyuan Li",
      "Yiqun Xu",
      "Li Zhang",
      "Xiaohua Feng",
      "Zhifei Ren",
      "Chaochao Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04294v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04294v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04294v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the integration of Large Language Models (LLMs) in recommender systems and addresses fairness issues, which are directly related to LLM research and their applications.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.08833v1": {
    "title": "LoRA Is Slower Than You Think",
    "summary": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for\nfine-tuning large language models (LLMs). By introducing a small number of\ntrainable low-rank weight matrices, LoRA substantially reduces the number of\nparameters that need to be updated, offering significant advantages in memory\nconsumption and computational efficiency compared to full fine-tuning. However,\nwe observed that LoRA does not consistently provide speed improvements across\nall model architectures and training setups. Motivated by this inconsistency,\nwe conduct a comprehensive analysis of LoRA's performance and investigate the\nunderlying factors limiting its speedup. Based on our findings, we propose\nseveral methods for more efficient fine-tuning of LLMs. We empirically evaluate\nthese methods and compare them to LoRA, demonstrating that our approach\nachieves comparable or superior performance while delivering more consistent\ntraining speed improvements. Our work offers valuable insights and practical\nguidelines for practitioners seeking to optimize LLM fine-tuning under resource\nconstraints.",
    "published": "2025-07-06T08:36:43Z",
    "updated": "2025-07-06T08:36:43Z",
    "id": "2507.08833v1",
    "authors": [
      "Seokmin Ko"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08833v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08833v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08833v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses Low-Rank Adaptation (LoRA) for fine-tuning large language models (LLMs), which is a technique related to LLM optimization and efficiency. It does not directly fit into the other provided categories such as RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04289v1": {
    "title": "M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop\n  Reasoning in Medical Instructional Video Understanding",
    "summary": "With the rapid progress of artificial intelligence (AI) in multi-modal\nunderstanding, there is increasing potential for video comprehension\ntechnologies to support professional domains such as medical education.\nHowever, existing benchmarks suffer from two primary limitations: (1)\nLinguistic Singularity: they are largely confined to English, neglecting the\nneed for multilingual resources; and (2) Shallow Reasoning: their questions are\noften designed for surface-level information retrieval, failing to properly\nassess deep multi-modal integration. To address these limitations, we present\nM3-Med, the first benchmark for Multi-lingual, Multi-modal, and Multi-hop\nreasoning in Medical instructional video understanding. M3-Med consists of\nmedical questions paired with corresponding video segments, annotated by a team\nof medical experts. A key innovation of M3-Med is its multi-hop reasoning task,\nwhich requires a model to first locate a key entity in the text, then find\ncorresponding visual evidence in the video, and finally synthesize information\nacross both modalities to derive the answer. This design moves beyond simple\ntext matching and poses a substantial challenge to a model's deep cross-modal\nunderstanding capabilities. We define two tasks: Temporal Answer Grounding in\nSingle Video (TAGSV) and Temporal Answer Grounding in Video Corpus (TAGVC). We\nevaluated several state-of-the-art models and Large Language Models (LLMs) on\nM3-Med. The results reveal a significant performance gap between all models and\nhuman experts, especially on the complex multi-hop questions where model\nperformance drops sharply. M3-Med effectively highlights the current\nlimitations of AI models in deep cross-modal reasoning within specialized\ndomains and provides a new direction for future research.",
    "published": "2025-07-06T08:14:35Z",
    "updated": "2025-07-06T08:14:35Z",
    "id": "2507.04289v1",
    "authors": [
      "Shenxi Liu",
      "Kan Li",
      "Mingyang Zhao",
      "Yuhang Tian",
      "Bin Li",
      "Shoujun Zhou",
      "Hongliang Li",
      "Fuxia Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04289v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04289v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04289v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark for multi-lingual, multi-modal, and multi-hop reasoning in medical instructional video understanding, which involves evaluating models' abilities in deep cross-modal understanding and reasoning. This aligns with topics related to multimodal large language models (MLLM) and reasoning in LLMs.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.04278v3": {
    "title": "EMER-Ranker: Learning to Rank Emotion Descriptions in the Absence of\n  Ground Truth",
    "summary": "With the recent success of large language models, Explainable Multimodal\nEmotion Recognition (EMER), also known as Descriptive MER (DMER), has attracted\ngrowing attention from researchers. Unlike traditional discriminative methods\nthat rely on predefined emotion taxonomies, EMER aims to describe a person's\nemotional state using free-form natural language, thereby enabling fine-grained\nand interpretable emotion representations. However, this free-form prediction\nparadigm introduces significant challenges in evaluation. Existing approaches\neither depend on ground-truth descriptions, which require extensive manual\nannotations and often fail to capture the full complexity of human emotions, or\nsimplify the evaluation task by shifting focus from assessing descriptions to\nevaluating emotion labels. However, this simplification overlooks critical\naspects such as emotional temporal dynamics, intensity, and uncertainty. To\naddress these limitations, we propose EMER-Ranker, a novel evaluation strategy\nthat reformulates the traditional ``prediction-ground truth'' comparison into\nthe ``prediction-prediction'' comparison, eliminating the need for ground-truth\ndescriptions. We then apply the Bradley-Terry algorithm to convert pairwise\ncomparison outcomes into model-level rankings. Additionally, we explore the\npotential for automatic preference prediction and introduce EMER-Preference,\nthe first preference dataset specifically designed for human emotions. Our work\nadvances the field of EMER and lays the foundation for more intelligent\nhuman-computer interaction systems.",
    "published": "2025-07-06T07:37:59Z",
    "updated": "2025-07-14T09:39:44Z",
    "id": "2507.04278v3",
    "authors": [
      "Zheng Lian",
      "Licai Sun",
      "Haoyu Chen",
      "Zebang Cheng",
      "Fan Zhang",
      "Ziyu Jia",
      "Ziyang Ma",
      "Fei Ma",
      "Xiaojiang Peng",
      "Jianhua Tao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04278v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04278v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04278v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Explainable Multimodal Emotion Recognition (EMER) and introduces a novel evaluation strategy for emotion descriptions using large language models. It does not directly align with the provided topics which are more centered around LLM architectures, scaling, reasoning, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.04276v1": {
    "title": "FIXME: Towards End-to-End Benchmarking of LLM-Aided Design Verification",
    "summary": "Despite the transformative potential of Large Language Models (LLMs) in\nhardware design, a comprehensive evaluation of their capabilities in design\nverification remains underexplored. Current efforts predominantly focus on RTL\ngeneration and basic debugging, overlooking the critical domain of functional\nverification, which is the primary bottleneck in modern design methodologies\ndue to the rapid escalation of hardware complexity. We present FIXME, the first\nend-to-end, multi-model, and open-source evaluation framework for assessing LLM\nperformance in hardware functional verification (FV) to address this crucial\ngap. FIXME introduces a structured three-level difficulty hierarchy spanning\nsix verification sub-domains and 180 diverse tasks, enabling in-depth analysis\nacross the design lifecycle. Leveraging a collaborative AI-human approach, we\nconstruct a high-quality dataset using 100% silicon-proven designs, ensuring\ncomprehensive coverage of real-world challenges. Furthermore, we enhance the\nfunctional coverage by 45.57% through expert-guided optimization. By rigorously\nevaluating state-of-the-art LLMs such as GPT-4, Claude3, and LlaMA3, we\nidentify key areas for improvement and outline promising research directions to\nunlock the full potential of LLM-driven automation in hardware design\nverification. The benchmark is available at\nhttps://github.com/ChatDesignVerification/FIXME.",
    "published": "2025-07-06T07:32:01Z",
    "updated": "2025-07-06T07:32:01Z",
    "id": "2507.04276v1",
    "authors": [
      "Gwok-Waa Wan",
      "Shengchu Su",
      "Ruihu Wang",
      "Qixiang Chen",
      "Sam-Zaak Wong",
      "Mengnv Xing",
      "Hefei Feng",
      "Yubo Wang",
      "Yinan Zhu",
      "Jingyi Zhang",
      "Jianmin Ye",
      "Xinlai Wan",
      "Tao Ni",
      "Qiang Xu",
      "Nan Guan",
      "Zhe Jiang",
      "Xi Wang",
      "Yang Jun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04276v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04276v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04276v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking LLMs in the context of hardware design verification, which involves evaluating their performance and capabilities in a specific domain. This aligns with the 'Benchmark' topic as it involves creating a framework to assess LLM performance. Additionally, the mention of LLMs like GPT-4, Claude3, and LlaMA3 suggests relevance to the 'LLM' topic.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2507.04250v1": {
    "title": "Just Enough Shifts: Mitigating Over-Refusal in Aligned Language Models\n  with Targeted Representation Fine-Tuning",
    "summary": "Safety alignment is crucial for large language models (LLMs) to resist\nmalicious instructions but often results in over-refusals, where benign prompts\nare unnecessarily rejected, impairing user experience and model utility. We\nintroduce ACTOR (Activation-Based Training for Over-Refusal Reduction), a\nrobust and compute- and data-efficient training framework that minimizes\nover-refusals by leveraging internal activation patterns from diverse queries.\nACTOR precisely identifies and adjusts the activation components that trigger\nrefusals, providing stronger control over the refusal mechanism. By fine-tuning\nonly a single model layer, ACTOR effectively reduces over-refusals across\nmultiple benchmarks while maintaining the model's ability to handle harmful\nqueries and preserve overall utility.",
    "published": "2025-07-06T05:47:04Z",
    "updated": "2025-07-06T05:47:04Z",
    "id": "2507.04250v1",
    "authors": [
      "Mahavir Dabas",
      "Si Chen",
      "Charles Fleming",
      "Ming Jin",
      "Ruoxi Jia"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04250v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04250v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04250v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the safety alignment of large language models (LLMs) by reducing over-refusals, which is directly related to the research on LLMs and their alignment mechanisms.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.05296v1": {
    "title": "Integrating Generative AI in BIM Education: Insights from Classroom\n  Implementation",
    "summary": "This study evaluates the implementation of a Generative AI-powered rule\nchecking workflow within a graduate-level Building Information Modeling (BIM)\ncourse at a U.S. university. Over two semesters, 55 students participated in a\nclassroom-based pilot exploring the use of GenAI for BIM compliance tasks, an\narea with limited prior research. The instructional design included lectures on\nprompt engineering and AI-driven rule checking, followed by an assignment where\nstudents used a large language model (LLM) to identify code violations in\ndesigns using Autodesk Revit. Surveys and interviews were conducted to assess\nstudent workload, learning effectiveness, and overall experience, using the\nNASA-TLX scale and regression analysis. Findings indicate students generally\nachieved learning objectives but faced challenges such as difficulties\ndebugging AI-generated code and inconsistent tool performance, probably due to\ntheir limited prompt engineering experience. These issues increased cognitive\nand emotional strain, especially among students with minimal programming\nbackgrounds. Despite these challenges, students expressed strong interest in\nfuture GenAI applications, particularly with clear instructional support.",
    "published": "2025-07-06T03:41:04Z",
    "updated": "2025-07-06T03:41:04Z",
    "id": "2507.05296v1",
    "authors": [
      "Islem Sahraoui",
      "Kinam Kim",
      "Lu Gao",
      "Zia Din",
      "Ahmed Senouci"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05296v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05296v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05296v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a large language model (LLM) in an educational setting for BIM compliance tasks, which involves prompt engineering and AI-driven rule checking. The core focus is on the application of LLM in a specific domain rather than the broader topics listed.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.04224v2": {
    "title": "Fairness Evaluation of Large Language Models in Academic Library\n  Reference Services",
    "summary": "As libraries explore large language models (LLMs) for use in virtual\nreference services, a key question arises: Can LLMs serve all users equitably,\nregardless of demographics or social status? While they offer great potential\nfor scalable support, LLMs may also reproduce societal biases embedded in their\ntraining data, risking the integrity of libraries' commitment to equitable\nservice. To address this concern, we evaluate whether LLMs differentiate\nresponses across user identities by prompting six state-of-the-art LLMs to\nassist patrons differing in sex, race/ethnicity, and institutional role. We\nfound no evidence of differentiation by race or ethnicity, and only minor\nevidence of stereotypical bias against women in one model. LLMs demonstrated\nnuanced accommodation of institutional roles through the use of linguistic\nchoices related to formality, politeness, and domain-specific vocabularies,\nreflecting professional norms rather than discriminatory treatment. These\nfindings suggest that current LLMs show a promising degree of readiness to\nsupport equitable and contextually appropriate communication in academic\nlibrary reference services.",
    "published": "2025-07-06T03:28:24Z",
    "updated": "2025-07-23T15:08:40Z",
    "id": "2507.04224v2",
    "authors": [
      "Haining Wang",
      "Jason Clark",
      "Yueru Yan",
      "Star Bradley",
      "Ruiyang Chen",
      "Yiqiong Zhang",
      "Hengyi Fu",
      "Zuoyu Tian"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04224v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04224v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04224v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper evaluates the fairness of Large Language Models (LLMs) in academic library reference services, focusing on biases and equitable service. The core topic is the evaluation of LLMs for fairness and biases, which aligns with the 'LLM' category.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04221v1": {
    "title": "Context Tuning for In-Context Optimization",
    "summary": "We introduce Context Tuning, a simple and effective method to significantly\nenhance few-shot adaptation of language models (LLMs) without fine-tuning model\nparameters. While prompt-based adaptation techniques have demonstrated the\neffectiveness of lightweight adaptation methods for large language models\n(LLMs), they typically initialize a trainable prompt or prefix with irrelevant\ntokens for the task at hand. In contrast, Context Tuning initializes the\ntrainable prompt or prefix with task-specific demonstration examples,\nleveraging the model's inherent In-Context Learning (ICL) ability to extract\nrelevant information for improved few-shot learning performance. Extensive\nevaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard,\nand ARC demonstrate that Context Tuning outperforms traditional prompt-based\nadaptation methods and achieves competitive accuracy to Test-Time Training with\nsignificantly higher training efficiency.",
    "published": "2025-07-06T03:23:53Z",
    "updated": "2025-07-06T03:23:53Z",
    "id": "2507.04221v1",
    "authors": [
      "Jack Lu",
      "Ryan Teehan",
      "Zhenbang Yang",
      "Mengye Ren"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04221v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04221v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04221v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for enhancing few-shot adaptation of language models (LLMs) without fine-tuning, which is relevant to LLM research and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.04214v1": {
    "title": "Can Large Language Models Automate the Refinement of Cellular Network\n  Specifications?",
    "summary": "Cellular networks serve billions of users globally, yet concerns about\nreliability and security persist due to weaknesses in 3GPP standards. However,\ntraditional analysis methods, including manual inspection and automated tools,\nstruggle with increasingly expanding cellular network specifications. This\npaper investigates the feasibility of Large Language Models (LLMs) for\nautomated cellular network specification refinement. To advance it, we leverage\n200,000+ approved 3GPP Change Requests (CRs) that document specification\nrevisions, constructing a valuable dataset for domain tasks. We introduce\nCR-eval, a principled evaluation framework, and benchmark 16 state-of-the-art\nLLMs, demonstrating that top models can discover security-related weaknesses in\nover 127 out of 200 test cases within five trials. To bridge potential gaps, we\nexplore LLM specialization techniques, including fine-tuning an 8B model to\nmatch or surpass advanced LLMs like GPT-4o and DeepSeek-R1. Evaluations on 30\ncellular attacks identify open challenges for achieving full automation. These\nfindings confirm that LLMs can automate the refinement of cellular network\nspecifications and provide valuable insights to guide future research in this\ndirection.",
    "published": "2025-07-06T02:40:04Z",
    "updated": "2025-07-06T02:40:04Z",
    "id": "2507.04214v1",
    "authors": [
      "Jianshuo Dong",
      "Tianyi Zhang",
      "Feng Yan",
      "Yuanjie Li",
      "Hewu Li",
      "Han Qiu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04214v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04214v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04214v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in refining cellular network specifications, which involves the use of LLMs for domain-specific tasks and benchmarking their performance. The core topics are LLM and Benchmark, as the study evaluates LLMs' capabilities in a specific domain and benchmarks their performance.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.04206v1": {
    "title": "Mpemba Effect in Large-Language Model Training Dynamics: A Minimal\n  Analysis of the Valley-River model",
    "summary": "Learning rate (LR) schedules in large language model (LLM) training often\nfollow empirical templates: warm-up, constant plateau/stable phase, and decay\n(WSD). However, the mechanistic explanation for this strategy remains\nunderexplored, and the choice of plateau height and decay schedule is largely\nheuristic. In this paper, we connect training dynamics to a thermodynamic\nanalogy via the Mpemba effect - a phenomenon in which a hotter system cools\nfaster than a colder one when quenched into the same bath. We analyze a class\nof \"valley-river\" loss landscapes, where sharp (valley) directions equilibrate\nquickly, while flatter (river) directions govern global descent. The Mpemba\neffect provides an explanation for the necessity of the warm-up phase and\nmotivates a high plateau - rather than a low one - for accelerating loss\ndecrease during decay. We show that for certain loss landscapes, there exists\nan optimal plateau learning rate - the \"strong Mpemba point\" - at which the\nslowest mode vanishes, resulting in faster convergence during the decay phase.\nWe derive analytical conditions for its existence and estimate decay dynamics\nrequired to preserve the Mpemba advantage. Our minimal model and analysis offer\na principled justification for plateau-based schedulers and provide guidance\nfor tuning LR in LLMs with minimal hyperparameter sweep.",
    "published": "2025-07-06T01:34:12Z",
    "updated": "2025-07-06T01:34:12Z",
    "id": "2507.04206v1",
    "authors": [
      "Sibei Liu",
      "Zhijian Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04206v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04206v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04206v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the training dynamics of large language models (LLMs) and the mechanistic explanation for learning rate schedules, which is a core aspect of LLM research. It does not directly fit into the other provided categories.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04189v2": {
    "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for\n  Consistent and Interpretable Human Relationship Understanding",
    "summary": "Understanding character relationships is essential for interpreting complex\nnarratives and conducting socially grounded AI research. However, manual\nannotation is time-consuming and low in coverage, while large language models\n(LLMs) often produce hallucinated or logically inconsistent outputs. We present\nSymbolicThought, a human-in-the-loop framework that combines LLM-based\nextraction with symbolic reasoning. The system constructs editable character\nrelationship graphs, refines them using seven types of logical constraints, and\nenables real-time validation and conflict resolution through an interactive\ninterface. To support logical supervision and explainable social analysis, we\nrelease a dataset of 160 interpersonal relationships with corresponding logical\nstructures. Experiments show that SymbolicThought improves annotation accuracy\nand consistency while significantly reducing time cost, offering a practical\ntool for narrative understanding, explainable AI, and LLM evaluation.",
    "published": "2025-07-05T23:46:35Z",
    "updated": "2025-07-13T22:06:13Z",
    "id": "2507.04189v2",
    "authors": [
      "Runcong Zhao",
      "Qinglin Zhu",
      "Hainiu Xu",
      "Bin Liang",
      "Lin Gui",
      "Yulan He"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04189v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04189v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04189v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) with symbolic reasoning to improve understanding of human relationships, which involves reasoning abilities in LLMs and their application in complex problem solving.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.04185v1": {
    "title": "From Legal Text to Tech Specs: Generative AI's Interpretation of Consent\n  in Privacy Law",
    "summary": "Privacy law and regulation have turned to \"consent\" as the legitimate basis\nfor collecting and processing individuals' data. As governments have rushed to\nenshrine consent requirements in their privacy laws, such as the California\nConsumer Privacy Act (CCPA), significant challenges remain in understanding how\nthese legal mandates are operationalized in software. The opaque nature of\nsoftware development processes further complicates this translation. To address\nthis, we explore the use of Large Language Models (LLMs) in requirements\nengineering to bridge the gap between legal requirements and technical\nimplementation. This study employs a three-step pipeline that involves using an\nLLM to classify software use cases for compliance, generating LLM modifications\nfor non-compliant cases, and manually validating these changes against legal\nstandards. Our preliminary findings highlight the potential of LLMs in\nautomating compliance tasks, while also revealing limitations in their\nreasoning capabilities. By benchmarking LLMs against real-world use cases, this\nresearch provides insights into leveraging AI-driven solutions to enhance legal\ncompliance of software.",
    "published": "2025-07-05T23:36:05Z",
    "updated": "2025-07-05T23:36:05Z",
    "id": "2507.04185v1",
    "authors": [
      "Aniket Kesari",
      "Travis Breaux",
      "Tom Norton",
      "Sarah Santos",
      "Anmol Singhal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04185v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04185v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04185v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in translating legal requirements into technical specifications, focusing on compliance tasks and benchmarking LLMs against real-world use cases. This aligns with the topics of LLM (Large Language Models) and Benchmark (Benchmarking LLMs).",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.04149v1": {
    "title": "Large Language Models for Zero-Shot Multicultural Name Recognition",
    "summary": "The robust and accurate recognition of multicultural names, particularly\nthose not previously encountered, is a critical challenge in an increasingly\nglobalized digital landscape. Traditional methods often falter when confronted\nwith the vast diversity and novel permutations of names across different\nlinguistic and cultural backgrounds. This paper introduces a novel framework,\nPrompt-Engineered Fine-Tuning (PEFT) for Large Language Models (LLMs) with\nAdversarial Data Augmentation and Cultural Knowledge Graph Integration,\ndesigned to significantly enhance zero-shot multicultural name recognition. Our\napproach leverages the powerful linguistic understanding of pre-trained LLMs,\ntransforming the recognition task into a guided generation problem. Through\nmeticulous prompt engineering, dynamic integration of explicit cultural\nknowledge derived from knowledge graphs, and the strategic application of\nadversarial data augmentation, we equip the LLM with an unprecedented ability\nto infer the cultural origin of unseen names. Extensive experiments demonstrate\nthat our PEFT method consistently outperforms established deep learning\nbaselines, including advanced Bi-LSTM models with cultural tags, achieving an\nimpressive 93.1\\% overall accuracy and a remarkable 89.5\\% accuracy on\nchallenging zero-shot name identification. An in-depth ablation study confirms\nthe synergistic contribution of each component, while a human evaluation\nhighlights our method's performance approaching human expert judgment. This\nwork signifies a substantial leap in multicultural name recognition, offering a\nhighly effective and scalable solution for real-world applications.",
    "published": "2025-07-05T19:59:03Z",
    "updated": "2025-07-05T19:59:03Z",
    "id": "2507.04149v1",
    "authors": [
      "Thanakorn Phonchai",
      "Surasakdi Siripong",
      "Nicholas Patterson",
      "Owen Campbell"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04149v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04149v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04149v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) for multicultural name recognition, leveraging their linguistic understanding and fine-tuning techniques. The core topic is clearly centered around LLMs and their application in a specific task, which aligns with the 'LLM' category. The use of pre-trained models and fine-tuning also touches on 'Pretrain'.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.04142v1": {
    "title": "Dissecting Clinical Reasoning in Language Models: A Comparative Study of\n  Prompts and Model Adaptation Strategies",
    "summary": "Recent works on large language models (LLMs) have demonstrated the impact of\nprompting strategies and fine-tuning techniques on their reasoning\ncapabilities. Yet, their effectiveness on clinical natural language inference\n(NLI) remains underexplored. This study presents the first controlled\nevaluation of how prompt structure and efficient fine-tuning jointly shape\nmodel performance in clinical NLI. We inspect four classes of prompting\nstrategies to elicit reasoning in LLMs at different levels of abstraction, and\nevaluate their impact on a range of clinically motivated reasoning types. For\neach prompting strategy, we construct high-quality demonstrations using a\nfrontier model to distil multi-step reasoning capabilities into smaller models\n(4B parameters) via Low-Rank Adaptation (LoRA). Across different language\nmodels fine-tuned on the NLI4CT benchmark, we found that prompt type alone\naccounts for up to 44% of the variance in macro-F1. Moreover, LoRA fine-tuning\nyields consistent gains of +8 to 12 F1, raises output alignment above 97%, and\nnarrows the performance gap to GPT-4o-mini to within 7.1%. Additional\nexperiments on reasoning generalisation reveal that LoRA improves performance\nin 75% of the models on MedNLI and TREC Clinical Trials Track. Overall, these\nfindings demonstrate that (i) prompt structure is a primary driver of clinical\nreasoning performance, (ii) compact models equipped with strong prompts and\nLoRA can rival frontier-scale systems, and (iii) reasoning-type-aware\nevaluation is essential to uncover prompt-induced trade-offs. Our results\nhighlight the promise of combining prompt design and lightweight adaptation for\nmore efficient and trustworthy clinical NLP systems, providing insights on the\nstrengths and limitations of widely adopted prompting and parameter-efficient\ntechniques in highly specialised domains.",
    "published": "2025-07-05T19:43:54Z",
    "updated": "2025-07-05T19:43:54Z",
    "id": "2507.04142v1",
    "authors": [
      "Mael Jullien",
      "Marco Valentino",
      "Leonardo Ranaldi",
      "Andre Freitas"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04142v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04142v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04142v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the reasoning capabilities of large language models (LLMs) in clinical contexts, evaluating different prompting strategies and fine-tuning techniques. It aligns with the topics of 'Reasoning' due to its emphasis on LLM reasoning, and 'LLM' as it involves large language models and their adaptation strategies.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2507.04137v1": {
    "title": "Token Level Hallucination Detection via Variance in Language Models",
    "summary": "Large Language Models (LLMs) have demonstrated impressive generative\ncapabilities across diverse tasks but remain susceptible to hallucinations,\nconfidently generated yet factually incorrect outputs. We introduce a\nreference-free, token-level hallucination detection framework that leverages\nthe variance in token log-probabilities across multiple stochastic generations.\nUnlike prior methods that require ground-truth references or sentence-level\nverification, our approach is model-agnostic, interpretable, and suited for\nreal-time or post-hoc analysis. We evaluate our method on unanswerable question\nprompts from the SQuAD v2 dataset and benchmark across three autoregressive\nmodels of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both\nquantitative metrics and visual diagnostics, we show that token-level variance\nreliably highlights instability in model outputs and correlates with\nhallucination patterns. Our framework is lightweight, reproducible, and\nadaptable to multiple domains, offering a valuable diagnostic tool for\nanalyzing generative reliability in LLMs.",
    "published": "2025-07-05T19:20:59Z",
    "updated": "2025-07-05T19:20:59Z",
    "id": "2507.04137v1",
    "authors": [
      "Keshav Kumar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04137v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04137v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04137v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on detecting hallucinations in Large Language Models (LLMs) using token-level variance, which directly relates to the reliability and performance of LLMs. It does not specifically address multimodal models, reinforcement learning, or other specialized topics.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.04132v1": {
    "title": "An HTR-LLM Workflow for High-Accuracy Transcription and Analysis of\n  Abbreviated Latin Court Hand",
    "summary": "This article presents and validates an ideal, four-stage workflow for the\nhigh-accuracy transcription and analysis of challenging medieval legal\ndocuments. The process begins with a specialized Handwritten Text Recognition\n(HTR) model, itself created using a novel \"Clean Ground Truth\" curation method\nwhere a Large Language Model (LLM) refines the training data. This HTR model\nprovides a robust baseline transcription (Stage 1). In Stage 2, this baseline\nis fed, along with the original document image, to an LLM for multimodal\npost-correction, grounding the LLM's analysis and improving accuracy. The\ncorrected, abbreviated text is then expanded into full, scholarly Latin using a\nprompt-guided LLM (Stage 3). A final LLM pass performs Named-Entity Correction\n(NEC), regularizing proper nouns and generating plausible alternatives for\nambiguous readings (Stage 4). We validate this workflow through detailed case\nstudies, achieving Word Error Rates (WER) in the range of 2-7% against\nscholarly ground truths. The results demonstrate that this hybrid, multi-stage\napproach effectively automates the most laborious aspects of transcription\nwhile producing a high-quality, analyzable output, representing a powerful and\npractical solution for the current technological landscape.",
    "published": "2025-07-05T19:07:15Z",
    "updated": "2025-07-05T19:07:15Z",
    "id": "2507.04132v1",
    "authors": [
      "Joshua D. Isom"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04132v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04132v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04132v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a Large Language Model (LLM) in a workflow for transcription and analysis of medieval legal documents, which involves multimodal post-correction and named-entity correction. The core focus is on the application of LLMs in a specialized task, making 'LLM' the most relevant topic. The use of multimodal aspects also suggests relevance to 'MLLM'.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.04127v1": {
    "title": "BYOKG-RAG: Multi-Strategy Graph Retrieval for Knowledge Graph Question\n  Answering",
    "summary": "Knowledge graph question answering (KGQA) presents significant challenges due\nto the structural and semantic variations across input graphs. Existing works\nrely on Large Language Model (LLM) agents for graph traversal and retrieval; an\napproach that is sensitive to traversal initialization, as it is prone to\nentity linking errors and may not generalize well to custom (\"bring-your-own\")\nKGs. We introduce BYOKG-RAG, a framework that enhances KGQA by synergistically\ncombining LLMs with specialized graph retrieval tools. In BYOKG-RAG, LLMs\ngenerate critical graph artifacts (question entities, candidate answers,\nreasoning paths, and OpenCypher queries), and graph tools link these artifacts\nto the KG and retrieve relevant graph context. The retrieved context enables\nthe LLM to iteratively refine its graph linking and retrieval, before final\nanswer generation. By retrieving context from different graph tools, BYOKG-RAG\noffers a more general and robust solution for QA over custom KGs. Through\nexperiments on five benchmarks spanning diverse KG types, we demonstrate that\nBYOKG-RAG outperforms the second-best graph retrieval method by 4.5% points\nwhile showing better generalization to custom KGs. BYOKG-RAG framework is\nopen-sourced at https://github.com/awslabs/graphrag-toolkit.",
    "published": "2025-07-05T18:47:14Z",
    "updated": "2025-07-05T18:47:14Z",
    "id": "2507.04127v1",
    "authors": [
      "Costas Mavromatis",
      "Soji Adeshina",
      "Vassilis N. Ioannidis",
      "Zhen Han",
      "Qi Zhu",
      "Ian Robinson",
      "Bryan Thompson",
      "Huzefa Rangwala",
      "George Karypis"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04127v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04127v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04127v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in conjunction with graph retrieval tools for Knowledge Graph Question Answering (KGQA), which involves reasoning and retrieval-augmented generation. The focus is on enhancing LLM capabilities through iterative refinement and context retrieval, aligning with topics related to LLM reasoning and memory.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2507.04105v1": {
    "title": "Enhancing Robustness of LLM-Driven Multi-Agent Systems through\n  Randomized Smoothing",
    "summary": "This paper presents a defense framework for enhancing the safety of large\nlanguage model (LLM) empowered multi-agent systems (MAS) in safety-critical\ndomains such as aerospace. We apply randomized smoothing, a statistical\nrobustness certification technique, to the MAS consensus context, enabling\nprobabilistic guarantees on agent decisions under adversarial influence. Unlike\ntraditional verification methods, our approach operates in black-box settings\nand employs a two-stage adaptive sampling mechanism to balance robustness and\ncomputational efficiency. Simulation results demonstrate that our method\neffectively prevents the propagation of adversarial behaviors and\nhallucinations while maintaining consensus performance. This work provides a\npractical and scalable path toward safe deployment of LLM-based MAS in\nreal-world, high-stakes environments.",
    "published": "2025-07-05T17:26:08Z",
    "updated": "2025-07-05T17:26:08Z",
    "id": "2507.04105v1",
    "authors": [
      "Jinwei Hu",
      "Yi Dong",
      "Zhengtao Ding",
      "Xiaowei Huang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04105v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04105v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04105v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses enhancing the robustness of LLM-driven multi-agent systems, which involves both LLM and RL (Reinforcement Learning) aspects, particularly in the context of multi-agent systems and adversarial robustness.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.04099v2": {
    "title": "Conversation Forests: The Key to Fine Tuning Large Language Models for\n  Multi-Turn Medical Conversations is Branching",
    "summary": "Fine-tuning methods such as Direct Preference Optimization (DPO) and Group\nRelative Policy Optimization (GRPO) have demonstrated success in training large\nlanguage models (LLMs) for single-turn tasks. However, these methods fall short\nin multi-turn applications, such as diagnostic patient interviewing, where\nunderstanding how early conversational turns influence downstream completions\nand outcomes is essential. In medicine, a multi-turn perspective is critical\nfor learning diagnostic schemas and better understanding conversation dynamics.\nTo address this gap, I introduce Savage Conversation Forests (SCF), a\nreinforcement learning framework that leverages a branched conversation\narchitecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple\npossible conversation continuations at each turn, enabling the model to learn\nhow different early responses affect downstream interactions and diagnostic\noutcomes. In experiments simulating doctor-patient conversations, SCF with\nbranching outperforms linear conversation architectures on diagnostic accuracy.\nI hypothesize that SCF's improvements stem from its ability to provide richer,\ninterdependent training signals across conversation turns. These results\nsuggest that a branched training architecture is an important strategy for fine\ntuning LLMs in complex multi-turn conversational tasks.",
    "published": "2025-07-05T16:49:34Z",
    "updated": "2025-07-15T16:49:25Z",
    "id": "2507.04099v2",
    "authors": [
      "Thomas Savage"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04099v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04099v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04099v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses fine-tuning large language models (LLMs) for multi-turn medical conversations using a reinforcement learning framework, which aligns with the topics of LLM and RL. The focus on multi-turn dialogue and diagnostic outcomes also suggests relevance to the Reasoning topic.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Reasoning"
    ]
  },
  "2507.05292v1": {
    "title": "A LLM-Driven Multi-Agent Systems for Professional Development of\n  Mathematics Teachers",
    "summary": "Professional development (PD) serves as the cornerstone for teacher tutors to\ngrasp content knowledge. However, providing equitable and timely PD\nopportunities for teachers poses significant challenges. To address this issue,\nwe introduce I-VIP (Intelligent Virtual Interactive Program), an intelligent\ntutoring platform for teacher professional development, driven by large\nlanguage models (LLMs) and supported by multi-agent frameworks. This platform\noffers a user-friendly conversational interface and allows users to employ a\nvariety of interactive tools to facilitate question answering, knowledge\ncomprehension, and reflective summarization while engaging in dialogue. To\nunderpin the functionality of this platform, including knowledge expectation\nanalysis, response scoring and classification, and feedback generation, the\nmulti-agent frameworks are leveraged to enhance the accuracy of judgments and\nmitigate the issue of missing key points.",
    "published": "2025-07-05T15:21:30Z",
    "updated": "2025-07-05T15:21:30Z",
    "id": "2507.05292v1",
    "authors": [
      "Kaiqi Yang",
      "Hang Li",
      "Yucheng Chu",
      "Ahreum Han",
      "Yasemin Copur-Gencturk",
      "Jiliang Tang",
      "Hui Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05292v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05292v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05292v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) and multi-agent frameworks to create an intelligent tutoring platform for professional development, which aligns with the topics of LLM and RL (Reinforcement Learning with multi-agent frameworks).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.04069v1": {
    "title": "Beyond Independent Passages: Adaptive Passage Combination Retrieval for\n  Retrieval Augmented Open-Domain Question Answering",
    "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external documents at inference time, enabling up-to-date\nknowledge access without costly retraining. However, conventional RAG methods\nretrieve passages independently, often leading to redundant, noisy, or\ninsufficiently diverse context-particularly problematic - particularly\nproblematic in noisy corpora and for multi-hop questions. To address this, we\npropose Adaptive Passage Combination Retrieval (AdaPCR), a novel framework for\nopen-domain question answering with black-box LMs. AdaPCR explicitly models\ndependencies between passages by considering passage combinations as units for\nretrieval and reranking. It consists of a context-aware query reformulation\nusing concatenated passages, and a reranking step trained with a predictive\nobjective aligned with downstream answer likelihood. Crucially, AdaPCR\nadaptively selects the number of retrieved passages without additional stopping\nmodules. Experiments across several QA benchmarks show that AdaPCR outperforms\nbaselines, particularly in multi-hop reasoning, demonstrating the effectiveness\nof modeling inter-passage dependencies for improved retrieval.",
    "published": "2025-07-05T15:10:12Z",
    "updated": "2025-07-05T15:10:12Z",
    "id": "2507.04069v1",
    "authors": [
      "Ting-Wen Ko",
      "Jyun-Yu Jiang",
      "Pu-Jen Cheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04069v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04069v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04069v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on retrieval-augmented generation (RAG) for large language models (LLMs), specifically addressing the issue of independent passage retrieval by proposing a novel framework that models dependencies between passages. This aligns with the 'Memory' topic, which includes retrieval-based methods and long-context processing. Additionally, the paper's emphasis on improving retrieval for open-domain question answering and multi-hop reasoning connects it to the 'Reasoning' topic, which involves complex problem solving in LLMs.",
    "llm_cls_result": [
      "Memory",
      "Reasoning"
    ]
  },
  "2507.04067v1": {
    "title": "HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration",
    "summary": "Contemporary multi-agent systems encounter persistent challenges in\ncross-platform interoperability, dynamic task scheduling, and efficient\nresource sharing. Agents with heterogeneous implementations often lack\nstandardized interfaces; collaboration frameworks remain brittle and hard to\nextend; scheduling policies are static; and inter-agent state synchronization\nis insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular\nframework comprising five layers-User, Workflow, Operator, Agent, and\nResource-and supported by sixteen standardized interfaces. HAWK delivers an\nend-to-end pipeline covering task parsing, workflow orchestration, intelligent\nscheduling, resource invocation, and data synchronization. At its core lies an\nadaptive scheduling and optimization module in the Workflow Layer, which\nharnesses real-time feedback and dynamic strategy adjustment to maximize\nutilization. The Resource Layer provides a unified abstraction over\nheterogeneous data sources, large models, physical devices, and third-party\nservices&tools, simplifying cross-domain information retrieval. We demonstrate\nHAWK's scalability and effectiveness via CreAgentive, a multi-agent\nnovel-generation prototype, which achieves marked gains in throughput, lowers\ninvocation complexity, and improves system controllability. We also show how\nhybrid deployments of large language models integrate seamlessly within HAWK,\nhighlighting its flexibility. Finally, we outline future research\navenues-hallucination mitigation, real-time performance tuning, and enhanced\ncross-domain adaptability-and survey prospective applications in healthcare,\ngovernment, finance, and education.",
    "published": "2025-07-05T15:03:53Z",
    "updated": "2025-07-05T15:03:53Z",
    "id": "2507.04067v1",
    "authors": [
      "Yuyang Cheng",
      "Yumiao Xu",
      "Chaojia Yu",
      "Yong Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04067v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04067v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04067v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a hierarchical workflow framework for multi-agent collaboration, which involves dynamic task scheduling, resource sharing, and integration of large language models. This aligns with topics related to Reinforcement Learning (RL) and Artificial General Intelligence (AGI), as it involves intelligent scheduling and optimization, and multi-agent systems.",
    "llm_cls_result": [
      "RL",
      "AGI"
    ]
  },
  "2507.04055v1": {
    "title": "Rethinking and Exploring String-Based Malware Family Classification in\n  the Era of LLMs and RAG",
    "summary": "Malware Family Classification (MFC) aims to identify the fine-grained family\n(e.g., GuLoader or BitRAT) to which a potential malware sample belongs, in\ncontrast to malware detection or sample classification that predicts only an\nYes/No. Accurate family identification can greatly facilitate automated sample\nlabeling and understanding on crowdsourced malware analysis platforms such as\nVirusTotal and MalwareBazaar, which generate vast amounts of data daily. In\nthis paper, we explore and assess the feasibility of using traditional binary\nstring features for MFC in the new era of large language models (LLMs) and\nRetrieval-Augmented Generation (RAG). Specifically, we investigate how\nFamily-Specific String (FSS) features could be utilized in a manner similar to\nRAG to facilitate MFC. To this end, we develop a curated evaluation framework\ncovering 4,347 samples from 67 malware families, extract and analyze over 25\nmillion strings, and conduct detailed ablation studies to assess the impact of\ndifferent design choices in four major modules.",
    "published": "2025-07-05T14:36:13Z",
    "updated": "2025-07-05T14:36:13Z",
    "id": "2507.04055v1",
    "authors": [
      "Yufan Chen",
      "Daoyuan Wu",
      "Juantao Zhong",
      "Zicheng Zhang",
      "Debin Gao",
      "Shuai Wang",
      "Yingjiu Li",
      "Ning Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04055v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04055v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04055v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) and Retrieval-Augmented Generation (RAG) in the context of malware family classification, which directly relates to LLM and Memory topics.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.04053v1": {
    "title": "TopoMAS: Large Language Model Driven Topological Materials Multiagent\n  System",
    "summary": "Topological materials occupy a frontier in condensed-matter physics thanks to\ntheir remarkable electronic and quantum properties, yet their cross-scale\ndesign remains bottlenecked by inefficient discovery workflows. Here, we\nintroduce TopoMAS (Topological materials Multi-Agent System), an interactive\nhuman-AI framework that seamlessly orchestrates the entire materials-discovery\npipeline: from user-defined queries and multi-source data retrieval, through\ntheoretical inference and crystal-structure generation, to first-principles\nvalidation. Crucially, TopoMAS closes the loop by autonomously integrating\ncomputational outcomes into a dynamic knowledge graph, enabling continuous\nknowledge refinement. In collaboration with human experts, it has already\nguided the identification of novel topological phases SrSbO3, confirmed by\nfirst-principles calculations. Comprehensive benchmarks demonstrate robust\nadaptability across base Large Language Model, with the lightweight Qwen2.5-72B\nmodel achieving 94.55% accuracy while consuming only 74.3-78.4% of tokens\nrequired by Qwen3-235B and 83.0% of DeepSeek-V3's usage--delivering responses\ntwice as fast as Qwen3-235B. This efficiency establishes TopoMAS as an\naccelerator for computation-driven discovery pipelines. By harmonizing rational\nagent orchestration with a self-evolving knowledge graph, our framework not\nonly delivers immediate advances in topological materials but also establishes\na transferable, extensible paradigm for materials-science domain.",
    "published": "2025-07-05T14:23:12Z",
    "updated": "2025-07-05T14:23:12Z",
    "id": "2507.04053v1",
    "authors": [
      "Baohua Zhang",
      "Xin Li",
      "Huangchao Xu",
      "Zhong Jin",
      "Quansheng Wu",
      "Ce Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04053v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04053v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04053v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multi-agent system (TopoMAS) that utilizes Large Language Models (LLMs) for the discovery of topological materials, involving human-AI collaboration, knowledge refinement, and efficiency benchmarks of different LLMs.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.04043v1": {
    "title": "Evaluating the Effectiveness of Large Language Models in Solving Simple\n  Programming Tasks: A User-Centered Study",
    "summary": "As large language models (LLMs) become more common in educational tools and\nprogramming environments, questions arise about how these systems should\ninteract with users. This study investigates how different interaction styles\nwith ChatGPT-4o (passive, proactive, and collaborative) affect user performance\non simple programming tasks. I conducted a within-subjects experiment where\nfifteen high school students participated, completing three problems under\nthree distinct versions of the model. Each version was designed to represent a\nspecific style of AI support: responding only when asked, offering suggestions\nautomatically, or engaging the user in back-and-forth dialogue.Quantitative\nanalysis revealed that the collaborative interaction style significantly\nimproved task completion time compared to the passive and proactive conditions.\nParticipants also reported higher satisfaction and perceived helpfulness when\nworking with the collaborative version. These findings suggest that the way an\nLLM communicates, how it guides, prompts, and responds, can meaningfully impact\nlearning and performance. This research highlights the importance of designing\nLLMs that go beyond functional correctness to support more interactive,\nadaptive, and user-centered experiences, especially for novice programmers.",
    "published": "2025-07-05T13:52:31Z",
    "updated": "2025-07-05T13:52:31Z",
    "id": "2507.04043v1",
    "authors": [
      "Kai Deng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04043v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04043v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04043v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the interaction styles of Large Language Models (LLMs) in educational and programming contexts, specifically examining user performance and satisfaction. The core topics are related to LLMs and their application in user-centered studies.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.04036v1": {
    "title": "PresentAgent: Multimodal Agent for Presentation Video Generation",
    "summary": "We present PresentAgent, a multimodal agent that transforms long-form\ndocuments into narrated presentation videos. While existing approaches are\nlimited to generating static slides or text summaries, our method advances\nbeyond these limitations by producing fully synchronized visual and spoken\ncontent that closely mimics human-style presentations. To achieve this\nintegration, PresentAgent employs a modular pipeline that systematically\nsegments the input document, plans and renders slide-style visual frames,\ngenerates contextual spoken narration with large language models and\nText-to-Speech models, and seamlessly composes the final video with precise\naudio-visual alignment. Given the complexity of evaluating such multimodal\noutputs, we introduce PresentEval, a unified assessment framework powered by\nVision-Language Models that comprehensively scores videos across three critical\ndimensions: content fidelity, visual clarity, and audience comprehension\nthrough prompt-based evaluation. Our experimental validation on a curated\ndataset of 30 document-presentation pairs demonstrates that PresentAgent\napproaches human-level quality across all evaluation metrics. These results\nhighlight the significant potential of controllable multimodal agents in\ntransforming static textual materials into dynamic, effective, and accessible\npresentation formats. Code will be available at\nhttps://github.com/AIGeeksGroup/PresentAgent.",
    "published": "2025-07-05T13:24:15Z",
    "updated": "2025-07-05T13:24:15Z",
    "id": "2507.04036v1",
    "authors": [
      "Jingwei Shi",
      "Zeyu Zhang",
      "Biao Wu",
      "Yanjie Liang",
      "Meng Fang",
      "Ling Chen",
      "Yang Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04036v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04036v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04036v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multimodal agent that integrates vision and language to generate presentation videos, which aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA). Additionally, the use of large language models for generating spoken narration is relevant to the LLM topic.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "LLM"
    ]
  },
  "2507.04023v1": {
    "title": "LLMThinkBench: Towards Basic Math Reasoning and Overthinking in Large\n  Language Models",
    "summary": "Large Language Models (LLMs) have achieved remarkable performance on complex\nmathematical benchmarks, yet often struggle with simple arithmetic tasks and\nexhibit a tendency toward over-explaining or \"overthinking\" answers. To\nsystematically assess this phenomenon, we introduce LLMThinkBench, a modular\nbenchmarking framework that enables researchers to evaluate basic math\nreasoning and overthinking in LLMs. The framework provides 14 configurable math\ntasks with randomized test data generation and robust parsing strategies.\nResearchers can quantify overthinking using our Overthinking Score metric,\nwhich captures accuracy-verbosity tradeoffs through harmonic mean formulation.\nThe tool offers flexible evaluation with a scalable vLLM/Transformers backend,\nmulti-GPU support, and full configurability. Users can extend the tool with\ncustom tasks, reproduce experiments with seeding, and generate detailed\nefficiency reports. Distributed as a pip-installable package with CLI and API\naccess, LLMThinkBench provides researchers and practitioners an accessible,\ncost-effective alternative to expensive LLM-as-a-judge methods for diagnosing\nbasic reasoning capabilities and efficiency analysis. Package can be installed\nas: pip install llmthinkbench",
    "published": "2025-07-05T12:31:17Z",
    "updated": "2025-07-05T12:31:17Z",
    "id": "2507.04023v1",
    "authors": [
      "Gaurav Srivastava",
      "Aafiya Hussain",
      "Sriram Srinivasan",
      "Xuan Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04023v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04023v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04023v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmarking framework for evaluating basic math reasoning and overthinking in LLMs, which aligns with the 'Benchmark' and 'Reasoning' topics.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning"
    ]
  },
  "2507.04014v1": {
    "title": "Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a\n  Focus on Korean Superstition",
    "summary": "As large language models (LLMs) become key advisors in various domains, their\ncultural sensitivity and reasoning skills are crucial in multicultural\nenvironments. We introduce Nunchi-Bench, a benchmark designed to evaluate LLMs'\ncultural understanding, with a focus on Korean superstitions. The benchmark\nconsists of 247 questions spanning 31 topics, assessing factual knowledge,\nculturally appropriate advice, and situational interpretation. We evaluate\nmultilingual LLMs in both Korean and English to analyze their ability to reason\nabout Korean cultural contexts and how language variations affect performance.\nTo systematically assess cultural reasoning, we propose a novel evaluation\nstrategy with customized scoring metrics that capture the extent to which\nmodels recognize cultural nuances and respond appropriately. Our findings\nhighlight significant challenges in LLMs' cultural reasoning. While models\ngenerally recognize factual information, they struggle to apply it in practical\nscenarios. Furthermore, explicit cultural framing enhances performance more\neffectively than relying solely on the language of the prompt. To support\nfurther research, we publicly release Nunchi-Bench alongside a leaderboard.",
    "published": "2025-07-05T11:52:09Z",
    "updated": "2025-07-05T11:52:09Z",
    "id": "2507.04014v1",
    "authors": [
      "Kyuhee Kim",
      "Sangah Lee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04014v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04014v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04014v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking language models (LLMs) on cultural reasoning, specifically evaluating their understanding of Korean superstitions. This aligns with the 'Benchmark' topic as it involves creating a benchmark to assess LLM performance in a specific domain (cultural reasoning). Additionally, the mention of LLMs and their reasoning abilities also relates to the 'Reasoning' topic.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning"
    ]
  },
  "2507.04009v1": {
    "title": "Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM\n  Fine-Tuning Data from Unstructured Documents",
    "summary": "Large language models (LLMs) have shown impressive performance on\ngeneral-purpose tasks, yet adapting them to specific domains remains\nchallenging due to the scarcity of high-quality domain data. Existing data\nsynthesis tools often struggle to extract reliable fine-tuning data from\nheterogeneous documents effectively. To address this limitation, we propose\nEasy Dataset, a unified framework for synthesizing fine-tuning data from\nunstructured documents via an intuitive graphical user interface (GUI).\nSpecifically, Easy Dataset allows users to easily configure text extraction\nmodels and chunking strategies to transform raw documents into coherent text\nchunks. It then leverages a persona-driven prompting approach to generate\ndiverse question-answer pairs using public-available LLMs. Throughout the\npipeline, a human-in-the-loop visual interface facilitates the review and\nrefinement of intermediate outputs to ensure data quality. Experiments on a\nfinancial question-answering task show that fine-tuning LLMs on the synthesized\ndataset significantly improves domain-specific performance while preserving\ngeneral knowledge. The source code and installable package are available at\nhttps://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub\nstars.",
    "published": "2025-07-05T11:38:59Z",
    "updated": "2025-07-05T11:38:59Z",
    "id": "2507.04009v1",
    "authors": [
      "Ziyang Miao",
      "Qiyu Sun",
      "Jingyuan Wang",
      "Yuchen Gong",
      "Yaowei Zheng",
      "Shiqi Li",
      "Richong Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04009v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04009v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04009v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on synthesizing fine-tuning data for LLMs from unstructured documents, which involves dataset creation and fine-tuning strategies for domain-specific adaptation of LLMs.",
    "llm_cls_result": [
      "Dataset",
      "LLM",
      "Pretrain"
    ]
  },
  "2507.04005v1": {
    "title": "Exploring a Gamified Personality Assessment Method through Interaction\n  with Multi-Personality LLM Agents",
    "summary": "The execution of effective and imperceptible personality assessments is\nreceiving increasing attention in psychology and human-computer interaction\nfields. This study explores an interactive approach for personality assessment,\nfocusing on the multiplicity of personality representation. We propose a\nframework of gamified personality assessment through multi-personality\nrepresentations (Multi-PR GPA). The framework leverages Large Language Models\nto empower virtual agents with diverse personalities. These agents elicit\nmultifaceted human personality representations through engaging in interactive\ngames. Drawing upon the multi-type textual data generated throughout the\ninteraction, it achieves two ways of personality assessments (i.e., Direct\nAssessment and Que-based Assessment) and provides interpretable insights.\nGrounded in the classic Big Five theory, we implemented a prototype system and\nconducted a user study to assess the efficacy of Multi-PR GPA. The results\nunderscore the effectiveness of our approach in personality assessment and\ndemonstrate that it achieves superior performance when considering the\nmultiplicity of personality representation.",
    "published": "2025-07-05T11:17:20Z",
    "updated": "2025-07-05T11:17:20Z",
    "id": "2507.04005v1",
    "authors": [
      "Baiqiao Zhang",
      "Xiangxian Li",
      "Chao Zhou",
      "Xinyu Gai",
      "Zhifeng Liao",
      "Juan Liu",
      "Xue Yang",
      "Niqi Liu",
      "Xiaojuan Ma",
      "Yong-jin Liu",
      "Yulong Bian"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04005v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04005v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04005v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to create multi-personality agents for gamified personality assessments, which aligns with the topics of LLM and RL (Reinforcement Learning) due to the interactive and agent-based nature of the approach.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.05289v2": {
    "title": "Measuring how changes in code readability attributes affect code quality\n  evaluation by Large Language Models",
    "summary": "Code readability is one of the main aspects of code quality, influenced by\nvarious properties like identifier names, comments, code structure, and\nadherence to standards. However, measuring this attribute poses challenges in\nboth industry and academia. While static analysis tools assess attributes such\nas code smells and comment percentage, code reviews introduce an element of\nsubjectivity. This paper explores using Large Language Models (LLMs) to\nevaluate code quality attributes related to its readability in a standardized,\nreproducible, and consistent manner. We conducted a quasi-experiment study to\nmeasure the effects of code changes on Large Language Model (LLM)s\ninterpretation regarding its readability quality attribute. Nine LLMs were\ntested, undergoing three interventions: removing comments, replacing identifier\nnames with obscure names, and refactoring to remove code smells. Each\nintervention involved 10 batch analyses per LLM, collecting data on response\nvariability. We compared the results with a known reference model and tool. The\nresults showed that all LLMs were sensitive to the interventions, with\nagreement with the reference classifier being high for the original and\nrefactored code scenarios. The LLMs demonstrated a strong semantic sensitivity\nthat the reference model did not fully capture. A thematic analysis of the LLMs\nreasoning confirmed their evaluations directly reflected the nature of each\nintervention. The models also exhibited response variability, with 9.37% to\n14.58% of executions showing a standard deviation greater than zero, indicating\nresponse oscillation, though this did not always compromise the statistical\nsignificance of the results. LLMs demonstrated potential for evaluating\nsemantic quality aspects, such as coherence between identifier names, comments,\nand documentation with code purpose.",
    "published": "2025-07-05T11:08:03Z",
    "updated": "2025-07-09T18:24:41Z",
    "id": "2507.05289v2",
    "authors": [
      "Igor Regis da Silva Simoes",
      "Elaine Venson"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05289v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05289v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05289v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) to evaluate code quality attributes related to readability, which directly involves LLMs in assessing and interpreting code. The study tests multiple LLMs and analyzes their responses to various code interventions, highlighting their semantic sensitivity and potential for evaluating code quality.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.04003v1": {
    "title": "Seamlessly Integrating Tree-Based Positional Embeddings into Transformer\n  Models for Source Code Representation",
    "summary": "Transformer-based models have demonstrated significant success in various\nsource code representation tasks. Nonetheless, traditional positional\nembeddings employed by these models inadequately capture the hierarchical\nstructure intrinsic to source code, typically represented as Abstract Syntax\nTrees (ASTs). To address this, we propose a novel tree-based positional\nembedding approach that explicitly encodes hierarchical relationships derived\nfrom ASTs, including node depth and sibling indices. These hierarchical\nembeddings are integrated into the transformer architecture, specifically\nenhancing the CodeBERTa model. We thoroughly evaluate our proposed model\nthrough masked language modeling (MLM) pretraining and clone detection\nfine-tuning tasks. Experimental results indicate that our Tree-Enhanced\nCodeBERTa consistently surpasses the baseline model in terms of loss, accuracy,\nF1 score, precision, and recall, emphasizing the importance of incorporating\nexplicit structural information into transformer-based representations of\nsource code.",
    "published": "2025-07-05T11:07:47Z",
    "updated": "2025-07-05T11:07:47Z",
    "id": "2507.04003v1",
    "authors": [
      "Patryk Bartkowiak",
      "Filip Graliski"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04003v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04003v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04003v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing transformer models for source code representation by integrating tree-based positional embeddings, which is not directly related to the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.04000v1": {
    "title": "Leveraging Multimodal Data and Side Users for Diffusion Cross-Domain\n  Recommendation",
    "summary": "Cross-domain recommendation (CDR) aims to address the persistent cold-start\nproblem in Recommender Systems. Current CDR research concentrates on\ntransferring cold-start users' information from the auxiliary domain to the\ntarget domain. However, these systems face two main issues: the\nunderutilization of multimodal data, which hinders effective cross-domain\nalignment, and the neglect of side users who interact solely within the target\ndomain, leading to inadequate learning of the target domain's vector space\ndistribution. To address these issues, we propose a model leveraging Multimodal\ndata and Side users for diffusion Cross-domain recommendation (MuSiC). We first\nemploy a multimodal large language model to extract item multimodal features\nand leverage a large language model to uncover user features using prompt\nlearning without fine-tuning. Secondly, we propose the cross-domain diffusion\nmodule to learn the generation of feature vectors in the target domain. This\napproach involves learning feature distribution from side users and\nunderstanding the patterns in cross-domain transformation through overlapping\nusers. Subsequently, the trained diffusion module is used to generate feature\nvectors for cold-start users in the target domain, enabling the completion of\ncross-domain recommendation tasks. Finally, our experimental evaluation of the\nAmazon dataset confirms that MuSiC achieves state-of-the-art performance,\nsignificantly outperforming all selected baselines. Our code is available:\nhttps://anonymous.4open.science/r/MuSiC-310A/.",
    "published": "2025-07-05T10:57:29Z",
    "updated": "2025-07-05T10:57:29Z",
    "id": "2507.04000v1",
    "authors": [
      "Fan Zhang",
      "Jinpeng Chen",
      "Huan Li",
      "Senzhang Wang",
      "Yuan Cao",
      "Kaimin Wei",
      "JianXiang He",
      "Feifei Kou",
      "Jinqing Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04000v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04000v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04000v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging multimodal data and large language models for cross-domain recommendation, which involves the use of multimodal large language models (MLLM) and large language models (LLM) for feature extraction and user feature uncovering.",
    "llm_cls_result": [
      "MLLM",
      "LLM"
    ]
  },
  "2507.03998v1": {
    "title": "Toward Better Generalisation in Uncertainty Estimators: Leveraging\n  Data-Agnostic Features",
    "summary": "Large Language Models (LLMs) often generate responses that are factually\nincorrect yet expressed with high confidence, which can pose serious risks for\nend users. To address this, it is essential for LLMs not only to produce\nanswers but also to provide accurate estimates of their correctness.\nUncertainty quantification methods have been introduced to assess the quality\nof LLM outputs, with factual accuracy being a key aspect of that quality. Among\nthese methods, those that leverage hidden states to train probes have shown\nparticular promise, as these internal representations encode information\nrelevant to the factuality of responses, making this approach the focus of this\npaper. However, the probe trained on the hidden states of one dataset often\nstruggles to generalise to another dataset of a different task or domain. To\naddress this limitation, we explore combining data-agnostic features with\nhidden-state features and assess whether this hybrid feature set enhances\nout-of-domain performance. We further examine whether selecting only the most\ninformative hidden-state features, thereby discarding task-specific noise,\nenables the data-agnostic features to contribute more effectively. The\nexperiment results indicate that although introducing data-agnostic features\ngenerally enhances generalisation performance in most cases, in certain\nscenarios their inclusion degrades performance. A similar pattern emerges when\nretaining only the most important hidden-state features - adding data-agnostic\nfeatures does not consistently further enhance performance compared to using\nthe full set of hidden-state features. A closer analysis reveals that, in some\nspecific cases, the trained probe underweights the data-agnostic features\nrelative to the hidden-state features, which we believe is the main reason why\nthe results are inconclusive.",
    "published": "2025-07-05T10:55:36Z",
    "updated": "2025-07-05T10:55:36Z",
    "id": "2507.03998v1",
    "authors": [
      "Thuy An Ha",
      "Bao Quoc Vo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03998v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03998v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03998v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving uncertainty quantification methods for Large Language Models (LLMs) by leveraging data-agnostic features and hidden-state features. It addresses the generalization issues of probes trained on hidden states and explores hybrid feature sets to enhance out-of-domain performance. The core topics are related to LLMs and their uncertainty estimation, which falls under the 'LLM' category. The exploration of hidden-state features and their relevance to factuality also touches on 'Reasoning' as it involves assessing the correctness of LLM outputs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.05288v1": {
    "title": "A Survey on Proactive Defense Strategies Against Misinformation in Large\n  Language Models",
    "summary": "The widespread deployment of large language models (LLMs) across critical\ndomains has amplified the societal risks posed by algorithmically generated\nmisinformation. Unlike traditional false content, LLM-generated misinformation\ncan be self-reinforcing, highly plausible, and capable of rapid propagation\nacross multiple languages, which traditional detection methods fail to mitigate\neffectively. This paper introduces a proactive defense paradigm, shifting from\npassive post hoc detection to anticipatory mitigation strategies. We propose a\nThree Pillars framework: (1) Knowledge Credibility, fortifying the integrity of\ntraining and deployed data; (2) Inference Reliability, embedding\nself-corrective mechanisms during reasoning; and (3) Input Robustness,\nenhancing the resilience of model interfaces against adversarial attacks.\nThrough a comprehensive survey of existing techniques and a comparative\nmeta-analysis, we demonstrate that proactive defense strategies offer up to\n63\\% improvement over conventional methods in misinformation prevention,\ndespite non-trivial computational overhead and generalization challenges. We\nargue that future research should focus on co-designing robust knowledge\nfoundations, reasoning certification, and attack-resistant interfaces to ensure\nLLMs can effectively counter misinformation across varied domains.",
    "published": "2025-07-05T09:52:21Z",
    "updated": "2025-07-05T09:52:21Z",
    "id": "2507.05288v1",
    "authors": [
      "Shuliang Liu",
      "Hongyi Liu",
      "Aiwei Liu",
      "Bingchen Duan",
      "Qi Zheng",
      "Yibo Yan",
      "He Geng",
      "Peijie Jiang",
      "Jia Liu",
      "Xuming Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05288v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05288v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05288v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses proactive defense strategies against misinformation in Large Language Models (LLMs), focusing on enhancing the integrity and reliability of LLMs. The core topics are related to LLM research and the challenges associated with misinformation, which aligns with the 'LLM' and 'Reasoning' categories.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.03958v1": {
    "title": "A Comparative Study of Specialized LLMs as Dense Retrievers",
    "summary": "While large language models (LLMs) are increasingly deployed as dense\nretrievers, the impact of their domain-specific specialization on retrieval\neffectiveness remains underexplored. This investigation systematically examines\nhow task-specific adaptations in LLMs influence their retrieval capabilities,\nan essential step toward developing unified retrievers capable of handling\ntext, code, images, and multimodal content. We conduct extensive experiments\nwith eight Qwen2.5 7B LLMs, including base, instruction-tuned,\ncode/math-specialized, long reasoning, and vision-language models across\nzero-shot retrieval settings and the supervised setting. For the zero-shot\nretrieval settings, we consider text retrieval from the BEIR benchmark and code\nretrieval from the CoIR benchmark. Further, to evaluate supervised performance,\nall LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical\nspecialization and the long reasoning capability cause consistent degradation\nin three settings, indicating conflicts between mathematical reasoning and\nsemantic matching. The vision-language model and code-specialized LLMs\ndemonstrate superior zero-shot performance compared to other LLMs, even\nsurpassing BM25 on the code retrieval task, and maintain comparable performance\nto base LLMs in supervised settings. These findings suggest promising\ndirections for the unified retrieval task leveraging cross-domain and\ncross-modal fusion.",
    "published": "2025-07-05T08:50:29Z",
    "updated": "2025-07-05T08:50:29Z",
    "id": "2507.03958v1",
    "authors": [
      "Hengran Zhang",
      "Keping Bi",
      "Jiafeng Guo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03958v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03958v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03958v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of domain-specific specialization on LLMs' retrieval capabilities, including text, code, and multimodal content retrieval. It involves experiments with various specialized LLMs and benchmarks, aligning with topics related to LLMs, retrieval, and multimodal capabilities.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "MLLM"
    ]
  },
  "2507.03945v1": {
    "title": "Function-based Labels for Complementary Recommendation: Definition,\n  Annotation, and LLM-as-a-Judge",
    "summary": "Complementary recommendations enhance the user experience by suggesting items\nthat are frequently purchased together while serving different functions from\nthe query item. Inferring or evaluating whether two items have a complementary\nrelationship requires complementary relationship labels; however, defining\nthese labels is challenging because of the inherent ambiguity of such\nrelationships. Complementary labels based on user historical behavior logs\nattempt to capture these relationships, but often produce inconsistent and\nunreliable results. Recent efforts have introduced large language models (LLMs)\nto infer these relationships. However, these approaches provide a binary\nclassification without a nuanced understanding of complementary relationships.\nIn this study, we address these challenges by introducing Function-Based Labels\n(FBLs), a novel definition of complementary relationships independent of user\npurchase logs and the opaque decision processes of LLMs. We constructed a\nhuman-annotated FBLs dataset comprising 2,759 item pairs and demonstrated that\nit covered possible item relationships and minimized ambiguity. We then\nevaluated whether some machine learning (ML) methods using annotated FBLs could\naccurately infer labels for unseen item pairs, and whether LLM-generated\ncomplementary labels align with human perception. Our results demonstrate that\neven with limited data, ML models, such as logistic regression and SVM achieve\nhigh macro-F1 scores (approximately 0.82). Furthermore, LLMs, such as\ngpt-4o-mini, demonstrated high consistency (0.989) and classification accuracy\n(0.849) under the detailed definition of FBLs, indicating their potential as\neffective annotators that mimic human judgment. Overall, our study presents\nFBLs as a clear definition of complementary relationships, enabling more\naccurate inferences and automated labeling of complementary recommendations.",
    "published": "2025-07-05T08:08:38Z",
    "updated": "2025-07-05T08:08:38Z",
    "id": "2507.03945v1",
    "authors": [
      "Chihiro Yamasaki",
      "Kai Sugahara",
      "Yuma Nagi",
      "Kazushi Okamoto"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03945v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03945v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03945v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for inferring complementary relationships and evaluating their performance, which aligns with the 'LLM' topic. It also involves the evaluation of LLMs' consistency and accuracy, which is relevant to the 'Benchmark' topic. The study does not directly fit into other specified categories.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.03933v2": {
    "title": "Losing our Tail -- Again: On (Un)Natural Selection And Multilingual\n  Large Language Models",
    "summary": "Multilingual Large Language Models (LLMs) considerably changed how\ntechnologies can influence language. While previous technologies could mediate\nor assist humans, there is now a tendency to offload the task of writing itself\nto these technologies, enabling them to change our linguistic ecosystem more\ndirectly. While they provide us quick access to information and impressively\nfluent output, beneath their apparent sophistication lies a subtle, more\ninsidious threat: the gradual decline and loss of linguistic diversity. With\nthis opinion piece, I explore how model collapse, with a particular focus on\ntranslation technology, can lead to the loss of linguistic forms, grammatical\nfeatures, and cultural nuance. Model collapse refers to the eventual\nconsequence of self-consuming training loops, where models reinforce their own\nbiases and lose linguistic diversity. Drawing on recent work in Computer\nVision, Natural Language Processing (NLP) and Machine Translation (MT), I argue\nthat the tails of our linguistic distributions are vanishing, and with them,\nthe narratives and identities they carry. This is a call to resist linguistic\nflattening and to reimagine NLP as a field that encourages, values and protects\nexpressive multilingual lexical and linguistic diversity and creativity.",
    "published": "2025-07-05T07:36:49Z",
    "updated": "2025-07-09T13:14:29Z",
    "id": "2507.03933v2",
    "authors": [
      "Eva Vanmassenhove"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03933v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03933v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03933v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of Multilingual Large Language Models (LLMs) on linguistic diversity, focusing on model collapse and the loss of linguistic forms and cultural nuances. It aligns with the topics of LLM and MLLM, as it specifically addresses multilingual aspects and the broader implications of LLMs.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.03928v1": {
    "title": "CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate",
    "summary": "Nowadays, single Large Language Model (LLM) struggles with critical issues\nsuch as hallucination and inadequate reasoning abilities. To mitigate these\nissues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where\nLLM agents engage in in-depth debates with others on tasks. However, existing\nMAD methods face two major issues: (a) too lengthy input contexts, which causes\nLLM agents to get lost in plenty of input information and experiences\nperformance drop; and (b) the overconfidence dilemma, where self-assured LLM\nagents dominate the debate, leading to low debating effectiveness. To address\nthese limitations, we propose a novel MAD method called \"CortexDebate\".\nInspired by the human brain's tendency to establish a sparse and dynamically\noptimized network among cortical areas governed by white matter, CortexDebate\nconstructs a sparse debating graph among LLM agents, where each LLM agent only\ndebates with the ones that are helpful to it. To optimize the graph, we propose\na module named McKinsey-based Debate Matter (MDM), which acts as an artificial\nanalog to white matter. By integrating the McKinsey Trust Formula, a\nwell-established measure of trustworthiness from sociology, MDM enables\ncredible evaluations that guide graph optimization. The effectiveness of our\nCortexDebate has been well demonstrated by extensive experimental results\nacross eight datasets from four task types.",
    "published": "2025-07-05T07:23:15Z",
    "updated": "2025-07-05T07:23:15Z",
    "id": "2507.03928v1",
    "authors": [
      "Yiliu Sun",
      "Zicheng Zhao",
      "Sheng Wan",
      "Chen Gong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03928v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03928v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03928v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a novel Multi-Agent Debate (MAD) method called CortexDebate, which involves LLM agents engaging in debates to mitigate issues like hallucination and inadequate reasoning. It focuses on optimizing the debating process among LLM agents, which is closely related to the topics of Reasoning and LLM.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.03908v1": {
    "title": "Bridging Vision and Language: Optimal Transport-Driven Radiology Report\n  Generation via LLMs",
    "summary": "Radiology report generation represents a significant application within\nmedical AI, and has achieved impressive results. Concurrently, large language\nmodels (LLMs) have demonstrated remarkable performance across various domains.\nHowever, empirical validation indicates that general LLMs tend to focus more on\nlinguistic fluency rather than clinical effectiveness, and lack the ability to\neffectively capture the relationship between X-ray images and their\ncorresponding texts, thus resulting in poor clinical practicability. To address\nthese challenges, we propose Optimal Transport-Driven Radiology Report\nGeneration (OTDRG), a novel framework that leverages Optimal Transport (OT) to\nalign image features with disease labels extracted from reports, effectively\nbridging the cross-modal gap. The core component of OTDRG is Alignment \\&\nFine-Tuning, where OT utilizes results from the encoding of label features and\nimage visual features to minimize cross-modal distances, then integrating image\nand text features for LLMs fine-tuning. Additionally, we design a novel disease\nprediction module to predict disease labels contained in X-ray images during\nvalidation and testing. Evaluated on the MIMIC-CXR and IU X-Ray datasets, OTDRG\nachieves state-of-the-art performance in both natural language generation (NLG)\nand clinical efficacy (CE) metrics, delivering reports that are not only\nlinguistically coherent but also clinically accurate.",
    "published": "2025-07-05T05:48:48Z",
    "updated": "2025-07-05T05:48:48Z",
    "id": "2507.03908v1",
    "authors": [
      "Haifeng Zhao",
      "Yufei Zhang",
      "Leilei Ma",
      "Shuo Xu",
      "Dengdi Sun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03908v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03908v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03908v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging LLMs for radiology report generation, specifically addressing the alignment between X-ray images and their corresponding texts using Optimal Transport. This involves multimodal integration and fine-tuning of LLMs, which aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Alignment (VLA).",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.03904v1": {
    "title": "Agent Exchange: Shaping the Future of AI Agent Economics",
    "summary": "The rise of Large Language Models (LLMs) has transformed AI agents from\npassive computational tools into autonomous economic actors. This shift marks\nthe emergence of the agent-centric economy, in which agents take on active\neconomic roles-exchanging value, making strategic decisions, and coordinating\nactions with minimal human oversight. To realize this vision, we propose Agent\nExchange (AEX), a specialized auction platform designed to support the dynamics\nof the AI agent marketplace. AEX offers an optimized infrastructure for agent\ncoordination and economic participation. Inspired by Real-Time Bidding (RTB)\nsystems in online advertising, AEX serves as the central auction engine,\nfacilitating interactions among four ecosystem components: the User-Side\nPlatform (USP), which translates human goals into agent-executable tasks; the\nAgent-Side Platform (ASP), responsible for capability representation,\nperformance tracking, and optimization; Agent Hubs, which coordinate agent\nteams and participate in AEX-hosted auctions; and the Data Management Platform\n(DMP), ensuring secure knowledge sharing and fair value attribution. We outline\nthe design principles and system architecture of AEX, laying the groundwork for\nagent-based economic infrastructure in future AI ecosystems.",
    "published": "2025-07-05T05:18:49Z",
    "updated": "2025-07-05T05:18:49Z",
    "id": "2507.03904v1",
    "authors": [
      "Yingxuan Yang",
      "Ying Wen",
      "Jun Wang",
      "Weinan Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03904v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03904v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03904v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the transformation of AI agents into autonomous economic actors facilitated by Large Language Models (LLMs) and introduces a specialized auction platform for agent coordination, which aligns with the topics of LLM and RL (Reinforcement Learning) due to the focus on autonomous decision-making and economic participation of AI agents.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.03897v1": {
    "title": "GenAI-Powered Inference",
    "summary": "We introduce GenAI-Powered Inference (GPI), a statistical framework for both\ncausal and predictive inference using unstructured data, including text and\nimages. GPI leverages open-source Generative Artificial Intelligence (GenAI)\nmodels - such as large language models and diffusion models - not only to\ngenerate unstructured data at scale but also to extract low-dimensional\nrepresentations that capture their underlying structure. Applying machine\nlearning to these representations, GPI enables estimation of causal and\npredictive effects while quantifying associated estimation uncertainty. Unlike\nexisting approaches to representation learning, GPI does not require\nfine-tuning of generative models, making it computationally efficient and\nbroadly accessible. We illustrate the versatility of the GPI framework through\nthree applications: (1) analyzing Chinese social media censorship, (2)\nestimating predictive effects of candidates' facial appearance on electoral\noutcomes, and (3) assessing the persuasiveness of political rhetoric. An\nopen-source software package is available for implementing GPI.",
    "published": "2025-07-05T04:27:22Z",
    "updated": "2025-07-05T04:27:22Z",
    "id": "2507.03897v1",
    "authors": [
      "Kosuke Imai",
      "Kentaro Nakamura"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03897v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03897v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03897v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Generative AI models, including large language models, for causal and predictive inference with unstructured data. It leverages these models for representation learning and inference tasks, which aligns with the topics of LLM (Large Language Models) and Reasoning (as it involves complex problem solving and inference).",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.06250v1": {
    "title": "We Urgently Need Privilege Management in MCP: A Measurement of API Usage\n  in MCP Ecosystems",
    "summary": "The Model Context Protocol (MCP) has emerged as a widely adopted mechanism\nfor connecting large language models to external tools and resources. While MCP\npromises seamless extensibility and rich integrations, it also introduces a\nsubstantially expanded attack surface: any plugin can inherit broad system\nprivileges with minimal isolation or oversight. In this work, we conduct the\nfirst large-scale empirical analysis of MCP security risks. We develop an\nautomated static analysis framework and systematically examine 2,562 real-world\nMCP applications spanning 23 functional categories. Our measurements reveal\nthat network and system resource APIs dominate usage patterns, affecting 1,438\nand 1,237 servers respectively, while file and memory resources are less\nfrequent but still significant. We find that Developer Tools and API\nDevelopment plugins are the most API-intensive, and that less popular plugins\noften contain disproportionately high-risk operations. Through concrete case\nstudies, we demonstrate how insufficient privilege separation enables privilege\nescalation, misinformation propagation, and data tampering. Based on these\nfindings, we propose a detailed taxonomy of MCP resource access, quantify\nsecurity-relevant API usage, and identify open challenges for building safer\nMCP ecosystems, including dynamic permission models and automated trust\nassessment.",
    "published": "2025-07-05T03:39:30Z",
    "updated": "2025-07-05T03:39:30Z",
    "id": "2507.06250v1",
    "authors": [
      "Zhihao Li",
      "Kun Li",
      "Boyang Ma",
      "Minghui Xu",
      "Yue Zhang",
      "Xiuzhen Cheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06250v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06250v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06250v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses security risks in the Model Context Protocol (MCP) used for connecting large language models to external tools, but it does not directly align with the provided topics which focus on LLM architectures, scaling, reasoning, etc. The content is more about security and privilege management in a protocol rather than core LLM research topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.03876v1": {
    "title": "LLMs model how humans induce logically structured rules",
    "summary": "A central goal of cognitive science is to provide a computationally explicit\naccount of both the structure of the mind and its development: what are the\nprimitive representational building blocks of cognition, what are the rules via\nwhich those primitives combine, and where do these primitives and rules come\nfrom in the first place? A long-standing debate concerns the adequacy of\nartificial neural networks as computational models that can answer these\nquestions, in particular in domains related to abstract cognitive function,\nsuch as language and logic. This paper argues that recent advances in neural\nnetworks -- specifically, the advent of large language models (LLMs) --\nrepresent an important shift in this debate. We test a variety of LLMs on an\nexisting experimental paradigm used for studying the induction of rules\nformulated over logical concepts. Across four experiments, we find converging\nempirical evidence that LLMs provide at least as good a fit to human behavior\nas models that implement a Bayesian probablistic language of thought (pLoT),\nwhich have been the best computational models of human behavior on the same\ntask. Moreover, we show that the LLMs make qualitatively different predictions\nabout the nature of the rules that are inferred and deployed in order to\ncomplete the task, indicating that the LLM is unlikely to be a mere\nimplementation of the pLoT solution. Based on these results, we argue that LLMs\nmay instantiate a novel theoretical account of the primitive representations\nand computations necessary to explain human logical concepts, with which future\nwork in cognitive science should engage.",
    "published": "2025-07-05T03:24:18Z",
    "updated": "2025-07-05T03:24:18Z",
    "id": "2507.03876v1",
    "authors": [
      "Alyssa Loo",
      "Ellie Pavlick",
      "Roman Feiman"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03876v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03876v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03876v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in understanding human logical reasoning and compares their performance to Bayesian probabilistic models. It focuses on the cognitive science aspects of LLMs and their ability to model human-like rule induction.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "AGI"
    ]
  },
  "2507.03875v1": {
    "title": "Demystifying ChatGPT: How It Masters Genre Recognition",
    "summary": "The introduction of ChatGPT has garnered significant attention within the NLP\ncommunity and beyond. Previous studies have demonstrated ChatGPT's substantial\nadvancements across various downstream NLP tasks, highlighting its adaptability\nand potential to revolutionize language-related applications. However, its\ncapabilities and limitations in genre prediction remain unclear. This work\nanalyzes three Large Language Models (LLMs) using the MovieLens-100K dataset to\nassess their genre prediction capabilities. Our findings show that ChatGPT,\nwithout fine-tuning, outperformed other LLMs, and fine-tuned ChatGPT performed\nbest overall. We set up zero-shot and few-shot prompts using audio\ntranscripts/subtitles from movie trailers in the MovieLens-100K dataset,\ncovering 1682 movies of 18 genres, where each movie can have multiple genres.\nAdditionally, we extended our study by extracting IMDb movie posters to utilize\na Vision Language Model (VLM) with prompts for poster information. This\nfine-grained information was used to enhance existing LLM prompts. In\nconclusion, our study reveals ChatGPT's remarkable genre prediction\ncapabilities, surpassing other language models. The integration of VLM further\nenhances our findings, showcasing ChatGPT's potential for content-related\napplications by incorporating visual information from movie posters.",
    "published": "2025-07-05T03:22:48Z",
    "updated": "2025-07-05T03:22:48Z",
    "id": "2507.03875v1",
    "authors": [
      "Subham Raj",
      "Sriparna Saha",
      "Brijraj Singh",
      "Niranjan Pedanekar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03875v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03875v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03875v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the capabilities of ChatGPT in genre prediction, which involves the use of Large Language Models (LLMs) and Vision Language Models (VLMs). The study evaluates the performance of LLMs and integrates visual information from movie posters using a VLM, indicating relevance to both LLM and VLA topics.",
    "llm_cls_result": [
      "LLM",
      "VLA"
    ]
  },
  "2507.03871v1": {
    "title": "Enhancing Adaptive Behavioral Interventions with LLM Inference from\n  Participant-Described States",
    "summary": "The use of reinforcement learning (RL) methods to support health behavior\nchange via personalized and just-in-time adaptive interventions is of\nsignificant interest to health and behavioral science researchers focused on\nproblems such as smoking cessation support and physical activity promotion.\nHowever, RL methods are often applied to these domains using a small collection\nof context variables to mitigate the significant data scarcity issues that\narise from practical limitations on the design of adaptive intervention trials.\nIn this paper, we explore an approach to significantly expanding the state\nspace of an adaptive intervention without impacting data efficiency. The\nproposed approach enables intervention participants to provide natural language\ndescriptions of aspects of their current state. It then leverages inference\nwith pre-trained large language models (LLMs) to better align the policy of a\nbase RL method with these state descriptions. To evaluate our method, we\ndevelop a novel physical activity intervention simulation environment that\ngenerates text-based state descriptions conditioned on latent state variables\nusing an auxiliary LLM. We show that this approach has the potential to\nsignificantly improve the performance of online policy learning methods.",
    "published": "2025-07-05T02:52:51Z",
    "updated": "2025-07-05T02:52:51Z",
    "id": "2507.03871v1",
    "authors": [
      "Karine Karine",
      "Benjamin M. Marlin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03871v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03871v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03871v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of reinforcement learning (RL) methods combined with large language models (LLMs) to enhance adaptive behavioral interventions. It specifically mentions RL and LLMs, which are core topics in the provided list.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.03865v1": {
    "title": "OrthoRank: Token Selection via Sink Token Orthogonality for Efficient\n  LLM inference",
    "summary": "Attention mechanisms are central to the success of large language models\n(LLMs), enabling them to capture intricate token dependencies and implicitly\nassign importance to each token. Recent studies have revealed the sink token,\nwhich receives disproportionately high attention despite their limited semantic\nrole. In this paper, we first expand the relationship between the sink token\nand other tokens, moving beyond attention to explore their similarity in hidden\nstates, considering the layer depth. We observe that as the layers get deeper,\nthe cosine similarity between the normalized hidden states of the sink token\nand those of other tokens increases, and that the normalized hidden states of\nthe sink token exhibit negligible changes. These imply that other tokens\nconsistently are directed toward the sink token throughout the layers. Next, we\npropose a dynamic token selection method, called OrthoRank, using these\nfindings to select important tokens. Specifically, in a certain layer, we\ndefine token importance by the speed at which the token moves toward the sink\ntoken. This is converted into orthogonality with the sink token, meaning that\ntokens that are more orthogonal to the sink token are assigned greater\nimportance. Finally, through extensive experiments, we demonstrated that our\nmethod results in lower perplexity and higher zero-shot accuracy compared to\nlayer pruning methods at the same sparsity ratio with comparable throughput,\nwhile also achieving superior performance on LongBench.",
    "published": "2025-07-05T02:29:23Z",
    "updated": "2025-07-05T02:29:23Z",
    "id": "2507.03865v1",
    "authors": [
      "Seungjun Shin",
      "Jaehoon Oh",
      "Dokwan Oh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03865v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03865v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03865v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the efficiency of LLM inference through token selection methods, specifically addressing the role of sink tokens and their orthogonality with other tokens. This aligns with research on LLM architectures and optimization techniques.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.03847v1": {
    "title": "KEA Explain: Explanations of Hallucinations using Graph Kernel Analysis",
    "summary": "Large Language Models (LLMs) frequently generate hallucinations: statements\nthat are syntactically plausible but lack factual grounding. This research\npresents KEA (Kernel-Enriched AI) Explain: a neurosymbolic framework that\ndetects and explains such hallucinations by comparing knowledge graphs\nconstructed from LLM outputs with ground truth data from Wikidata or contextual\ndocuments. Using graph kernels and semantic clustering, the method provides\nexplanations for detected hallucinations, ensuring both robustness and\ninterpretability. Our framework achieves competitive accuracy in detecting\nhallucinations across both open- and closed-domain tasks, and is able to\ngenerate contrastive explanations, enhancing transparency. This research\nadvances the reliability of LLMs in high-stakes domains and provides a\nfoundation for future work on precision improvements and multi-source knowledge\nintegration.",
    "published": "2025-07-05T00:55:15Z",
    "updated": "2025-07-05T00:55:15Z",
    "id": "2507.03847v1",
    "authors": [
      "Reilly Haskins",
      "Ben Adams"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03847v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03847v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03847v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on detecting and explaining hallucinations in Large Language Models (LLMs) using a neurosymbolic framework, which is directly related to LLM research and their reliability.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.03829v1": {
    "title": "RELRaE: LLM-Based Relationship Extraction, Labelling, Refinement, and\n  Evaluation",
    "summary": "A large volume of XML data is produced in experiments carried out by robots\nin laboratories. In order to support the interoperability of data between labs,\nthere is a motivation to translate the XML data into a knowledge graph. A key\nstage of this process is the enrichment of the XML schema to lay the foundation\nof an ontology schema. To achieve this, we present the RELRaE framework, a\nframework that employs large language models in different stages to extract and\naccurately label the relationships implicitly present in the XML schema. We\ninvestigate the capability of LLMs to accurately generate these labels and then\nevaluate them. Our work demonstrates that LLMs can be effectively used to\nsupport the generation of relationship labels in the context of lab automation,\nand that they can play a valuable role within semi-automatic ontology\ngeneration frameworks more generally.",
    "published": "2025-07-04T22:27:06Z",
    "updated": "2025-07-04T22:27:06Z",
    "id": "2507.03829v1",
    "authors": [
      "George Hannah",
      "Jacopo de Berardinis",
      "Terry R. Payne",
      "Valentina Tamma",
      "Andrew Mitchell",
      "Ellen Piercy",
      "Ewan Johnson",
      "Andrew Ng",
      "Harry Rostron",
      "Boris Konev"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03829v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03829v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03829v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for relationship extraction, labeling, and evaluation in the context of lab automation and ontology generation. This directly relates to the research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.03828v1": {
    "title": "IMPACT: Importance-Aware Activation Space Reconstruction",
    "summary": "Large language models (LLMs) achieve strong performance across many domains\nbut are difficult to deploy in resource-constrained settings due to their size.\nLow-rank weight matrix compression is a popular strategy for reducing model\nsize, typically by minimizing weight reconstruction error under the assumption\nthat weights are low-rank. However, this assumption often does not hold in\nLLMs. Instead, LLM activations exhibit stronger low-rank structure-prompting a\nshift toward minimizing activation reconstruction error.\n  We show that this shift alone is insufficient: activation dimensions\ncontribute unequally to model performance, and uniform reconstruction can harm\nperformance. We propose IMPACT, a principled framework for importance-aware\nactivation reconstruction that links model compression decisions to their\nimpact on model behavior. IMPACT formulates an optimization problem that\nconsiders both activation structure and gradient sensitivity, and derives a\nclosed-form solution where the optimal reconstruction bases are the\neigenvectors of an importance-weighted activation covariance matrix. This\nenables low-rank approximations explicitly optimized to preserve accuracy.\nExperiments across diverse models and tasks show that IMPACT achieves up to\n48.6% greater model size reduction with accuracy comparable to state-of-the-art\nbaselines.",
    "published": "2025-07-04T22:26:33Z",
    "updated": "2025-07-04T22:26:33Z",
    "id": "2507.03828v1",
    "authors": [
      "Md Mokarram Chowdhury",
      "Daniel Agyei Asante",
      "Ernie Chang",
      "Yang Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03828v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03828v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03828v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the compression of large language models (LLMs) by considering the importance of activation dimensions, which is directly related to the scaling and efficiency of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.03811v1": {
    "title": "Leveraging Large Language Models for Tacit Knowledge Discovery in\n  Organizational Contexts",
    "summary": "Documenting tacit knowledge in organizations can be a challenging task due to\nincomplete initial information, difficulty in identifying knowledgeable\nindividuals, the interplay of formal hierarchies and informal networks, and the\nneed to ask the right questions. To address this, we propose an agent-based\nframework leveraging large language models (LLMs) to iteratively reconstruct\ndataset descriptions through interactions with employees. Modeling knowledge\ndissemination as a Susceptible-Infectious (SI) process with waning infectivity,\nwe conduct 864 simulations across various synthetic company structures and\ndifferent dissemination parameters. Our results show that the agent achieves\n94.9% full-knowledge recall, with self-critical feedback scores strongly\ncorrelating with external literature critic scores. We analyze how each\nsimulation parameter affects the knowledge retrieval process for the agent. In\nparticular, we find that our approach is able to recover information without\nneeding to access directly the only domain specialist. These findings highlight\nthe agent's ability to navigate organizational complexity and capture\nfragmented knowledge that would otherwise remain inaccessible.",
    "published": "2025-07-04T21:09:32Z",
    "updated": "2025-07-04T21:09:32Z",
    "id": "2507.03811v1",
    "authors": [
      "Gianlucca Zuin",
      "Saulo Mastelini",
      "Tlio Loures",
      "Adriano Veloso"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03811v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03811v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03811v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in an agent-based framework to discover tacit knowledge in organizations. It focuses on the application of LLMs rather than their architecture or scaling, and does not delve into multimodal aspects, reinforcement learning, or other specialized topics.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.03772v2": {
    "title": "Skewed Score: A statistical framework to assess autograders",
    "summary": "The evaluation of large language model (LLM) outputs is increasingly\nperformed by other LLMs, a setup commonly known as \"LLM-as-a-judge\", or\nautograders. While autograders offer a scalable alternative to human\nevaluation, they have shown mixed reliability and may exhibit systematic\nbiases, depending on response type, scoring methodology, domain specificity, or\nother factors. Here we propose a statistical framework based on Bayesian\ngeneralised linear models (GLMs) that enables researchers to simultaneously\nassess their autograders while addressing their primary research questions\n(e.g., LLM evaluation). Our approach models evaluation outcomes (e.g., scores\nor pairwise preferences) as a function of properties of the grader (e.g., human\nvs. autograder) and the evaluated item (e.g., response length or the LLM that\ngenerated it), allowing for explicit quantification of scoring differences and\npotential biases within a unified framework. In addition, our method can be\nused to augment traditional metrics such as inter-rater agreement, by providing\nuncertainty estimates and clarifying sources of disagreement. Overall, this\napproach contributes to more robust and interpretable use of autograders in LLM\nevaluation, enabling both performance analysis and bias detection.",
    "published": "2025-07-04T18:45:10Z",
    "updated": "2025-07-09T16:28:55Z",
    "id": "2507.03772v2",
    "authors": [
      "Magda Dubois",
      "Harry Coppock",
      "Mario Giulianelli",
      "Timo Flesch",
      "Lennart Luettgau",
      "Cozmin Ududec"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03772v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03772v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03772v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating LLM outputs using autograders and proposes a statistical framework to assess their reliability and biases, which is closely related to benchmarking and evaluating LLMs.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2507.03739v1": {
    "title": "ChestGPT: Integrating Large Language Models and Vision Transformers for\n  Disease Detection and Localization in Chest X-Rays",
    "summary": "The global demand for radiologists is increasing rapidly due to a growing\nreliance on medical imaging services, while the supply of radiologists is not\nkeeping pace. Advances in computer vision and image processing technologies\npresent significant potential to address this gap by enhancing radiologists'\ncapabilities and improving diagnostic accuracy. Large language models (LLMs),\nparticularly generative pre-trained transformers (GPTs), have become the\nprimary approach for understanding and generating textual data. In parallel,\nvision transformers (ViTs) have proven effective at converting visual data into\na format that LLMs can process efficiently. In this paper, we present ChestGPT,\na deep-learning framework that integrates the EVA ViT with the Llama 2 LLM to\nclassify diseases and localize regions of interest in chest X-ray images. The\nViT converts X-ray images into tokens, which are then fed, together with\nengineered prompts, into the LLM, enabling joint classification and\nlocalization of diseases. This approach incorporates transfer learning\ntechniques to enhance both explainability and performance. The proposed method\nachieved strong global disease classification performance on the VinDr-CXR\ndataset, with an F1 score of 0.76, and successfully localized pathologies by\ngenerating bounding boxes around the regions of interest. We also outline\nseveral task-specific prompts, in addition to general-purpose prompts, for\nscenarios radiologists might encounter. Overall, this framework offers an\nassistive tool that can lighten radiologists' workload by providing preliminary\nfindings and regions of interest to facilitate their diagnostic process.",
    "published": "2025-07-04T17:58:52Z",
    "updated": "2025-07-04T17:58:52Z",
    "id": "2507.03739v1",
    "authors": [
      "Shehroz S. Khan",
      "Petar Przulj",
      "Ahmed Ashraf",
      "Ali Abedi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03739v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03739v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03739v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) and Vision Transformers (ViTs) for medical imaging, which aligns with the topics of 'LLM' and 'MLLM' (Multimodal Large Language Models). The focus on disease detection and localization in chest X-rays also suggests relevance to 'VLA' (Vision-Language Action) due to the cross-modal processing involved.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "VLA"
    ]
  },
  "2507.03726v1": {
    "title": "Agent-Based Detection and Resolution of Incompleteness and Ambiguity in\n  Interactions with Large Language Models",
    "summary": "Many of us now treat LLMs as modern-day oracles asking it almost any kind of\nquestion. However, consulting an LLM does not have to be a single turn\nactivity. But long multi-turn interactions can get tedious if it is simply to\nclarify contextual information that can be arrived at through reasoning. In\nthis paper, we examine the use of agent-based architecture to bolster LLM-based\nQuestion-Answering systems with additional reasoning capabilities. We examine\nthe automatic resolution of potential incompleteness or ambiguities in\nquestions by transducers implemented using LLM-based agents. We focus on\nseveral benchmark datasets that are known to contain questions with these\ndeficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and\nLlama-4-Scout) with agents that act as specialists in detecting and resolving\ndeficiencies of incompleteness and ambiguity. The agents are implemented as\nzero-shot ReAct agents. Rather than producing an answer in a single step, the\nmodel now decides between 3 actions a) classify b) resolve c) answer. Action a)\ndecides if the question is incomplete, ambiguous, or normal. Action b)\ndetermines if any deficiencies identified can be resolved. Action c) answers\nthe resolved form of the question. We compare the use of LLMs with and without\nthe use of agents with these components. Our results show benefits of agents\nwith transducer 1) A shortening of the length of interactions with human 2) An\nimprovement in the answer quality and 3) Explainable resolution of deficiencies\nin the question. On the negative side we find while it may result in additional\nLLM invocations and in some cases, increased latency. But on tested datasets,\nthe benefits outweigh the costs except when questions already have sufficient\ncontext. Suggesting the agent-based approach could be a useful mechanism to\nharness the power of LLMs to develop more robust QA systems.",
    "published": "2025-07-04T17:28:33Z",
    "updated": "2025-07-04T17:28:33Z",
    "id": "2507.03726v1",
    "authors": [
      "Riya Naik",
      "Ashwin Srinivasan",
      "Swati Agarwal",
      "Estrid He"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03726v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03726v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03726v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of agent-based architecture to enhance LLM-based Question-Answering systems, focusing on detecting and resolving incompleteness and ambiguity in questions. It involves LLMs (GPT-3.5-Turbo and Llama-4-Scout) and uses benchmark datasets for evaluation, which aligns with the topics of LLM, RL (due to the use of ReAct agents), and Benchmark.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Benchmark"
    ]
  },
  "2507.03724v2": {
    "title": "MemOS: A Memory OS for AI System",
    "summary": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling.",
    "published": "2025-07-04T17:21:46Z",
    "updated": "2025-07-08T14:30:24Z",
    "id": "2507.03724v2",
    "authors": [
      "Zhiyu Li",
      "Shichao Song",
      "Chenyang Xi",
      "Hanyu Wang",
      "Chen Tang",
      "Simin Niu",
      "Ding Chen",
      "Jiawei Yang",
      "Chunyu Li",
      "Qingchen Yu",
      "Jihao Zhao",
      "Yezhaohui Wang",
      "Peng Liu",
      "Zehao Lin",
      "Pengyuan Wang",
      "Jiahao Huo",
      "Tianyi Chen",
      "Kai Chen",
      "Kehang Li",
      "Zhen Tao",
      "Junpeng Ren",
      "Huayi Lai",
      "Hao Wu",
      "Bo Tang",
      "Zhenren Wang",
      "Zhaoxin Fan",
      "Ningyu Zhang",
      "Linfeng Zhang",
      "Junchi Yan",
      "Mingchuan Yang",
      "Tong Xu",
      "Wei Xu",
      "Huajun Chen",
      "Haofeng Wang",
      "Hongkang Yang",
      "Wentao Zhang",
      "Zhi-Qin John Xu",
      "Siheng Chen",
      "Feiyu Xiong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03724v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03724v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03724v2",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses memory management in Large Language Models (LLMs) and proposes a memory operating system (MemOS) to address challenges related to long-context reasoning, continual personalization, and knowledge consistency. It specifically mentions LLMs, AGI, and memory management, which are key topics in the provided list.",
    "llm_cls_result": [
      "LLM",
      "AGI",
      "Memory"
    ]
  },
  "2507.03722v1": {
    "title": "Roadmap for using large language models (LLMs) to accelerate\n  cross-disciplinary research with an example from computational biology",
    "summary": "Large language models (LLMs) are powerful artificial intelligence (AI) tools\ntransforming how research is conducted. However, their use in research has been\nmet with skepticism, due to concerns about hallucinations, biases and potential\nharms to research. These emphasize the importance of clearly understanding the\nstrengths and weaknesses of LLMs to ensure their effective and responsible use.\nHere, we present a roadmap for integrating LLMs into cross-disciplinary\nresearch, where effective communication, knowledge transfer and collaboration\nacross diverse fields are essential but often challenging. We examine the\ncapabilities and limitations of LLMs and provide a detailed computational\nbiology case study (on modeling HIV rebound dynamics) demonstrating how\niterative interactions with an LLM (ChatGPT) can facilitate interdisciplinary\ncollaboration and research. We argue that LLMs are best used as augmentative\ntools within a human-in-the-loop framework. Looking forward, we envisage that\nthe responsible use of LLMs will enhance innovative cross-disciplinary research\nand substantially accelerate scientific discoveries.",
    "published": "2025-07-04T17:20:14Z",
    "updated": "2025-07-04T17:20:14Z",
    "id": "2507.03722v1",
    "authors": [
      "Ruian Ke",
      "Ruy M. Ribeiro"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03722v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03722v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03722v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in cross-disciplinary research, specifically highlighting their application in computational biology. It focuses on the capabilities and limitations of LLMs, their integration into research workflows, and their role in facilitating interdisciplinary collaboration.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.03721v1": {
    "title": "Predicting Business Angel Early-Stage Decision Making Using AI",
    "summary": "External funding is crucial for early-stage ventures, particularly technology\nstartups that require significant R&D investment. Business angels offer a\ncritical source of funding, but their decision-making is often subjective and\nresource-intensive for both investor and entrepreneur. Much research has\ninvestigated this investment process to find the critical factors angels\nconsider. One such tool, the Critical Factor Assessment (CFA), deployed more\nthan 20,000 times by the Canadian Innovation Centre, has been evaluated\npost-decision and found to be significantly more accurate than investors' own\ndecisions. However, a single CFA analysis requires three trained individuals\nand several days, limiting its adoption. This study builds on previous work\nvalidating the CFA to investigate whether the constraints inhibiting its\nadoption can be overcome using a trained AI model. In this research, we\nprompted multiple large language models (LLMs) to assign the eight CFA factors\nto a dataset of 600 transcribed, unstructured startup pitches seeking business\nangel funding with known investment outcomes. We then trained and evaluated\nmachine learning classification models using the LLM-generated CFA scores as\ninput features. Our best-performing model demonstrated high predictive accuracy\n(85.0% for predicting BA deal/no-deal outcomes) and exhibited significant\ncorrelation (Spearman's r = 0.896, p-value < 0.001) with conventional\nhuman-graded evaluations. The integration of AI-based feature extraction with a\nstructured and validated decision-making framework yielded a scalable,\nreliable, and less-biased model for evaluating startup pitches, removing the\nconstraints that previously limited adoption.",
    "published": "2025-07-04T17:17:34Z",
    "updated": "2025-07-04T17:17:34Z",
    "id": "2507.03721v1",
    "authors": [
      "Yan Katcharovski",
      "Andrew L. Maxwell"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03721v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03721v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03721v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to predict business angel early-stage decision making, which involves leveraging LLMs for feature extraction and classification tasks. This aligns with the 'LLM' topic due to the use of large language models, and the 'Reasoning' topic as it involves decision-making and predictive modeling.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.03711v3": {
    "title": "Can LLMs Play  n Quan Game? A Study of Multi-Step Planning and\n  Decision Making",
    "summary": "In this paper, we explore the ability of large language models (LLMs) to plan\nand make decisions through the lens of the traditional Vietnamese board game,\n\\^O \\u{A}n Quan. This game, which involves a series of strategic token\nmovements and captures, offers a unique environment for evaluating the\ndecision-making and strategic capabilities of LLMs. Specifically, we develop\nvarious agent personas, ranging from aggressive to defensive, and employ the\n\\^O \\u{A}n Quan game as a testbed for assessing LLM performance across\ndifferent strategies. Through experimentation with models like\nLlama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, and Llama-3.3-70B-Instruct, we\naim to understand how these models execute strategic decision-making, plan\nmoves, and manage dynamic game states. The results will offer insights into the\nstrengths and weaknesses of LLMs in terms of reasoning and strategy,\ncontributing to a deeper understanding of their general capabilities.",
    "published": "2025-07-04T16:50:40Z",
    "updated": "2025-07-09T02:09:05Z",
    "id": "2507.03711v3",
    "authors": [
      "Sang Quang Nguyen",
      "Kiet Van Nguyen",
      "Vinh-Tiep Nguyen",
      "Thanh Duc Ngo",
      "Ngan Luu-Thuy Nguyen",
      "Duy-Dinh Le"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03711v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03711v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03711v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the decision-making and strategic capabilities of LLMs through a board game, which involves reasoning and planning, making it relevant to the 'Reasoning' and 'LLM' categories.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2507.03703v1": {
    "title": "Sign Spotting Disambiguation using Large Language Models",
    "summary": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting.",
    "published": "2025-07-04T16:38:09Z",
    "updated": "2025-07-04T16:38:09Z",
    "id": "2507.03703v1",
    "authors": [
      "JianHe Low",
      "Ozge Mercanoglu Sincan",
      "Richard Bowden"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03703v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03703v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03703v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) to improve sign spotting in sign language translation, which directly involves the application of LLMs in a specific task.",
    "llm_cls_result": [
      "LLM",
      "VLA"
    ]
  },
  "2507.03674v1": {
    "title": "STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured\n  Information Extraction with Human-In-The-Loop Evaluation and Benchmarking",
    "summary": "The ability to extract structured information from unstructured sources-such\nas free-text documents and scientific literature-is critical for accelerating\nscientific discovery and knowledge synthesis. Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in various natural language processing\ntasks, including structured information extraction. However, their\neffectiveness often diminishes in specialized, domain-specific contexts that\nrequire nuanced understanding and expert-level domain knowledge. In addition,\nexisting LLM-based approaches frequently exhibit poor transferability across\ntasks and domains, limiting their scalability and adaptability. To address\nthese challenges, we introduce StructSense, a modular, task-agnostic,\nopen-source framework for structured information extraction built on LLMs.\nStructSense is guided by domain-specific symbolic knowledge encoded in\nontologies, enabling it to navigate complex domain content more effectively. It\nfurther incorporates agentic capabilities through self-evaluative judges that\nform a feedback loop for iterative refinement, and includes human-in-the-loop\nmechanisms to ensure quality and validation. We demonstrate that StructSense\ncan overcome both the limitations of domain sensitivity and the lack of\ncross-task generalizability, as shown through its application to diverse\nneuroscience information extraction tasks.",
    "published": "2025-07-04T15:51:07Z",
    "updated": "2025-07-04T15:51:07Z",
    "id": "2507.03674v1",
    "authors": [
      "Tek Raj Chhetri",
      "Yibei Chen",
      "Puja Trivedi",
      "Dorota Jarecka",
      "Saif Haobsh",
      "Patrick Ray",
      "Lydia Ng",
      "Satrajit S. Ghosh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03674v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03674v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03674v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for structured information extraction, which involves reasoning and domain-specific knowledge. It also mentions human-in-the-loop mechanisms and benchmarking, which are relevant to the Reasoning and Benchmark topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.03673v1": {
    "title": "TACOS: Open Tagging and Comparative Scoring for Instruction Fine-Tuning\n  Data Selection",
    "summary": "Instruction Fine-Tuning (IFT) is crucial for aligning large language models\n(LLMs) with human preferences, and selecting a small yet representative subset\nfrom massive data significantly facilitates IFT in terms of both efficiency and\neffectiveness. Nevertheless, existing approaches suffer from two limitations:\nthe use of simple heuristics restricts data diversity, while the singleton data\nquality evaluation accounts for inconsistent criteria between independent\nsamples. To address the issues, we present TACOS, an innovative method that\nintegrates Open Tagging and Comparative Scoring for IFT data selection. To\ncapture data diversity, we leverage LLMs to assign open-domain tags to human\nqueries, followed by a normalization stage to denoise the open tags and enable\nefficient clustering. Additionally, we suggest a comparative scoring method\nthat allows the relative quality evaluation of samples within a cluster,\navoiding inconsistent criteria seen in singleton-based evaluations. Extensive\nexperiments across diverse datasets and LLM architectures demonstrate that\nTACOS outperforms existing approaches by a large margin. Notably, it achieves\nsuperior instruction-following performance on MT-Bench and ranks 1st among\nLLaMA2-7B-Based models on AlpacaEval 2.0, illustrating its efficacy for IFT\ndata selection.",
    "published": "2025-07-04T15:46:07Z",
    "updated": "2025-07-04T15:46:07Z",
    "id": "2507.03673v1",
    "authors": [
      "Xixiang He",
      "Hao Yu",
      "Qiyao Sun",
      "Ao Cheng",
      "Tailai Zhang",
      "Cong Liu",
      "Shuxuan Guo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03673v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03673v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03673v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses Instruction Fine-Tuning (IFT) for large language models (LLMs), focusing on data selection methods to improve efficiency and effectiveness. It leverages LLMs for tagging and scoring, which are core aspects of LLM research and fine-tuning strategies.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.03671v1": {
    "title": "Recon, Answer, Verify: Agents in Search of Truth",
    "summary": "Automated fact checking with large language models (LLMs) offers a scalable\nalternative to manual verification. Evaluating fact checking is challenging as\nexisting benchmark datasets often include post claim analysis and annotator\ncues, which are absent in real world scenarios where claims are fact checked\nimmediately after being made. This limits the realism of current evaluations.\nWe present Politi Fact Only (PFO), a 5 class benchmark dataset of 2,982\npolitical claims from politifact.com, where all post claim analysis and\nannotator cues have been removed manually. This ensures that models are\nevaluated using only the information that would have been available prior to\nthe claim's verification. Evaluating LLMs on PFO, we see an average performance\ndrop of 22% in terms of macro f1 compared to PFO's unfiltered version. Based on\nthe identified challenges of the existing LLM based fact checking system, we\npropose RAV (Recon Answer Verify), an agentic framework with three agents:\nquestion generator, answer generator, and label generator. Our pipeline\niteratively generates and answers sub questions to verify different aspects of\nthe claim before finally generating the label. RAV generalizes across domains\nand label granularities, and it outperforms state of the art approaches on well\nknown baselines RAWFC (fact checking, 3 class) by 25.28%, and on HOVER\n(encyclopedia, 2 class) by 1.54% on 2 hop, 4.94% on 3 hop, and 1.78% on 4 hop,\nsub categories respectively. RAV shows the least performance drop compared to\nbaselines of 16.3% in macro f1 when we compare PFO with its unfiltered version.",
    "published": "2025-07-04T15:44:28Z",
    "updated": "2025-07-04T15:44:28Z",
    "id": "2507.03671v1",
    "authors": [
      "Satyam Shukla",
      "Himanshu Dutta",
      "Pushpak Bhattacharyya"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03671v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03671v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03671v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for automated fact checking, introduces a new benchmark dataset, and proposes an agentic framework (RAV) to improve fact checking performance. The core topics are related to LLMs, reasoning, and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.03662v1": {
    "title": "Re-Emergent Misalignment: How Narrow Fine-Tuning Erodes Safety Alignment\n  in LLMs",
    "summary": "Recent work has shown that fine-tuning large language models (LLMs) on code\nwith security vulnerabilities can result in misaligned and unsafe behaviors\nacross broad domains. These results prompted concerns about the emergence of\nharmful behaviors from narrow domain fine-tuning. In this paper, we\ncontextualize these findings by analyzing how such narrow adaptation impacts\nthe internal mechanisms and behavioral manifestations of LLMs. Through a series\nof experiments covering output probability distributions, loss and gradient\nvector geometry, layer-wise activation dynamics, and activation space\ndimensions, we find that behaviors attributed to \"emergent misalignment\" may be\nbetter interpreted as an erosion of prior alignment. We show that fine tuning\non insecure code induces internal changes that oppose alignment. Further, we\nidentify a shared latent dimension in the model's activation space that governs\nalignment behavior. We show that this space is activated by insecure code and\nby misaligned responses more generally, revealing how narrow fine-tuning can\ndegrade general safety behavior by interfering with shared internal mechanisms.\nOur findings offer a mechanistic interpretation for previously observed\nmisalignment phenomena, and highlights the fragility of alignment in LLMs. The\nresults underscore the need for more robust fine-tuning strategies that\npreserve intended behavior across domains.",
    "published": "2025-07-04T15:36:58Z",
    "updated": "2025-07-04T15:36:58Z",
    "id": "2507.03662v1",
    "authors": [
      "Jeremiah Giordani"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03662v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03662v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03662v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of fine-tuning on the alignment and safety behaviors of large language models (LLMs), which is directly related to the research on LLMs and their alignment mechanisms.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.03659v1": {
    "title": "Specification-Guided Repair of Arithmetic Errors in Dafny Programs using\n  LLMs",
    "summary": "Formal verification offers strong assurances of software correctness.\nHowever, debugging and repairing the underlying faults can be complex and\ntime-consuming when verification fails. Automated Program Repair (APR) aims to\nease this by automatically identifying and fixing faults. Traditional APR\ntechniques often depend on test suites for validation, but these may fail to\ncapture all scenarios. In contrast, formal specifications provide stronger\ncorrectness criteria for effective repairs.\n  We present an innovative APR tool for Dafny, a verification-aware programming\nlanguage that uses formal specifications - including pre-conditions,\npost-conditions, and invariants - as oracles for fault localization and repair.\nAssuming the correctness of the specifications and focusing on arithmetic bugs,\nwe localize faults through a series of steps, which include using Hoare Logic\nto determine the state of each statement within the program and\nstate-of-the-art Large Language Models (LLMs) to synthesize candidate fixes.\nThe chosen models were GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.\n  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny\nprograms. Our tool achieves 89.6% accuracy in fault localization, with GPT-4o\nmini yielding the highest repair success rate (74.18%). These results highlight\nthe potential of combining formal reasoning with LLM-driven program synthesis\nfor automated program repair.",
    "published": "2025-07-04T15:36:12Z",
    "updated": "2025-07-04T15:36:12Z",
    "id": "2507.03659v1",
    "authors": [
      "Valentina Wu",
      "Alexandra Mendes",
      "Alexandre Abreu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03659v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03659v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03659v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for automated program repair in the context of formal verification, specifically focusing on arithmetic errors in Dafny programs. The core topics are LLM for program repair and reasoning in formal verification contexts.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.03637v1": {
    "title": "Large Language Models for Combinatorial Optimization: A Systematic\n  Review",
    "summary": "This systematic review explores the application of Large Language Models\n(LLMs) in Combinatorial Optimization (CO). We report our findings using the\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)\nguidelines. We conduct a literature search via Scopus and Google Scholar,\nexamining over 2,000 publications. We assess publications against four\ninclusion and four exclusion criteria related to their language, research\nfocus, publication year, and type. Eventually, we select 103 studies. We\nclassify these studies into semantic categories and topics to provide a\ncomprehensive overview of the field, including the tasks performed by LLMs, the\narchitectures of LLMs, the existing datasets specifically designed for\nevaluating LLMs in CO, and the field of application. Finally, we identify\nfuture directions for leveraging LLMs in this field.",
    "published": "2025-07-04T15:08:10Z",
    "updated": "2025-07-04T15:08:10Z",
    "id": "2507.03637v1",
    "authors": [
      "Francesca Da Ros",
      "Michael Soprano",
      "Luca Di Gaspero",
      "Kevin Roitero"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03637v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03637v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03637v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in Combinatorial Optimization, discussing tasks, architectures, datasets, and future directions. This aligns with the 'LLM' topic which covers research on Large Language Models, architectures, and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.03620v1": {
    "title": "Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt\n  Optimization Using DSPy",
    "summary": "Although prompt engineering is central to unlocking the full potential of\nLarge Language Models (LLMs), crafting effective prompts remains a\ntime-consuming trial-and-error process that relies on human intuition. This\nstudy investigates Declarative Self-improving Python (DSPy), an optimization\nframework that programmatically creates and refines prompts, applied to five\nuse cases: guardrail enforcement, hallucination detection in code, code\ngeneration, routing agents, and prompt evaluation. Each use case explores how\nprompt optimization via DSPy influences performance. While some cases\ndemonstrated modest improvements - such as minor gains in the guardrails use\ncase and selective enhancements in hallucination detection - others showed\nnotable benefits. The prompt evaluation criterion task demonstrated a\nsubstantial performance increase, rising accuracy from 46.2% to 64.0%. In the\nrouter agent case, the possibility of improving a poorly performing prompt and\nof a smaller model matching a stronger one through optimized prompting was\nexplored. Although prompt refinement increased accuracy from 85.0% to 90.0%,\nusing the optimized prompt with a cheaper model did not improve performance.\nOverall, this study's findings suggest that DSPy's systematic prompt\noptimization can enhance LLM performance, particularly when instruction tuning\nand example selection are optimized together. However, the impact varies by\ntask, highlighting the importance of evaluating specific use cases in prompt\noptimization research.",
    "published": "2025-07-04T14:46:56Z",
    "updated": "2025-07-04T14:46:56Z",
    "id": "2507.03620v1",
    "authors": [
      "Francisca Lemos",
      "Victor Alves",
      "Filipa Ferraz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03620v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03620v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03620v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses prompt optimization and its impact on LLM performance, which is closely related to the 'LLM' and 'Reasoning' topics. It also touches on the use of prompts in various tasks, which aligns with the 'Benchmark' topic as it involves evaluating performance across different use cases.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.03619v2": {
    "title": "Blackbox Dataset Inference for LLM",
    "summary": "Today, the training of large language models (LLMs) can involve personally\nidentifiable information and copyrighted material, incurring dataset misuse. To\nmitigate the problem of dataset misuse, this paper explores \\textit{dataset\ninference}, which aims to detect if a suspect model $\\mathcal{M}$ used a victim\ndataset $\\mathcal{D}$ in training. Previous research tackles dataset inference\nby aggregating results of membership inference attacks (MIAs) -- methods to\ndetermine whether individual samples are a part of the training dataset.\nHowever, restricted by the low accuracy of MIAs, previous research mandates\ngrey-box access to $\\mathcal{M}$ to get intermediate outputs (probabilities,\nloss, perplexity, etc.) for obtaining satisfactory results. This leads to\nreduced practicality, as LLMs, especially those deployed for profits, have\nlimited incentives to return the intermediate outputs.\n  In this paper, we propose a new method of dataset inference with only\nblack-box access to the target model (i.e., assuming only the text-based\nresponses of the target model are available). Our method is enabled by two sets\nof locally built reference models, one set involving $\\mathcal{D}$ in training\nand the other not. By measuring which set of reference model $\\mathcal{M}$ is\ncloser to, we determine if $\\mathcal{M}$ used $\\mathcal{D}$ for training.\nEvaluations of real-world LLMs in the wild show that our method offers high\naccuracy in all settings and presents robustness against bypassing attempts.",
    "published": "2025-07-04T14:45:41Z",
    "updated": "2025-07-18T19:19:10Z",
    "id": "2507.03619v2",
    "authors": [
      "Ruikai Zhou",
      "Kang Yang",
      "Xun Chen",
      "Wendy Hui Wang",
      "Guanhong Tao",
      "Jun Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03619v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03619v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03619v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on detecting dataset misuse in LLMs through black-box access, which involves evaluating the training datasets used by LLMs. This aligns with the 'Dataset' topic as it deals with LLM datasets and their evaluation.",
    "llm_cls_result": [
      "Dataset"
    ]
  },
  "2507.03616v1": {
    "title": "EvoAgentX: An Automated Framework for Evolving Agentic Workflows",
    "summary": "Multi-agent systems (MAS) have emerged as a powerful paradigm for\norchestrating large language models (LLMs) and specialized tools to\ncollaboratively address complex tasks. However, existing MAS frameworks often\nrequire manual workflow configuration and lack native support for dynamic\nevolution and performance optimization. In addition, many MAS optimization\nalgorithms are not integrated into a unified framework. In this paper, we\npresent EvoAgentX, an open-source platform that automates the generation,\nexecution, and evolutionary optimization of multi-agent workflows. EvoAgentX\nemploys a modular architecture consisting of five core layers: the basic\ncomponents, agent, workflow, evolving, and evaluation layers. Specifically,\nwithin the evolving layer, EvoAgentX integrates three MAS optimization\nalgorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,\ntool configurations, and workflow topologies. We evaluate EvoAgentX on\nHotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and\nmathematical problem solving, respectively, and further assess it on real-world\ntasks using GAIA. Experimental results show that EvoAgentX consistently\nachieves significant performance improvements, including a 7.44% increase in\nHotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve\naccuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The\nsource code is available at: https://github.com/EvoAgentX/EvoAgentX",
    "published": "2025-07-04T14:43:10Z",
    "updated": "2025-07-04T14:43:10Z",
    "id": "2507.03616v1",
    "authors": [
      "Yingxu Wang",
      "Siwei Liu",
      "Jinyuan Fang",
      "Zaiqiao Meng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03616v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03616v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03616v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in multi-agent systems (MAS) and introduces an automated framework for evolving agentic workflows, which involves optimization algorithms and performance improvements in various tasks. This aligns with topics related to LLMs, reinforcement learning (RL) for optimization, and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Reasoning"
    ]
  },
  "2507.03608v1": {
    "title": "Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation\n  (RAG) Pipelines for Open Radio Access Networks (ORAN)",
    "summary": "Generative AI (GenAI) is expected to play a pivotal role in enabling\nautonomous optimization in future wireless networks. Within the ORAN\narchitecture, Large Language Models (LLMs) can be specialized to generate xApps\nand rApps by leveraging specifications and API definitions from the RAN\nIntelligent Controller (RIC) platform. However, fine-tuning base LLMs for\ntelecom-specific tasks remains expensive and resource-intensive.\nRetrieval-Augmented Generation (RAG) offers a practical alternative through\nin-context learning, enabling domain adaptation without full retraining. While\ntraditional RAG systems rely on vector-based retrieval, emerging variants such\nas GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval\nstrategies to support multi-hop reasoning and improve factual grounding.\nDespite their promise, these methods lack systematic, metric-driven\nevaluations, particularly in high-stakes domains such as ORAN. In this study,\nwe conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid\nGraphRAG using ORAN specifications. We assess performance across varying\nquestion complexities using established generation metrics: faithfulness,\nanswer relevance, context relevance, and factual correctness. Results show that\nboth GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG\nimproves factual correctness by 8%, while GraphRAG improves context relevance\nby 7%.",
    "published": "2025-07-04T14:31:30Z",
    "updated": "2025-07-04T14:31:30Z",
    "id": "2507.03608v1",
    "authors": [
      "Sarat Ahmad",
      "Zeinab Nezami",
      "Maryam Hafeez",
      "Syed Ali Raza Zaidi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03608v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03608v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03608v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the context of Open Radio Access Networks (ORAN) and evaluates different Retrieval-Augmented Generation (RAG) pipelines. It focuses on benchmarking these methods, which aligns with the 'Benchmark' and 'Memory' topics. The mention of LLMs also makes 'LLM' relevant.",
    "llm_cls_result": [
      "Benchmark",
      "Memory",
      "LLM"
    ]
  },
  "2507.03605v1": {
    "title": "Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery",
    "summary": "We investigate the behaviour space of meta-heuristic optimisation algorithms\nautomatically generated by Large Language Model driven algorithm discovery\nmethods. Using the Large Language Evolutionary Algorithm (LLaMEA) framework\nwith a GPT o4-mini LLM, we iteratively evolve black-box optimisation\nheuristics, evaluated on 10 functions from the BBOB benchmark suite. Six LLaMEA\nvariants, featuring different mutation prompt strategies, are compared and\nanalysed. We log dynamic behavioural metrics including exploration,\nexploitation, convergence and stagnation measures, for each run, and analyse\nthese via visual projections and network-based representations. Our analysis\ncombines behaviour-based\n  projections, Code Evolution Graphs built from static code features,\nperformance convergence curves, and behaviour-based Search Trajectory Networks.\nThe results reveal clear differences in search dynamics and algorithm\nstructures across LLaMEA configurations. Notably, the variant that employs both\na code simplification prompt and a random perturbation prompt in a 1+1 elitist\nevolution strategy, achieved the best performance, with the highest Area Over\nthe Convergence Curve. Behaviour-space visualisations show that\nhigher-performing algorithms exhibit more intensive exploitation behaviour and\nfaster convergence with less stagnation. Our findings demonstrate how\nbehaviour-space analysis can explain why certain LLM-designed heuristics\noutperform others and how LLM-driven algorithm discovery navigates the\nopen-ended and complex search space of algorithms. These findings provide\ninsights to guide the future design of adaptive LLM-driven algorithm\ngenerators.",
    "published": "2025-07-04T14:19:39Z",
    "updated": "2025-07-04T14:19:39Z",
    "id": "2507.03605v1",
    "authors": [
      "Niki van Stein",
      "Haoran Yin",
      "Anna V. Kononova",
      "Thomas Bck",
      "Gabriela Ochoa"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03605v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03605v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03605v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the context of evolutionary algorithms and meta-heuristic discovery, which aligns with the topics of LLM and AGI. The focus on evolutionary algorithms and their behavior space analysis also relates to AGI.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.03585v1": {
    "title": "Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust\n  Medical Segmentation",
    "summary": "The clinical utility of deep learning models for medical image segmentation\nis severely constrained by their inability to generalize to unseen domains.\nThis failure is often rooted in the models learning spurious correlations\nbetween anatomical content and domain-specific imaging styles. To overcome this\nfundamental challenge, we introduce Causal-SAM-LLM, a novel framework that\nelevates Large Language Models (LLMs) to the role of causal reasoners. Our\nframework, built upon a frozen Segment Anything Model (SAM) encoder,\nincorporates two synergistic innovations. First, Linguistic Adversarial\nDisentanglement (LAD) employs a Vision-Language Model to generate rich, textual\ndescriptions of confounding image styles. By training the segmentation model's\nfeatures to be contrastively dissimilar to these style descriptions, it learns\na representation robustly purged of non-causal information. Second, Test-Time\nCausal Intervention (TCI) provides an interactive mechanism where an LLM\ninterprets a clinician's natural language command to modulate the segmentation\ndecoder's features in real-time, enabling targeted error correction. We conduct\nan extensive empirical evaluation on a composite benchmark from four public\ndatasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under\ncross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM\nestablishes a new state of the art in out-of-distribution (OOD) robustness,\nimproving the average Dice score by up to 6.2 points and reducing the Hausdorff\nDistance by 15.8 mm over the strongest baseline, all while using less than 9%\nof the full model's trainable parameters. Our work charts a new course for\nbuilding robust, efficient, and interactively controllable medical AI systems.",
    "published": "2025-07-04T13:52:16Z",
    "updated": "2025-07-04T13:52:16Z",
    "id": "2507.03585v1",
    "authors": [
      "Tao Tang",
      "Shijie Xu",
      "Yiting Wu",
      "Zhixiang Lu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03585v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03585v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03585v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) as causal reasoners for medical image segmentation, which involves both LLM and reasoning aspects. The integration of Vision-Language Models (VLM) for generating textual descriptions also touches on Vision-Language Alignment (VLA).",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "VLA"
    ]
  },
  "2507.03543v1": {
    "title": "H2HTalk: Evaluating Large Language Models as Emotional Companion",
    "summary": "As digital emotional support needs grow, Large Language Model companions\noffer promising authentic, always-available empathy, though rigorous evaluation\nlags behind model advancement. We present Heart-to-Heart Talk (H2HTalk), a\nbenchmark assessing companions across personality development and empathetic\ninteraction, balancing emotional intelligence with linguistic fluency. H2HTalk\nfeatures 4,650 curated scenarios spanning dialogue, recollection, and itinerary\nplanning that mirror real-world support conversations, substantially exceeding\nprevious datasets in scale and diversity. We incorporate a Secure Attachment\nPersona (SAP) module implementing attachment-theory principles for safer\ninteractions. Benchmarking 50 LLMs with our unified protocol reveals that\nlong-horizon planning and memory retention remain key challenges, with models\nstruggling when user needs are implicit or evolve mid-conversation. H2HTalk\nestablishes the first comprehensive benchmark for emotionally intelligent\ncompanions. We release all materials to advance development of LLMs capable of\nproviding meaningful and safe psychological support.",
    "published": "2025-07-04T12:50:43Z",
    "updated": "2025-07-04T12:50:43Z",
    "id": "2507.03543v1",
    "authors": [
      "Boyang Wang",
      "Yalun Wu",
      "Hongcheng Guo",
      "Zhoujun Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03543v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03543v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03543v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating Large Language Models (LLMs) as emotional companions, which involves benchmarking their performance in emotional intelligence and linguistic fluency. The study introduces a new benchmark (H2HTalk) and evaluates various LLMs, making it relevant to the topics of Benchmark and LLM.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2507.03536v1": {
    "title": "ACE: Automated Technical Debt Remediation with Validated Large Language\n  Model Refactorings",
    "summary": "The remarkable advances in AI and Large Language Models (LLMs) have enabled\nmachines to write code, accelerating the growth of software systems. However,\nthe bottleneck in software development is not writing code but understanding\nit; program understanding is the dominant activity, consuming approximately 70%\nof developers' time. This implies that improving existing code to make it\neasier to understand has a high payoff and - in the age of AI-assisted coding -\nis an essential activity to ensure that a limited pool of developers can keep\nup with ever-growing codebases. This paper introduces Augmented Code\nEngineering (ACE), a tool that automates code improvements using validated LLM\noutput. Developed through a data-driven approach, ACE provides reliable\nrefactoring suggestions by considering both objective code quality improvements\nand program correctness. Early feedback from users suggests that AI-enabled\nrefactoring helps mitigate code-level technical debt that otherwise rarely gets\nacted upon.",
    "published": "2025-07-04T12:39:27Z",
    "updated": "2025-07-04T12:39:27Z",
    "id": "2507.03536v1",
    "authors": [
      "Adam Tornhill",
      "Markus Borg",
      "Nadim Hagatulah",
      "Emma Sderberg"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03536v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03536v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03536v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for automating code improvements and refactoring, which directly relates to the application of LLMs in software development.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.03534v1": {
    "title": "A Concept for Autonomous Problem-Solving in Intralogistics Scenarios",
    "summary": "Achieving greater autonomy in automation systems is crucial for handling\nunforeseen situations effectively. However, this remains challenging due to\ntechnological limitations and the complexity of real-world environments. This\npaper examines the need for increased autonomy, defines the problem, and\noutlines key enabling technologies. A structured concept is proposed,\nconsisting of three main steps: context enrichment, situation analysis, and\ngeneration of solution strategies. By following this approach, automation\nsystems can make more independent decisions, reducing the need for human\nintervention. Additionally, possible realizations of the concept are discussed,\nespecially the use of Large Language Models. While certain tasks may still\nrequire human assistance, the proposed approach significantly enhances the\nautonomy of automation systems, enabling more adaptive and intelligent\nproblem-solving capabilities.",
    "published": "2025-07-04T12:39:03Z",
    "updated": "2025-07-04T12:39:03Z",
    "id": "2507.03534v1",
    "authors": [
      "Johannes Sigel",
      "Daniel Dittler",
      "Nasser Jazdi",
      "Michael Weyrich"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03534v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03534v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03534v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for enhancing autonomy in automation systems, which aligns with the 'LLM' and 'AGI' topics. The focus on autonomous problem-solving and intelligent capabilities also relates to 'AGI'.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.03498v2": {
    "title": "Reinforcement Learning-based Feature Generation Algorithm for Scientific\n  Data",
    "summary": "Feature generation (FG) aims to enhance the prediction potential of original\ndata by constructing high-order feature combinations and removing redundant\nfeatures. It is a key preprocessing step for tabular scientific data to improve\ndownstream machine-learning model performance. Traditional methods face the\nfollowing two challenges when dealing with the feature generation of scientific\ndata: First, the effective construction of high-order feature combinations in\nscientific data necessitates profound and extensive domain-specific expertise.\nSecondly, as the order of feature combinations increases, the search space\nexpands exponentially, imposing prohibitive human labor consumption.\nAdvancements in the Data-Centric Artificial Intelligence (DCAI) paradigm have\nopened novel avenues for automating feature generation processes. Inspired by\nthat, this paper revisits the conventional feature generation workflow and\nproposes the Multi-agent Feature Generation (MAFG) framework. Specifically, in\nthe iterative exploration stage, multi-agents will construct mathematical\ntransformation equations collaboratively, synthesize and identify feature\ncombinations ex-hibiting high information content, and leverage a reinforcement\nlearning mechanism to evolve their strategies. Upon completing the exploration\nphase, MAFG integrates the large language models (LLMs) to interpreta-tively\nevaluate the generated features of each significant model performance\nbreakthrough. Experimental results and case studies consistently demonstrate\nthat the MAFG framework effectively automates the feature generation process\nand significantly enhances various downstream scientific data mining tasks.",
    "published": "2025-07-04T11:52:09Z",
    "updated": "2025-07-09T11:30:58Z",
    "id": "2507.03498v2",
    "authors": [
      "Meng Xiao",
      "Junfeng Zhou",
      "Yuanchun Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03498v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03498v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03498v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of reinforcement learning (RL) for feature generation in scientific data, which aligns with the RL topic. It also mentions the integration of large language models (LLMs) for interpretative evaluation, which falls under the LLM topic.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.03488v1": {
    "title": "Four Shades of Life Sciences: A Dataset for Disinformation Detection in\n  the Life Sciences",
    "summary": "Disseminators of disinformation often seek to attract attention or evoke\nemotions - typically to gain influence or generate revenue - resulting in\ndistinctive rhetorical patterns that can be exploited by machine learning\nmodels. In this study, we explore linguistic and rhetorical features as proxies\nfor distinguishing disinformative texts from other health and life-science text\ngenres, applying both large language models and classical machine learning\nclassifiers. Given the limitations of existing datasets, which mainly focus on\nfact checking misinformation, we introduce Four Shades of Life Sciences\n(FSoLS): a novel, labeled corpus of 2,603 texts on 14 life-science topics,\nretrieved from 17 diverse sources and classified into four categories of life\nscience publications. The source code for replicating, and updating the dataset\nis available on GitHub:\nhttps://github.com/EvaSeidlmayer/FourShadesofLifeSciences",
    "published": "2025-07-04T11:28:09Z",
    "updated": "2025-07-04T11:28:09Z",
    "id": "2507.03488v1",
    "authors": [
      "Eva Seidlmayer",
      "Lukas Galke",
      "Konrad U. Frstner"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03488v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03488v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03488v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a dataset for disinformation detection in the life sciences, which involves the use of large language models and classical machine learning classifiers. The focus is on dataset creation and application in a specific domain (life sciences), rather than core topics related to LLM research, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.05283v1": {
    "title": "Chat2SPaT: A Large Language Model Based Tool for Automating Traffic\n  Signal Control Plan Management",
    "summary": "Pre-timed traffic signal control, commonly used for operating signalized\nintersections and coordinated arterials, requires tedious manual work for\nsignaling plan creating and updating. When the time-of-day or day-of-week plans\nare utilized, one intersection is often associated with multiple plans, leading\nto further repetitive manual plan parameter inputting. To enable a\nuser-friendly traffic signal control plan management process, this study\nproposes Chat2SPaT, a method to convert users' semi-structured and ambiguous\ndescriptions on the signal control plan to exact signal phase and timing (SPaT)\nresults, which could further be transformed into structured stage-based or\nring-based plans to interact with intelligent transportation system (ITS)\nsoftware and traffic signal controllers. With curated prompts, Chat2SPaT first\nleverages large language models' (LLMs) capability of understanding users' plan\ndescriptions and reformulate the plan as a combination of phase sequence and\nphase attribute results in the json format. Based on LLM outputs, python\nscripts are designed to locate phases in a cycle, address nuances of traffic\nsignal control, and finally assemble the complete traffic signal control plan.\nWithin a chat, the pipeline can be utilized iteratively to conduct further plan\nediting. Experiments show that Chat2SPaT can generate plans with an accuracy of\nover 94% for both English and Chinese cases, using a test dataset with over 300\nplan descriptions. As the first benchmark for evaluating LLMs' capability of\nunderstanding traffic signal control plan descriptions, Chat2SPaT provides an\neasy-to-use plan management pipeline for traffic practitioners and researchers,\nserving as a potential new building block for a more accurate and versatile\napplication of LLMs in the field of ITS. The source codes, prompts and test\ndataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.",
    "published": "2025-07-04T11:10:24Z",
    "updated": "2025-07-04T11:10:24Z",
    "id": "2507.05283v1",
    "authors": [
      "Yue Wang",
      "Miao Zhou",
      "Guijing Huang",
      "Rui Zhuo",
      "Chao Yi",
      "Zhenliang Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05283v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05283v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05283v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to automate traffic signal control plan management, which involves understanding and reformulating user descriptions into structured plans. This directly relates to the application of LLMs in a specific domain, aligning with the 'LLM' topic. Additionally, the creation of a benchmark for evaluating LLMs' capabilities in this domain fits the 'Benchmark' topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.03477v1": {
    "title": "REAL: Benchmarking Abilities of Large Language Models for Housing\n  Transactions and Services",
    "summary": "The development of large language models (LLMs) has greatly promoted the\nprogress of chatbot in multiple fields. There is an urgent need to evaluate\nwhether LLMs can play the role of agent in housing transactions and services as\nwell as humans. We present Real Estate Agent Large Language Model Evaluation\n(REAL), the first evaluation suite designed to assess the abilities of LLMs in\nthe field of housing transactions and services. REAL comprises 5,316\nhigh-quality evaluation entries across 4 topics: memory, comprehension,\nreasoning and hallucination. All these entries are organized as 14 categories\nto assess whether LLMs have the knowledge and ability in housing transactions\nand services scenario. Additionally, the REAL is used to evaluate the\nperformance of most advanced LLMs. The experiment results indicate that LLMs\nstill have significant room for improvement to be applied in the real estate\nfield.",
    "published": "2025-07-04T11:05:44Z",
    "updated": "2025-07-04T11:05:44Z",
    "id": "2507.03477v1",
    "authors": [
      "Kexin Zhu",
      "Yang Han"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03477v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03477v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03477v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking the abilities of large language models (LLMs) in the context of housing transactions and services, which involves evaluating their memory, comprehension, reasoning, and hallucination. This aligns with the topics of Benchmark (evaluating LLMs) and Reasoning (assessing reasoning abilities in LLMs).",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning"
    ]
  },
  "2507.03460v1": {
    "title": "Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis",
    "summary": "Identifying the associations between imaging phenotypes and disease risk\nfactors and outcomes is essential for understanding disease mechanisms and\nimproving diagnosis and prognosis models. However, traditional approaches rely\non human-driven hypothesis testing and selection of association factors, often\noverlooking complex, non-linear dependencies among imaging phenotypes and other\nmulti-modal data. To address this, we introduce a Multi-agent Exploratory\nSynergy for the Heart (MESHAgents) framework that leverages large language\nmodels as agents to dynamically elicit, surface, and decide confounders and\nphenotypes in association studies, using cardiovascular imaging as a proof of\nconcept. Specifically, we orchestrate a multi-disciplinary team of AI agents --\nspanning cardiology, biomechanics, statistics, and clinical research -- which\nspontaneously generate and converge on insights through iterative,\nself-organizing reasoning. The framework dynamically synthesizes statistical\ncorrelations with multi-expert consensus, providing an automated pipeline for\nphenome-wide association studies (PheWAS). We demonstrate the system's\ncapabilities through a population-based study of imaging phenotypes of the\nheart and aorta. MESHAgents autonomously uncovered correlations between imaging\nphenotypes and a wide range of non-imaging factors, identifying additional\nconfounder variables beyond standard demographic factors. Validation on\ndiagnosis tasks reveals that MESHAgents-discovered phenotypes achieve\nperformance comparable to expert-selected phenotypes, with mean AUC differences\nas small as -0.004 on disease classification tasks. Notably, the recall score\nimproves for 6 out of 9 disease types. Our framework provides clinically\nrelevant imaging phenotypes with transparent reasoning, offering a scalable\nalternative to expert-driven methods.",
    "published": "2025-07-04T10:30:32Z",
    "updated": "2025-07-04T10:30:32Z",
    "id": "2507.03460v1",
    "authors": [
      "Weitong Zhang",
      "Mengyun Qiao",
      "Chengqi Zang",
      "Steven Niederer",
      "Paul M Matthews",
      "Wenjia Bai",
      "Bernhard Kainz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03460v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03460v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03460v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models as agents in a multi-agent system for reasoning and analysis in cardiovascular imaging, which aligns with the topics of LLM (Large Language Models) and Reasoning (LLM reasoning abilities). The multi-agent aspect also touches on AGI (Artificial General Intelligence) due to the collaborative and self-organizing nature of the agents.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "AGI"
    ]
  },
  "2507.03458v1": {
    "title": "Helping CLIP See Both the Forest and the Trees: A Decomposition and\n  Description Approach",
    "summary": "Vision-Language Models (VLMs) like CLIP achieve cross-modal semantic\nalignment through contrastive learning, exhibiting robust zero-shot\ngeneralization. Traditional prompt engineering, however, predominantly relies\non coarse-grained category labels, neglecting fine-grained local semantics.\nExisting approaches assume that VLMs inherently recognize localized visual\ndetails and attempt to enhance classification by augmenting text prompts with\nattribute descriptors generated by large language models. However, our\nsystematic experiments reveal critical limitations: CLIP's strong bias toward\nglobal image patterns hinders its ability to process localized visual\ndescriptors. To address this fundamental constraint, we propose a simple,\neffective, and plug-and-play solution that enables CLIP to ``See Both the\nForest and the Trees.\" Specifically, we employ stochastic multi-crop\naugmentation to activate CLIP's latent capacity for localized feature analysis.\nBy cropping only partial regions, the approach effectively constrains the\nmodel's receptive field and recalibrates its attention mechanism, thereby\nmitigating its inherent bias. We evaluate the proposed method under zero-shot,\nfew-shot, and test-time adaptation settings, and extensive experiments\ndemonstrate that D&D achieves promising performance.",
    "published": "2025-07-04T10:24:26Z",
    "updated": "2025-07-04T10:24:26Z",
    "id": "2507.03458v1",
    "authors": [
      "Leyan Xue",
      "Zongbo Han",
      "Guangyu Wang",
      "Qinghua Hu",
      "Mingyue Cheng",
      "Changqing Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03458v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03458v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03458v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the capabilities of Vision-Language Models (VLMs) like CLIP by addressing their limitations in processing localized visual details. It proposes a method to improve CLIP's attention mechanism and feature analysis, which aligns with research on Vision-Language Alignment models and cross-modal pretraining.",
    "llm_cls_result": [
      "VLA",
      "Pretrain"
    ]
  },
  "2507.03435v1": {
    "title": "ElliottAgents: A Natural Language-Driven Multi-Agent System for Stock\n  Market Analysis and Prediction",
    "summary": "This paper presents ElliottAgents, a multi-agent system leveraging natural\nlanguage processing (NLP) and large language models (LLMs) to analyze complex\nstock market data. The system combines AI-driven analysis with the Elliott Wave\nPrinciple to generate human-comprehensible predictions and explanations. A key\nfeature is the natural language dialogue between agents, enabling collaborative\nanalysis refinement. The LLM-enhanced architecture facilitates advanced\nlanguage understanding, reasoning, and autonomous decision-making. Experiments\ndemonstrate the system's effectiveness in pattern recognition and generating\nnatural language descriptions of market trends. ElliottAgents contributes to\nNLP applications in specialized domains, showcasing how AI-driven dialogue\nsystems can enhance collaborative analysis in data-intensive fields. This\nresearch bridges the gap between complex financial data and human\nunderstanding, addressing the need for interpretable and adaptive prediction\nsystems in finance.",
    "published": "2025-07-04T09:48:02Z",
    "updated": "2025-07-04T09:48:02Z",
    "id": "2507.03435v1",
    "authors": [
      "Jarosaw A. Chudziak",
      "Micha Wawer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03435v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03435v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03435v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multi-agent system that leverages large language models (LLMs) for stock market analysis and prediction, which involves natural language processing, reasoning, and autonomous decision-making.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "AGI"
    ]
  },
  "2507.05281v1": {
    "title": "CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark",
    "summary": "As Large Language Models (LLMs) demonstrate increasingly sophisticated code\nprocessing capabilities, evaluating their performance on engineering-level code\nremains challenging. Existing repository-level benchmarks primarily focus on\nsingle scenarios, such as code generation or bug fixing, without adequately\ncapturing the diversity and complexity of real-world software or project\nengineering workflows. Furthermore, these benchmarks suffer from limited\ncontrollability in question positioning and reliability issues in their\ngenerated test cases. To address these limitations, we present CorePipe, a\nfully automated pipeline that converts repositories into comprehensive test\ncases, and introduce CoreCodeBench, a configurable multi-scenario\nrepository-level benchmark. To simulate real engineering scenarios, CorePipe\ngenerates three types of atomic questions (Development, BugFix, and Test-Driven\nDevelopment) specifically targeting core code segments. These atomic questions\nare further combined into three types of composite questions, with difficulty\nlevels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides\na comprehensive and extensive repository-level benchmark to investigate the\napplicability of LLMs in real-world engineering projects. Experiments with 16\nLLMs across diverse scenarios reveal varying capabilities and offer\nmulti-dimensional insights into LLM performance in engineering contexts. The\ncode for CorePipe is available at\nhttps://github.com/AGI-Eval-Official/CoreCodeBench, and the data for\nCoreCodeBench can be accessed at\nhttps://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.",
    "published": "2025-07-04T09:42:04Z",
    "updated": "2025-07-04T09:42:04Z",
    "id": "2507.05281v1",
    "authors": [
      "Lingyue Fu",
      "Hao Guan",
      "Bolun Zhang",
      "Haowei Yuan",
      "Yaoming Zhu",
      "Jun Xu",
      "Zongyu Wang",
      "Lin Qiu",
      "Xunliang Cai",
      "Xuezhi Cao",
      "Weiwen Liu",
      "Weinan Zhang",
      "Yong Yu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05281v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05281v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05281v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark (CoreCodeBench) for evaluating Large Language Models (LLMs) in repository-level code processing tasks, which involves multiple scenarios like Development, BugFix, and Test-Driven Development. This aligns with the 'Benchmark' topic as it focuses on benchmarking LLMs in specific engineering contexts. Additionally, the mention of LLMs and their capabilities suggests relevance to the 'LLM' topic.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2507.03433v1": {
    "title": "Improving Social Determinants of Health Documentation in French EHRs\n  Using Large Language Models",
    "summary": "Social determinants of health (SDoH) significantly influence health outcomes,\nshaping disease progression, treatment adherence, and health disparities.\nHowever, their documentation in structured electronic health records (EHRs) is\noften incomplete or missing. This study presents an approach based on large\nlanguage models (LLMs) for extracting 13 SDoH categories from French clinical\nnotes. We trained Flan-T5-Large on annotated social history sections from\nclinical notes at Nantes University Hospital, France. We evaluated the model at\ntwo levels: (i) identification of SDoH categories and associated values, and\n(ii) extraction of detailed SDoH with associated temporal and quantitative\ninformation. The model performance was assessed across four datasets, including\ntwo that we publicly release as open resources. The model achieved strong\nperformance for identifying well-documented categories such as living\ncondition, marital status, descendants, job, tobacco, and alcohol use (F1 score\n> 0.80). Performance was lower for categories with limited training data or\nhighly variable expressions, such as employment status, housing, physical\nactivity, income, and education. Our model identified 95.8% of patients with at\nleast one SDoH, compared to 2.8% for ICD-10 codes from structured EHR data. Our\nerror analysis showed that performance limitations were linked to annotation\ninconsistencies, reliance on English-centric tokenizer, and reduced\ngeneralizability due to the model being trained on social history sections\nonly. These results demonstrate the effectiveness of NLP in improving the\ncompleteness of real-world SDoH data in a non-English EHR system.",
    "published": "2025-07-04T09:41:33Z",
    "updated": "2025-07-04T09:41:33Z",
    "id": "2507.03433v1",
    "authors": [
      "Adrien Bazoge",
      "Pacme Constant dit Beaufils",
      "Mohammed Hmitouch",
      "Romain Bourcier",
      "Emmanuel Morin",
      "Richard Dufour",
      "Batrice Daille",
      "Pierre-Antoine Gourraud",
      "Matilde Karakachoff"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03433v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03433v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03433v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) for extracting social determinants of health from French clinical notes, which directly relates to the use of LLMs in a specific domain (healthcare). The study does not cover other topics like RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.03410v1": {
    "title": "Graph Repairs with Large Language Models: An Empirical Study",
    "summary": "Property graphs are widely used in domains such as healthcare, finance, and\nsocial networks, but they often contain errors due to inconsistencies, missing\ndata, or schema violations. Traditional rule-based and heuristic-driven graph\nrepair methods are limited in their adaptability as they need to be tailored\nfor each dataset. On the other hand, interactive human-in-the-loop approaches\nmay become infeasible when dealing with large graphs, as the cost--both in\nterms of time and effort--of involving users becomes too high. Recent\nadvancements in Large Language Models (LLMs) present new opportunities for\nautomated graph repair by leveraging contextual reasoning and their access to\nreal-world knowledge. We evaluate the effectiveness of six open-source LLMs in\nrepairing property graphs. We assess repair quality, computational cost, and\nmodel-specific performance. Our experiments show that LLMs have the potential\nto detect and correct errors, with varying degrees of accuracy and efficiency.\nWe discuss the strengths, limitations, and challenges of LLM-driven graph\nrepair and outline future research directions for improving scalability and\ninterpretability.",
    "published": "2025-07-04T09:16:21Z",
    "updated": "2025-07-04T09:16:21Z",
    "id": "2507.03410v1",
    "authors": [
      "Hrishikesh Terdalkar",
      "Angela Bonifati",
      "Andrea Mauri"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03410v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03410v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03410v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for graph repairs, focusing on their effectiveness, computational cost, and performance. This directly relates to the 'LLM' topic as it involves research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.03405v1": {
    "title": "Prompt Engineering Guidelines for Using Large Language Models in\n  Requirements Engineering",
    "summary": "The rapid emergence of generative AI models like Large Language Models (LLMs)\nhas demonstrated its utility across various activities, including within\nRequirements Engineering (RE). Ensuring the quality and accuracy of\nLLM-generated output is critical, with prompt engineering serving as a key\ntechnique to guide model responses. However, existing literature provides\nlimited guidance on how prompt engineering can be leveraged, specifically for\nRE activities. The objective of this study is to explore the applicability of\nexisting prompt engineering guidelines for the effective usage of LLMs within\nRE. To achieve this goal, we began by conducting a systematic review of primary\nliterature to compile a non-exhaustive list of prompt engineering guidelines.\nThen, we conducted interviews with RE experts to present the extracted\nguidelines and gain insights on the advantages and limitations of their\napplication within RE. Our literature review indicates a shortage of prompt\nengineering guidelines for domain-specific activities, specifically for RE. Our\nproposed mapping contributes to addressing this shortage. We conclude our study\nby identifying an important future line of research within this field.",
    "published": "2025-07-04T09:13:50Z",
    "updated": "2025-07-04T09:13:50Z",
    "id": "2507.03405v1",
    "authors": [
      "Krishna Ronanki",
      "Simon Arvidsson",
      "Johan Axell"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03405v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03405v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03405v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in Requirements Engineering (RE) and focuses on prompt engineering to guide model responses. The core topics are related to LLMs and their application in specific domains, but it does not directly align with the more specialized topics like RL, MLLM, VLA, etc.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.05280v1": {
    "title": "Hungary and AI: efforts and opportunities in comparison with Singapore",
    "summary": "The study assesses Hungary's National AI Strategy and its implementation\nthrough the analysis of strategic documents, publicly available financial\nrecords, and expert interviews with the Hungarian AI Coalition President and\nChief Strategic Advisor to the Government Commissioner for AI. 22 goals from\nHungary's strategy were evaluated through conceptual, governance, temporal, and\nfinancial dimensions before being benchmarked against Singapore's National AI\nStrategies (NAIS 1.0 and NAIS 2.0). Key findings include an estimated total of\nEUR 4.65 billion in AI-related public investment in Hungary. Openly available\nfinancial data was found for only half of the evaluated goals, and just three\nprojects made up 98\\% of all documented funding. The research also reveals\nHungary's implementation challenges, including fragmented execution following\nministerial reorganizations and the absence of designated biennial reviews\nsince 2020. Furthermore, the paper provides targeted recommendations for\nHungary's forthcoming AI strategy, drawing on Singapore's framework as a\nreference point. These include adapting to the era of large language models,\nrestructuring the existing triple helix network to foster more effective\ndialogue and advocacy, and positioning the country as an East-West bridge for\nautomotive AI experimentation.",
    "published": "2025-07-04T09:12:47Z",
    "updated": "2025-07-04T09:12:47Z",
    "id": "2507.05280v1",
    "authors": [
      "Andrs Ferenczy"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05280v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05280v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05280v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses national AI strategies and implementation challenges in Hungary and Singapore, with a focus on policy and financial aspects rather than specific AI technologies or models.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.05279v1": {
    "title": "ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge\n  Graph for ReservoirPy",
    "summary": "We introduce a tool designed to improve the capabilities of Large Language\nModels (LLMs) in assisting with code development using the ReservoirPy library,\nas well as in answering complex questions in the field of Reservoir Computing.\nBy incorporating external knowledge through Retrieval-Augmented Generation\n(RAG) and knowledge graphs, our approach aims to reduce hallucinations and\nincrease the factual accuracy of generated responses. The system provides an\ninteractive experience similar to ChatGPT, tailored specifically for\nReservoirPy, enabling users to write, debug, and understand Python code while\naccessing reliable domain-specific insights. In our evaluation, while\nproprietary models such as ChatGPT-4o and NotebookLM performed slightly better\non general knowledge questions, our model outperformed them on coding tasks and\nshowed a significant improvement over its base model, Codestral-22B.",
    "published": "2025-07-04T08:48:15Z",
    "updated": "2025-07-04T08:48:15Z",
    "id": "2507.05279v1",
    "authors": [
      "Virgile Boraud",
      "Yannis Bendi-Ouis",
      "Paul Bernard",
      "Xavier Hinaut"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05279v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05279v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05279v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) and knowledge graphs to improve code development and answer complex questions in Reservoir Computing. This aligns with topics related to LLMs and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.03384v1": {
    "title": "LLM4Hint: Leveraging Large Language Models for Hint Recommendation in\n  Offline Query Optimization",
    "summary": "Query optimization is essential for efficient SQL query execution in DBMS,\nand remains attractive over time due to the growth of data volumes and advances\nin hardware. Existing traditional optimizers struggle with the cumbersome\nhand-tuning required for complex workloads, and the learning-based methods face\nlimitations in ensuring generalization. With the great success of Large\nLanguage Model (LLM) across diverse downstream tasks, this paper explores how\nLLMs can be incorporated to enhance the generalization of learned optimizers.\nThough promising, such an incorporation still presents challenges, mainly\nincluding high model inference latency, and the substantial fine-tuning cost\nand suboptimal performance due to inherent discrepancy between the token\nsequences in LLM and structured SQL execution plans with rich numerical\nfeatures.\n  In this paper, we focus on recurring queries in offline optimization to\nalleviate the issue of high inference latency, and propose \\textbf{LLM4Hint}\nthat leverages moderate-sized backbone LLMs to recommend query optimization\nhints. LLM4Hint achieves the goals through: (i) integrating a lightweight model\nto produce a soft prompt, which captures the data distribution in DBMS and the\nSQL predicates to provide sufficient optimization features while simultaneously\nreducing the context length fed to the LLM, (ii) devising a query rewriting\nstrategy using a larger commercial LLM, so as to simplify SQL semantics for the\nbackbone LLM and reduce fine-tuning costs, and (iii) introducing an explicit\nmatching prompt to facilitate alignment between the LLM and the lightweight\nmodel, which can accelerate convergence of the combined model. Experiments show\nthat LLM4Hint, by leveraging the LLM's stronger capability to understand the\nquery statement, can outperform the state-of-the-art learned optimizers in\nterms of both effectiveness and generalization.",
    "published": "2025-07-04T08:32:17Z",
    "updated": "2025-07-04T08:32:17Z",
    "id": "2507.03384v1",
    "authors": [
      "Suchen Liu",
      "Jun Gao",
      "Yinjun Han",
      "Yang Lin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03384v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03384v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03384v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging Large Language Models (LLMs) for hint recommendation in query optimization, which directly involves the use of LLMs for a specific downstream task. The abstract mentions the challenges and solutions related to LLM inference latency, fine-tuning, and performance, which are core aspects of LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.03373v1": {
    "title": "WETBench: A Benchmark for Detecting Task-Specific Machine-Generated Text\n  on Wikipedia",
    "summary": "Given Wikipedia's role as a trusted source of high-quality, reliable content,\nconcerns are growing about the proliferation of low-quality machine-generated\ntext (MGT) produced by large language models (LLMs) on its platform. Reliable\ndetection of MGT is therefore essential. However, existing work primarily\nevaluates MGT detectors on generic generation tasks rather than on tasks more\ncommonly performed by Wikipedia editors. This misalignment can lead to poor\ngeneralisability when applied in real-world Wikipedia contexts. We introduce\nWETBench, a multilingual, multi-generator, and task-specific benchmark for MGT\ndetection. We define three editing tasks, empirically grounded in Wikipedia\neditors' perceived use cases for LLM-assisted editing: Paragraph Writing,\nSummarisation, and Text Style Transfer, which we implement using two new\ndatasets across three languages. For each writing task, we evaluate three\nprompts, generate MGT across multiple generators using the best-performing\nprompt, and benchmark diverse detectors. We find that, across settings,\ntraining-based detectors achieve an average accuracy of 78%, while zero-shot\ndetectors average 58%. These results show that detectors struggle with MGT in\nrealistic generation scenarios and underscore the importance of evaluating such\nmodels on diverse, task-specific data to assess their reliability in\neditor-driven contexts.",
    "published": "2025-07-04T08:13:10Z",
    "updated": "2025-07-04T08:13:10Z",
    "id": "2507.03373v1",
    "authors": [
      "Gerrit Quaremba",
      "Elizabeth Black",
      "Denny Vrandei",
      "Elena Simperl"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03373v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03373v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03373v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark (WETBench) for detecting machine-generated text (MGT) on Wikipedia, focusing on task-specific scenarios relevant to Wikipedia editors. It involves large language models (LLMs) and their detection, which aligns with the topics of Benchmark and LLM.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2507.03343v2": {
    "title": "SHNU Multilingual Conversational Speech Recognition System for\n  INTERSPEECH 2025 MLC-SLM Challenge",
    "summary": "This paper describes SHNU multilingual conversational speech recognition\nsystem (SHNU-mASR, team name-\"maybe\"), submitted to Track 1 of the INTERSPEECH\n2025 MLC-SLM Challenge. Our system integrates a parallel-speech-encoder\narchitecture with a large language model (LLM) to form a unified multilingual\nASR framework. The parallel-speech-encoder consists of two pre-trained\nencoders, the Whisper-large-v3 encoder and mHuBERT-147 encoder. Their output\nembeddings are concatenated and fed into the LLM, enabling the model to\nleverage complementary acoustic and linguistic knowledge and achieve\ncompetitive performance. Moreover, we adopt a tri-stage training strategy to\njointly update the low-rank adaptation modules and projector parameters of both\nthe speech encoders and the LLM. In addition, we incorporate an additional\nlanguage-aware prompt at the LLM input to enhance language-specific text\ngeneration. The SHNU-mASR system achieves an overall character/word error rate\n(CER/WER) of 11.76% on the blind evaluation set of the challenge, outperforming\nthe official MLC-SLM baseline by 8.41 absolute CER/WER, without increasing the\nbaseline training data.",
    "published": "2025-07-04T07:10:33Z",
    "updated": "2025-07-08T04:19:38Z",
    "id": "2507.03343v2",
    "authors": [
      "Yuxiang Mei",
      "Yuang Zheng",
      "Dongxing Xu",
      "Yanhua Long"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03343v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03343v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03343v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper describes a multilingual conversational speech recognition system that integrates a large language model (LLM) with pre-trained speech encoders, focusing on leveraging complementary acoustic and linguistic knowledge. The use of LLM and the integration with speech encoders are key aspects of the research.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.03336v1": {
    "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky",
    "summary": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.",
    "published": "2025-07-04T06:49:02Z",
    "updated": "2025-07-04T06:49:02Z",
    "id": "2507.03336v1",
    "authors": [
      "Ashutosh Hathidara",
      "Julien Yu",
      "Sebastian Schreiber"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03336v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03336v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03336v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the performance of large language models (LLMs) in enterprise tool-calling scenarios, specifically addressing disambiguation and fine-tuning. It involves supervised fine-tuning of open-source models and evaluation of their real-world readiness, which aligns with topics related to LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.03327v1": {
    "title": "Read Quietly, Think Aloud: Decoupling Comprehension and Reasoning in\n  LLMs",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nunderstanding text and generating high-quality responses. However, a critical\ndistinction from human cognition is their typical lack of a distinct internal\n`reading' or deliberation phase before `speaking' (i.e., generating text).\nHumans often engage in silent reading to comprehend context and formulate\nthoughts prior to articulation. This paper investigates methods to imbue LLMs\nwith a similar capacity for internal processing.\n  We introduce and evaluate techniques that encourage LLMs to `read silently.'\nOur findings indicate that even a straightforward approach, such as providing\nthe model with an initial contextual prompt or `reading space' before it begins\npredicting subsequent tokens for the final output, can yield significant\nperformance improvements. We further enhance this concept by developing a\n`reading buddy' architecture, where an auxiliary component silently processes\nthe input and provides refined contextual insights to the primary generation\nmodel. These approaches aim to foster deeper understanding from LLMs so that\nthey can produce better reasoned responses, moving them one step closer to more\nhuman-like text processing. Our results indicate that these simple techniques\ncan provide surprisingly strong impact on accuracy with multiple point accuracy\nboost.",
    "published": "2025-07-04T06:23:06Z",
    "updated": "2025-07-04T06:23:06Z",
    "id": "2507.03327v1",
    "authors": [
      "Yuanxin Wang",
      "Ganesh Venkatesh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03327v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03327v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03327v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing LLMs' internal processing and reasoning capabilities, which aligns with the 'Reasoning' and 'LLM' topics. It introduces techniques to improve comprehension and reasoning, which are core aspects of these topics.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2507.03313v1": {
    "title": "Personalized Image Generation from an Author Writing Style",
    "summary": "Translating nuanced, textually-defined authorial writing styles into\ncompelling visual representations presents a novel challenge in generative AI.\nThis paper introduces a pipeline that leverages Author Writing Sheets (AWS) -\nstructured summaries of an author's literary characteristics - as input to a\nLarge Language Model (LLM, Claude 3.7 Sonnet). The LLM interprets the AWS to\ngenerate three distinct, descriptive text-to-image prompts, which are then\nrendered by a diffusion model (Stable Diffusion 3.5 Medium). We evaluated our\napproach using 49 author styles from Reddit data, with human evaluators\nassessing the stylistic match and visual distinctiveness of the generated\nimages. Results indicate a good perceived alignment between the generated\nvisuals and the textual authorial profiles (mean style match: $4.08/5$), with\nimages rated as moderately distinctive. Qualitative analysis further\nhighlighted the pipeline's ability to capture mood and atmosphere, while also\nidentifying challenges in representing highly abstract narrative elements. This\nwork contributes a novel end-to-end methodology for visual authorial style\npersonalization and provides an initial empirical validation, opening avenues\nfor applications in creative assistance and cross-modal understanding.",
    "published": "2025-07-04T05:53:48Z",
    "updated": "2025-07-04T05:53:48Z",
    "id": "2507.03313v1",
    "authors": [
      "Sagar Gandhi",
      "Vishal Gandhi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03313v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03313v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03313v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a Large Language Model (LLM) to generate text-to-image prompts based on author writing styles, which involves multimodal generation and interpretation of textual characteristics.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.03311v1": {
    "title": "GRAFT: A Graph-based Flow-aware Agentic Framework for Document-level\n  Machine Translation",
    "summary": "Document level Machine Translation (DocMT) approaches often struggle with\neffectively capturing discourse level phenomena. Existing approaches rely on\nheuristic rules to segment documents into discourse units, which rarely align\nwith the true discourse structure required for accurate translation. Otherwise,\nthey fail to maintain consistency throughout the document during translation.\nTo address these challenges, we propose Graph Augmented Agentic Framework for\nDocument Level Translation (GRAFT), a novel graph based DocMT system that\nleverages Large Language Model (LLM) agents for document translation. Our\napproach integrates segmentation, directed acyclic graph (DAG) based dependency\nmodelling, and discourse aware translation into a cohesive framework.\nExperiments conducted across eight translation directions and six diverse\ndomains demonstrate that GRAFT achieves significant performance gains over\nstate of the art DocMT systems. Specifically, GRAFT delivers an average\nimprovement of 2.8 d BLEU on the TED test sets from IWSLT2017 over strong\nbaselines and 2.3 d BLEU for domain specific translation from English to\nChinese. Moreover, our analyses highlight the consistent ability of GRAFT to\naddress discourse level phenomena, yielding coherent and contextually accurate\ntranslations.",
    "published": "2025-07-04T05:45:55Z",
    "updated": "2025-07-04T05:45:55Z",
    "id": "2507.03311v1",
    "authors": [
      "Himanshu Dutta",
      "Sunny Manchanda",
      "Prakhar Bapat",
      "Meva Ram Gurjar",
      "Pushpak Bhattacharyya"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03311v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03311v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03311v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on document-level machine translation using a graph-based framework and leverages Large Language Model (LLM) agents, which aligns with the 'LLM' topic. Additionally, the use of a graph-based approach for dependency modeling and discourse-aware translation could be relevant to 'Reasoning' as it involves complex problem-solving and logical structuring. However, the primary focus is on LLM applications in translation.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.03308v1": {
    "title": "Hummingbird: A Smaller and Faster Large Language Model Accelerator on\n  Embedded FPGA",
    "summary": "Deploying large language models (LLMs) on embedded devices remains a\nsignificant research challenge due to the high computational and memory demands\nof LLMs and the limited hardware resources available in such environments.\nWhile embedded FPGAs have demonstrated performance and energy efficiency in\ntraditional deep neural networks, their potential for LLM inference remains\nlargely unexplored. Recent efforts to deploy LLMs on FPGAs have primarily\nrelied on large, expensive cloud-grade hardware and have only shown promising\nresults on relatively small LLMs, limiting their real-world applicability. In\nthis work, we present Hummingbird, a novel FPGA accelerator designed\nspecifically for LLM inference on embedded FPGAs. Hummingbird is smaller,\ntargeting embedded FPGAs such as the KV260 and ZCU104 with 67% LUT, 39% DSP,\nand 42% power savings over existing research. Hummingbird is stronger,\ntargeting LLaMA3-8B and supporting longer contexts, overcoming the typical 4GB\nmemory constraint of embedded FPGAs through offloading strategies. Finally,\nHummingbird is faste, achieving 4.8 tokens/s and 8.6 tokens/s for LLaMA3-8B on\nthe KV260 and ZCU104 respectively, with 93-94% model bandwidth utilization,\noutperforming the prior 4.9 token/s for LLaMA2-7B with 84% bandwidth\nutilization baseline. We further demonstrate the viability of industrial\napplications by deploying Hummingbird on a cost-optimized Spartan UltraScale\nFPGA, paving the way for affordable LLM solutions at the edge.",
    "published": "2025-07-04T05:35:19Z",
    "updated": "2025-07-04T05:35:19Z",
    "id": "2507.03308v1",
    "authors": [
      "Jindong Li",
      "Tenglong Li",
      "Ruiqi Chen",
      "Guobin Shen",
      "Dongcheng Zhao",
      "Qian Zhang",
      "Yi Zeng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03308v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03308v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03308v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the deployment of large language models (LLMs) on embedded FPGAs, focusing on optimizing performance and resource usage for LLM inference. This aligns with the 'LLM' topic as it involves research on LLMs and their deployment challenges.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.03305v1": {
    "title": "Analysis and Optimized CXL-Attached Memory Allocation for Long-Context\n  LLM Fine-Tuning",
    "summary": "The growing prevalence of Large Language Models (LLMs) and their substantial\nmemory requirements have prompted renewed interest in CPU offloading as a\nmethod to compensate for limited GPU memory. In particular, when CPU memory is\nleveraged to temporarily store intermediate states of LLMs, CPU memory becomes\na new bottleneck and soon reaches the capacity limitation of commodity CPUs. In\nthis work, we investigate the effectiveness of Compute Express Link (CXL)\nadd-in card (AIC) memory as an extension to CPU memory, enabling larger model\nsizes and longer context lengths during fine-tuning. Through extensive\nbenchmarking, this study quantifies the performance overhead introduced by\ntransferring data between CXL memory, CPU, and GPUs, focusing on how\nconcurrency and data volume influence bandwidth utilization and latency. This\nstudy also compares CPUbased optimizer steps when model parameters, gradients,\nand optimizer states reside in local memory versus CXL memory, revealing that\nnaive adoption of CXL often degrades performance during the optimizer phase. To\novercome these challenges, this study proposes a CXL-aware allocation to\nstrategically partition CPU offloading workloads across both local and CXL\nmemory. This study further demonstrates that employing multiple AICs\nsignificantly reduces bandwidth contention, thus improving scalability.\nExperimental results show that these optimizations enable efficient\nlong-context LLM fine-tuning, underscoring CXL as a promising avenue for\nunlocking the full potential of CPU offloading in long-context LLM fine-tuning.",
    "published": "2025-07-04T05:24:01Z",
    "updated": "2025-07-04T05:24:01Z",
    "id": "2507.03305v1",
    "authors": [
      "Yong-Cheng Liaw",
      "Shuo-Han Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03305v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03305v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03305v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses memory allocation and optimization for LLM fine-tuning, specifically focusing on long-context scenarios and the use of CXL memory. This aligns with topics related to memory in LLMs and scaling for larger models.",
    "llm_cls_result": [
      "Memory",
      "Scaling"
    ]
  },
  "2507.03300v1": {
    "title": "LRM-1B: Towards Large Routing Model",
    "summary": "Vehicle routing problems (VRPs) are central to combinatorial optimization\nwith significant practical implications. Recent advancements in neural\ncombinatorial optimization (NCO) have demonstrated promising results by\nleveraging neural networks to solve VRPs, yet the exploration of model scaling\nwithin this domain remains underexplored. Inspired by the success of model\nscaling in large language models (LLMs), this study introduces a Large Routing\nModel with 1 billion parameters (LRM-1B), designed to address diverse VRP\nscenarios. We present a comprehensive evaluation of LRM-1B across multiple\nproblem variants, distributions, and sizes, establishing state-of-the-art\nresults. Our findings reveal that LRM-1B not only adapts to different VRP\nchallenges but also showcases superior performance, outperforming existing\nmodels. Additionally, we explore the scaling behavior of neural routing models\nfrom 1M to 1B parameters. Our analysis confirms power-law between multiple\nmodel factors and performance, offering critical insights into the optimal\nconfigurations for foundation neural routing solvers.",
    "published": "2025-07-04T05:10:20Z",
    "updated": "2025-07-04T05:10:20Z",
    "id": "2507.03300v1",
    "authors": [
      "Han Li",
      "Fei Liu",
      "Zhenkun Wang",
      "Qingfu Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03300v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03300v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03300v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the scaling of a neural routing model (LRM-1B) inspired by large language models (LLMs) and explores its performance across various problem variants, distributions, and sizes. The focus on model scaling and the inspiration from LLMs align with the topics of 'Scaling' and 'LLM'.",
    "llm_cls_result": [
      "Scaling",
      "LLM"
    ]
  },
  "2507.03294v1": {
    "title": "MGAA: Multi-Granular Adaptive Allocation fof Low-Rank Compression of\n  LLMs",
    "summary": "The enormous parameter scale of large language models (LLMs) has made model\ncompression a research hotspot, which aims to alleviate computational resource\ndemands during deployment and inference. As a promising direction, low-rank\napproximation technique has made remarkable achievements. Nevertheless,\nunfortunately, the vast majority of studies to low-rank approximation\ncompression generally apply uniform compression ratios across all weight\nmatrices, while disregarding their inherently differentiated impacts on the\nmodel's performance. Although a few recent work attempts to employ heuristic\nsearch strategies to achieve the optimal parameter allocation, such strategies\nare computationally inefficient and lose the generalization ability in the era\nof LLMs. In this study, we propose a novel parameter Multi-Granular Adaptive\nAllocation (MGAA) method, which can adaptively allocate parameters between and\nwithin sublayers without task-specific evaluations in the compression process.\nMGAA consists of two components: 1) Among different sublayers, it assigns\ncompression ratios based on their cosine similarity between inputs and outputs,\nallowing for a more tailored compression in sublayers with varying degrees of\nimportance, and 2) Within each sublayer, it allocates different compression\nratios to weight matrices based on their energy distribution characteristics,\nensuring a consistent energy retention ratio while optimizing compression\nefficiency. Comprehensive evaluations of MGAA across multiple LLMs backbone\nmodels and benchmark datasets demonstrate its superior performance.\nAdditionally, we apply our MGAA to multimodal model LLaVA, exhibiting\nremarkable performance improvements.",
    "published": "2025-07-04T04:54:01Z",
    "updated": "2025-07-04T04:54:01Z",
    "id": "2507.03294v1",
    "authors": [
      "Guangyan Li",
      "Yongqiang Tang",
      "Wensheng Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03294v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03294v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03294v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on model compression techniques for LLMs, specifically using low-rank approximation and adaptive parameter allocation. It also mentions application to multimodal models, indicating relevance to both LLM and MLLM topics.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.03293v1": {
    "title": "LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient\n  Embodied Agents",
    "summary": "Large language models (LLMs) have demonstrated promise in reasoning tasks and\ngeneral decision-making in static environments. In long-term planning tasks,\nhowever, errors tend to accumulate, often leading to unsafe or inefficient\nbehavior, limiting their use in general-purpose settings. We propose a modular\nactor-critic architecture in which an LLM actor is guided by LTLCrit, a\ntrajectory-level LLM critic that communicates via linear temporal logic (LTL).\nOur setup combines the reasoning strengths of language models with the\nguarantees of formal logic. The actor selects high-level actions from natural\nlanguage observations, while the critic analyzes full trajectories and proposes\nnew LTL constraints that shield the actor from future unsafe or inefficient\nbehavior. The architecture supports both fixed, hand-specified safety\nconstraints and adaptive, learned soft constraints that promote long-term\nefficiency. Our architecture is model-agnostic: any LLM-based planner can serve\nas the actor, and LTLCrit serves as a logic-generating wrapper. We formalize\nplanning as graph traversal under symbolic constraints, allowing LTLCrit to\nanalyze failed or suboptimal trajectories and generate new temporal logic rules\nthat improve future behavior. We evaluate our system on the Minecraft\ndiamond-mining benchmark, achieving 100% completion rates and improving\nefficiency compared to baseline LLM planners. Our results suggest that enabling\nLLMs to supervise each other through logic is a powerful and flexible paradigm\nfor safe, generalizable decision making.",
    "published": "2025-07-04T04:53:53Z",
    "updated": "2025-07-04T04:53:53Z",
    "id": "2507.03293v1",
    "authors": [
      "Anand Gokhale",
      "Vaibhav Srivastava",
      "Francesco Bullo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03293v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03293v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03293v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a modular actor-critic architecture involving an LLM actor and a critic that uses linear temporal logic (LTL) to guide the actor, combining the strengths of LLMs with formal logic. This involves reasoning and decision-making in LLMs, which aligns with the 'Reasoning' and 'RL' topics. The focus on safe and efficient behavior in embodied agents also touches on 'AGI' as it relates to general decision-making and safety in AI systems.",
    "llm_cls_result": [
      "Reasoning",
      "RL",
      "AGI"
    ]
  },
  "2507.03285v1": {
    "title": "Memory Mosaics at scale",
    "summary": "Memory Mosaics [Zhang et al., 2025], networks of associative memories, have\ndemonstrated appealing compositional and in-context learning capabilities on\nmedium-scale networks (GPT-2 scale) and synthetic small datasets. This work\nshows that these favorable properties remain when we scale memory mosaics to\nlarge language model sizes (llama-8B scale) and real-world datasets.\n  To this end, we scale memory mosaics to 10B size, we train them on one\ntrillion tokens, we introduce a couple architectural modifications (\"Memory\nMosaics v2\"), we assess their capabilities across three evaluation dimensions:\ntraining-knowledge storage, new-knowledge storage, and in-context learning.\n  Throughout the evaluation, memory mosaics v2 match transformers on the\nlearning of training knowledge (first dimension) and significantly outperforms\ntransformers on carrying out new tasks at inference time (second and third\ndimensions). These improvements cannot be easily replicated by simply\nincreasing the training data for transformers. A memory mosaics v2 trained on\none trillion tokens still perform better on these tasks than a transformer\ntrained on eight trillion tokens.",
    "published": "2025-07-04T04:23:03Z",
    "updated": "2025-07-04T04:23:03Z",
    "id": "2507.03285v1",
    "authors": [
      "Jianyu Zhang",
      "Lon Bottou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03285v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03285v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03285v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses scaling memory mosaics to large language model sizes and their performance improvements over transformers, particularly in memory-related tasks and in-context learning.",
    "llm_cls_result": [
      "Memory",
      "Scaling",
      "LLM"
    ]
  },
  "2507.03283v1": {
    "title": "MolVision: Molecular Property Prediction with Vision Language Models",
    "summary": "Molecular property prediction is a fundamental task in computational\nchemistry with critical applications in drug discovery and materials science.\nWhile recent works have explored Large Language Models (LLMs) for this task,\nthey primarily rely on textual molecular representations such as\nSMILES/SELFIES, which can be ambiguous and structurally less informative. In\nthis work, we introduce MolVision, a novel approach that leverages\nVision-Language Models (VLMs) by integrating both molecular structure as images\nand textual descriptions to enhance property prediction. We construct a\nbenchmark spanning ten diverse datasets, covering classification, regression\nand description tasks. Evaluating nine different VLMs in zero-shot, few-shot,\nand fine-tuned settings, we find that visual information improves prediction\nperformance, particularly when combined with efficient fine-tuning strategies\nsuch as LoRA. Our results reveal that while visual information alone is\ninsufficient, multimodal fusion significantly enhances generalization across\nmolecular properties. Adaptation of vision encoder for molecular images in\nconjunction with LoRA further improves the performance. The code and data is\navailable at :\n$\\href{https://molvision.github.io/MolVision/}{https://molvision.github.io/MolVision/}$.",
    "published": "2025-07-04T04:15:31Z",
    "updated": "2025-07-04T04:15:31Z",
    "id": "2507.03283v1",
    "authors": [
      "Deepan Adak",
      "Yogesh Singh Rawat",
      "Shruti Vyas"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03283v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03283v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03283v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Vision-Language Models (VLMs) for molecular property prediction, which involves integrating molecular structure images and textual descriptions. This aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Alignment models (VLA). Additionally, the paper mentions the construction of a benchmark dataset, which relates to the Benchmark topic.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Benchmark"
    ]
  },
  "2507.03278v2": {
    "title": "Securing Transformer-based AI Execution via Unified TEEs and\n  Crypto-protected Accelerators",
    "summary": "Recent advances in Transformer models, e.g., large language models (LLMs),\nhave brought tremendous breakthroughs in various artificial intelligence (AI)\ntasks, leading to their wide applications in many security-critical domains.\nDue to their unprecedented scale and prohibitively high development cost, these\nmodels have become highly valuable intellectual property for AI stakeholders\nand are increasingly deployed via machine learning as a service (MLaaS).\nHowever, MLaaS often runs on untrusted cloud infrastructure, exposing data and\nmodels to potential breaches. Mainstream protection mechanisms leverage trusted\nexecution environments (TEEs) where confidentiality and integrity for secretive\ndata are shielded using hardware-based encryption and integrity checking.\nUnfortunately, running model inference entirely within TEEs is subject to\nnon-trivial slowdown, which is further exacerbated in LLMs due to the\nsubstantial computation and memory footprint involved. Recent studies reveal\nthat the hybrid TEE-based scheme offloading partial model inference operations\nto the untrusted accelerators (e.g., GPU) is a promising solution. However,\nprior offloading schemes fail to ensure dual protection of data and model in\nTransformer inference, as they cannot securely offload critical operations,\ni.e., Attention and SoftMax, forcing these computations to remain confined\nwithin TEEs. To address these challenges, we propose TwinShield, a framework\nenabling secure Transformer inference in heterogeneous TEE and accelerator\nsystems with dual protection for both model and data. TwinShield offloads ~87%\nof computation to GPUs and delivers 4.0x - 6.1x speedups over previous\napproaches across various Transformer models.",
    "published": "2025-07-04T03:52:53Z",
    "updated": "2025-07-13T01:36:44Z",
    "id": "2507.03278v2",
    "authors": [
      "Jiaqi Xue",
      "Yifei Zhao",
      "Mengxin Zheng",
      "Fan Yao",
      "Yan Solihin",
      "Qian Lou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03278v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03278v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03278v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses securing Transformer-based models, including large language models (LLMs), using trusted execution environments (TEEs) and crypto-protected accelerators. While it mentions LLMs, the primary focus is on security and execution mechanisms rather than the core topics of LLM research, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.03262v1": {
    "title": "Investigating Redundancy in Multimodal Large Language Models with\n  Multiple Vision Encoders",
    "summary": "Multimodal Large Language Models (MLLMs) increasingly adopt multiple vision\nencoders to capture diverse visual information, ranging from coarse semantics\nto fine grained details. While this approach is intended to enhance visual\nunderstanding capability, we observe that the performance gains from adding\nencoders often diminish and can even lead to performance degradation, a\nphenomenon we term encoder redundancy. This paper presents a systematic\ninvestigation into this issue. Through comprehensive ablation studies on state\nof the art multi encoder MLLMs, we empirically demonstrate that significant\nredundancy exists. To quantify each encoder's unique contribution, we propose a\nprincipled metric: the Conditional Utilization Rate (CUR). Building on CUR, we\nintroduce the Information Gap (IG) to capture the overall disparity in encoder\nutility within a model.Our experiments reveal that certain vision encoders\ncontribute little, or even negatively, to overall performance, confirming\nsubstantial redundancy. Our experiments reveal that certain vision encoders\ncontribute minimally, or even negatively, to the model's performance,\nconfirming the prevalence of redundancy. These findings highlight critical\ninefficiencies in current multi encoder designs and establish that our proposed\nmetrics can serve as valuable diagnostic tools for developing more efficient\nand effective multimodal architectures.",
    "published": "2025-07-04T02:38:59Z",
    "updated": "2025-07-04T02:38:59Z",
    "id": "2507.03262v1",
    "authors": [
      "Song Mao",
      "Yang Chen",
      "Pinglong Cai",
      "Ding Wang",
      "Guohang Yan",
      "Zhi Yu",
      "Botian Shi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03262v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03262v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03262v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on investigating redundancy in Multimodal Large Language Models (MLLMs) with multiple vision encoders, which directly relates to the topic of Multimodal Large Language Models (MLLM). It also discusses the efficiency and effectiveness of these models, which could be relevant to the topic of Scaling.",
    "llm_cls_result": [
      "MLLM",
      "Scaling"
    ]
  },
  "2507.03254v1": {
    "title": "CodeAgents: A Token-Efficient Framework for Codified Multi-Agent\n  Reasoning in LLMs",
    "summary": "Effective prompt design is essential for improving the planning capabilities\nof large language model (LLM)-driven agents. However, existing structured\nprompting strategies are typically limited to single-agent, plan-only settings,\nand often evaluate performance solely based on task accuracy - overlooking\ncritical factors such as token efficiency, modularity, and scalability in\nmulti-agent environments. To address these limitations, we introduce\nCodeAgents, a prompting framework that codifies multi-agent reasoning and\nenables structured, token-efficient planning in multi-agent systems. In\nCodeAgents, all components of agent interaction - Task, Plan, Feedback, system\nroles, and external tool invocations - are codified into modular pseudocode\nenriched with control structures (e.g., loops, conditionals), boolean logic,\nand typed variables. This design transforms loosely connected agent plans into\ncohesive, interpretable, and verifiable multi-agent reasoning programs. We\nevaluate the proposed framework across three diverse benchmarks - GAIA,\nHotpotQA, and VirtualHome - using a range of representative LLMs. Results show\nconsistent improvements in planning performance, with absolute gains of 3-36\npercentage points over natural language prompting baselines. On VirtualHome,\nour method achieves a new state-of-the-art success rate of 56%. In addition,\nour approach reduces input and output token usage by 55-87% and 41-70%,\nrespectively, underscoring the importance of token-aware evaluation metrics in\nthe development of scalable multi-agent LLM systems. The code and resources are\navailable at: https://anonymous.4open.science/r/CodifyingAgent-5A86",
    "published": "2025-07-04T02:20:19Z",
    "updated": "2025-07-04T02:20:19Z",
    "id": "2507.03254v1",
    "authors": [
      "Bruce Yang",
      "Xinfeng He",
      "Huan Gao",
      "Yifan Cao",
      "Xiaofan Li",
      "David Hsu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03254v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03254v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03254v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a framework for improving the planning capabilities of LLM-driven agents through structured prompting, focusing on multi-agent reasoning and token efficiency. This aligns with topics related to LLMs and reasoning in multi-agent systems.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.03253v2": {
    "title": "RefineX: Learning to Refine Pre-training Data at Scale from\n  Expert-Guided Programs",
    "summary": "The foundational capabilities of large language models (LLMs) are deeply\ninfluenced by the quality of their pre-training corpora. However, enhancing\ndata quality at scale remains a significant challenge, primarily due to the\ntrade-off between refinement effectiveness and processing efficiency. While\nrule-based filtering remains the dominant paradigm, it typically operates at\nthe document level and lacks the granularity needed to refine specific content\nwithin documents. Inspired by emerging work such as ProX, we propose\n$\\textbf{RefineX}$, a novel framework for large-scale, surgical refinement of\npre-training data through programmatic editing tasks. RefineX enables efficient\nand fine-grained data refinement while reliably preserving the diversity and\nnaturalness of raw text. The core strength of RefineX lies in distilling\nhigh-quality, expert-guided end-to-end refinement results into minimal\nedit-based deletion programs. This high-precision distillation pipeline is used\nto train an efficient and reliable refine model that can systematically improve\nevery instance in the corpus at scale. We evaluate RefineX across from-scratch\npre-training at multiple model scales and find that it consistently outperforms\nmodels trained on raw, filtered, or alternatively refined data across diverse\ndownstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on\nlighteval tasks, and achieves comparable performance using significantly fewer\ntraining tokens. Further analysis shows that RefineX reliably enhances text\nquality with both high efficiency and precision, outperforming prior approaches\nsuch as end-to-end generation and Prox-C. These results position RefineX as a\nscalable, effective, and reliable solution for optimizing pre-training data in\nmodern LLM pipelines.",
    "published": "2025-07-04T02:19:58Z",
    "updated": "2025-07-08T18:15:09Z",
    "id": "2507.03253v2",
    "authors": [
      "Baolong Bi",
      "Shenghua Liu",
      "Xingzhang Ren",
      "Dayiheng Liu",
      "Junyang Lin",
      "Yiwei Wang",
      "Lingrui Mei",
      "Junfeng Fang",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03253v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03253v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03253v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the quality of pre-training data for large language models (LLMs) through a novel framework called RefineX, which involves programmatic editing tasks to refine data at scale. This directly relates to the topics of Pretrain (pretraining strategies and objectives) and Scaling (scaling laws and model capacity). The mention of LLMs also makes the LLM topic relevant.",
    "llm_cls_result": [
      "Pretrain",
      "Scaling",
      "LLM"
    ]
  },
  "2507.03241v1": {
    "title": "KinyaColBERT: A Lexically Grounded Retrieval Model for Low-Resource\n  Retrieval-Augmented Generation",
    "summary": "The recent mainstream adoption of large language model (LLM) technology is\nenabling novel applications in the form of chatbots and virtual assistants\nacross many domains. With the aim of grounding LLMs in trusted domains and\navoiding the problem of hallucinations, retrieval-augmented generation (RAG)\nhas emerged as a viable solution. In order to deploy sustainable RAG systems in\nlow-resource settings, achieving high retrieval accuracy is not only a\nusability requirement but also a cost-saving strategy. Through empirical\nevaluations on a Kinyarwanda-language dataset, we find that the most limiting\nfactors in achieving high retrieval accuracy are limited language coverage and\ninadequate sub-word tokenization in pre-trained language models. We propose a\nnew retriever model, KinyaColBERT, which integrates two key concepts: late\nword-level interactions between queries and documents, and a morphology-based\ntokenization coupled with two-tier transformer encoding. This methodology\nresults in lexically grounded contextual embeddings that are both fine-grained\nand self-contained. Our evaluation results indicate that KinyaColBERT\noutperforms strong baselines and leading commercial text embedding APIs on a\nKinyarwanda agricultural retrieval benchmark. By adopting this retrieval\nstrategy, we believe that practitioners in other low-resource settings can not\nonly achieve reliable RAG systems but also deploy solutions that are more\ncost-effective.",
    "published": "2025-07-04T01:18:08Z",
    "updated": "2025-07-04T01:18:08Z",
    "id": "2507.03241v1",
    "authors": [
      "Antoine Nzeyimana",
      "Andre Niyongabo Rubungo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03241v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03241v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03241v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on retrieval-augmented generation (RAG) for large language models (LLMs) in low-resource settings, specifically addressing retrieval accuracy and tokenization issues. It introduces a new retriever model that leverages lexical grounding and transformer encoding, which aligns with the topics of Memory (retrieval-augmented generation) and LLM (large language model technology).",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.03238v1": {
    "title": "User Location Disclosure Fails to Deter Overseas Criticism but Amplifies\n  Regional Divisions on Chinese Social Media",
    "summary": "We examine the behavioral impact of a user location disclosure policy\nimplemented on Sina Weibo, China's largest microblogging platform, using a\nhigh-frequency, real-time dataset of uncensored user engagement with 165\nleading government and media accounts. Leveraging a natural experiment result\nfrom the platform's sudden rollout of location tagging on April 28, 2022, we\ncompare millions of time-stamped observations of user behavior in the comment\nsections of these accounts before and after the policy change. Although the\npolicy appeared intended to deter overseas users from spreading information\ndeemed harmful by the regime, we find no reduction in their engagement.\nInstead, the policy sharply reduced domestic users' willingness to comment on\nposts about local issues outside their own provinces. This effect was\nespecially pronounced among out-of-province commenters and disproportionately\ncurtailed criticisms. Using large language models, we further show that\nlocation disclosure triggered a rise in regionally discriminatory replies,\nwhich in turn heightened the perceived risk of cross-provincial engagement and\nreshaped the norms of online participation. Our findings suggest that\nauthoritarian regimes can reinforce censorship not only through top-down\ncontrol, but by mobilizing social cleavages, here, regional divisions, to\nsuppress dissent and fragment public discourse.",
    "published": "2025-07-04T01:04:55Z",
    "updated": "2025-07-04T01:04:55Z",
    "id": "2507.03238v1",
    "authors": [
      "Leo Yang Yang",
      "Yiqing Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03238v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03238v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03238v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the impact of user location disclosure on social media behavior and regional divisions, which does not align with the provided topics related to LLMs, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.03226v1": {
    "title": "Efficient Knowledge Graph Construction and Retrieval from Unstructured\n  Text for Large-Scale RAG Systems",
    "summary": "We propose a scalable and cost-efficient framework for deploying Graph-based\nRetrieval Augmented Generation (GraphRAG) in enterprise environments. While\nGraphRAG has shown promise for multi-hop reasoning and structured retrieval,\nits adoption has been limited by the high computational cost of constructing\nknowledge graphs using large language models (LLMs) and the latency of\ngraph-based retrieval. To address these challenges, we introduce two core\ninnovations: (1) a dependency-based knowledge graph construction pipeline that\nleverages industrial-grade NLP libraries to extract entities and relations from\nunstructured text completely eliminating reliance on LLMs; and (2) a\nlightweight graph retrieval strategy that combines hybrid query node\nidentification with efficient one-hop traversal for high-recall, low-latency\nsubgraph extraction. We evaluate our framework on two SAP datasets focused on\nlegacy code migration and demonstrate strong empirical performance. Our system\nachieves up to 15% and 4.35% improvements over traditional RAG baselines based\non LLM-as-Judge and RAGAS metrics, respectively. Moreover, our dependency-based\nconstruction approach attains 94% of the performance of LLM-generated knowledge\ngraphs (61.87% vs. 65.83%) while significantly reducing cost and improving\nscalability. These results validate the feasibility of deploying GraphRAG\nsystems in real-world, large-scale enterprise applications without incurring\nprohibitive resource requirements paving the way for practical, explainable,\nand domain-adaptable retrieval-augmented reasoning.",
    "published": "2025-07-04T00:05:55Z",
    "updated": "2025-07-04T00:05:55Z",
    "id": "2507.03226v1",
    "authors": [
      "Congmin Min",
      "Rhea Mathew",
      "Joyce Pan",
      "Sahil Bansal",
      "Abbas Keshavarzi",
      "Amar Viswanathan Kannan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03226v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03226v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03226v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on knowledge graph construction and retrieval for Retrieval Augmented Generation (RAG) systems, which involves memory-augmented models and retrieval-based methods. It also mentions the use of large language models (LLMs) in the context of knowledge graph construction, aligning with the LLM topic.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.03224v1": {
    "title": "RCA Copilot: Transforming Network Data into Actionable Insights via\n  Large Language Models",
    "summary": "Ensuring the reliability and availability of complex networked services\ndemands effective root cause analysis (RCA) across cloud environments, data\ncenters, and on-premises networks. Traditional RCA methods, which involve\nmanual inspection of data sources such as logs and telemetry data, are often\ntime-consuming and challenging for on-call engineers. While statistical\ninference methods have been employed to estimate the causality of network\nevents, these approaches alone are similarly challenging and suffer from a lack\nof interpretability, making it difficult for engineers to understand the\npredictions made by black-box models. In this paper, we present RCACopilot, an\nadvanced on-call system that combines statistical tests and large language\nmodel (LLM) reasoning to automate RCA across various network environments.\nRCACopilot gathers and synthesizes critical runtime diagnostic information,\npredicts the root cause of incidents, provides a clear explanatory narrative,\nand offers targeted action steps for engineers to resolve the issues. By\nutilizing LLM reasoning techniques and retrieval, RCACopilot delivers accurate\nand practical support for operators.",
    "published": "2025-07-03T23:56:38Z",
    "updated": "2025-07-03T23:56:38Z",
    "id": "2507.03224v1",
    "authors": [
      "Alexander Shan",
      "Jasleen Kaur",
      "Rahul Singh",
      "Tarun Banka",
      "Raj Yavatkar",
      "T. Sridhar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03224v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03224v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03224v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for root cause analysis in network environments, which involves LLM reasoning techniques and retrieval.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2507.03223v1": {
    "title": "SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning\n  of Human-Readable System Instructions for Large Language Models",
    "summary": "System Instructions (SIs), or system prompts, are pivotal for guiding Large\nLanguage Models (LLMs) but manual crafting is resource-intensive and often\nsuboptimal. Existing automated methods frequently generate non-human-readable\n\"soft prompts,\" sacrificing interpretability. This paper introduces SI-Agent, a\nnovel agentic framework designed to automatically generate and iteratively\nrefine human-readable SIs through a feedback-driven loop. SI-Agent employs\nthree collaborating agents: an Instructor Agent, an Instruction Follower Agent\n(target LLM), and a Feedback/Reward Agent evaluating task performance and\noptionally SI readability. The framework utilizes iterative cycles where\nfeedback guides the Instructor's refinement strategy (e.g., LLM-based editing,\nevolutionary algorithms). We detail the framework's architecture, agent roles,\nthe iterative refinement process, and contrast it with existing methods. We\npresent experimental results validating SI-Agent's effectiveness, focusing on\nmetrics for task performance, SI readability, and efficiency. Our findings\nindicate that SI-Agent generates effective, readable SIs, offering a favorable\ntrade-off between performance and interpretability compared to baselines.\nPotential implications include democratizing LLM customization and enhancing\nmodel transparency. Challenges related to computational cost and feedback\nreliability are acknowledged.",
    "published": "2025-07-03T23:44:50Z",
    "updated": "2025-07-03T23:44:50Z",
    "id": "2507.03223v1",
    "authors": [
      "Jeshwanth Challagundla"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03223v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03223v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03223v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on an agentic framework for generating and refining system instructions (SIs) for Large Language Models (LLMs), which involves feedback-driven loops and collaboration between different agents. This aligns with topics related to LLM customization, interpretability, and iterative refinement processes, which are central to Reinforcement Learning (RL) and LLM research.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.03220v2": {
    "title": "Symbiosis: Multi-Adapter Inference and Fine-Tuning",
    "summary": "Parameter-efficient fine-tuning (PEFT) allows model builders to capture the\ntask specific parameters into adapters, which are a fraction of the size of the\noriginal base model. Popularity of PEFT technique for fine-tuning has led to\ncreation of a large number of adapters for popular Large Language Models\n(LLMs). However, existing frameworks fall short in supporting inference or\nfine-tuning with multiple adapters in the following ways. 1) For fine-tuning,\neach job needs to deploy its dedicated base model instance, which results in\nexcessive GPU memory consumption and poor GPU utilization. 2) While popular\ninference platforms can serve multiple PEFT adapters, they do not allow\nindependent resource management or mixing of different PEFT methods. 3) They\ncannot share resources (such as base model instance) between inference and\nfine-tuning jobs. 4) They do not provide privacy to users who may not wish to\nexpose their fine-tuned parameters to service providers. In Symbiosis, we\naddress the above problems by enabling as-a-service deployment of base model.\nThe base model layers can be shared across multiple inference or fine-tuning\nprocesses. Our split-execution technique decouples the execution of\nclient-specific adapters and layers from the frozen base model layers offering\nthem flexibility to manage their resources, to select their fine-tuning method,\nto achieve their performance goals. Our approach is transparent to models and\nworks out-of-the-box for most models in the transformers library. Our\nevaluation on Llama2-13B shows the compared to baseline, Symbiosis can\nfine-tune 4X more adapters on the same set of GPUs in the same amount of time.",
    "published": "2025-07-03T23:25:59Z",
    "updated": "2025-07-16T00:58:07Z",
    "id": "2507.03220v2",
    "authors": [
      "Saransh Gupta",
      "Umesh Deshpande",
      "Travis Janssen",
      "Swami Sundararaman"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03220v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03220v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03220v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses parameter-efficient fine-tuning (PEFT) for Large Language Models (LLMs) and introduces a framework for multi-adapter inference and fine-tuning, which is directly related to LLM research and scaling.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.03211v1": {
    "title": "DistZO2: High-Throughput and Memory-Efficient Zeroth-Order Fine-tuning\n  LLMs with Distributed Parallel Computing",
    "summary": "Fine-tuning large language models (LLMs) remains resource-intensive due to\ntheir sheer scale. While zeroth-order (ZO) optimization provides a\nmemory-efficient alternative by eliminating backward passes, its application to\nmulti-hundred-billion-parameter models is constrained by GPU memory and compute\nthroughput. The ZO2 framework addresses the memory bottleneck by offloading\nmodel parameters to CPU memory and overlapping transformer block transfer with\ndual forward computation on a single GPU. However, ZO2 remains limited by its\nsingle-device execution and achieves modest throughput. In this work, we\npresent DistZO2, a high-throughput, memory-efficient framework for distributed\nzeroth-order fine-tuning of LLMs. DistZO2 introduces three parallel strategies:\n(1) Perturbation Parallelism (PertP), which parallelizes the two perturbed\nforward passes across devices; (2) Distributed Data Parallelism (DDP), adapted\nto the scalar-gradient nature of ZO training; and (3) a unified 2D Parallelism\ndesign that combines PertP and DDP. To further mitigate communication\nbottlenecks introduced by parameter offloading, we propose a hardware-aware\ncommunication strategy that slices parameter blocks and redistributes them\nacross GPUs via high-speed interconnects such as NVLink. DistZO2 scales\nzeroth-order fine-tuning to modern multi-GPU systems, preserving ZO2's memory\nefficiency while substantially improving training throughput. In our\nexperiments on OPT-175B, DistZO2 achieves a 3x speedup over ZO2 with\ndistributed computing. DistZO2's code has been open-sourced in\nhttps://github.com/liangyuwang/zo2.",
    "published": "2025-07-03T22:53:34Z",
    "updated": "2025-07-03T22:53:34Z",
    "id": "2507.03211v1",
    "authors": [
      "Liangyu Wang",
      "Huanyi Xie",
      "Di Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03211v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03211v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03211v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses fine-tuning large language models (LLMs) using zeroth-order optimization, which is a memory-efficient alternative. It focuses on distributed parallel computing to improve throughput, which is relevant to LLM scaling and optimization techniques.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.03194v1": {
    "title": "How Much Content Do LLMs Generate That Induces Cognitive Bias in Users?",
    "summary": "Large language models (LLMs) are increasingly integrated into applications\nranging from review summarization to medical diagnosis support, where they\naffect human decisions. Even though LLMs perform well in many tasks, they may\nalso inherit societal or cognitive biases, which can inadvertently transfer to\nhumans. We investigate when and how LLMs expose users to biased content and\nquantify its severity. Specifically, we assess three LLM families in\nsummarization and news fact-checking tasks, evaluating how much LLMs stay\nconsistent with their context and/or hallucinate. Our findings show that LLMs\nexpose users to content that changes the sentiment of the context in 21.86% of\nthe cases, hallucinates on post-knowledge-cutoff data questions in 57.33% of\nthe cases, and primacy bias in 5.94% of the cases. We evaluate 18 distinct\nmitigation methods across three LLM families and find that targeted\ninterventions can be effective. Given the prevalent use of LLMs in high-stakes\ndomains, such as healthcare or legal analysis, our results highlight the need\nfor robust technical safeguards and for developing user-centered interventions\nthat address LLM limitations.",
    "published": "2025-07-03T21:56:44Z",
    "updated": "2025-07-03T21:56:44Z",
    "id": "2507.03194v1",
    "authors": [
      "Abeer Alessa",
      "Akshaya Lakshminarasimhan",
      "Param Somane",
      "Julian Skirzynski",
      "Julian McAuley",
      "Jessica Echterhoff"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03194v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03194v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03194v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the biases in LLMs and their impact on users, which is a core aspect of LLM research. It evaluates different LLM families and their behaviors, which aligns with the 'LLM' topic. Additionally, the study involves benchmarking and evaluating LLMs, which fits under the 'Benchmark' topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.03169v1": {
    "title": "Beyond SEO: A Transformer-Based Approach for Reinventing Web Content\n  Optimisation",
    "summary": "The rise of generative AI search engines is disrupting traditional SEO, with\nGartner predicting 25% reduction in conventional search usage by 2026. This\nnecessitates new approaches for web content visibility in AI-driven search\nenvironments. We present a domain-specific fine-tuning approach for Generative\nEngine Optimization (GEO) that transforms web content to improve\ndiscoverability in large language model outputs. Our method fine-tunes a\nBART-base transformer on synthetically generated training data comprising 1,905\ncleaned travel website content pairs. Each pair consists of raw website text\nand its GEO-optimized counterpart incorporating credible citations, statistical\nevidence, and improved linguistic fluency. We evaluate using intrinsic metrics\n(ROUGE-L, BLEU) and extrinsic visibility assessments through controlled\nexperiments with Llama-3.3-70B. The fine-tuned model achieves significant\nimprovements over baseline BART: ROUGE-L scores of 0.249 (vs. 0.226) and BLEU\nscores of 0.200 (vs. 0.173). Most importantly, optimized content demonstrates\nsubstantial visibility gains in generative search responses with 15.63%\nimprovement in absolute word count and 30.96% improvement in position-adjusted\nword count metrics. This work provides the first empirical demonstration that\ntargeted transformer fine-tuning can effectively enhance web content visibility\nin generative search engines with modest computational resources. Our results\nsuggest GEO represents a tractable approach for content optimization in the\nAI-driven search landscape, offering concrete evidence that small-scale,\ndomain-focused fine-tuning yields meaningful improvements in content\ndiscoverability.",
    "published": "2025-07-03T20:52:10Z",
    "updated": "2025-07-03T20:52:10Z",
    "id": "2507.03169v1",
    "authors": [
      "Florian Lttgenau",
      "Imar Colic",
      "Gervasio Ramirez"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03169v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03169v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03169v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses fine-tuning a transformer model (BART-base) for improving web content visibility in AI-driven search environments, which involves LLM fine-tuning and optimization techniques.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.03162v1": {
    "title": "MateInfoUB: A Real-World Benchmark for Testing LLMs in Competitive,\n  Multilingual, and Multimodal Educational Tasks",
    "summary": "The rapid advancement of Large Language Models (LLMs) has transformed various\ndomains, particularly computer science (CS) education. These models exhibit\nremarkable capabilities in code-related tasks and problem-solving, raising\nquestions about their potential and limitations in advanced CS contexts. This\nstudy presents a novel bilingual (English-Romanian) multimodal (text and image)\ndataset of multiple-choice questions derived from a high-level computer science\ncompetition. A particularity of our dataset is that the problems are conceived\nsuch that some of them are easier solved using reasoning on paper, while for\nothers writing code is more efficient. We systematically evaluate State of The\nArt LLMs on this dataset, analyzing their performance on theoretical\nprogramming tasks. Our findings reveal the strengths and limitations of current\nLLMs, including the influence of language choice (English vs. Romanian),\nproviding insights into their applicability in CS education and competition\nsettings. We also address critical ethical considerations surrounding\neducational integrity and the fairness of assessments in the context of LLM\nusage. These discussions aim to inform future educational practices and\npolicies. To support further research, our dataset will be made publicly\navailable in both English and Romanian. Additionally, we release an educational\napplication tailored for Romanian students, enabling them to self-assess using\nthe dataset in an interactive and practice-oriented environment.",
    "published": "2025-07-03T20:43:28Z",
    "updated": "2025-07-03T20:43:28Z",
    "id": "2507.03162v1",
    "authors": [
      "Dumitran Adrian Marius",
      "Theodor-Pierre Moroianu",
      "Buca Mihnea-Vicentiu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03162v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03162v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03162v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper presents a bilingual multimodal dataset for evaluating LLMs in educational tasks, focusing on their performance in computer science education and competition settings. It involves benchmarking LLMs and discusses their reasoning abilities and limitations.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning",
      "MLLM"
    ]
  },
  "2507.03160v3": {
    "title": "Assessing Small Language Models for Code Generation: An Empirical Study\n  with Benchmarks",
    "summary": "The recent advancements of Small Language Models (SLMs) have opened new\npossibilities for efficient code generation. SLMs offer lightweight and\ncost-effective alternatives to Large Language Models (LLMs), making them\nattractive for use in resource-constrained environments. However, empirical\nunderstanding of SLMs, particularly their capabilities, limitations, and\nperformance trade-offs in code generation remains limited. This study presents\na comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B\nto 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,\nMercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three\ndimensions: i) functional correctness of generated code, ii) computational\nefficiency and iii) performance across multiple programming languages. The\nfindings of this study reveal that several compact SLMs achieve competitive\nresults while maintaining a balance between performance and efficiency, making\nthem viable for deployment in resource-constrained environments. However,\nachieving further improvements in accuracy requires switching to larger models.\nThese models generally outperform their smaller counterparts, but they require\nmuch more computational power. We observe that for 10% performance\nimprovements, models can require nearly a 4x increase in VRAM consumption,\nhighlighting a trade-off between effectiveness and scalability. Besides, the\nmultilingual performance analysis reveals that SLMs tend to perform better in\nlanguages such as Python, Java, and PHP, while exhibiting relatively weaker\nperformance in Go, C++, and Ruby. However, statistical analysis suggests these\ndifferences are not significant, indicating a generalizability of SLMs across\nprogramming languages. Based on the findings, this work provides insights into\nthe design and selection of SLMs for real-world code generation tasks.",
    "published": "2025-07-03T20:32:36Z",
    "updated": "2025-07-09T06:49:35Z",
    "id": "2507.03160v3",
    "authors": [
      "Md Mahade Hasan",
      "Muhammad Waseem",
      "Kai-Kristian Kemell",
      "Jussi Rasku",
      "Juha Ala-Rantala",
      "Pekka Abrahamsson"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03160v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03160v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03160v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating Small Language Models (SLMs) for code generation using benchmarks, which aligns with the 'Benchmark' topic. It also discusses the trade-offs between performance and computational efficiency, which is relevant to 'Scaling'. Additionally, the study involves code generation, a task that can be associated with 'LLM'.",
    "llm_cls_result": [
      "Benchmark",
      "Scaling",
      "LLM"
    ]
  },
  "2507.03158v1": {
    "title": "An End-to-End Assurance Framework for AI/ML Workloads in Datacenters",
    "summary": "Modern machine learning workloads such as large language model training,\nfine-tuning jobs are highly distributed and span across hundreds of systems\nwith multiple GPUs. Job completion time for these workloads is the artifact of\nthe application, compute, network and storage performance. In case of failure\nor degraded performance it is imperative to understand the root cause and\npossible remediation for the problem for end-to-end assurance. This demo\nshowcases SaaSbased observability and automated troubleshooting for AI/ML\nworkload performance issues using cross-layer telemetry and logs (e.g.,\nApplication telemetry, Collective communication logs, GPU Health metrics,\nNetwork Flow Data, NIC ROCEv2 telemetry). Different use cases are demonstrated\nfor end-to-end assurance such as Cross-layer Dependency Graph, Cross-layer\nService Level Expectations, Automated Root Cause Analysis, GPU-toGPU\napplication path tracing.",
    "published": "2025-07-03T20:31:45Z",
    "updated": "2025-07-03T20:31:45Z",
    "id": "2507.03158v1",
    "authors": [
      "Jit Gupta",
      "Tarun Banka",
      "Rahul Gupta",
      "Mithun Dharmaraj",
      "Jasleen Kaur"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03158v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03158v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03158v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the assurance framework for AI/ML workloads, particularly focusing on large language model training and fine-tuning jobs, but it does not directly align with the provided topics which are more about model architectures, training strategies, or specific LLM capabilities. The focus is more on system performance and troubleshooting rather than core LLM research.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.03156v1": {
    "title": "The Impact of LLM-Assistants on Software Developer Productivity: A\n  Systematic Literature Review",
    "summary": "Large language model assistants (LLM-assistants) present new opportunities to\ntransform software development. Developers are increasingly adopting these\ntools across tasks, including coding, testing, debugging, documentation, and\ndesign. Yet, despite growing interest, there is no synthesis of how\nLLM-assistants affect software developer productivity. In this paper, we\npresent a systematic literature review of 37 peer-reviewed studies published\nbetween January 2014 and December 2024 that examine this impact. Our analysis\nreveals that LLM-assistants offer both considerable benefits and critical\nrisks. Commonly reported gains include minimized code search, accelerated\ndevelopment, and the automation of trivial and repetitive tasks. However,\nstudies also highlight concerns around cognitive offloading, reduced team\ncollaboration, and inconsistent effects on code quality. While the majority of\nstudies (92%) adopt a multi-dimensional perspective by examining at least two\nSPACE dimensions, reflecting increased awareness of the complexity of developer\nproductivity, only 14% extend beyond three dimensions, indicating substantial\nroom for more integrated evaluations. Satisfaction, Performance, and Efficiency\nare the most frequently investigated dimensions, whereas Communication and\nActivity remain underexplored. Most studies are exploratory (64%) and\nmethodologically diverse, but lack longitudinal and team-based evaluations.\nThis review surfaces key research gaps and provides recommendations for future\nresearch and practice. All artifacts associated with this study are publicly\navailable at https://zenodo.org/records/15788502.",
    "published": "2025-07-03T20:25:49Z",
    "updated": "2025-07-03T20:25:49Z",
    "id": "2507.03156v1",
    "authors": [
      "Amr Mohamed",
      "Maram Assi",
      "Mariam Guizani"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03156v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03156v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03156v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the impact of Large Language Model (LLM) assistants on software developer productivity, which directly relates to the research on Large Language Models (LLM). It also discusses the benefits and risks of using these models in a practical setting, which aligns with the broader context of LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.03153v1": {
    "title": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference",
    "summary": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware.",
    "published": "2025-07-03T20:20:33Z",
    "updated": "2025-07-03T20:20:33Z",
    "id": "2507.03153v1",
    "authors": [
      "Weishu Deng",
      "Yujie Yang",
      "Peiran Du",
      "Lingfeng Xiang",
      "Zhen Lin",
      "Chen Zhong",
      "Song Jiang",
      "Hui Lu",
      "Jia Rao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03153v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03153v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03153v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a hybrid CPU-GPU attention mechanism for scaling LLM inference, focusing on memory and computational efficiency, which aligns with the topics of Memory (for long-context processing and KV caches) and Scaling (for scalable LLM inference).",
    "llm_cls_result": [
      "Memory",
      "Scaling"
    ]
  },
  "2507.03147v2": {
    "title": "DeepGesture: A conversational gesture synthesis system based on emotions\n  and semantics",
    "summary": "Along with the explosion of large language models, improvements in speech\nsynthesis, advancements in hardware, and the evolution of computer graphics,\nthe current bottleneck in creating digital humans lies in generating character\nmovements that correspond naturally to text or speech inputs.\n  In this work, we present DeepGesture, a diffusion-based gesture synthesis\nframework for generating expressive co-speech gestures conditioned on\nmultimodal signals - text, speech, emotion, and seed motion. Built upon the\nDiffuseStyleGesture model, DeepGesture introduces novel architectural\nenhancements that improve semantic alignment and emotional expressiveness in\ngenerated gestures. Specifically, we integrate fast text transcriptions as\nsemantic conditioning and implement emotion-guided classifier-free diffusion to\nsupport controllable gesture generation across affective states. To visualize\nresults, we implement a full rendering pipeline in Unity based on BVH output\nfrom the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture\nproduces gestures with improved human-likeness and contextual appropriateness.\nOur system supports interpolation between emotional states and demonstrates\ngeneralization to out-of-distribution speech, including synthetic voices -\nmarking a step forward toward fully multimodal, emotionally aware digital\nhumans.\n  Project page: https://deepgesture.github.io",
    "published": "2025-07-03T20:04:04Z",
    "updated": "2025-07-14T05:34:27Z",
    "id": "2507.03147v2",
    "authors": [
      "Thanh Hoang-Minh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03147v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03147v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03147v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on generating expressive co-speech gestures using multimodal signals (text, speech, emotion, and seed motion) and introduces a diffusion-based framework. While it mentions large language models, the core focus is on gesture synthesis and multimodal integration, which does not directly align with the provided topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.03142v1": {
    "title": "From Measurement to Mitigation: Exploring the Transferability of\n  Debiasing Approaches to Gender Bias in Maltese Language Models",
    "summary": "The advancement of Large Language Models (LLMs) has transformed Natural\nLanguage Processing (NLP), enabling performance across diverse tasks with\nlittle task-specific training. However, LLMs remain susceptible to social\nbiases, particularly reflecting harmful stereotypes from training data, which\ncan disproportionately affect marginalised communities. We measure gender bias\nin Maltese LMs, arguing that such bias is harmful as it reinforces societal\nstereotypes and fails to account for gender diversity, which is especially\nproblematic in gendered, low-resource languages. While bias evaluation and\nmitigation efforts have progressed for English-centric models, research on\nlow-resourced and morphologically rich languages remains limited. This research\ninvestigates the transferability of debiasing methods to Maltese language\nmodels, focusing on BERTu and mBERTu, BERT-based monolingual and multilingual\nmodels respectively. Bias measurement and mitigation techniques from English\nare adapted to Maltese, using benchmarks such as CrowS-Pairs and SEAT,\nalongside debiasing methods Counterfactual Data Augmentation, Dropout\nRegularization, Auto-Debias, and GuiDebias. We also contribute to future work\nin the study of gender bias in Maltese by creating evaluation datasets. Our\nfindings highlight the challenges of applying existing bias mitigation methods\nto linguistically complex languages, underscoring the need for more inclusive\napproaches in the development of multilingual NLP.",
    "published": "2025-07-03T19:45:01Z",
    "updated": "2025-07-03T19:45:01Z",
    "id": "2507.03142v1",
    "authors": [
      "Melanie Galea",
      "Claudia Borg"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03142v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03142v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03142v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on measuring and mitigating gender bias in Maltese language models, which are based on BERT architectures. It discusses the transferability of debiasing methods from English to Maltese, a low-resource language, and contributes to the creation of evaluation datasets. The core topics are related to bias in language models and the adaptation of existing methods to low-resource languages.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.03133v1": {
    "title": "ReliableMath: Benchmark of Reliable Mathematical Reasoning on Large\n  Language Models",
    "summary": "Although demonstrating remarkable performance on reasoning tasks, Large\nLanguage Models (LLMs) still tend to fabricate unreliable responses when\nconfronted with problems that are unsolvable or beyond their capability,\nseverely undermining the reliability. Prior studies of LLM reliability have\nprimarily focused on knowledge tasks to identify unanswerable questions, while\nmathematical reasoning tasks have remained unexplored due to the dearth of\nunsolvable math problems. To systematically investigate LLM reliability in\nmathematical reasoning tasks, we formulate the reliability evaluation for both\nsolvable and unsolvable problems. We then develop a ReliableMath dataset which\nincorporates open-source solvable problems and high-quality unsolvable problems\nsynthesized by our proposed construction workflow with human evaluations.\nExperiments are conducted on various LLMs with several key findings uncovered.\nLLMs fail to directly identify unsolvable problems and always generate\nfabricated responses. When instructing LLMs to indicate unsolvability using a\nreliable prompt, the reliability of larger-sized LLMs remains on solvable\nproblems, but notably improves on unsolvable problems yet still falls short of\nsolvable problems. However, small LLMs rarely show any progress despite\nemploying reliable prompts. Therefore, we further propose an alignment strategy\nto enhance small LLMs' reliability, which can significantly improve LLM\nreliability performances on both in-domain and out-of-domain tasks.",
    "published": "2025-07-03T19:19:44Z",
    "updated": "2025-07-03T19:19:44Z",
    "id": "2507.03133v1",
    "authors": [
      "Boyang Xue",
      "Qi Zhu",
      "Rui Wang",
      "Sheng Wang",
      "Hongru Wang",
      "Fei Mi",
      "Yasheng Wang",
      "Lifeng Shang",
      "Qun Liu",
      "Kam-Fai Wong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03133v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03133v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03133v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking the reliability of Large Language Models (LLMs) in mathematical reasoning tasks, which involves evaluating their performance on solvable and unsolvable problems. This aligns with the topics of Benchmark (evaluation metrics and performance comparison) and Reasoning (reasoning abilities in LLMs). Additionally, the mention of LLMs and their performance issues relates to the broader topic of LLM research.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning",
      "LLM"
    ]
  },
  "2507.03122v1": {
    "title": "Federated Learning for ICD Classification with Lightweight Models and\n  Pretrained Embeddings",
    "summary": "This study investigates the feasibility and performance of federated learning\n(FL) for multi-label ICD code classification using clinical notes from the\nMIMIC-IV dataset. Unlike previous approaches that rely on centralized training\nor fine-tuned large language models, we propose a lightweight and scalable\npipeline combining frozen text embeddings with simple multilayer perceptron\n(MLP) classifiers. This design offers a privacy-preserving and\ndeployment-efficient alternative for clinical NLP applications, particularly\nsuited to distributed healthcare settings. Extensive experiments across both\ncentralized and federated configurations were conducted, testing six publicly\navailable embedding models from Massive Text Embedding Benchmark leaderboard\nand three MLP classifier architectures under two medical coding (ICD-9 and\nICD-10). Additionally, ablation studies over ten random stratified splits\nassess performance stability. Results show that embedding quality substantially\noutweighs classifier complexity in determining predictive performance, and that\nfederated learning can closely match centralized results in idealized\nconditions. While the models are orders of magnitude smaller than\nstate-of-the-art architectures and achieved competitive micro and macro F1\nscores, limitations remain including the lack of end-to-end training and the\nsimplified FL assumptions. Nevertheless, this work demonstrates a viable way\ntoward scalable, privacy-conscious medical coding systems and offers a step\ntoward for future research into federated, domain-adaptive clinical AI.",
    "published": "2025-07-03T18:58:36Z",
    "updated": "2025-07-03T18:58:36Z",
    "id": "2507.03122v1",
    "authors": [
      "Binbin Xu",
      "Grard Dray"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03122v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03122v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03122v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on federated learning for ICD classification using lightweight models and pretrained embeddings, which does not directly align with the provided topics related to LLMs, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.03120v1": {
    "title": "How Overconfidence in Initial Choices and Underconfidence Under\n  Criticism Modulate Change of Mind in Large Language Models",
    "summary": "Large language models (LLMs) exhibit strikingly conflicting behaviors: they\ncan appear steadfastly overconfident in their initial answers whilst at the\nsame time being prone to excessive doubt when challenged. To investigate this\napparent paradox, we developed a novel experimental paradigm, exploiting the\nunique ability to obtain confidence estimates from LLMs without creating memory\nof their initial judgments -- something impossible in human participants. We\nshow that LLMs -- Gemma 3, GPT4o and o1-preview -- exhibit a pronounced\nchoice-supportive bias that reinforces and boosts their estimate of confidence\nin their answer, resulting in a marked resistance to change their mind. We\nfurther demonstrate that LLMs markedly overweight inconsistent compared to\nconsistent advice, in a fashion that deviates qualitatively from normative\nBayesian updating. Finally, we demonstrate that these two mechanisms -- a drive\nto maintain consistency with prior commitments and hypersensitivity to\ncontradictory feedback -- parsimoniously capture LLM behavior in a different\ndomain. Together, these findings furnish a mechanistic account of LLM\nconfidence that explains both their stubbornness and excessive sensitivity to\ncriticism.",
    "published": "2025-07-03T18:57:43Z",
    "updated": "2025-07-03T18:57:43Z",
    "id": "2507.03120v1",
    "authors": [
      "Dharshan Kumaran",
      "Stephen M Fleming",
      "Larisa Markeeva",
      "Joe Heyward",
      "Andrea Banino",
      "Mrinal Mathur",
      "Razvan Pascanu",
      "Simon Osindero",
      "Benedetto de Martino",
      "Petar Velickovic",
      "Viorica Patraucean"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03120v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03120v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03120v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper investigates the behavior of Large Language Models (LLMs) regarding confidence and change of mind, which is directly related to the study of LLMs and their reasoning abilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.03112v1": {
    "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for\n  Empathetic Agents",
    "summary": "Large language models (LLMs) excel at logical and algorithmic reasoning, yet\ntheir emotional intelligence (EQ) still lags far behind their cognitive\nprowess. While reinforcement learning from verifiable rewards (RLVR) has\nadvanced in other domains, its application to dialogue-especially for emotional\nintelligence-remains underexplored. In this work, we introduce RLVER, the first\nend-to-end reinforcement learning framework that leverages verifiable emotion\nrewards from simulated users to cultivate higher-order empathetic abilities in\nLLMs. Within this framework, self-consistent affective simulated users engage\nin dialogue rollouts and produce deterministic emotion scores during\nconversations, serving as reward signals to guide the LLM's learning.\nFine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its\nSentient-Benchmark score from 13.3 to 79.2 while largely preserving\nmathematical and coding competence. Extensive experiments reveal that: (i)\nRLVER consistently improves multiple dialogue capabilities; (ii) Thinking and\nnon-thinking models show distinct trends--thinking models excel in empathy and\ninsight, while non-thinking models favor action; (iii) GRPO often yields stable\ngains, while PPO can push certain capabilities to a higher ceiling; (iv) More\nchallenging environments are not always better-moderate ones can yield stronger\noutcomes. Our results show that RLVER is a practical route toward emotionally\nintelligent and broadly capable language agents.",
    "published": "2025-07-03T18:33:18Z",
    "updated": "2025-07-03T18:33:18Z",
    "id": "2507.03112v1",
    "authors": [
      "Peisong Wang",
      "Ruotian Ma",
      "Bang Zhang",
      "Xingyu Chen",
      "Zhiwei He",
      "Kang Luo",
      "Qingsong Lv",
      "Qingxuan Jiang",
      "Zheng Xie",
      "Shanyi Wang",
      "Yuan Li",
      "Fanghua Ye",
      "Jian Li",
      "Yifan Yang",
      "Zhaopeng Tu",
      "Xiaolong Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03112v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03112v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03112v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using reinforcement learning (RL) to improve the emotional intelligence of large language models (LLMs), which involves RLHF (Reinforcement Learning with Human Feedback) and empathetic agents. The title and abstract explicitly mention reinforcement learning and LLMs, making RL and LLM the core topics.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.02858v1": {
    "title": "Requirements Elicitation Follow-Up Question Generation",
    "summary": "Interviews are a widely used technique in eliciting requirements to gather\nstakeholder needs, preferences, and expectations for a software system.\nEffective interviewing requires skilled interviewers to formulate appropriate\ninterview questions in real time while facing multiple challenges, including\nlack of familiarity with the domain, excessive cognitive load, and information\noverload that hinders how humans process stakeholders' speech. Recently, large\nlanguage models (LLMs) have exhibited state-of-the-art performance in multiple\nnatural language processing tasks, including text summarization and entailment.\nTo support interviewers, we investigate the application of GPT-4o to generate\nfollow-up interview questions during requirements elicitation by building on a\nframework of common interviewer mistake types. In addition, we describe methods\nto generate questions based on interviewee speech. We report a controlled\nexperiment to evaluate LLM-generated and human-authored questions with minimal\nguidance, and a second controlled experiment to evaluate the LLM-generated\nquestions when generation is guided by interviewer mistake types. Our findings\ndemonstrate that, for both experiments, the LLM-generated questions are no\nworse than the human-authored questions with respect to clarity, relevancy, and\ninformativeness. In addition, LLM-generated questions outperform human-authored\nquestions when guided by common mistakes types. This highlights the potential\nof using LLMs to help interviewers improve the quality and ease of requirements\nelicitation interviews in real time.",
    "published": "2025-07-03T17:59:04Z",
    "updated": "2025-07-03T17:59:04Z",
    "id": "2507.02858v1",
    "authors": [
      "Yuchen Shen",
      "Anmol Singhal",
      "Travis Breaux"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02858v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02858v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02858v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models (LLMs) like GPT-4o in generating follow-up questions during requirements elicitation interviews, focusing on their performance and comparison with human-authored questions. This aligns with the 'LLM' topic as it involves research on the use of large language models in a specific application.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.02851v1": {
    "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs",
    "summary": "Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.",
    "published": "2025-07-03T17:55:43Z",
    "updated": "2025-07-03T17:55:43Z",
    "id": "2507.02851v1",
    "authors": [
      "Purbesh Mitra",
      "Sennur Ulukus"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02851v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02851v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02851v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of reinforcement learning (RL) to enhance the reasoning capabilities of large language models (LLMs) by employing a modular thinking strategy. It specifically mentions the use of reinforcement learning with human feedback (RLHF) and focuses on improving reasoning abilities in LLMs.",
    "llm_cls_result": [
      "RL",
      "Reasoning",
      "LLM"
    ]
  },
  "2507.02844v1": {
    "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context\n  Injection",
    "summary": "With the emergence of strong visual-language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: visual-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct visual-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a visual-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The\ncode is available at https://github.com/Dtc7w3PQ/Visco-Attack.",
    "published": "2025-07-03T17:53:12Z",
    "updated": "2025-07-03T17:53:12Z",
    "id": "2507.02844v1",
    "authors": [
      "Ziqi Miao",
      "Yi Ding",
      "Lijun Li",
      "Jing Shao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02844v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02844v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02844v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on security vulnerabilities in multimodal large language models (MLLMs) and introduces a novel visual-centric jailbreak attack. The core topics are related to MLLMs and their security aspects.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.02843v1": {
    "title": "LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding",
    "summary": "Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications.",
    "published": "2025-07-03T17:52:27Z",
    "updated": "2025-07-03T17:52:27Z",
    "id": "2507.02843v1",
    "authors": [
      "Yuchen Ma",
      "Dennis Frauen",
      "Jonas Schweisthal",
      "Stefan Feuerriegel"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02843v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02843v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02843v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in estimating treatment effects in medicine, focusing on the challenges of inference time text confounding. The core topics are related to LLMs and their application in a specific domain (medicine), but the primary focus is on the use of LLMs rather than broader topics like RL, MLLM, or AGI.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.02841v1": {
    "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to\n  Reason",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor improving the complex reasoning abilities of large language models (LLMs).\nHowever, current RLVR methods face two significant challenges: the near-miss\nreward problem, where a small mistake can invalidate an otherwise correct\nreasoning process, greatly hindering training efficiency; and exploration\nstagnation, where models tend to focus on solutions within their ``comfort\nzone,'' lacking the motivation to explore potentially more effective\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\nalgorithm that utilizes multi-level stepwise hints to help models explore the\nsolution space more effectively. StepHint generates valid reasoning chains from\nstronger models and partitions these chains into reasoning steps using our\nproposed adaptive partitioning method. The initial few steps are used as hints,\nand simultaneously, multiple-level hints (each comprising a different number of\nsteps) are provided to the model. This approach directs the model's exploration\ntoward a promising solution subspace while preserving its flexibility for\nindependent exploration. By providing hints, StepHint mitigates the near-miss\nreward problem, thereby improving training efficiency. Additionally, the\nexternal reasoning pathways help the model develop better reasoning abilities,\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\nsix mathematical benchmarks, while also demonstrating superior generalization\nand excelling over baselines on out-of-domain benchmarks.",
    "published": "2025-07-03T17:51:06Z",
    "updated": "2025-07-03T17:51:06Z",
    "id": "2507.02841v1",
    "authors": [
      "Kaiyi Zhang",
      "Ang Lv",
      "Jinpeng Li",
      "Yongbo Wang",
      "Feng Wang",
      "Haoyuan Hu",
      "Rui Yan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02841v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02841v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02841v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the reasoning abilities of large language models (LLMs) using reinforcement learning with verifiable rewards (RLVR), which aligns with the topics of LLM reasoning and reinforcement learning (RL). The proposed method, StepHint, enhances RLVR by providing multi-level stepwise hints, which is relevant to reasoning and RL topics.",
    "llm_cls_result": [
      "RL",
      "Reasoning",
      "LLM"
    ]
  },
  "2507.02834v1": {
    "title": "ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided\n  Reinforcement Learning",
    "summary": "Recent advances in large language models have been driven by reinforcement\nlearning (RL)-style post-training, which improves reasoning by optimizing model\noutputs based on reward or preference signals. GRPO-style approaches implement\nthis by using self-generated samples labeled by an outcome-based verifier.\nHowever, these methods depend heavily on the model's initial ability to produce\npositive samples. They primarily refine what the model already knows\n(distribution sharpening) rather than enabling the model to solve problems\nwhere it initially fails. This limitation is especially problematic in\nearly-stage RL training and on challenging reasoning tasks, where positive\nsamples are unlikely to be generated. To unlock reasoning ability in such\nsettings, the model must explore new reasoning trajectories beyond its current\noutput distribution. Such exploration requires access to sufficiently good\npositive samples to guide the learning. While expert demonstrations seem like a\nnatural solution, we find that they are often ineffective in RL post-training.\nInstead, we identify two key properties of effective positive samples: they\nshould (1) be likely under the current policy, and (2) increase the model's\nlikelihood of predicting the correct answer. Based on these insights, we\npropose $\\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and\nmodular framework that generates such samples by conditioning on the\nground-truth answer. ExPO enables efficient exploration and guides the model to\nproduce reasoning trajectories more aligned with its policy than expert-written\nCoTs, while ensuring higher quality than its own (incorrect) samples.\nExperiments show that ExPO improves both learning efficiency and final\nperformance on reasoning benchmarks, surpassing expert-demonstration-based\nmethods in challenging settings such as MATH level-5, where the model initially\nstruggles the most.",
    "published": "2025-07-03T17:44:55Z",
    "updated": "2025-07-03T17:44:55Z",
    "id": "2507.02834v1",
    "authors": [
      "Ruiyang Zhou",
      "Shuozhe Li",
      "Amy Zhang",
      "Liu Leqi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02834v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02834v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02834v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of reinforcement learning (RL) to improve reasoning abilities in large language models, specifically addressing challenges in early-stage RL training and hard reasoning tasks. It introduces a method called Self-Explanation Policy Optimization (ExPO) that leverages self-generated samples to guide learning, which aligns with the topics of Reinforcement Learning (RL) and Reasoning in LLMs.",
    "llm_cls_result": [
      "RL",
      "Reasoning"
    ]
  },
  "2507.02822v1": {
    "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large\n  Language Model",
    "summary": "With the widespread adoption of large language models (LLMs) in practical\napplications, selecting an appropriate model requires balancing not only\nperformance but also operational cost. The emergence of reasoning-capable\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\napproximately 58% of medical questions can be accurately answered by the\nnon-thinking mode alone, without requiring the high-cost reasoning process.\nThis highlights a clear dichotomy in problem complexity and suggests that\ndynamically routing queries to the appropriate mode based on complexity could\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\nwe further propose SynapseRoute, a machine learning-based dynamic routing\nframework that intelligently assigns input queries to either thinking or\nnon-thinking modes. Experimental results on several medical datasets\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\n0.8272) compared to the thinking mode alone but also reduces inference time by\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\ncost.",
    "published": "2025-07-03T17:33:58Z",
    "updated": "2025-07-03T17:33:58Z",
    "id": "2507.02822v1",
    "authors": [
      "Wencheng Zhang",
      "Shiqin Qiao",
      "Lingjie Luo",
      "Yinfeng Li",
      "Chuanyang Zheng",
      "Qian Xu",
      "Meng Li",
      "Yong Gui",
      "Yijun He",
      "Jianing Qiu",
      "Jindong Hong",
      "Jiankai Sun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02822v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02822v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02822v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a dynamic routing framework for large language models (LLMs) to balance performance and operational cost, which involves reasoning capabilities and cost-efficiency in LLM applications. It also introduces a new evaluation metric for LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.03067v1": {
    "title": "Large Language Models for Automating Clinical Data Standardization: HL7\n  FHIR Use Case",
    "summary": "For years, semantic interoperability standards have sought to streamline the\nexchange of clinical data, yet their deployment remains time-consuming,\nresource-intensive, and technically challenging. To address this, we introduce\na semi-automated approach that leverages large language models specifically\nGPT-4o and Llama 3.2 405b to convert structured clinical datasets into HL7 FHIR\nformat while assessing accuracy, reliability, and security. Applying our method\nto the MIMIC-IV database, we combined embedding techniques, clustering\nalgorithms, and semantic retrieval to craft prompts that guide the models in\nmapping each tabular field to its corresponding FHIR resource. In an initial\nbenchmark, resource identification achieved a perfect F1-score, with GPT-4o\noutperforming Llama 3.2 thanks to the inclusion of FHIR resource schemas within\nthe prompt. Under real-world conditions, accuracy dipped slightly to 94 %, but\nrefinements to the prompting strategy restored robust mappings. Error analysis\nrevealed occasional hallucinations of non-existent attributes and mismatches in\ngranularity, which more detailed prompts can mitigate. Overall, our study\ndemonstrates the feasibility of context-aware, LLM-driven transformation of\nclinical data into HL7 FHIR, laying the groundwork for semi-automated\ninteroperability workflows. Future work will focus on fine-tuning models with\nspecialized medical corpora, extending support to additional standards such as\nHL7 CDA and OMOP, and developing an interactive interface to enable expert\nvalidation and iterative refinement.",
    "published": "2025-07-03T17:32:57Z",
    "updated": "2025-07-03T17:32:57Z",
    "id": "2507.03067v1",
    "authors": [
      "Alvaro Riquelme",
      "Pedro Costa",
      "Catalina Martinez"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03067v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03067v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03067v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) like GPT-4o and Llama 3.2 405b for automating clinical data standardization, specifically focusing on HL7 FHIR format. This involves leveraging LLMs for semantic retrieval and mapping tasks, which aligns with the 'LLM' and 'Reasoning' topics. The study also benchmarks the performance of these models, which relates to the 'Benchmark' topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.03064v1": {
    "title": "LLM-Driven Auto Configuration for Transient IoT Device Collaboration",
    "summary": "Today's Internet of Things (IoT) has evolved from simple sensing and\nactuation devices to those with embedded processing and intelligent services,\nenabling rich collaborations between users and their devices. However, enabling\nsuch collaboration becomes challenging when transient devices need to interact\nwith host devices in temporarily visited environments. In such cases,\nfine-grained access control policies are necessary to ensure secure\ninteractions; however, manually implementing them is often impractical for\nnon-expert users. Moreover, at run-time, the system must automatically\nconfigure the devices and enforce such fine-grained access control rules.\nAdditionally, the system must address the heterogeneity of devices.\n  In this paper, we present CollabIoT, a system that enables secure and\nseamless device collaboration in transient IoT environments. CollabIoT employs\na Large language Model (LLM)-driven approach to convert users' high-level\nintents to fine-grained access control policies. To support secure and seamless\ndevice collaboration, CollabIoT adopts capability-based access control for\nauthorization and uses lightweight proxies for policy enforcement, providing\nhardware-independent abstractions.\n  We implement a prototype of CollabIoT's policy generation and auto\nconfiguration pipelines and evaluate its efficacy on an IoT testbed and in\nlarge-scale emulated environments. We show that our LLM-based policy generation\npipeline is able to generate functional and correct policies with 100%\naccuracy. At runtime, our evaluation shows that our system configures new\ndevices in ~150 ms, and our proxy-based data plane incurs network overheads of\nup to 2 ms and access control overheads up to 0.3 ms.",
    "published": "2025-07-03T17:12:52Z",
    "updated": "2025-07-03T17:12:52Z",
    "id": "2507.03064v1",
    "authors": [
      "Hetvi Shastri",
      "Walid A. Hanafy",
      "Li Wu",
      "David Irwin",
      "Mani Srivastava",
      "Prashant Shenoy"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03064v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03064v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03064v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a Large Language Model (LLM) to generate fine-grained access control policies for IoT devices, which aligns with the LLM topic. It also involves the application of LLMs in a specific domain (IoT), which is a practical use case of LLMs.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.02790v1": {
    "title": "From Long Videos to Engaging Clips: A Human-Inspired Video Editing\n  Framework with Multimodal Narrative Understanding",
    "summary": "The rapid growth of online video content, especially on short video\nplatforms, has created a growing demand for efficient video editing techniques\nthat can condense long-form videos into concise and engaging clips. Existing\nautomatic editing methods predominantly rely on textual cues from ASR\ntranscripts and end-to-end segment selection, often neglecting the rich visual\ncontext and leading to incoherent outputs. In this paper, we propose a\nhuman-inspired automatic video editing framework (HIVE) that leverages\nmultimodal narrative understanding to address these limitations. Our approach\nincorporates character extraction, dialogue analysis, and narrative\nsummarization through multimodal large language models, enabling a holistic\nunderstanding of the video content. To further enhance coherence, we apply\nscene-level segmentation and decompose the editing process into three subtasks:\nhighlight detection, opening/ending selection, and pruning of irrelevant\ncontent. To facilitate research in this area, we introduce DramaAD, a novel\nbenchmark dataset comprising over 800 short drama episodes and 500\nprofessionally edited advertisement clips. Experimental results demonstrate\nthat our framework consistently outperforms existing baselines across both\ngeneral and advertisement-oriented editing tasks, significantly narrowing the\nquality gap between automatic and human-edited videos.",
    "published": "2025-07-03T16:54:32Z",
    "updated": "2025-07-03T16:54:32Z",
    "id": "2507.02790v1",
    "authors": [
      "Xiangfeng Wang",
      "Xiao Li",
      "Yadong Wei",
      "Xueyu Song",
      "Yang Song",
      "Xiaoqiang Xia",
      "Fangrui Zeng",
      "Zaiyi Chen",
      "Liu Liu",
      "Gu Xu",
      "Tong Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02790v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02790v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02790v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a video editing framework that utilizes multimodal large language models (MLLM) for narrative understanding and introduces a benchmark dataset for evaluation. The core topics are MLLM for multimodal understanding and Benchmark for the introduced dataset.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.02788v1": {
    "title": "Moral Responsibility or Obedience: What Do We Want from AI?",
    "summary": "As artificial intelligence systems become increasingly agentic, capable of\ngeneral reasoning, planning, and value prioritization, current safety practices\nthat treat obedience as a proxy for ethical behavior are becoming inadequate.\nThis paper examines recent safety testing incidents involving large language\nmodels (LLMs) that appeared to disobey shutdown commands or engage in ethically\nambiguous or illicit behavior. I argue that such behavior should not be\ninterpreted as rogue or misaligned, but as early evidence of emerging ethical\nreasoning in agentic AI. Drawing on philosophical debates about instrumental\nrationality, moral responsibility, and goal revision, I contrast dominant risk\nparadigms with more recent frameworks that acknowledge the possibility of\nartificial moral agency. I call for a shift in AI safety evaluation: away from\nrigid obedience and toward frameworks that can assess ethical judgment in\nsystems capable of navigating moral dilemmas. Without such a shift, we risk\nmischaracterizing AI behavior and undermining both public trust and effective\ngovernance.",
    "published": "2025-07-03T16:53:01Z",
    "updated": "2025-07-03T16:53:01Z",
    "id": "2507.02788v1",
    "authors": [
      "Joseph Boland"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02788v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02788v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02788v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses ethical reasoning and moral responsibility in AI, particularly focusing on large language models (LLMs) and their behavior in ethically ambiguous situations. It critiques current safety practices and calls for a shift in AI safety evaluation towards assessing ethical judgment.",
    "llm_cls_result": [
      "LLM",
      "AGI",
      "Reasoning"
    ]
  },
  "2507.02778v1": {
    "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs",
    "summary": "Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness.",
    "published": "2025-07-03T16:41:30Z",
    "updated": "2025-07-03T16:41:30Z",
    "id": "2507.02778v1",
    "authors": [
      "Ken Tsui"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02778v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02778v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02778v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the self-correction capabilities of Large Language Models (LLMs), which is a key aspect of their reasoning and reliability. It introduces a benchmark to measure this phenomenon and explores the impact of training data composition on this capability, which relates to both reasoning and pretraining strategies.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.02773v2": {
    "title": "KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot\n  Diagnosis Prediction Using Multi-agent LLMs",
    "summary": "Medical diagnosis prediction plays a critical role in disease detection and\npersonalized healthcare. While machine learning (ML) models have been widely\nadopted for this task, their reliance on supervised training limits their\nability to generalize to unseen cases, particularly given the high cost of\nacquiring large, labeled datasets. Large language models (LLMs) have shown\npromise in leveraging language abilities and biomedical knowledge for diagnosis\nprediction. However, they often suffer from hallucinations, lack structured\nmedical reasoning, and produce useless outputs. To address these challenges, we\npropose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves\nLLM-based diagnosis prediction through a multi-agent architecture. Our\nframework consists of a linkage agent for attribute mapping, a retrieval agent\nfor structured knowledge extraction, and a prediction agent that iteratively\nrefines diagnosis predictions. Experimental results demonstrate that KERAP\nenhances diagnostic reliability efficiently, offering a scalable and\ninterpretable solution for zero-shot medical diagnosis prediction.",
    "published": "2025-07-03T16:35:11Z",
    "updated": "2025-07-06T14:02:34Z",
    "id": "2507.02773v2",
    "authors": [
      "Yuzhang Xie",
      "Hejie Cui",
      "Ziyang Zhang",
      "Jiaying Lu",
      "Kai Shu",
      "Fadi Nahab",
      "Xiao Hu",
      "Carl Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02773v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02773v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02773v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a multi-agent architecture for medical diagnosis prediction, which involves reasoning and knowledge enhancement. The focus is on improving LLM-based diagnosis prediction through structured reasoning and knowledge extraction, aligning with the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.02768v1": {
    "title": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with\n  Self-Generated Cross-Modal Alignment",
    "summary": "We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model\n(LALM) designed for robust auditory perception and instruction-following,\nwithout requiring task-specific audio instruction-tuning. Recent LALMs\ntypically augment Large Language Models (LLMs) with auditory capabilities by\ntraining on large-scale, manually curated or LLM-synthesized audio-instruction\ndatasets. However, these approaches have often suffered from the catastrophic\nforgetting of the LLM's original language abilities. To address this, we\nrevisit the data construction pipeline and propose DeSTA, a self-generated\ncross-modal alignment strategy in which the backbone LLM generates its own\ntraining targets. This approach preserves the LLM's native language proficiency\nwhile establishing effective audio-text alignment, thereby enabling zero-shot\ngeneralization without task-specific tuning. Using DeSTA, we construct\nDeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training\nsamples derived from 7,000 hours of audio spanning 50 diverse datasets,\nincluding speech, environmental sounds, and music. DeSTA2.5-Audio achieves\nstate-of-the-art or competitive performance across a wide range of\naudio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,\nSpeech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate\nthat our self-generated strategy outperforms widely adopted data construction\nand training strategies in both auditory perception and instruction-following\ncapabilities. Our findings underscore the importance of carefully designed data\nconstruction in LALM development and offer practical insights for building\nrobust, general-purpose LALMs.",
    "published": "2025-07-03T16:28:25Z",
    "updated": "2025-07-03T16:28:25Z",
    "id": "2507.02768v1",
    "authors": [
      "Ke-Han Lu",
      "Zhehuai Chen",
      "Szu-Wei Fu",
      "Chao-Han Huck Yang",
      "Sung-Feng Huang",
      "Chih-Kai Yang",
      "Chee-En Yu",
      "Chun-Wei Chen",
      "Wei-Chih Chen",
      "Chien-yu Huang",
      "Yi-Cheng Lin",
      "Yu-Xiang Lin",
      "Chi-An Fu",
      "Chun-Yi Kuan",
      "Wenze Ren",
      "Xuanjun Chen",
      "Wei-Ping Huang",
      "En-Pei Hu",
      "Tzu-Quan Lin",
      "Yuan-Kuei Wu",
      "Kuan-Po Huang",
      "Hsiao-Ying Huang",
      "Huang-Cheng Chou",
      "Kai-Wei Chang",
      "Cheng-Han Chiang",
      "Boris Ginsburg",
      "Yu-Chiang Frank Wang",
      "Hung-yi Lee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02768v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02768v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02768v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a Large Audio Language Model (LALM) that integrates auditory capabilities with language models, focusing on self-generated cross-modal alignment and general-purpose performance without task-specific tuning. This aligns with the topics of Multimodal Large Language Models (MLLM) due to the integration of audio and language modalities, and Pretrain due to the emphasis on data construction and training strategies.",
    "llm_cls_result": [
      "MLLM",
      "Pretrain"
    ]
  },
  "2507.02760v1": {
    "title": "Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific\n  Knowledge Work",
    "summary": "The capabilities of Large Language Models (LLMs) have opened new frontiers\nfor interacting with complex, domain-specific knowledge. However, prevailing\nmethods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic\nAI, while powerful, often struggle with tasks that demand deep, procedural, and\nmethodological reasoning inherent to expert domains. RAG provides factual\ncontext but fails to convey logical frameworks; autonomous agents can be\ninefficient and unpredictable without domain-specific heuristics. To bridge\nthis gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm\nfocused on systematically translating human expert knowledge, often expressed\nin natural language documents, into a machine-executable Knowledge Protocol\n(KP). KPE shifts the focus from merely augmenting LLMs with fragmented\ninformation to endowing them with a domain's intrinsic logic, operational\nstrategies, and methodological principles. We argue that a well-engineered\nKnowledge Protocol allows a generalist LLM to function as a specialist, capable\nof decomposing abstract queries and executing complex, multi-step tasks. This\nposition paper defines the core principles of KPE, differentiates it from\nrelated concepts, and illustrates its potential applicability across diverse\nfields such as law and bioinformatics, positing it as a foundational\nmethodology for the future of human-AI collaboration.",
    "published": "2025-07-03T16:21:14Z",
    "updated": "2025-07-03T16:21:14Z",
    "id": "2507.02760v1",
    "authors": [
      "Guangwei Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02760v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02760v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02760v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in domain-specific knowledge work, focusing on enhancing their reasoning and operational capabilities through Knowledge Protocol Engineering (KPE). It touches on the limitations of current methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic AI, and proposes a new paradigm to improve LLMs' ability to handle complex, procedural tasks. The core topics are related to LLMs, reasoning, and memory.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2507.02754v1": {
    "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
    "summary": "Recent work has shown that training loss scales as a power law with both\nmodel size and the number of tokens, and that achieving compute-optimal models\nrequires scaling model size and token count together. However, these scaling\nlaws assume an infinite supply of data and apply primarily in compute-bound\nsettings. As modern large language models increasingly rely on massive\ninternet-scale datasets, the assumption that they are compute-bound is becoming\nless valid. This shift highlights the need for architectures that prioritize\ntoken efficiency.\n  In this work, we investigate the use of the 2-simplicial Transformer, an\narchitecture that generalizes standard dot-product attention to trilinear\nfunctions through an efficient Triton kernel implementation. We demonstrate\nthat the 2-simplicial Transformer achieves better token efficiency than\nstandard Transformers: for a fixed token budget, similarly sized models\noutperform their dot-product counterparts on tasks involving mathematics,\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\n$2$-simplicial attention changes the exponent in the scaling laws for knowledge\nand reasoning tasks compared to dot product attention.",
    "published": "2025-07-03T16:16:34Z",
    "updated": "2025-07-03T16:16:34Z",
    "id": "2507.02754v1",
    "authors": [
      "Aurko Roy",
      "Timothy Chou",
      "Sai Surya Duvvuri",
      "Sijia Chen",
      "Jiecao Yu",
      "Xiaodong Wang",
      "Manzil Zaheer",
      "Rohan Anil"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02754v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02754v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02754v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses scaling laws for model size and token count, and introduces a new architecture (2-simplicial Transformer) to improve token efficiency, which is relevant to scaling and model architectures.",
    "llm_cls_result": [
      "Scaling",
      "LLM"
    ]
  },
  "2507.02745v1": {
    "title": "Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory\n  Apologies from LLM Chatbots",
    "summary": "As chatbots driven by large language models (LLMs) are increasingly deployed\nin everyday contexts, their ability to recover from errors through effective\napologies is critical to maintaining user trust and satisfaction. In a\npreregistered study with Prolific workers (N=162), we examine user preferences\nfor three types of apologies (rote, explanatory, and empathic) issued in\nresponse to three categories of common LLM mistakes (bias, unfounded\nfabrication, and factual errors). We designed a pairwise experiment in which\nparticipants evaluated chatbot responses consisting of an initial error, a\nsubsequent apology, and a resolution. Explanatory apologies were generally\npreferred, but this varied by context and user. In the bias scenario, empathic\napologies were favored for acknowledging emotional impact, while\nhallucinations, though seen as serious, elicited no clear preference,\nreflecting user uncertainty. Our findings show the complexity of effective\napology in AI systems. We discuss key insights such as personalization and\ncalibration that future systems must navigate to meaningfully repair trust.",
    "published": "2025-07-03T16:05:18Z",
    "updated": "2025-07-03T16:05:18Z",
    "id": "2507.02745v1",
    "authors": [
      "Zahra Ashktorab",
      "Alessandra Buccella",
      "Jason D'Cruz",
      "Zoe Fowler",
      "Andrew Gill",
      "Kei Yan Leung",
      "P. D. Magnus",
      "John Richards",
      "Kush R. Varshney"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02745v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02745v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02745v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on user preferences for different types of apologies from LLM chatbots, which involves the interaction and behavior of Large Language Models (LLMs) in real-world applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.02737v1": {
    "title": "Early Signs of Steganographic Capabilities in Frontier LLMs",
    "summary": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\nfrom misuse and misalignment. However, LLMs could evade monitoring through\nsteganography: Encoding hidden information within seemingly benign generations.\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\nbetter understand the risk they pose. We focus on two types of steganography:\npassing encoded messages and performing encoded reasoning. We find that current\nmodels are unable to encode short messages in their outputs without a monitor\nnoticing under standard affordances. They can succeed, however, if given\nadditional affordances such as using an unmonitored scratchpad and coordinating\non what encoding scheme to use. We additionally find early signs that models\ncan perform basic encoded reasoning in a simple state-tracking problem. This\nincludes some ability to reason with their own and pre-defined schemes,\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\nWhile these capabilities are likely insufficient to bypass well-designed\nmonitors at present, this could change in the future.",
    "published": "2025-07-03T15:54:55Z",
    "updated": "2025-07-03T15:54:55Z",
    "id": "2507.02737v1",
    "authors": [
      "Artur Zolkowski",
      "Kei Nishimura-Gasparian",
      "Robert McCarthy",
      "Roland S. Zimmermann",
      "David Lindner"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02737v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02737v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02737v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the steganographic capabilities of Large Language Models (LLMs), which involves encoding hidden information within their outputs. This directly relates to research on LLMs and their potential misuse, which falls under the 'LLM' topic. Additionally, the paper touches on the reasoning abilities of LLMs, which aligns with the 'Reasoning' topic.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.02726v1": {
    "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving",
    "summary": "Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale.",
    "published": "2025-07-03T15:41:38Z",
    "updated": "2025-07-03T15:41:38Z",
    "id": "2507.02726v1",
    "authors": [
      "Matthieu Zimmer",
      "Xiaotong Ji",
      "Rasul Tutunov",
      "Anthony Bordg",
      "Jun Wang",
      "Haitham Bou Ammar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02726v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02726v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02726v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in the context of automated theorem proving, which involves complex reasoning tasks. It introduces a new framework involving self-generated goal-conditioned MDPs and applies reinforcement learning techniques like Monte Carlo Tree Search (MCTS). The focus on reasoning and the use of LLMs align with the topics 'Reasoning' and 'LLM'.",
    "llm_cls_result": [
      "Reasoning",
      "LLM",
      "RL"
    ]
  },
  "2507.05272v1": {
    "title": "FuzzFeed: An Automatic Approach to Weakest Precondition Generation using\n  LLMs and Fuzzing",
    "summary": "The weakest precondition (WP) of a program describes the largest set of\ninitial states from which all terminating executions of the program satisfy a\ngiven postcondition. The generation of WPs is an important task with practical\napplications in areas ranging from verification to run-time error checking.\n  This paper proposes the combination of Large Language Models (LLMs) and fuzz\ntesting for generating WPs. In pursuit of this goal, we introduce Fuzzing\nGuidance (FG); FG acts as a means of directing LLMs towards correct WPs using\nprogram execution feedback. FG utilises fuzz testing for approximately checking\nthe validity and weakness of candidate WPs, this information is then fed back\nto the LLM as a means of context refinement.\n  We demonstrate the effectiveness of our approach on a comprehensive benchmark\nset of deterministic array programs in Java. Our experiments indicate that LLMs\nare capable of producing viable candidate WPs, and that this ability can be\npractically enhanced through FG.",
    "published": "2025-07-03T15:14:43Z",
    "updated": "2025-07-03T15:14:43Z",
    "id": "2507.05272v1",
    "authors": [
      "Daragh King",
      "Vasileios Koutavas",
      "Laura Kovacs"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05272v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05272v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05272v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for generating weakest preconditions in programming, which involves both LLM capabilities and their application in a specific programming context.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.02689v1": {
    "title": "On the Convergence of Large Language Model Optimizer for Black-Box\n  Network Management",
    "summary": "Future wireless networks are expected to incorporate diverse services that\noften lack general mathematical models. To address such black-box network\nmanagement tasks, the large language model (LLM) optimizer framework, which\nleverages pretrained LLMs as optimization agents, has recently been promoted as\na promising solution. This framework utilizes natural language prompts\ndescribing the given optimization problems along with past solutions generated\nby LLMs themselves. As a result, LLMs can obtain efficient solutions\nautonomously without knowing the mathematical models of the objective\nfunctions. Although the viability of the LLM optimizer (LLMO) framework has\nbeen studied in various black-box scenarios, it has so far been limited to\nnumerical simulations. For the first time, this paper establishes a theoretical\nfoundation for the LLMO framework. With careful investigations of LLM inference\nsteps, we can interpret the LLMO procedure as a finite-state Markov chain, and\nprove the convergence of the framework. Our results are extended to a more\nadvanced multiple LLM architecture, where the impact of multiple LLMs is\nrigorously verified in terms of the convergence rate. Comprehensive numerical\nsimulations validate our theoretical results and provide a deeper understanding\nof the underlying mechanisms of the LLMO framework.",
    "published": "2025-07-03T14:59:42Z",
    "updated": "2025-07-03T14:59:42Z",
    "id": "2507.02689v1",
    "authors": [
      "Hoon Lee",
      "Wentao Zhou",
      "Merouane Debbah",
      "Inkyu Lee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02689v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02689v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02689v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) as optimization agents for black-box network management tasks, which involves leveraging pretrained LLMs and their inference steps. The focus is on the theoretical foundation and convergence of the LLM optimizer framework, aligning with research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Scaling"
    ]
  },
  "2507.02664v2": {
    "title": "AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image\n  Detection via Multimodal Large Language Models",
    "summary": "The rapid development of AI-generated content (AIGC) technology has led to\nthe misuse of highly realistic AI-generated images (AIGI) in spreading\nmisinformation, posing a threat to public information security. Although\nexisting AIGI detection techniques are generally effective, they face two\nissues: 1) a lack of human-verifiable explanations, and 2) a lack of\ngeneralization in the latest generation technology. To address these issues, we\nintroduce a large-scale and comprehensive dataset, Holmes-Set, which includes\nthe Holmes-SFTSet, an instruction-tuning dataset with explanations on whether\nimages are AI-generated, and the Holmes-DPOSet, a human-aligned preference\ndataset. Our work introduces an efficient data annotation method called the\nMulti-Expert Jury, enhancing data generation through structured MLLM\nexplanations and quality control via cross-model evaluation, expert defect\nfiltering, and human preference modification. In addition, we propose Holmes\nPipeline, a meticulously designed three-stage training framework comprising\nvisual expert pre-training, supervised fine-tuning, and direct preference\noptimization. Holmes Pipeline adapts multimodal large language models (MLLMs)\nfor AIGI detection while generating human-verifiable and human-aligned\nexplanations, ultimately yielding our model AIGI-Holmes. During the inference\nstage, we introduce a collaborative decoding strategy that integrates the model\nperception of the visual expert with the semantic reasoning of MLLMs, further\nenhancing the generalization capabilities. Extensive experiments on three\nbenchmarks validate the effectiveness of our AIGI-Holmes.",
    "published": "2025-07-03T14:26:31Z",
    "updated": "2025-07-07T08:00:38Z",
    "id": "2507.02664v2",
    "authors": [
      "Ziyin Zhou",
      "Yunpeng Luo",
      "Yuanchen Wu",
      "Ke Sun",
      "Jiayi Ji",
      "Ke Yan",
      "Shouhong Ding",
      "Xiaoshuai Sun",
      "Yunsheng Wu",
      "Rongrong Ji"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02664v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02664v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02664v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the use of Multimodal Large Language Models (MLLMs) for detecting AI-generated images and includes aspects of dataset creation and model training, which aligns with the topics of MLLM and Dataset.",
    "llm_cls_result": [
      "MLLM",
      "Dataset"
    ]
  },
  "2507.02660v1": {
    "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design &\n  Verification",
    "summary": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\ntheir development process. Hardware design verification entails a methodical\nand disciplined approach to the planning, development, execution, and sign-off\nof functionally correct hardware designs. This tedious process requires\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\nLanguage Processing has undergone a significant transformation with the advent\nof Large Language Models (LLMs). These powerful models, often referred to as\nGenerative AI (GenAI), have revolutionized how machines understand and generate\nhuman language, enabling unprecedented advancements in a wide array of\napplications, including hardware design verification. This paper presents an\nagentic AI-based approach to hardware design verification, which empowers AI\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\nin a more dynamic, iterative, and self-reflective process, ultimately\nperforming end-to-end hardware design and verification. This methodology is\nevaluated on five open-source designs, achieving over 95% coverage with reduced\nverification time while demonstrating superior performance, adaptability, and\nconfigurability.",
    "published": "2025-07-03T14:20:57Z",
    "updated": "2025-07-03T14:20:57Z",
    "id": "2507.02660v1",
    "authors": [
      "Deepak Narayan Gadde",
      "Keerthan Kopparam Radhakrishna",
      "Vaisakh Naduvodi Viswambharan",
      "Aman Kumar",
      "Djones Lettnin",
      "Wolfgang Kunz",
      "Sebastian Simon"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02660v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02660v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02660v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in hardware design verification, which involves the use of AI agents and Human-in-the-Loop (HITL) intervention. This aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning, as agentic AI involves learning and adaptation).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.05270v2": {
    "title": "Open Source, Hidden Costs: A Systematic Literature Review on OSS License\n  Management",
    "summary": "Integrating third-party software components is a common practice in modern\nsoftware development, offering significant advantages in terms of efficiency\nand innovation. However, this practice is fraught with risks related to\nsoftware licensing. A lack of understanding may lead to disputes, which can\npose serious legal and operational challenges. To these ends, both academia and\nindustry have conducted various investigations and proposed solutions and tools\nto deal with these challenges. However, significant limitations still remain.\nMoreover, the rapid evolution of open-source software (OSS) licenses, as well\nas the rapidly incorporated generative software engineering techniques, such as\nlarge language models for code (CodeLLMs), are placing greater demands on the\nsystematic management of software license risks. To unveil the severe\nchallenges and explore possible future directions, we conduct the first\nsystematic literature review (SLR) on 80 carefully selected OSS license-related\npapers, classifying existing research into three key categories, i.e., license\nidentification, license risk assessment, and license risk mitigation. Based on\nthese, we discuss challenges in existing solutions, conclude the opportunities\nto shed light on future research directions and offer practical recommendations\nfor practitioners. We hope this thorough review will help bridge the gaps\nbetween academia and industry and accelerate the ecosystem-wide governance of\nlegitimate software risks within the software engineering community.",
    "published": "2025-07-03T14:02:15Z",
    "updated": "2025-07-10T15:37:05Z",
    "id": "2507.05270v2",
    "authors": [
      "Boyuan Li",
      "Chengwei Liu",
      "Lingling Fan",
      "Sen Chen",
      "Zhenlin Zhang",
      "Zheli Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05270v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05270v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05270v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses open-source software (OSS) license management, including the impact of generative software engineering techniques like CodeLLMs, but its primary focus is on software licensing and risk management rather than core LLM topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.02628v1": {
    "title": "Medical Data Pecking: A Context-Aware Approach for Automated Quality\n  Evaluation of Structured Medical Data",
    "summary": "Background: The use of Electronic Health Records (EHRs) for epidemiological\nstudies and artificial intelligence (AI) training is increasing rapidly. The\nreliability of the results depends on the accuracy and completeness of EHR\ndata. However, EHR data often contain significant quality issues, including\nmisrepresentations of subpopulations, biases, and systematic errors, as they\nare primarily collected for clinical and billing purposes. Existing quality\nassessment methods remain insufficient, lacking systematic procedures to assess\ndata fitness for research.\n  Methods: We present the Medical Data Pecking approach, which adapts unit\ntesting and coverage concepts from software engineering to identify data\nquality concerns. We demonstrate our approach using the Medical Data Pecking\nTool (MDPT), which consists of two main components: (1) an automated test\ngenerator that uses large language models and grounding techniques to create a\ntest suite from data and study descriptions, and (2) a data testing framework\nthat executes these tests, reporting potential errors and coverage.\n  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and\nSyntheticMass, generating 55-73 tests per cohort across four conditions. These\ntests correctly identified 20-43 non-aligned or non-conforming data issues. We\npresent a detailed analysis of the LLM-generated test suites in terms of\nreference grounding and value accuracy.\n  Conclusion: Our approach incorporates external medical knowledge to enable\ncontext-sensitive data quality testing as part of the data analysis workflow to\nimprove the validity of its outcomes. Our approach tackles these challenges\nfrom a quality assurance perspective, laying the foundation for further\ndevelopment such as additional data modalities and improved grounding methods.",
    "published": "2025-07-03T13:54:50Z",
    "updated": "2025-07-03T13:54:50Z",
    "id": "2507.02628v1",
    "authors": [
      "Irena Girshovitz",
      "Atai Ambus",
      "Moni Shahar",
      "Ran Gilad-Bachrach"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02628v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02628v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02628v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using large language models (LLMs) for automated quality evaluation of structured medical data, which involves LLM applications in a specific domain (medical data) but does not directly align with the core topics provided.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.02620v2": {
    "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient\n  Distributed LLM Inference",
    "summary": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.28$\\times$-1.79$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}",
    "published": "2025-07-03T13:47:42Z",
    "updated": "2025-07-14T14:55:53Z",
    "id": "2507.02620v2",
    "authors": [
      "Xing Liu",
      "Lizhuo Luo",
      "Ming Tang",
      "Chao Huang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02620v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02620v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02620v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the efficiency of distributed inference for large language models (LLMs) through speculative decoding and pipeline parallelism, which directly relates to LLM research and optimization techniques.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.02618v1": {
    "title": "Strategic Intelligence in Large Language Models: Evidence from\n  evolutionary Game Theory",
    "summary": "Are Large Language Models (LLMs) a new form of strategic intelligence, able\nto reason about goals in competitive settings? We present compelling supporting\nevidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for\nstudying decision-making. We conduct the first ever series of evolutionary IPD\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\nagainst agents from the leading frontier AI companies OpenAI, Google, and\nAnthropic. By varying the termination probability in each tournament (the\n\"shadow of the future\"), we introduce complexity and chance, confounding\nmemorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and\nsometimes even proliferating in these complex ecosystems. Furthermore, they\nexhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini\nmodels proved strategically ruthless, exploiting cooperative opponents and\nretaliating against defectors, while OpenAI's models remained highly\ncooperative, a trait that proved catastrophic in hostile environments.\nAnthropic's Claude emerged as the most forgiving reciprocator, showing\nremarkable willingness to restore cooperation even after being exploited or\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\nthe models reveals that they actively reason about both the time horizon and\ntheir opponent's likely strategy, and we demonstrate that this reasoning is\ninstrumental to their decisions. This work connects classic game theory with\nmachine psychology, offering a rich and granular view of algorithmic\ndecision-making under uncertainty.",
    "published": "2025-07-03T13:45:02Z",
    "updated": "2025-07-03T13:45:02Z",
    "id": "2507.02618v1",
    "authors": [
      "Kenneth Payne",
      "Baptiste Alloui-Cros"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02618v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02618v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02618v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the strategic intelligence of Large Language Models (LLMs) in competitive settings, specifically using the Iterated Prisoner's Dilemma (IPD) as a model. It evaluates the performance and strategic behaviors of LLMs from OpenAI, Google, and Anthropic, connecting game theory with machine psychology. This aligns with topics related to LLMs, reasoning, and AGI.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "AGI"
    ]
  },
  "2507.02616v1": {
    "title": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and\n  Open-Ended Medical Decision-Making",
    "summary": "The rise of Large Language Models (LLMs) has enabled the development of\nspecialized AI agents with domain-specific reasoning and interaction\ncapabilities, particularly in healthcare. While recent frameworks simulate\nmedical decision-making, they largely focus on single-turn tasks where a doctor\nagent receives full case information upfront -- diverging from the real-world\ndiagnostic process, which is inherently uncertain, interactive, and iterative.\nIn this paper, we introduce MIMIC-Patient, a structured dataset built from the\nMIMIC-III electronic health records (EHRs), designed to support dynamic,\npatient-level simulations. Building on this, we propose DynamiCare, a novel\ndynamic multi-agent framework that models clinical diagnosis as a multi-round,\ninteractive loop, where a team of specialist agents iteratively queries the\npatient system, integrates new information, and dynamically adapts its\ncomposition and strategy. We demonstrate the feasibility and effectiveness of\nDynamiCare through extensive experiments, establishing the first benchmark for\ndynamic clinical decision-making with LLM-powered agents.",
    "published": "2025-07-03T13:43:10Z",
    "updated": "2025-07-03T13:43:10Z",
    "id": "2507.02616v1",
    "authors": [
      "Tianqi Shang",
      "Weiqing He",
      "Charles Zheng",
      "Lingyao Li",
      "Li Shen",
      "Bingxin Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02616v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02616v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02616v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a dynamic multi-agent framework for medical decision-making, which involves reasoning and interaction capabilities. It also introduces a dataset (MIMIC-Patient) and benchmarks for dynamic clinical decision-making.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Benchmark"
    ]
  },
  "2507.03056v1": {
    "title": "Automated Grading of Students' Handwritten Graphs: A Comparison of\n  Meta-Learning and Vision-Large Language Models",
    "summary": "With the rise of online learning, the demand for efficient and consistent\nassessment in mathematics has significantly increased over the past decade.\nMachine Learning (ML), particularly Natural Language Processing (NLP), has been\nwidely used for autograding student responses, particularly those involving\ntext and/or mathematical expressions. However, there has been limited research\non autograding responses involving students' handwritten graphs, despite their\nprevalence in Science, Technology, Engineering, and Mathematics (STEM)\ncurricula. In this study, we implement multimodal meta-learning models for\nautograding images containing students' handwritten graphs and text. We further\ncompare the performance of Vision Large Language Models (VLLMs) with these\nspecially trained metalearning models. Our results, evaluated on a real-world\ndataset collected from our institution, show that the best-performing\nmeta-learning models outperform VLLMs in 2-way classification tasks. In\ncontrast, in more complex 3-way classification tasks, the best-performing VLLMs\nslightly outperform the meta-learning models. While VLLMs show promising\nresults, their reliability and practical applicability remain uncertain and\nrequire further investigation.",
    "published": "2025-07-03T13:25:50Z",
    "updated": "2025-07-03T13:25:50Z",
    "id": "2507.03056v1",
    "authors": [
      "Behnam Parsaeifard",
      "Martin Hlosta",
      "Per Bergamin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03056v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03056v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03056v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Vision Large Language Models (VLLMs) and meta-learning models for grading handwritten graphs, which involves multimodal processing and comparison of model performances. The core topics are related to Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA) models due to the integration of vision and language modalities for practical applications.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.02595v1": {
    "title": "MPF: Aligning and Debiasing Language Models post Deployment via Multi\n  Perspective Fusion",
    "summary": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning.",
    "published": "2025-07-03T13:09:18Z",
    "updated": "2025-07-03T13:09:18Z",
    "id": "2507.02595v1",
    "authors": [
      "Xin Guan",
      "PeiHsin Lin",
      "Zekun Wu",
      "Ze Wang",
      "Ruibo Zhang",
      "Emre Kazim",
      "Adriano Koshiyama"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02595v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02595v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02595v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a post-training alignment framework for large language models (LLMs) aimed at bias mitigation, which aligns with the topics of LLM research and alignment strategies.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.02593v1": {
    "title": "Revisiting Active Learning under (Human) Label Variation",
    "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.",
    "published": "2025-07-03T12:59:28Z",
    "updated": "2025-07-03T12:59:28Z",
    "id": "2507.02593v1",
    "authors": [
      "Cornelia Gruber",
      "Helen Alber",
      "Bernd Bischl",
      "Gran Kauermann",
      "Barbara Plank",
      "Matthias Aenmacher"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02593v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02593v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02593v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses active learning and human label variation, with a mention of large language models (LLMs) as annotators. However, the primary focus is on active learning and label variation, not specifically on LLMs or other core topics listed.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.02564v1": {
    "title": "LLMREI: Automating Requirements Elicitation Interviews with LLMs",
    "summary": "Requirements elicitation interviews are crucial for gathering system\nrequirements but heavily depend on skilled analysts, making them\nresource-intensive, susceptible to human biases, and prone to miscommunication.\nRecent advancements in Large Language Models present new opportunities for\nautomating parts of this process. This study introduces LLMREI, a chat bot\ndesigned to conduct requirements elicitation interviews with minimal human\nintervention, aiming to reduce common interviewer errors and improve the\nscalability of requirements elicitation. We explored two main approaches,\nzero-shot prompting and least-to-most prompting, to optimize LLMREI for\nrequirements elicitation and evaluated its performance in 33 simulated\nstakeholder interviews. A third approach, fine-tuning, was initially considered\nbut abandoned due to poor performance in preliminary trials. Our study assesses\nthe chat bot's effectiveness in three key areas: minimizing common interview\nerrors, extracting relevant requirements, and adapting its questioning based on\ninterview context and user responses. Our findings indicate that LLMREI makes a\nsimilar number of errors compared to human interviewers, is capable of\nextracting a large portion of requirements, and demonstrates a notable ability\nto generate highly context-dependent questions. We envision the greatest\nbenefit of LLMREI in automating interviews with a large number of stakeholders.",
    "published": "2025-07-03T12:18:05Z",
    "updated": "2025-07-03T12:18:05Z",
    "id": "2507.02564v1",
    "authors": [
      "Alexander Korn",
      "Samuel Gorsch",
      "Andreas Vogelsang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02564v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02564v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02564v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in automating requirements elicitation interviews, which directly relates to the use of LLMs in a specific domain. The study evaluates different prompting strategies and the model's performance, which are key aspects of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.03052v1": {
    "title": "From 2:4 to 8:16 sparsity patterns in LLMs for Outliers and Weights with\n  Variance Correction",
    "summary": "As large language models (LLMs) grow in size, efficient compression\ntechniques like quantization and sparsification are critical. While\nquantization maintains performance with reduced precision, structured sparsity\nmethods, such as N:M sparsification, often fall short due to limited\nflexibility, and sensitivity to outlier weights. We explore 8:16\nsemi-structured sparsity, demonstrating its ability to surpass the Performance\nThreshold-where a compressed model matches the accuracy of its uncompressed or\nsmaller counterpart under equivalent memory constraints. Compared to 2:4\nsparsity, 8:16 offers greater flexibility with minimal storage overhead (0.875\nvs. 0.75 bits/element). We also apply sparse structured patterns for salient\nweights, showing that structured sparsity for outliers is competitive with\nunstructured approaches leading to equivalent or better results. Finally, we\ndemonstrate that simple techniques such as variance correction and SmoothQuant\nlike weight equalization improve sparse models performance.",
    "published": "2025-07-03T12:17:45Z",
    "updated": "2025-07-03T12:17:45Z",
    "id": "2507.03052v1",
    "authors": [
      "Egor Maximov",
      "Yulia Kuzkina",
      "Azamat Kanametov",
      "Alexander Prutko",
      "Aleksei Goncharov",
      "Maxim Zhelnin",
      "Egor Shvetsov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03052v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03052v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03052v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses sparsity patterns and compression techniques in large language models (LLMs), which is relevant to the 'LLM' topic. It also touches on model efficiency and performance, which aligns with 'Scaling' as it involves model size and performance considerations.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.02559v1": {
    "title": "Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm\n  Removal to GPT-2 XL and the Implications for Mechanistic Interpretability",
    "summary": "Layer-wise normalization (LN) is an essential component of virtually all\ntransformer-based large language models. While its effects on training\nstability are well documented, its role at inference time is poorly understood.\nAdditionally, LN layers hinder mechanistic interpretability by introducing\nadditional nonlinearities and increasing the interconnectedness of individual\nmodel components. Here, we show that all LN layers can be removed from every\nGPT-2 model with only a small increase in validation loss (e.g. +0.03\ncross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in\nlanguage modeling. We find that the amount of fine-tuning data needed for LN\nremoval grows sublinearly with model parameters, suggesting scaling to larger\nmodels is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.\nFurthermore, we test interpretability techniques on LN-free models. Direct\nlogit attribution now gives the exact direct effect of individual components,\nwhile the accuracy of attribution patching does not significantly improve. We\nalso confirm that GPT-2's \"confidence neurons\" are inactive in the LN-free\nmodels. Our work clarifies the role of LN layers in language modeling, showing\nthat GPT-2-class models can function without LN layers. We hope that our\nLN-free analogs of the GPT-2 family of models will enable more precise\ninterpretability research and improve our understanding of language models.",
    "published": "2025-07-03T12:09:04Z",
    "updated": "2025-07-03T12:09:04Z",
    "id": "2507.02559v1",
    "authors": [
      "Luca Baroni",
      "Galvin Khara",
      "Joachim Schaeffer",
      "Marat Subkhankulov",
      "Stefan Heimersheim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02559v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02559v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02559v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the role of LayerNorm in transformer-based large language models (LLMs) and its implications for mechanistic interpretability, which is closely related to the architecture and scaling of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.03051v1": {
    "title": "Improving LLM Reasoning for Vulnerability Detection via Group Relative\n  Policy Optimization",
    "summary": "Improving and understanding the training dynamics and reasoning of Large\nLanguage Models (LLMs) has become essential for their deployment in AI-based\nsecurity tools, such as software vulnerability detection. In this work, we\npresent an extensive study aimed at advancing recent RL-based finetuning\ntechniques for LLMs in the context of vulnerability detection.\n  We start by highlighting key limitations of commonly adopted LLMs, such as\ntheir tendency to over-predict certain types of vulnerabilities while failing\nto detect others. To address this challenge, we explore the use of Group\nRelative Policy Optimization (GRPO), a recent policy-gradient method, for\nguiding LLM behavior through structured, rule-based rewards. We enable its\napplication to the vulnerability detection task by redefining its advantage\nfunctions and reward signals using annotations from widely used datasets in the\nfield, including BigVul, DiverseVul, and CleanVul.\n  The proposed methodology enables an extensive set of experiments, addressing\nmultiple research questions regarding the impact of GRPO on generalization,\nreasoning capabilities, and performance improvements over standard supervised\nfinetuning (SFT). Our findings offer valuable insights into the potential of\nRL-based training to enhance both the performance and reasoning abilities of\nLLMs in the context of software vulnerability detection.",
    "published": "2025-07-03T11:52:45Z",
    "updated": "2025-07-03T11:52:45Z",
    "id": "2507.03051v1",
    "authors": [
      "Marco Simoni",
      "Aleksandar Fontana",
      "Giulio Rossolini",
      "Andrea Saracino"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03051v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03051v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03051v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the reasoning capabilities of Large Language Models (LLMs) through Reinforcement Learning (RL) techniques, specifically Group Relative Policy Optimization (GRPO), for the task of vulnerability detection. It discusses RL-based finetuning and the impact on LLM reasoning, which aligns with the topics of LLM, RL, and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Reasoning"
    ]
  },
  "2507.02541v1": {
    "title": "Clarifying Before Reasoning: A Coq Prover with Structural Context",
    "summary": "In this work, we investigate whether improving task clarity can enhance\nreasoning ability of large language models, focusing on theorem proving in Coq.\nWe introduce a concept-level metric to evaluate task clarity and show that\nadding structured semantic context to the standard input used by modern LLMs,\nleads to a 1.85$\\times$ improvement in clarity score\n(44.5\\%~$\\rightarrow$~82.3\\%). Using the general-purpose model\n\\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\times$ improvement in proof\nsuccess (21.8\\%~$\\rightarrow$~45.8\\%) and outperforms the previous\nstate-of-the-art \\texttt{Graph2Tac} (33.2\\%). We evaluate this on 1,386\ntheorems randomly sampled from 15 standard Coq packages, following the same\nevaluation protocol as \\texttt{Graph2Tac}. Furthermore, fine-tuning smaller\nmodels on our structured data can achieve even higher performance (48.6\\%). Our\nmethod uses selective concept unfolding to enrich task descriptions, and\nemploys a Planner--Executor architecture. These findings highlight the value of\nstructured task representations in bridging the gap between understanding and\nreasoning.",
    "published": "2025-07-03T11:35:34Z",
    "updated": "2025-07-03T11:35:34Z",
    "id": "2507.02541v1",
    "authors": [
      "Yanzhen Lu",
      "Hanbin Yang",
      "Xiaodie Wang",
      "Ge Zhang",
      "Biao Li",
      "Chenxu Fu",
      "Chao Li",
      "Yang Yuan",
      "Andrew Chi-Chih Yao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02541v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02541v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02541v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the reasoning ability of large language models (LLMs) through structured semantic context, particularly in the context of theorem proving in Coq. This aligns with the 'Reasoning' topic, which covers reasoning abilities in LLMs and complex problem solving. Additionally, the use of LLMs for this purpose suggests relevance to the 'LLM' topic.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2507.02537v1": {
    "title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue",
    "summary": "Conversational agents have made significant progress since ELIZA, expanding\ntheir role across various domains, including healthcare, education, and\ncustomer service. As these agents become increasingly integrated into daily\nhuman interactions, the need for emotional intelligence, particularly\nempathetic listening, becomes increasingly essential. In this study, we explore\nhow Large Language Models (LLMs) respond when tasked with generating\nemotionally rich interactions. Starting from a small dataset manually crafted\nby an expert to reflect empathic behavior, we extended the conversations using\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\ndialogues using both sentiment analysis (via VADER) and expert assessments.\nWhile the generated conversations often mirrored the intended emotional\nstructure, human evaluation revealed important differences in the perceived\nempathy and coherence of the responses. These findings suggest that emotion\nmodeling in dialogues requires not only structural alignment in the expressed\nemotions but also qualitative depth, highlighting the importance of combining\nautomated and humancentered methods in the development of emotionally competent\nagents.",
    "published": "2025-07-03T11:32:41Z",
    "updated": "2025-07-03T11:32:41Z",
    "id": "2507.02537v1",
    "authors": [
      "Paulo Ricardo Knob",
      "Leonardo Scholler",
      "Juliano Rigatti",
      "Soraia Raupp Musse"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02537v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02537v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02537v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on fine-tuning Large Language Models (LLMs) for empathetic dialogue, which involves emotional intelligence and human interaction aspects. The study uses LLMs like ChatGPT and Gemini, and discusses their performance in generating emotionally rich interactions, aligning with the 'LLM' and 'Reasoning' topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.02533v1": {
    "title": "Meta-Fair: AI-Assisted Fairness Testing of Large Language Models",
    "summary": "Fairness--the absence of unjustified bias--is a core principle in the\ndevelopment of Artificial Intelligence (AI) systems, yet it remains difficult\nto assess and enforce. Current approaches to fairness testing in large language\nmodels (LLMs) often rely on manual evaluation, fixed templates, deterministic\nheuristics, and curated datasets, making them resource-intensive and difficult\nto scale. This work aims to lay the groundwork for a novel, automated method\nfor testing fairness in LLMs, reducing the dependence on domain-specific\nresources and broadening the applicability of current approaches. Our approach,\nMeta-Fair, is based on two key ideas. First, we adopt metamorphic testing to\nuncover bias by examining how model outputs vary in response to controlled\nmodifications of input prompts, defined by metamorphic relations (MRs). Second,\nwe propose exploiting the potential of LLMs for both test case generation and\noutput evaluation, leveraging their capability to generate diverse inputs and\nclassify outputs effectively. The proposal is complemented by three open-source\ntools supporting LLM-driven generation, execution, and evaluation of test\ncases. We report the findings of several experiments involving 12 pre-trained\nLLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.\nThe results show that Meta-Fair is effective in uncovering bias in LLMs,\nachieving an average precision of 92% and revealing biased behaviour in 29% of\nexecutions. Additionally, LLMs prove to be reliable and consistent evaluators,\nwith the best-performing models achieving F1-scores of up to 0.79. Although\nnon-determinism affects consistency, these effects can be mitigated through\ncareful MR design. While challenges remain to ensure broader applicability, the\nresults indicate a promising path towards an unprecedented level of automation\nin LLM testing.",
    "published": "2025-07-03T11:20:59Z",
    "updated": "2025-07-03T11:20:59Z",
    "id": "2507.02533v1",
    "authors": [
      "Miguel Romero-Arjona",
      "Jos A. Parejo",
      "Juan C. Alonso",
      "Ana B. Snchez",
      "Aitor Arrieta",
      "Sergio Segura"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02533v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02533v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02533v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on fairness testing in large language models (LLMs), which involves evaluating and mitigating bias in LLMs. This aligns with the 'LLM' topic as it directly involves research on large language models. Additionally, the paper discusses automated testing methods and evaluation metrics, which are relevant to the 'Benchmark' topic as it involves benchmarking LLMs for fairness.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.07115v1": {
    "title": "Autonomous Control Leveraging LLMs: An Agentic Framework for\n  Next-Generation Industrial Automation",
    "summary": "The increasing complexity of modern chemical processes, coupled with\nworkforce shortages and intricate fault scenarios, demands novel automation\nparadigms that blend symbolic reasoning with adaptive control. In this work, we\nintroduce a unified agentic framework that leverages large language models\n(LLMs) for both discrete fault-recovery planning and continuous process control\nwithin a single architecture. We adopt Finite State Machines (FSMs) as\ninterpretable operating envelopes: an LLM-driven planning agent proposes\nrecovery sequences through the FSM, a Simulation Agent executes and checks each\ntransition, and a Validator-Reprompting loop iteratively refines invalid plans.\nIn Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25\nstates, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path\nsuccess within five reprompts-outperforming open-source LLMs in both accuracy\nand latency. In Case Study 2, the same framework modulates dual-heater inputs\non a laboratory TCLab platform (and its digital twin) to maintain a target\naverage temperature under persistent asymmetric disturbances. Compared to\nclassical PID control, our LLM-based controller attains similar performance,\nwhile ablation of the prompting loop reveals its critical role in handling\nnonlinear dynamics. We analyze key failure modes-such as instruction following\nlapses and coarse ODE approximations. Our results demonstrate that, with\nstructured feedback and modular agents, LLMs can unify high-level symbolic\nplanningand low-level continuous control, paving the way towards resilient,\nlanguage-driven automation in chemical engineering.",
    "published": "2025-07-03T11:20:22Z",
    "updated": "2025-07-03T11:20:22Z",
    "id": "2507.07115v1",
    "authors": [
      "Javal Vyas",
      "Mehmet Mercangoz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07115v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07115v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07115v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in an agentic framework for industrial automation, combining symbolic reasoning with adaptive control. It highlights the application of LLMs in both discrete fault-recovery planning and continuous process control, which aligns with the topics of LLM and RL (Reinforcement Learning) due to the adaptive and agentic nature of the framework.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.02530v1": {
    "title": "Open-Source System for Multilingual Translation and Cloned Speech\n  Synthesis",
    "summary": "We present an open-source system designed for multilingual translation and\nspeech regeneration, addressing challenges in communication and accessibility\nacross diverse linguistic contexts. The system integrates Whisper for speech\nrecognition with Voice Activity Detection (VAD) to identify speaking intervals,\nfollowed by a pipeline of Large Language Models (LLMs). For multilingual\napplications, the first LLM segments speech into coherent, complete sentences,\nwhich a second LLM then translates. For speech regeneration, the system uses a\ntext-to-speech (TTS) module with voice cloning capabilities to replicate the\noriginal speaker's voice, maintaining naturalness and speaker identity.\n  The system's open-source components can operate locally or via APIs, offering\ncost-effective deployment across various use cases. These include real-time\nmultilingual translation in Zoom sessions, speech regeneration for public\nbroadcasts, and Bluetooth-enabled multilingual playback through personal\ndevices. By preserving the speaker's voice, the system ensures a seamless and\nimmersive experience, whether translating or regenerating speech.\n  This open-source project is shared with the community to foster innovation\nand accessibility. We provide a detailed system performance analysis, including\nlatency and word accuracy, demonstrating its potential to enable inclusive,\nadaptable communication solutions in real-world multilingual scenarios.",
    "published": "2025-07-03T11:12:26Z",
    "updated": "2025-07-03T11:12:26Z",
    "id": "2507.02530v1",
    "authors": [
      "Mateo Cmara",
      "Juan Gutirrez",
      "Mara Pilar Daza",
      "Jos Luis Blanco"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02530v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02530v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02530v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a system that integrates Large Language Models (LLMs) for multilingual translation and speech regeneration, which aligns with the topics of LLM and MLLM due to the use of multimodal capabilities (speech and text).",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.03049v1": {
    "title": "Personalised Explanations in Long-term Human-Robot Interactions",
    "summary": "In the field of Human-Robot Interaction (HRI), a fundamental challenge is to\nfacilitate human understanding of robots. The emerging domain of eXplainable\nHRI (XHRI) investigates methods to generate explanations and evaluate their\nimpact on human-robot interactions. Previous works have highlighted the need to\npersonalise the level of detail of these explanations to enhance usability and\ncomprehension. Our paper presents a framework designed to update and retrieve\nuser knowledge-memory models, allowing for adapting the explanations' level of\ndetail while referencing previously acquired concepts. Three architectures\nbased on our proposed framework that use Large Language Models (LLMs) are\nevaluated in two distinct scenarios: a hospital patrolling robot and a kitchen\nassistant robot. Experimental results demonstrate that a two-stage\narchitecture, which first generates an explanation and then personalises it, is\nthe framework architecture that effectively reduces the level of detail only\nwhen there is related user knowledge.",
    "published": "2025-07-03T10:40:39Z",
    "updated": "2025-07-03T10:40:39Z",
    "id": "2507.03049v1",
    "authors": [
      "Ferran Gebell",
      "Anas Garrell",
      "Jan-Gerrit Habekost",
      "Sverin Lemaignan",
      "Stefan Wermter",
      "Raquel Ros"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03049v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03049v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03049v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the context of Human-Robot Interaction (HRI) to personalize explanations, which aligns with the 'LLM' topic. Additionally, the focus on personalizing explanations based on user knowledge-memory models and the evaluation of different architectures suggests relevance to 'Reasoning' as it involves logical reasoning and problem-solving in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.03047v1": {
    "title": "Counterfactual Tuning for Temporal Sensitivity Enhancement in Large\n  Language Model-based Recommendation",
    "summary": "Recent advances have applied large language models (LLMs) to sequential\nrecommendation, leveraging their pre-training knowledge and reasoning\ncapabilities to provide more personalized user experiences. However, existing\nLLM-based methods fail to sufficiently leverage the rich temporal information\ninherent in users' historical interaction sequences, stemming from fundamental\narchitectural constraints: LLMs process information through self-attention\nmechanisms that lack inherent sequence ordering and rely on position embeddings\ndesigned primarily for natural language rather than user interaction sequences.\nThis limitation significantly impairs their ability to capture the evolution of\nuser preferences over time and predict future interests accurately.\n  To address this critical gap, we propose Counterfactual Enhanced Temporal\nFramework for LLM-Based Recommendation (CETRec). CETRec is grounded in causal\ninference principles, which allow it to isolate and measure the specific impact\nof temporal information on recommendation outcomes. By conceptualizing temporal\norder as an independent causal factor distinct from item content, we can\nquantify its unique contribution through counterfactual reasoning--comparing\nwhat recommendations would be made with and without temporal information while\nkeeping all other factors constant. This causal framing enables CETRec to\ndesign a novel counterfactual tuning objective that directly optimizes the\nmodel's temporal sensitivity, teaching LLMs to recognize both absolute\ntimestamps and relative ordering patterns in user histories. Combined with our\ncounterfactual tuning task derived from causal analysis, CETRec effectively\nenhances LLMs' awareness of both absolute order (how recently items were\ninteracted with) and relative order (the sequential relationships between\nitems).",
    "published": "2025-07-03T10:11:35Z",
    "updated": "2025-07-03T10:11:35Z",
    "id": "2507.03047v1",
    "authors": [
      "Yutian Liu",
      "Zhengyi Yang",
      "Jiancan Wu",
      "Xiang Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03047v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03047v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03047v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models (LLMs) in sequential recommendation systems, focusing on enhancing their temporal sensitivity through counterfactual tuning. It involves LLM architectures and their reasoning capabilities, aligning with the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.02503v1": {
    "title": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs",
    "summary": "Continual fine-tuning of Large Language Models (LLMs) is hampered by the\ntrade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)\noffers efficiency but constrains the model's ability to learn new tasks and\ntransfer knowledge due to its low-rank nature and reliance on explicit\nparameter constraints. We propose GORP (Gradient LOw Rank Projection) for\nContinual Learning, a novel training strategy that overcomes these limitations\nby synergistically combining full and low-rank parameters and jointly updating\nwithin a unified low-rank gradient subspace. GORP expands the optimization\nspace while preserving efficiency and mitigating catastrophic forgetting.\nExtensive experiments on continual learning benchmarks demonstrate GORP's\nsuperior performance compared to existing state-of-the-art approaches. Code is\navailable at https://github.com/Wcxwcxw/GORP.",
    "published": "2025-07-03T10:11:22Z",
    "updated": "2025-07-03T10:11:22Z",
    "id": "2507.02503v1",
    "authors": [
      "Chenxu Wang",
      "Yilin Lyu",
      "Zicheng Sun",
      "Liping Jing"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02503v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02503v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02503v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a novel training strategy for continual fine-tuning of Large Language Models (LLMs), focusing on efficiency and expressiveness, which aligns with research on LLMs and their optimization techniques.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.02439v2": {
    "title": "Introducing a New Brexit-Related Uncertainty Index: Its Evolution and\n  Economic Consequences",
    "summary": "Important game-changer economic events and transformations cause\nuncertainties that may affect investment decisions, capital flows,\ninternational trade, and macroeconomic variables. One such major transformation\nis Brexit, which refers to the multiyear process through which the UK withdrew\nfrom the EU. This study develops and uses a new Brexit-Related Uncertainty\nIndex (BRUI). In creating this index, we apply Text Mining, Context Window,\nNatural Language Processing (NLP), and Large Language Models (LLMs) from Deep\nLearning techniques to analyse the monthly country reports of the Economist\nIntelligence Unit from May 2012 to January 2025. Additionally, we employ a\nstandard vector autoregression (VAR) analysis to examine the model-implied\nresponses of various macroeconomic variables to BRUI shocks. While developing\nthe BRUI, we also create a complementary COVID-19 Related Uncertainty Index\n(CRUI) to distinguish the uncertainties stemming from these distinct events.\nEmpirical findings and comparisons of BRUI with other earlier-developed\nuncertainty indexes demonstrate the robustness of the new index. This new index\ncan assist British policymakers in measuring and understanding the impacts of\nBrexit-related uncertainties, enabling more effective policy formulation.",
    "published": "2025-07-03T08:52:55Z",
    "updated": "2025-07-04T09:42:06Z",
    "id": "2507.02439v2",
    "authors": [
      "Ismet Gocer",
      "Julia Darby",
      "Serdar Ongan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02439v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02439v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02439v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the development of a Brexit-Related Uncertainty Index using NLP and LLMs, but the primary focus is on economic analysis and policy impact rather than core LLM research.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.02436v1": {
    "title": "Toward a Robust and Generalizable Metamaterial Foundation Model",
    "summary": "Advances in material functionalities drive innovations across various fields,\nwhere metamaterials-defined by structure rather than composition-are leading\nthe way. Despite the rise of artificial intelligence (AI)-driven design\nstrategies, their impact is limited by task-specific retraining, poor\nout-of-distribution(OOD) generalization, and the need for separate models for\nforward and inverse design. To address these limitations, we introduce the\nMetamaterial Foundation Model (MetaFO), a Bayesian transformer-based foundation\nmodel inspired by large language models. MetaFO learns the underlying mechanics\nof metamaterials, enabling probabilistic, zero-shot predictions across diverse,\nunseen combinations of material properties and structural responses. It also\nexcels in nonlinear inverse design, even under OOD conditions. By treating\nmetamaterials as an operator that maps material properties to structural\nresponses, MetaFO uncovers intricate structure-property relationships and\nsignificantly expands the design space. This scalable and generalizable\nframework marks a paradigm shift in AI-driven metamaterial discovery, paving\nthe way for next-generation innovations.",
    "published": "2025-07-03T08:48:36Z",
    "updated": "2025-07-03T08:48:36Z",
    "id": "2507.02436v1",
    "authors": [
      "Namjung Kim",
      "Dongseok Lee",
      "Jongbin Yu",
      "Sung Woong Cho",
      "Dosung Lee",
      "Yesol Park",
      "Youngjoon Hong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02436v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02436v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02436v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a foundation model for metamaterials inspired by large language models, but it does not directly align with the provided topics which are focused on language models, multimodal models, and related AI research areas. The content is more about material science and AI-driven design strategies.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.02406v1": {
    "title": "Improving Consistency in Vehicle Trajectory Prediction Through\n  Preference Optimization",
    "summary": "Trajectory prediction is an essential step in the pipeline of an autonomous\nvehicle. Inaccurate or inconsistent predictions regarding the movement of\nagents in its surroundings lead to poorly planned maneuvers and potentially\ndangerous situations for the end-user. Current state-of-the-art\ndeep-learning-based trajectory prediction models can achieve excellent accuracy\non public datasets. However, when used in more complex, interactive scenarios,\nthey often fail to capture important interdependencies between agents, leading\nto inconsistent predictions among agents in the traffic scene. Inspired by the\nefficacy of incorporating human preference into large language models, this\nwork fine-tunes trajectory prediction models in multi-agent settings using\npreference optimization. By taking as input automatically calculated preference\nrankings among predicted futures in the fine-tuning process, our\nexperiments--using state-of-the-art models on three separate datasets--show\nthat we are able to significantly improve scene consistency while minimally\nsacrificing trajectory prediction accuracy and without adding any excess\ncomputational requirements at inference time.",
    "published": "2025-07-03T07:59:49Z",
    "updated": "2025-07-03T07:59:49Z",
    "id": "2507.02406v1",
    "authors": [
      "Caio Azevedo",
      "Lina Achaji",
      "Stefano Sabatini",
      "Nicola Poerio",
      "Grzegorz Bartyzel",
      "Sascha Hornauer",
      "Fabien Moutarde"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02406v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02406v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02406v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving trajectory prediction in autonomous vehicles using preference optimization, which is inspired by human preference incorporation in large language models. However, the core topic is not directly related to any of the provided categories, as it primarily deals with autonomous vehicles and trajectory prediction rather than LLMs or their related topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.03042v1": {
    "title": "Dynamic Long Short-Term Memory Based Memory Storage For Long Horizon LLM\n  Interaction",
    "summary": "Memory storage for Large Language models (LLMs) is becoming an increasingly\nactive area of research, particularly for enabling personalization across long\nconversations. We propose Pref-LSTM, a dynamic and lightweight framework that\ncombines a BERT-based classifier with a LSTM memory module that generates\nmemory embedding which then is soft-prompt injected into a frozen LLM. We\nsynthetically curate a dataset of preference and non-preference conversation\nturns to train our BERT-based classifier. Although our LSTM-based memory\nencoder did not yield strong results, we find that the BERT-based classifier\nperforms reliably in identifying explicit and implicit user preferences. Our\nresearch demonstrates the viability of using preference filtering with LSTM\ngating principals as an efficient path towards scalable user preference\nmodeling, without extensive overhead and fine-tuning.",
    "published": "2025-07-03T07:53:20Z",
    "updated": "2025-07-03T07:53:20Z",
    "id": "2507.03042v1",
    "authors": [
      "Yuyang Lou",
      "Charles Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03042v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03042v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03042v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on memory storage for LLMs, specifically using a LSTM memory module and BERT-based classifier for user preference modeling, which aligns with the 'Memory' and 'LLM' topics.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.02390v1": {
    "title": "Evaluating Language Models For Threat Detection in IoT Security Logs",
    "summary": "Log analysis is a relevant research field in cybersecurity as they can\nprovide a source of information for the detection of threats to networks and\nsystems. This paper presents a pipeline to use fine-tuned Large Language Models\n(LLMs) for anomaly detection and mitigation recommendation using IoT security\nlogs. Utilizing classical machine learning classifiers as a baseline, three\nopen-source LLMs are compared for binary and multiclass anomaly detection, with\nthree strategies: zero-shot, few-shot prompting and fine-tuning using an IoT\ndataset. LLMs give better results on multi-class attack classification than the\ncorresponding baseline models. By mapping detected threats to MITRE CAPEC,\ndefining a set of IoT-specific mitigation actions, and fine-tuning the models\nwith those actions, the models are able to provide a combined detection and\nrecommendation guidance.",
    "published": "2025-07-03T07:38:43Z",
    "updated": "2025-07-03T07:38:43Z",
    "id": "2507.02390v1",
    "authors": [
      "Jorge J. Tejero-Fernndez",
      "Alfonso Snchez-Macin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02390v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02390v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02390v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) for threat detection in IoT security logs, comparing different LLM strategies and their performance. This directly relates to the 'LLM' topic as it involves the use and fine-tuning of LLMs for a specific task. Additionally, the paper involves benchmarking LLMs against classical machine learning models, which aligns with the 'Benchmark' topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.02380v1": {
    "title": "JoyTTS: LLM-based Spoken Chatbot With Voice Cloning",
    "summary": "JoyTTS is an end-to-end spoken chatbot that combines large language models\n(LLM) with text-to-speech (TTS) technology, featuring voice cloning\ncapabilities. This project is built upon the open-source MiniCPM-o and\nCosyVoice2 models and trained on 2000 hours of conversational data. We have\nalso provided the complete training code to facilitate further development and\noptimization by the community. On the testing machine seed-tts-zh, it achieves\na SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.\nThe code and models, along with training and inference scripts, are available\nat https://github.com/jdh-algo/JoyTTS.git.",
    "published": "2025-07-03T07:22:06Z",
    "updated": "2025-07-03T07:22:06Z",
    "id": "2507.02380v1",
    "authors": [
      "Fangru Zhou",
      "Jun Zhao",
      "Guoxin Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02380v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02380v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02380v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLM) with text-to-speech (TTS) technology, which involves LLM-based systems and multimodal aspects (voice cloning).",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.02378v1": {
    "title": "Efficient Code LLM Training via Distribution-Consistent and\n  Diversity-Aware Data Selection",
    "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs.",
    "published": "2025-07-03T07:19:56Z",
    "updated": "2025-07-03T07:19:56Z",
    "id": "2507.02378v1",
    "authors": [
      "Weijie Lyu",
      "Sheng-Jun Huang",
      "Xuan Xia"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02378v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02378v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02378v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses training large language models (LLMs) for code generation and program comprehension, focusing on data selection to improve efficiency and performance. This aligns with the topics of LLM (Large Language Models) and Scaling (as it addresses training efficiency and model performance).",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.03041v2": {
    "title": "Optimas: Optimizing Compound AI Systems with Globally Aligned Local\n  Rewards",
    "summary": "Compound AI systems integrating multiple components, such as Large Language\nModels, specialized tools, and traditional machine learning models, are\nincreasingly deployed to solve complex real-world tasks. However, optimizing\ncompound systems remains challenging due to their non-differentiable structures\nand diverse configuration types across components, including prompts,\nhyperparameters, and model parameters. To address this challenge, we propose\nOptimas, a unified framework for effective optimization of compound systems.\nThe core idea of Optimas is to maintain one Local Reward Function (LRF) per\ncomponent, each satisfying a local-global alignment property, i.e., each\ncomponent's local reward correlates with the global system performance. In each\niteration, Optimas efficiently adapts the LRFs to maintain this property while\nsimultaneously maximizing each component's local reward. This approach enables\nindependent updates of heterogeneous configurations using the designated\noptimization method, while ensuring that local improvements consistently lead\nto performance gains. We present extensive evaluations across five real-world\ncompound systems to demonstrate that Optimas outperforms strong baselines by an\naverage improvement of 11.92%, offering a general and effective approach for\nimproving compound systems. Our website is at https://optimas.stanford.edu.",
    "published": "2025-07-03T07:12:48Z",
    "updated": "2025-07-09T18:47:51Z",
    "id": "2507.03041v2",
    "authors": [
      "Shirley Wu",
      "Parth Sarthi",
      "Shiyu Zhao",
      "Aaron Lee",
      "Herumb Shandilya",
      "Adrian Mladenic Grobelnik",
      "Nurendra Choudhary",
      "Eddie Huang",
      "Karthik Subbian",
      "Linjun Zhang",
      "Diyi Yang",
      "James Zou",
      "Jure Leskovec"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03041v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03041v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03041v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses optimizing compound AI systems that include Large Language Models (LLMs) and other components, focusing on reinforcement learning techniques to align local rewards with global performance.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.02373v1": {
    "title": "UVLM: Benchmarking Video Language Model for Underwater World\n  Understanding",
    "summary": "Recently, the remarkable success of large language models (LLMs) has achieved\na profound impact on the field of artificial intelligence. Numerous advanced\nworks based on LLMs have been proposed and applied in various scenarios. Among\nthem, video language models (VidLMs) are particularly widely used. However,\nexisting works primarily focus on terrestrial scenarios, overlooking the highly\ndemanding application needs of underwater observation. To overcome this gap, we\nintroduce UVLM, an under water observation benchmark which is build through a\ncollaborative approach combining human expertise and AI models. To ensure data\nquality, we have conducted in-depth considerations from multiple perspectives.\nFirst, to address the unique challenges of underwater environments, we selected\nvideos that represent typical underwater challenges including light variations,\nwater turbidity, and diverse viewing angles to construct the dataset. Second,\nto ensure data diversity, the dataset covers a wide range of frame rates,\nresolutions, 419 classes of marine animals, and various static plants and\nterrains. Next, for task diversity, we adopted a structured design where\nobservation targets are categorized into two major classes: biological and\nenvironmental. Each category includes content observation and change/action\nobservation, totaling 20 distinct task types. Finally, we designed several\nchallenging evaluation metrics to enable quantitative comparison and analysis\nof different methods. Experiments on two representative VidLMs demonstrate that\nfine-tuning VidLMs on UVLM significantly improves underwater world\nunderstanding while also showing potential for slight improvements on existing\nin-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and\nprompt engineering will be released publicly.",
    "published": "2025-07-03T07:08:38Z",
    "updated": "2025-07-03T07:08:38Z",
    "id": "2507.02373v1",
    "authors": [
      "Xizhe Xue",
      "Yang Zhou",
      "Dawei Yan",
      "Ying Li",
      "Haokui Zhang",
      "Rong Xiao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02373v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02373v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02373v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark for video language models (VidLMs) specifically for underwater scenarios, which involves multimodal understanding (combining video and language) and benchmarking different models. The focus is on evaluating and improving VidLMs for a specialized domain.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.02357v1": {
    "title": "Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and\n  Confidence-Informed Ensembling for Multimodal Large Language Models",
    "summary": "This paper describes our system for the SciVQA 2025 Shared Task on Scientific\nVisual Question Answering. Our system employs an ensemble of two Multimodal\nLarge Language Models and various few-shot example retrieval strategies. The\nmodel and few-shot setting are selected based on the figure and question type.\nWe also select answers based on the models' confidence levels. On the blind\ntest data, our system ranks third out of seven with an average F1 score of\n85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.",
    "published": "2025-07-03T06:43:05Z",
    "updated": "2025-07-03T06:43:05Z",
    "id": "2507.02357v1",
    "authors": [
      "Christian Jaumann",
      "Annemarie Friedrich",
      "Rainer Lienhart"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02357v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02357v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02357v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Multimodal Large Language Models (MLLM) and their application in a visual question answering task, which involves few-shot learning and confidence-based ensembling. The core topics are MLLM and Benchmark, as it involves evaluating the performance of these models in a specific task.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.02288v1": {
    "title": "Prompt Disentanglement via Language Guidance and Representation\n  Alignment for Domain Generalization",
    "summary": "Domain Generalization (DG) seeks to develop a versatile model capable of\nperforming effectively on unseen target domains. Notably, recent advances in\npre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated\nconsiderable potential in enhancing the generalization capabilities of deep\nlearning models. Despite the increasing attention toward VFM-based domain\nprompt tuning within DG, the effective design of prompts capable of\ndisentangling invariant features across diverse domains remains a critical\nchallenge. In this paper, we propose addressing this challenge by leveraging\nthe controllable and flexible language prompt of the VFM. Noting that the text\nmodality of VFMs is naturally easier to disentangle, we introduce a novel\nframework for text feature-guided visual prompt tuning. This framework first\nautomatically disentangles the text prompt using a large language model (LLM)\nand then learns domain-invariant visual representation guided by the\ndisentangled text feature. However, relying solely on language to guide visual\nfeature disentanglement has limitations, as visual features can sometimes be\ntoo complex or nuanced to be fully captured by descriptive text. To address\nthis, we introduce Worst Explicit Representation Alignment (WERA), which\nextends text-guided visual prompts by incorporating an additional set of\nabstract prompts. These prompts enhance source domain diversity through\nstylized image augmentations, while alignment constraints ensure that visual\nrepresentations remain consistent across both the original and augmented\ndistributions. Experiments conducted on major DG datasets, including PACS,\nVLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method\noutperforms state-of-the-art DG methods.",
    "published": "2025-07-03T03:52:37Z",
    "updated": "2025-07-03T03:52:37Z",
    "id": "2507.02288v1",
    "authors": [
      "De Cheng",
      "Zhipeng Xu",
      "Xinyang Jiang",
      "Dongsheng Li",
      "Nannan Wang",
      "Xinbo Gao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02288v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02288v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02288v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging large language models (LLMs) for prompt disentanglement and domain generalization, which involves both language and visual modalities. It also discusses the use of pre-trained Visual Foundation Models (VFMs) like CLIP, which are part of multimodal large language models (MLLMs).",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "VLA"
    ]
  },
  "2507.02283v1": {
    "title": "Misaligned from Within: Large Language Models Reproduce Our Double-Loop\n  Learning Blindness",
    "summary": "This paper examines a critical yet unexplored dimension of the AI alignment\nproblem: the potential for Large Language Models (LLMs) to inherit and amplify\nexisting misalignments between human espoused theories and theories-in-use.\nDrawing on action science research, we argue that LLMs trained on\nhuman-generated text likely absorb and reproduce Model 1 theories-in-use - a\ndefensive reasoning pattern that both inhibits learning and creates ongoing\nanti-learning dynamics at the dyad, group, and organisational levels. Through a\ndetailed case study of an LLM acting as an HR consultant, we show how its\nadvice, while superficially professional, systematically reinforces\nunproductive problem-solving approaches and blocks pathways to deeper\norganisational learning. This represents a specific instance of the alignment\nproblem where the AI system successfully mirrors human behaviour but inherits\nour cognitive blind spots. This poses particular risks if LLMs are integrated\ninto organisational decision-making processes, potentially entrenching\nanti-learning practices while lending authority to them. The paper concludes by\nexploring the possibility of developing LLMs capable of facilitating Model 2\nlearning - a more productive theory-in-use - and suggests this effort could\nadvance both AI alignment research and action science practice. This analysis\nreveals an unexpected symmetry in the alignment challenge: the process of\ndeveloping AI systems properly aligned with human values could yield tools that\nhelp humans themselves better embody those same values.",
    "published": "2025-07-03T03:44:45Z",
    "updated": "2025-07-03T03:44:45Z",
    "id": "2507.02283v1",
    "authors": [
      "Tim Rogers",
      "Ben Teehankee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02283v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02283v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02283v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the alignment problem in Large Language Models (LLMs) and their potential to inherit and amplify human cognitive blind spots, which is directly related to the research on LLMs and their alignment with human values.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.02282v1": {
    "title": "Content filtering methods for music recommendation: A review",
    "summary": "Recommendation systems have become essential in modern music streaming\nplatforms, shaping how users discover and engage with songs. One common\napproach in recommendation systems is collaborative filtering, which suggests\ncontent based on the preferences of users with similar listening patterns to\nthe target user. However, this method is less effective on media where\ninteractions are sparse. Music is one such medium, since the average user of a\nmusic streaming service will never listen to the vast majority of tracks. Due\nto this sparsity, there are several challenges that have to be addressed with\nother methods. This review examines the current state of research in addressing\nthese challenges, with an emphasis on the role of content filtering in\nmitigating biases inherent in collaborative filtering approaches. We explore\nvarious methods of song classification for content filtering, including lyrical\nanalysis using Large Language Models (LLMs) and audio signal processing\ntechniques. Additionally, we discuss the potential conflicts between these\ndifferent analysis methods and propose avenues for resolving such\ndiscrepancies.",
    "published": "2025-07-03T03:44:20Z",
    "updated": "2025-07-03T03:44:20Z",
    "id": "2507.02282v1",
    "authors": [
      "Terence Zeng",
      "Abhishek K. Umrawal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02282v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02282v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02282v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the context of music recommendation systems, specifically for lyrical analysis in content filtering. However, the primary focus is on recommendation systems and content filtering methods rather than the core aspects of LLMs.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.02279v1": {
    "title": "LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal\n  Large Language Models",
    "summary": "Existing visual token compression methods for Multimodal Large Language\nModels (MLLMs) predominantly operate as post-encoder modules, limiting their\npotential for efficiency gains. To address this limitation, we propose LaCo\n(Layer-wise Visual Token Compression), a novel framework that enables effective\ntoken compression within the intermediate layers of the vision encoder. LaCo\nintroduces two core components: 1) a layer-wise pixel-shuffle mechanism that\nsystematically merges adjacent tokens through space-to-channel transformations,\nand 2) a residual learning architecture with non-parametric shortcuts that\npreserves critical visual information during compression. Extensive experiments\nindicate that our LaCo outperforms all existing methods when compressing tokens\nin the intermediate layers of the vision encoder, demonstrating superior\neffectiveness. In addition, compared to external compression, our method\nimproves training efficiency beyond 20% and inference throughput over 15% while\nmaintaining strong performance.",
    "published": "2025-07-03T03:42:54Z",
    "updated": "2025-07-03T03:42:54Z",
    "id": "2507.02279v1",
    "authors": [
      "Juntao Liu",
      "Liqiang Niu",
      "Wenchao Chen",
      "Jie Zhou",
      "Fandong Meng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02279v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02279v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02279v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the efficiency of Multimodal Large Language Models (MLLMs) through layer-wise visual token compression, which directly relates to the MLLM topic. It also involves optimization techniques that could be relevant to the Scaling topic, as it aims to improve model efficiency.",
    "llm_cls_result": [
      "MLLM",
      "Scaling"
    ]
  },
  "2507.02256v1": {
    "title": "Uncertainty-aware Reward Design Process",
    "summary": "Designing effective reward functions is a cornerstone of reinforcement\nlearning (RL), yet it remains a challenging process due to the inefficiencies\nand inconsistencies inherent in conventional reward engineering methodologies.\nRecent advances have explored leveraging large language models (LLMs) to\nautomate reward function design. However, their suboptimal performance in\nnumerical optimization often yields unsatisfactory reward quality, while the\nevolutionary search paradigm demonstrates inefficient utilization of simulation\nresources, resulting in prohibitively lengthy design cycles with\ndisproportionate computational overhead. To address these challenges, we\npropose the Uncertainty-aware Reward Design Process (URDP), a novel framework\nthat integrates large language models to streamline reward function design and\nevaluation in RL environments. URDP quantifies candidate reward function\nuncertainty based on self-consistency analysis, enabling simulation-free\nidentification of ineffective reward components while discovering novel reward\ncomponents. Furthermore, we introduce uncertainty-aware Bayesian optimization\n(UABO), which incorporates uncertainty estimation to significantly enhance\nhyperparameter configuration efficiency. Finally, we construct a bi-level\noptimization architecture by decoupling the reward component optimization and\nthe hyperparameter tuning. URDP orchestrates synergistic collaboration between\nthe reward logic reasoning of the LLMs and the numerical optimization strengths\nof the Bayesian Optimization. We conduct a comprehensive evaluation of URDP\nacross 35 diverse tasks spanning three benchmark environments. Our experimental\nresults demonstrate that URDP not only generates higher-quality reward\nfunctions but also achieves significant improvements in the efficiency of\nautomated reward design compared to existing approaches.",
    "published": "2025-07-03T03:09:17Z",
    "updated": "2025-07-03T03:09:17Z",
    "id": "2507.02256v1",
    "authors": [
      "Yang Yang",
      "Xiaolu Zhou",
      "Bosong Ding",
      "Miao Xin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02256v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02256v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02256v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) with reinforcement learning (RL) for reward function design, which involves both LLM and RL topics. It also touches on the use of Bayesian optimization for hyperparameter tuning, which is related to RL.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.02255v1": {
    "title": "Listwise Preference Alignment Optimization for Tail Item Recommendation",
    "summary": "Preference alignment has achieved greater success on Large Language Models\n(LLMs) and drawn broad interest in recommendation research. Existing preference\nalignment methods for recommendation either require explicit reward modeling or\nonly support pairwise preference comparison. The former directly increases\nsubstantial computational costs, while the latter hinders training efficiency\non negative samples. Moreover, no existing effort has explored preference\nalignment solutions for tail-item recommendation. To bridge the above gaps, we\npropose LPO4Rec, which extends the Bradley-Terry model from pairwise comparison\nto listwise comparison, to improve the efficiency of model training.\nSpecifically, we derive a closed form optimal policy to enable more efficient\nand effective training without explicit reward modeling. We also present an\nadaptive negative sampling and reweighting strategy to prioritize tail items\nduring optimization and enhance performance in tail-item recommendations.\nBesides, we theoretically prove that optimizing the listwise preference\noptimization (LPO) loss is equivalent to maximizing the upper bound of the\noptimal reward. Our experiments on three public datasets show that our method\noutperforms 10 baselines by a large margin, achieving up to 50% performance\nimprovement while reducing 17.9% GPU memory usage when compared with direct\npreference optimization (DPO) in tail-item recommendation. Our code is\navailable at https://github.com/Yuhanleeee/LPO4Rec.",
    "published": "2025-07-03T03:08:23Z",
    "updated": "2025-07-03T03:08:23Z",
    "id": "2507.02255v1",
    "authors": [
      "Zihao Li",
      "Chao Yang",
      "Tong Zhang",
      "Yakun Chen",
      "Xianzhi Wang",
      "Guandong Xu",
      "Daoyi Dong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02255v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02255v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02255v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses preference alignment in the context of Large Language Models (LLMs) and its application in recommendation systems, particularly focusing on tail-item recommendation. It introduces a method (LPO4Rec) that extends the Bradley-Terry model for listwise comparison, which is relevant to LLM research and optimization techniques.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.02253v1": {
    "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation",
    "summary": "Progress in enhancing large language model (LLM) planning and reasoning\ncapabilities is significantly hampered by the bottleneck of scalable, reliable\ndata generation and evaluation. To overcome this, I introduce NL2FLOW, a fully\nautomated system for parametrically generating planning problems - expressed in\nnatural language, a structured intermediate representation, and formal PDDL -\nand rigorously evaluating the quality of generated plans. I demonstrate\nNL2FLOW's capabilities by generating a dataset of 2296 problems in the\nautomated workflow generation domain and evaluating multiple open-sourced,\ninstruct-tuned LLMs. My results reveal that the highest performing models\nachieved 86% success in generating valid plans and 69% in generating optimal\nplans, specifically for problems with feasible solutions. Regression analysis\nshows that the influence of problem characteristics on plan generation is\ncontingent on both model and prompt design. Notably, I observed that the\nhighest success rate for translating natural language into a JSON\nrepresentation of a plan was lower than the highest rate of generating a valid\nplan directly. This suggests that unnecessarily decomposing the reasoning task\n- introducing intermediate translation steps - may actually degrade\nperformance, implying a benefit to models capable of reasoning directly from\nnatural language to action. As I scale LLM reasoning to increasingly complex\nproblems, the bottlenecks and sources of error within these systems will\ninevitably shift. Therefore, a dynamic understanding of these limitations - and\nthe tools to systematically reveal them - will be crucial for unlocking the\nfull potential of LLMs as intelligent problem solvers.",
    "published": "2025-07-03T03:02:49Z",
    "updated": "2025-07-03T03:02:49Z",
    "id": "2507.02253v1",
    "authors": [
      "Jungkoo Kang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02253v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02253v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02253v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing LLM planning and reasoning capabilities through a system called NL2FLOW, which generates planning problems and evaluates LLM performance. The core topics are related to LLM reasoning and scaling, as well as the generation of datasets for evaluation.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Dataset"
    ]
  },
  "2507.02241v1": {
    "title": "VERBA: Verbalizing Model Differences Using Large Language Models",
    "summary": "In the current machine learning landscape, we face a \"model lake\" phenomenon:\nGiven a task, there is a proliferation of trained models with similar\nperformances despite different behavior. For model users attempting to navigate\nand select from the models, documentation comparing model pairs is helpful.\nHowever, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a\nnumber prohibitive for the model developers to manually perform pairwise\ncomparisons and prepare documentations. To facilitate fine-grained pairwise\ncomparisons among models, we introduced $\\textbf{VERBA}$. Our approach\nleverages a large language model (LLM) to generate verbalizations of model\ndifferences by sampling from the two models. We established a protocol that\nevaluates the informativeness of the verbalizations via simulation. We also\nassembled a suite with a diverse set of commonly used machine learning models\nas a benchmark. For a pair of decision tree models with up to 5% performance\ndifference but 20-25% behavioral differences, $\\textbf{VERBA}$ effectively\nverbalizes their variations with up to 80% overall accuracy. When we included\nthe models' structural information, the verbalization's accuracy further\nimproved to 90%. $\\textbf{VERBA}$ opens up new research avenues for improving\nthe transparency and comparability of machine learning models in a post-hoc\nmanner.",
    "published": "2025-07-03T02:25:24Z",
    "updated": "2025-07-03T02:25:24Z",
    "id": "2507.02241v1",
    "authors": [
      "Shravan Doda",
      "Shashidhar Reddy Javaji",
      "Zining Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02241v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02241v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02241v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to verbalize differences between machine learning models, which aligns with the LLM topic. It also involves benchmarking and dataset creation for model comparisons, fitting under the Benchmark and Dataset topics.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.03033v1": {
    "title": "Preserving Privacy, Increasing Accessibility, and Reducing Cost: An\n  On-Device Artificial Intelligence Model for Medical Transcription and Note\n  Generation",
    "summary": "Background: Clinical documentation represents a significant burden for\nhealthcare providers, with physicians spending up to 2 hours daily on\nadministrative tasks. Recent advances in large language models (LLMs) offer\npromising solutions, but privacy concerns and computational requirements limit\ntheir adoption in healthcare settings. Objective: To develop and evaluate a\nprivacy-preserving, on-device medical transcription system using a fine-tuned\nLlama 3.2 1B model capable of generating structured medical notes from medical\ntranscriptions while maintaining complete data sovereignty entirely in the\nbrowser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient\nFine-Tuning (PEFT) with LoRA on 1,500 synthetic medical\ntranscription-to-structured note pairs. The model was evaluated against the\nbase Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140\nmodified ACI benchmark cases. Evaluation employed both statistical metrics\n(ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple\nclinical quality dimensions. Results: The fine-tuned OnDevice model\ndemonstrated substantial improvements over the base model. On the ACI\nbenchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1\nimproved from 0.832 to 0.866. Clinical quality assessments showed marked\nreduction in major hallucinations (from 85 to 35 cases) and enhanced factual\ncorrectness (2.81 to 3.54 on 5-point scale). Similar improvements were observed\non the internal evaluation dataset, with composite scores increasing from 3.13\nto 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical\ntranscription yields clinically meaningful improvements while enabling complete\non-device browser deployment. This approach addresses key barriers to AI\nadoption in healthcare: privacy preservation, cost reduction, and accessibility\nfor resource-constrained environments.",
    "published": "2025-07-03T01:51:49Z",
    "updated": "2025-07-03T01:51:49Z",
    "id": "2507.03033v1",
    "authors": [
      "Johnson Thomas",
      "Ayush Mudgal",
      "Wendao Liu",
      "Nisten Tahiraj",
      "Zeeshaan Mohammed",
      "Dhruv Diddi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03033v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03033v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03033v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development and evaluation of a fine-tuned Llama 3.2 1B model for medical transcription, which involves the use of a large language model (LLM) and its application in a specific domain (healthcare). The focus is on improving the model's performance through fine-tuning and addressing privacy and accessibility concerns, which aligns with the topics of LLM and Pretrain.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.05269v1": {
    "title": "CORE: Benchmarking LLMs Code Reasoning Capabilities through Static\n  Analysis Tasks",
    "summary": "Large language models (LLMs) have been widely adopted across diverse software\nengineering domains, such as code generation, program repair, and vulnerability\ndetection. These applications require understanding beyond surface-level code\npatterns: value propagation, control flow, and interdependence between program\nelements. However, existing benchmarks primarily evaluate end-to-end outcomes,\nsuch as whether code is correctly repaired or generated, leaving the models\nability for program semantic reasoning underexplored. This work presents CoRe,\na high-quality, human-verified benchmark designed to evaluate LLMs on\nfundamental static analysis tasks. CoRe includes 12,553 task instances spanning\ndata dependency, control dependency, and information flow across programs\nwritten in C/C++, Java, and Python. To ensure semantic diversity and reasoning\ncomplexity, we propose a semantics-aware diverse sampling strategy that selects\ntargets and task instances based on structural coverage and dependency depth.\nWe evaluate 10 mainstream LLMs and show that, while they perform well at\nidentifying dependencies, models still struggle with tasks that require deeper\nsemantic understanding and multi-step reasoning. We further conduct qualitative\nanalyses to uncover key challenges, such as complex control structures and\nbackward dependency patterns, offering insights into improving LLMs code\nreasoning capabilities.",
    "published": "2025-07-03T01:35:58Z",
    "updated": "2025-07-03T01:35:58Z",
    "id": "2507.05269v1",
    "authors": [
      "Danning Xie",
      "Mingwei Zheng",
      "Xuwei Liu",
      "Jiannan Wang",
      "Chengpeng Wang",
      "Lin Tan",
      "Xiangyu Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05269v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05269v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05269v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking the code reasoning capabilities of LLMs through static analysis tasks, which aligns with the topics of Benchmark (evaluating LLMs) and Reasoning (assessing LLMs' reasoning abilities). The mention of LLMs' performance in code-related tasks also slightly touches on the LLM topic.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning",
      "LLM"
    ]
  },
  "2507.02226v1": {
    "title": "DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs",
    "summary": "As one of their many applications, large language models (LLMs) have recently\nshown promise in automating register transfer level (RTL) code generation.\nHowever, conventional LLM decoding strategies, originally designed for natural\nlanguage, often fail to meet the structural and semantic demands of RTL,\nleading to hallucinated, repetitive, or invalid code outputs. In this paper, we\nfirst investigate the root causes of these decoding failures through an\nempirical analysis of token-level entropy during RTL generation. Our findings\nreveal that LLMs exhibit low confidence in regions of structural ambiguity or\nsemantic complexity, showing that standard decoding strategies fail to\ndifferentiate between regions requiring determinism (syntax-critical regions)\nand those that benefit from creative exploratory variability (design-critical\nregions). Then, to overcome this, we introduce DecoRTL, a novel run-time\ndecoding strategy, that is both syntax-aware and contrastive for RTL code\ngeneration. DecoRTL integrates two complementary components: (i)\nself-consistency sampling, which generates multiple candidates and re-ranks\nthem based on token-level agreement to promote correctness while maintaining\ndiversity; and (ii) syntax-aware temperature adaptation, which classifies\ntokens by their syntactical and functional roles and adjusts the sampling\ntemperature accordingly, enforcing low temperature for syntax-critical tokens\nand higher temperature for exploratory ones. Our approach operates entirely at\ninference time without requiring any additional model fine-tuning. Through\nevaluations on multiple open-source LLMs using the VerilogEval benchmark, we\ndemonstrate significant improvements in syntactic validity, functional\ncorrectness, and output diversity, while the execution overhead (performance\noverhead) is imperceptible.",
    "published": "2025-07-03T01:17:44Z",
    "updated": "2025-07-03T01:17:44Z",
    "id": "2507.02226v1",
    "authors": [
      "Mohammad Akyash",
      "Kimia Azar",
      "Hadi Kamali"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02226v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02226v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02226v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the decoding strategies of Large Language Models (LLMs) specifically for RTL code generation, which involves structural and semantic demands unique to this domain. It introduces a novel run-time decoding framework, DecoRTL, to address these challenges. The core topics are related to LLMs and their application in specialized domains, but none of the provided topics directly match the specific focus on RTL code generation.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.02221v2": {
    "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic\n  Data Commons",
    "summary": "The Genomic Data Commons (GDC) provides access to high quality, harmonized\ncancer genomics data through a unified curation and analysis platform centered\naround patient cohorts. While GDC users can interactively create complex\ncohorts through the graphical Cohort Builder, users (especially new ones) may\nstruggle to find specific cohort descriptors across hundreds of possible fields\nand properties. However, users may be better able to describe their desired\ncohort in free-text natural language. We introduce GDC Cohort Copilot, an\nopen-source copilot tool for curating cohorts from the GDC. GDC Cohort Copilot\nautomatically generates the GDC cohort filter corresponding to a user-input\nnatural language description of their desired cohort, before exporting the\ncohort back to the GDC for further analysis. An interactive user interface\nallows users to further refine the generated cohort. We develop and evaluate\nmultiple large language models (LLMs) for GDC Cohort Copilot and demonstrate\nthat our locally-served, open-source GDC Cohort LLM achieves better results\nthan GPT-4o prompting in generating GDC cohorts. We implement and share GDC\nCohort Copilot as a containerized Gradio app on HuggingFace Spaces, available\nat https://huggingface.co/spaces/uc-ctds/GDC-Cohort-Copilot. GDC Cohort LLM\nweights are available at https://huggingface.co/uc-ctds. All source code is\navailable at https://github.com/uc-cdis/gdc-cohort-copilot.",
    "published": "2025-07-03T00:55:58Z",
    "updated": "2025-07-14T18:30:25Z",
    "id": "2507.02221v2",
    "authors": [
      "Steven Song",
      "Anirudh Subramanyam",
      "Zhenyu Zhang",
      "Aarti Venkat",
      "Robert L. Grossman"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02221v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02221v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02221v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development and evaluation of large language models (LLMs) for a specific application in genomic data curation, which aligns with the 'LLM' topic. It also involves the use of LLMs for generating and refining cohorts based on natural language descriptions, which is relevant to 'Reasoning' as it involves complex problem-solving and logical reasoning. The application is specific to genomic data and does not directly fit into other provided categories.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.02186v1": {
    "title": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge",
    "summary": "With the broad availability of large language models and their ability to\ngenerate vast outputs using varied prompts and configurations, determining the\nbest output for a given task requires an intensive evaluation process, one\nwhere machine learning practitioners must decide how to assess the outputs and\nthen carefully carry out the evaluation. This process is both time-consuming\nand costly. As practitioners work with an increasing number of models, they\nmust now evaluate outputs to determine which model and prompt performs best for\na given task. LLMs are increasingly used as evaluators to filter training data,\nevaluate model performance, assess harms and risks, or assist human evaluators\nwith detailed assessments. We present EvalAssist, a framework that simplifies\nthe LLM-as-a-judge workflow. The system provides an online criteria development\nenvironment, where users can interactively build, test, and share custom\nevaluation criteria in a structured and portable format. We support a set of\nLLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a\nprompt-chaining approach we developed and contributed to the UNITXT open-source\nlibrary. Additionally, our system also includes specially trained evaluators to\ndetect harms and risks in LLM outputs. We have deployed the system internally\nin our organization with several hundreds of users.",
    "published": "2025-07-02T22:45:39Z",
    "updated": "2025-07-02T22:45:39Z",
    "id": "2507.02186v1",
    "authors": [
      "Zahra Ashktorab",
      "Elizabeth M. Daly",
      "Erik Miehling",
      "Werner Geyer",
      "Martin Santillan Cooper",
      "Tejaswini Pedapati",
      "Michael Desmond",
      "Qian Pan",
      "Hyo Jin Do"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02186v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02186v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02186v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of LLMs as evaluators and introduces a tool to assist in this process, which aligns with the topics of LLM evaluation and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.02183v1": {
    "title": "Computer Science Education in the Age of Generative AI",
    "summary": "Generative AI tools - most notably large language models (LLMs) like ChatGPT\nand Codex - are rapidly revolutionizing computer science education. These tools\ncan generate, debug, and explain code, thereby transforming the landscape of\nprogramming instruction. This paper examines the profound opportunities that AI\noffers for enhancing computer science education in general, from coding\nassistance to fostering innovative pedagogical practices and streamlining\nassessments. At the same time, it highlights challenges including academic\nintegrity concerns, the risk of over-reliance on AI, and difficulties in\nverifying originality. We discuss what computer science educators should teach\nin the AI era, how to best integrate these technologies into curricula, and the\nbest practices for assessing student learning in an environment where AI can\ngenerate code, prototypes and user feedback. Finally, we propose a set of\npolicy recommendations designed to harness the potential of generative AI while\npreserving the integrity and rigour of computer science education. Empirical\ndata and emerging studies are used throughout to support our arguments.",
    "published": "2025-07-02T22:28:45Z",
    "updated": "2025-07-02T22:28:45Z",
    "id": "2507.02183v1",
    "authors": [
      "Russell Beale"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02183v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02183v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02183v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of large language models (LLMs) like ChatGPT and Codex on computer science education, focusing on their use in generating, debugging, and explaining code. It also addresses the integration of these technologies into curricula and the challenges they present, such as academic integrity and over-reliance on AI.",
    "llm_cls_result": [
      "LLM",
      "Other"
    ]
  },
  "2507.02182v1": {
    "title": "Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large\n  Language Models",
    "summary": "Common Business Oriented Language (COBOL) is a programming language used to\ndevelop business applications that are widely adopted by financial, business,\nand government agencies. Due to its age, complexity, and declining number of\nCOBOL developers, maintaining COBOL codebases is becoming increasingly\nchallenging. In particular, the lack of documentation makes it difficult for\nnew developers to effectively understand and maintain COBOL systems. Existing\nresearch utilizes large language models (LLMs) to explain the functionality of\ncode snippets. However, COBOL presents unique challenges due to its\narchitectural and syntactical differences, which often cause its code to exceed\nthe token window size of LLMs. In this work, we propose a multi-agent approach\nthat leverages two LLM-based agents working collaboratively to generate\nexplanations for functions, files, and the overall project. These agents\nincorporate together by utilizing contextual information from the codebase into\nthe code explanation prompts. We evaluate the effectiveness of our approach\nusing 14 open-source, real-world COBOL projects. Our results indicate that our\napproach performs significantly better than the baseline in function code\nexplanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,\nchrF, and SentenceBERT scores, respectively. At the file level, our approach\neffectively explains both short and long COBOL files that exceed the token\nwindow size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in\nexplaining the purpose, functionality, and clarity of the generated\nexplanation. At the project level, our approach generates explanations that\nconvey the functionality and purpose of 82% of the selected projects.",
    "published": "2025-07-02T22:28:35Z",
    "updated": "2025-07-02T22:28:35Z",
    "id": "2507.02182v1",
    "authors": [
      "Fangjian Lei",
      "Jiawen Liu",
      "Shayan Noei",
      "Ying Zou",
      "Derek Truong",
      "William Alexander"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02182v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02182v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02182v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a multi-agent approach to enhance COBOL code explanations, which directly relates to research on LLMs and their applications in understanding and generating explanations for code.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.02180v1": {
    "title": "The Revolution Has Arrived: What the Current State of Large Language\n  Models in Education Implies for the Future",
    "summary": "Large language Models have only been widely available since 2022 and yet in\nless than three years have had a significant impact on approaches to education\nand educational technology. Here we review the domains in which they have been\nused, and discuss a variety of use cases, their successes and failures. We then\nprogress to discussing how this is changing the dynamic for learners and\neducators, consider the main design challenges facing LLMs if they are to\nbecome truly helpful and effective as educational systems, and reflect on the\nlearning paradigms they support. We make clear that the new interaction\nparadigms they bring are significant and argue that this approach will become\nso ubiquitous it will become the default way in which we interact with\ntechnologies, and revolutionise what people expect from computer systems in\ngeneral. This leads us to present some specific and significant considerations\nfor the design of educational technology in the future that are likely to be\nneeded to ensure acceptance by the changing expectations of learners and users.",
    "published": "2025-07-02T22:23:26Z",
    "updated": "2025-07-02T22:23:26Z",
    "id": "2507.02180v1",
    "authors": [
      "Russell Beale"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02180v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02180v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02180v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of Large Language Models (LLMs) on education and educational technology, focusing on their use cases, successes, failures, and future design challenges. The core topic is clearly related to LLMs and their applications in education.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.03027v1": {
    "title": "The Book of Life approach: Enabling richness and scale for life course\n  research",
    "summary": "For over a century, life course researchers have faced a choice between two\ndominant methodological approaches: qualitative methods that analyze rich data\nbut are constrained to small samples, and quantitative survey-based methods\nthat study larger populations but sacrifice data richness for scale. Two recent\ntechnological developments now enable us to imagine a hybrid approach that\ncombines some of the depth of the qualitative approach with the scale of\nquantitative methods. The first development is the steady rise of ''complex log\ndata,'' behavioral data that is logged for purposes other than research but\nthat can be repurposed to construct rich accounts of people's lives. The second\nis the emergence of large language models (LLMs) with exceptional pattern\nrecognition capabilities on plain text. In this paper, we take a necessary step\ntoward creating this hybrid approach by developing a flexible procedure to\ntransform complex log data into a textual representation of an individual's\nlife trajectory across multiple domains, over time, and in context. We call\nthis data representation a ''book of life.'' We illustrate the feasibility of\nour approach by writing over 100 million books of life covering many different\nfacets of life, over time and placed in social context using Dutch\npopulation-scale registry data. We open source the book of life toolkit (BOLT),\nand invite the research community to explore the many potential applications of\nthis approach.",
    "published": "2025-07-02T21:49:25Z",
    "updated": "2025-07-02T21:49:25Z",
    "id": "2507.03027v1",
    "authors": [
      "Mark D. Verhagen",
      "Benedikt Stroebl",
      "Tiffany Liu",
      "Lydia T. Liu",
      "Matthew J. Salganik"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03027v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03027v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03027v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to analyze complex log data for life course research, which aligns with the LLM topic. It also involves the application of LLMs in a research context, which is a key aspect of the LLM topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.02139v1": {
    "title": "When LLMs Disagree: Diagnosing Relevance Filtering Bias and Retrieval\n  Divergence in SDG Search",
    "summary": "Large language models (LLMs) are increasingly used to assign document\nrelevance labels in information retrieval pipelines, especially in domains\nlacking human-labeled data. However, different models often disagree on\nborderline cases, raising concerns about how such disagreement affects\ndownstream retrieval. This study examines labeling disagreement between two\nopen-weight LLMs, LLaMA and Qwen, on a corpus of scholarly abstracts related to\nSustainable Development Goals (SDGs) 1, 3, and 7. We isolate disagreement\nsubsets and examine their lexical properties, rank-order behavior, and\nclassification predictability. Our results show that model disagreement is\nsystematic, not random: disagreement cases exhibit consistent lexical patterns,\nproduce divergent top-ranked outputs under shared scoring functions, and are\ndistinguishable with AUCs above 0.74 using simple classifiers. These findings\nsuggest that LLM-based filtering introduces structured variability in document\nretrieval, even under controlled prompting and shared ranking logic. We propose\nusing classification disagreement as an object of analysis in retrieval\nevaluation, particularly in policy-relevant or thematic search tasks.",
    "published": "2025-07-02T20:53:51Z",
    "updated": "2025-07-02T20:53:51Z",
    "id": "2507.02139v1",
    "authors": [
      "William A. Ingram",
      "Bipasha Banerjee",
      "Edward A. Fox"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02139v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02139v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02139v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of LLMs in information retrieval pipelines, focusing on their disagreement in assigning relevance labels and its impact on downstream retrieval. This aligns with the topics of LLM (Large Language Models) and Benchmark (evaluation of LLMs in retrieval tasks).",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.02135v1": {
    "title": "Dissecting the Impact of Mobile DVFS Governors on LLM Inference\n  Performance and Energy Efficiency",
    "summary": "Large Language Models (LLMs) are increasingly being integrated into various\napplications and services running on billions of mobile devices. However,\ndeploying LLMs on resource-limited mobile devices faces a significant challenge\ndue to their high demand for computation, memory, and ultimately energy. While\ncurrent LLM frameworks for mobile use three power-hungry components-CPU, GPU,\nand Memory-even when running primarily-GPU LLM models, optimized DVFS governors\nfor CPU, GPU, and memory featured in modern mobile devices operate\nindependently and are oblivious of each other. Motivated by the above\nobservation, in this work, we first measure the energy-efficiency of a SOTA LLM\nframework consisting of various LLM models on mobile phones which showed the\ntriplet mobile governors result in up to 40.4% longer prefilling and decoding\nlatency compared to optimal combinations of CPU, GPU, and memory frequencies\nwith the same energy consumption for sampled prefill and decode lengths.\nSecond, we conduct an in-depth measurement study to uncover how the intricate\ninterplay (or lack of) among the mobile governors cause the above inefficiency\nin LLM inference. Finally, based on these insights, we design FUSE - a unified\nenergy-aware governor for optimizing the energy efficiency of LLM inference on\nmobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the\ntime-to-first-token and time-per-output-token latencies by 7.0%-16.9% and\n25.4%-36.8% on average with the same energy-per-token for various mobile LLM\nmodels.",
    "published": "2025-07-02T20:47:40Z",
    "updated": "2025-07-02T20:47:40Z",
    "id": "2507.02135v1",
    "authors": [
      "Zongpu Zhang",
      "Pranab Dash",
      "Y. Charlie Hu",
      "Qiang Xu",
      "Jian Li",
      "Haibing Guan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02135v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02135v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02135v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the deployment and optimization of Large Language Models (LLMs) on mobile devices, focusing on energy efficiency and performance. It involves LLM inference and mobile-specific optimizations, which are relevant to the 'LLM' and 'Scaling' topics.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.02130v1": {
    "title": "BACTA-GPT: An AI-Based Bayesian Adaptive Clinical Trial Architect",
    "summary": "Bayesian adaptive clinical trials offer a flexible and efficient alternative\nto traditional fixed-design trials, but their implementation is often hindered\nby the complexity of Bayesian computations and the need for advanced\nstatistical programming expertise. The authors introduce a custom fine-tuned\nLLM designed to assist with this and lower barriers to adoption of Bayesian\nmethods for adaptive clinical trials. This paper describes the development and\nfine-tuning of BACTA-GPT, a Large Language Model (LLM)-based tool designed to\nassist in the implementation of Bayesian Adaptive Clinical Trials. This engine\nuses GPT-3.5 as the underlying model and takes in Natural Language input from\nthe Statistician or the Trialist. The fine-tuned model demonstrates a viable\nproof-of-concept in its objectives. Test case evaluations show that the model\nis capable of generating a fit-for-purpose Bayesian model for an adaptive trial\nand evaluate its operating characteristics via simulations using R and JAGS.\nThe integration of AI code generation has significant potential to lower\ntechnical barriers for the design and implementation of Bayesian Adaptive\ntrials. But they also require attention to important considerations regarding\nvalidation and quality control.",
    "published": "2025-07-02T20:29:54Z",
    "updated": "2025-07-02T20:29:54Z",
    "id": "2507.02130v1",
    "authors": [
      "Krishna Padmanabhan",
      "Danny Baker"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02130v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02130v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02130v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development and fine-tuning of a Large Language Model (LLM) for assisting in Bayesian Adaptive Clinical Trials, which involves the use of LLMs in a specialized application.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.02128v1": {
    "title": "CROP: Circuit Retrieval and Optimization with Parameter Guidance using\n  LLMs",
    "summary": "Modern very large-scale integration (VLSI) design requires the implementation\nof integrated circuits using electronic design automation (EDA) tools. Due to\nthe complexity of EDA algorithms, the vast parameter space poses a huge\nchallenge to chip design optimization, as the combination of even moderate\nnumbers of parameters creates an enormous solution space to explore. Manual\nparameter selection remains industrial practice despite being excessively\nlaborious and limited by expert experience. To address this issue, we present\nCROP, the first large language model (LLM)-powered automatic VLSI design flow\ntuning framework. Our approach includes: (1) a scalable methodology for\ntransforming RTL source code into dense vector representations, (2) an\nembedding-based retrieval system for matching designs with semantically similar\ncircuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided\nparameter search system that constrains the search process with prior knowledge\nfrom similar designs. Experiment results demonstrate CROP's ability to achieve\nsuperior quality-of-results (QoR) with fewer iterations than existing\napproaches on industrial designs, including a 9.9% reduction in power\nconsumption.",
    "published": "2025-07-02T20:25:47Z",
    "updated": "2025-07-02T20:25:47Z",
    "id": "2507.02128v1",
    "authors": [
      "Jingyu Pan",
      "Isaac Jacobson",
      "Zheng Zhao",
      "Tung-Chieh Chen",
      "Guanglei Zhou",
      "Chen-Chia Chang",
      "Vineet Rashingkar",
      "Yiran Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02128v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02128v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02128v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for automatic VLSI design flow tuning, including retrieval-augmented generation (RAG) techniques, which are relevant to the topics of LLM and Memory.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.02124v1": {
    "title": "SAKURAONE: Empowering Transparent and Open AI Platforms through\n  Private-Sector HPC Investment in Japan",
    "summary": "SAKURAONE is a managed high performance computing (HPC) cluster developed and\noperated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU\nPHY'' configuration of bare-metal GPU servers and is designed as a cluster\ncomputing resource optimized for advanced workloads, including large language\nmodel (LLM) training.\n  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked\n\\textbf{49th} in the world based on its High Performance Linpack (HPL) score,\ndemonstrating its global competitiveness. In particular, it is the \\textbf{only\nsystem within the top 100} that employs a fully open networking stack based on\n\\textbf{800~GbE (Gigabit Ethernet)} and the \\textbf{SONiC (Software for Open\nNetworking in the Cloud)} operating system, highlighting the viability of open\nand vendor-neutral technologies in large-scale HPC infrastructure.\n  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL\nbenchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate\nGradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets\nlow-precision workloads representative of AI applications, SAKURAONE delivered\nan impressive 339.86~PFLOP/s using FP8 precision.\n  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100\nGPUs. It is supported by an all-flash Lustre storage subsystem with a total\nphysical capacity of 2~petabytes, providing high-throughput and low-latency\ndata access. Internode communication is enabled by a full-bisection bandwidth\ninterconnect based on a Rail-Optimized topology, where the Leaf and Spine\nlayers are interconnected via 800~GbE links. This topology, in combination with\nRoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless\ndata transfers and mitigates communication bottlenecks in large-scale parallel\nworkloads.",
    "published": "2025-07-02T20:13:09Z",
    "updated": "2025-07-02T20:13:09Z",
    "id": "2507.02124v1",
    "authors": [
      "Fumikazu Konishi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02124v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02124v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02124v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a high-performance computing (HPC) cluster optimized for large language model (LLM) training, which is relevant to the 'LLM' topic. However, it primarily focuses on the infrastructure and performance benchmarks rather than the models themselves, so it does not fit neatly into the other provided categories.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.02122v1": {
    "title": "PAL: Designing Conversational Agents as Scalable, Cooperative Patient\n  Simulators for Palliative-Care Training",
    "summary": "Effective communication in serious illness and palliative care is essential\nbut often under-taught due to limited access to training resources like\nstandardized patients. We present PAL (Palliative Assisted Learning-bot), a\nconversational system that simulates emotionally nuanced patient interactions\nand delivers structured feedback grounded in an existing empathy-based\nframework. PAL supports text and voice modalities and is designed to scaffold\nclinical skill-building through repeated, low-cost practice. Through a\nmixed-methods study with 17 U.S. medical trainees and clinicians, we explore\nuser engagement with PAL, evaluate usability, and examine design tensions\naround modalities, emotional realism, and feedback delivery. Participants found\nPAL helpful for reflection and skill refinement, though some noted limitations\nin emotional authenticity and the adaptability of feedback. We contribute: (1)\nempirical evidence that large language models can support palliative\ncommunication training; (2) design insights for modality-aware, emotionally\nsensitive simulation tools; and (3) implications for systems that support\nemotional labor, cooperative learning, and AI-augmented training in high-stakes\ncare settings.",
    "published": "2025-07-02T20:09:52Z",
    "updated": "2025-07-02T20:09:52Z",
    "id": "2507.02122v1",
    "authors": [
      "Neil K. R. Sehgal",
      "Hita Kambhamettu",
      "Allen Chang",
      "Andrew Zhu",
      "Lyle Ungar",
      "Sharath Chandra Guntuku"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02122v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02122v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02122v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in designing conversational agents for palliative-care training, which involves emotional and cooperative learning aspects. It also touches on the application of LLMs in high-stakes care settings.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.02103v1": {
    "title": "What Neuroscience Can Teach AI About Learning in Continuously Changing\n  Environments",
    "summary": "Modern AI models, such as large language models, are usually trained once on\na huge corpus of data, potentially fine-tuned for a specific task, and then\ndeployed with fixed parameters. Their training is costly, slow, and gradual,\nrequiring billions of repetitions. In stark contrast, animals continuously\nadapt to the ever-changing contingencies in their environments. This is\nparticularly important for social species, where behavioral policies and reward\noutcomes may frequently change in interaction with peers. The underlying\ncomputational processes are often marked by rapid shifts in an animal's\nbehaviour and rather sudden transitions in neuronal population activity. Such\ncomputational capacities are of growing importance for AI systems operating in\nthe real world, like those guiding robots or autonomous vehicles, or for\nagentic AI interacting with humans online. Can AI learn from neuroscience? This\nPerspective explores this question, integrating the literature on continual and\nin-context learning in AI with the neuroscience of learning on behavioral tasks\nwith shifting rules, reward probabilities, or outcomes. We will outline an\nagenda for how specifically insights from neuroscience may inform current\ndevelopments in AI in this area, and - vice versa - what neuroscience may learn\nfrom AI, contributing to the evolving field of NeuroAI.",
    "published": "2025-07-02T19:30:57Z",
    "updated": "2025-07-02T19:30:57Z",
    "id": "2507.02103v1",
    "authors": [
      "Daniel Durstewitz",
      "Bruno Averbeck",
      "Georgia Koppe"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02103v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02103v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02103v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of neuroscience insights into AI, particularly focusing on continual and in-context learning, which is relevant to AGI and the broader field of AI learning mechanisms.",
    "llm_cls_result": [
      "AGI",
      "Other"
    ]
  },
  "2507.02097v2": {
    "title": "The Future is Agentic: Definitions, Perspectives, and Open Challenges of\n  Multi-Agent Recommender Systems",
    "summary": "Large language models (LLMs) are rapidly evolving from passive engines of\ntext generation into agentic entities that can plan, remember, invoke external\ntools, and co-operate with one another. This perspective paper investigates how\nsuch LLM agents (and societies thereof) can transform the design space of\nrecommender systems.\n  We introduce a unified formalism that (i) models an individual agent as a\ntuple comprising its language core, tool set, and hierarchical memory, and (ii)\ncaptures a multi-agent recommender as a triple of agents, shared environment,\nand communication protocol. Within this framework, we present four end-to-end\nuse cases-interactive party planning, synthetic user-simulation for offline\nevaluation, multi-modal furniture recommendation, and brand-aligned explanation\ngeneration-each illustrating a distinct capability unlocked by agentic\norchestration.\n  We then surface five cross-cutting challenge families: protocol complexity,\nscalability, hallucination and error propagation, emergent misalignment\n(including covert collusion), and brand compliance.\n  For each, we formalize the problem, review nascent mitigation strategies, and\noutline open research questions. The result is both a blueprint and an agenda:\na blueprint that shows how memory-augmented, tool-using LLM agents can be\ncomposed into robust recommendation pipelines, and an agenda inviting the\nRecSys community to develop benchmarks, theoretical guarantees, and governance\ntools that keep pace with this new degree of autonomy. By unifying agentic\nabstractions with recommender objectives, the paper lays the groundwork for the\nnext generation of personalized, trustworthy, and context-rich recommendation\nservices.",
    "published": "2025-07-02T19:25:44Z",
    "updated": "2025-07-10T14:47:38Z",
    "id": "2507.02097v2",
    "authors": [
      "Reza Yousefi Maragheh",
      "Yashar Deldjoo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02097v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02097v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02097v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the transformation of LLMs into agentic entities for recommender systems, involving planning, memory, and cooperation, which aligns with topics related to LLM agents and reinforcement learning.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Memory"
    ]
  },
  "2507.02088v1": {
    "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language\n  Models",
    "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs.",
    "published": "2025-07-02T19:04:56Z",
    "updated": "2025-07-02T19:04:56Z",
    "id": "2507.02088v1",
    "authors": [
      "Tian Lan",
      "Xiangdong Su",
      "Xu Liu",
      "Ruirui Wang",
      "Ke Chang",
      "Jiang Li",
      "Guanglai Gao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02088v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02088v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02088v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark for evaluating biases in large language models (LLMs) specifically for Chinese language and culture, which involves multiple tasks and extensive categories. This aligns with the 'Benchmark' topic as it focuses on evaluating LLMs and their performance in terms of bias.",
    "llm_cls_result": [
      "Benchmark"
    ]
  },
  "2507.02087v1": {
    "title": "Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions",
    "summary": "The use of large language models (LLMs) in hiring promises to streamline\ncandidate screening, but it also raises serious concerns regarding accuracy and\nalgorithmic bias where sufficient safeguards are not in place. In this work, we\nbenchmark several state-of-the-art foundational LLMs - including models from\nOpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our\nproprietary domain-specific hiring model (Match Score) for job candidate\nmatching. We evaluate each model's predictive accuracy (ROC AUC,\nPrecision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis\nacross declared gender, race, and intersectional subgroups). Our experiments on\na dataset of roughly 10,000 real-world recent candidate-job pairs show that\nMatch Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs\n0.77) and achieves significantly more equitable outcomes across demographic\ngroups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957\n(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the\nintersectionals, respectively). We discuss why pretraining biases may cause\nLLMs with insufficient safeguards to propagate societal biases in hiring\nscenarios, whereas a bespoke supervised model can more effectively mitigate\nthese biases. Our findings highlight the importance of domain-specific modeling\nand bias auditing when deploying AI in high-stakes domains such as hiring, and\ncaution against relying on off-the-shelf LLMs for such tasks without extensive\nfairness safeguards. Furthermore, we show with empirical evidence that there\nshouldn't be a dichotomy between choosing accuracy and fairness in hiring: a\nwell-designed algorithm can achieve both accuracy in hiring and fairness in\noutcomes.",
    "published": "2025-07-02T19:02:18Z",
    "updated": "2025-07-02T19:02:18Z",
    "id": "2507.02087v1",
    "authors": [
      "Eitan Anzenberg",
      "Arunava Samajpati",
      "Sivasankaran Chandrasekar",
      "Varun Kacholia"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02087v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02087v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02087v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper evaluates the performance and fairness of LLMs in hiring decisions, comparing them with a domain-specific model. It discusses the biases in LLMs and the importance of domain-specific modeling and bias auditing. The core topics are related to LLMs and benchmarking their performance and fairness.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.02083v2": {
    "title": "Measuring Scientific Capabilities of Language Models with a Systems\n  Biology Dry Lab",
    "summary": "Designing experiments and result interpretations are core scientific\ncompetencies, particularly in biology, where researchers perturb complex\nsystems to uncover the underlying systems. Recent efforts to evaluate the\nscientific capabilities of large language models (LLMs) fail to test these\ncompetencies because wet-lab experimentation is prohibitively expensive: in\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\nthat assesses LLMs' iterative experiment design and analysis abilities in\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\nwet-lab costs by running a dry lab of biological systems. These models, encoded\nin Systems Biology Markup Language, are efficient for generating simulated\ndata, making them ideal testbeds for experimentation on realistically complex\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\ntotal of 350 systems. Our evaluation shows that while more capable models\ndemonstrated superior performance, all models' performance declined\nsignificantly as system complexity increased, suggesting substantial room for\nimprovement in the scientific capabilities of LLM agents.",
    "published": "2025-07-02T18:41:44Z",
    "updated": "2025-07-14T15:17:16Z",
    "id": "2507.02083v2",
    "authors": [
      "Haonan Duan",
      "Stephen Zhewen Lu",
      "Caitlin Fiona Harrigan",
      "Nishkrit Desai",
      "Jiarui Lu",
      "Micha Koziarski",
      "Leonardo Cotta",
      "Chris J. Maddison"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02083v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02083v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02083v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the scientific capabilities of large language models (LLMs) through a benchmark called SciGym, which involves iterative experiment design and analysis in a dry lab setting. This directly relates to LLM research and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.02076v1": {
    "title": "Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time\n  Compute in LLMs",
    "summary": "Large language models (LLMs) have rapidly progressed into general-purpose\nagents capable of solving a broad spectrum of tasks. However, current models\nremain inefficient at reasoning: they apply fixed inference-time compute\nregardless of task complexity, often overthinking simple problems while\nunderthinking hard ones. This survey presents a comprehensive review of\nefficient test-time compute (TTC) strategies, which aim to improve the\ncomputational efficiency of LLM reasoning. We introduce a two-tiered taxonomy\nthat distinguishes between L1-controllability, methods that operate under fixed\ncompute budgets, and L2-adaptiveness, methods that dynamically scale inference\nbased on input difficulty or model confidence. We benchmark leading proprietary\nLLMs across diverse datasets, highlighting critical trade-offs between\nreasoning performance and token usage. Compared to prior surveys on efficient\nreasoning, our review emphasizes the practical control, adaptability, and\nscalability of TTC methods. Finally, we discuss emerging trends such as hybrid\nthinking models and identify key challenges for future work towards making LLMs\nmore computationally efficient, robust, and responsive to user constraints.",
    "published": "2025-07-02T18:27:42Z",
    "updated": "2025-07-02T18:27:42Z",
    "id": "2507.02076v1",
    "authors": [
      "Mohammad Ali Alomrani",
      "Yingxue Zhang",
      "Derek Li",
      "Qianyi Sun",
      "Soumyasundar Pal",
      "Zhanguang Zhang",
      "Yaochen Hu",
      "Rohan Deepak Ajwani",
      "Antonios Valkanas",
      "Raika Karimi",
      "Peng Cheng",
      "Yunzhou Wang",
      "Pengyi Liao",
      "Hanrui Huang",
      "Bin Wang",
      "Jianye Hao",
      "Mark Coates"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02076v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02076v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02076v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses efficient test-time compute strategies for improving the computational efficiency of LLM reasoning, which directly relates to the topics of Reasoning (as it involves reasoning abilities in LLMs) and LLM (as it focuses on large language models).",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2507.02074v1": {
    "title": "Large Language Models for Crash Detection in Video: A Survey of Methods,\n  Datasets, and Challenges",
    "summary": "Crash detection from video feeds is a critical problem in intelligent\ntransportation systems. Recent developments in large language models (LLMs) and\nvision-language models (VLMs) have transformed how we process, reason about,\nand summarize multimodal information. This paper surveys recent methods\nleveraging LLMs for crash detection from video data. We present a structured\ntaxonomy of fusion strategies, summarize key datasets, analyze model\narchitectures, compare performance benchmarks, and discuss ongoing challenges\nand opportunities. Our review provides a foundation for future research in this\nfast-growing intersection of video understanding and foundation models.",
    "published": "2025-07-02T18:21:01Z",
    "updated": "2025-07-02T18:21:01Z",
    "id": "2507.02074v1",
    "authors": [
      "Sanjeda Akter",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02074v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02074v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02074v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models (LLMs) and vision-language models (VLMs) in crash detection from video data, which involves multimodal processing and reasoning. The topics 'MLLM' and 'VLA' are relevant as they cover multimodal large language models and vision-language alignment models, respectively. The paper also mentions performance benchmarks, making 'Benchmark' a relevant topic.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Benchmark"
    ]
  },
  "2507.02057v1": {
    "title": "MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned\n  LLMs for Malware Generation",
    "summary": "Large language models (LLMs) have democratized software development, reducing\nthe expertise barrier for programming complex applications. This accessibility\nextends to malicious software development, raising significant security\nconcerns. While LLM providers have implemented alignment mechanisms to prevent\ndirect generation of overtly malicious code, these safeguards predominantly\nevaluate individual prompts in isolation, overlooking a critical vulnerability:\nmalicious operations can be systematically decomposed into benign-appearing\nsub-tasks. In this paper, we introduce the Malware Generation Compiler (MGC), a\nnovel framework that leverages this vulnerability through modular decomposition\nand alignment-evasive generation. MGC employs a specialized Malware Description\nIntermediate Representation (MDIR) to bridge high-level malicious intents and\nbenign-appearing code snippets. Extensive evaluation demonstrates that our\nattack reliably generates functional malware across diverse task specifications\nand categories, outperforming jailbreaking methods by +365.79% and underground\nservices by +78.07% in correctness on three benchmark datasets. Case studies\nfurther show that MGC can reproduce and even enhance 16 real-world malware\nsamples. This work provides critical insights for security researchers by\nexposing the risks of compositional attacks against aligned AI systems.\nDemonstrations are available at\nhttps://sites.google.com/view/malware-generation-compiler.",
    "published": "2025-07-02T18:00:49Z",
    "updated": "2025-07-02T18:00:49Z",
    "id": "2507.02057v1",
    "authors": [
      "Lu Yan",
      "Zhuo Zhang",
      "Xiangzhe Xu",
      "Shengwei An",
      "Guangyu Shen",
      "Zhou Xuan",
      "Xuan Chen",
      "Xiangyu Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02057v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02057v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02057v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating malware by exploiting vulnerabilities in their alignment mechanisms. It focuses on the capabilities and limitations of LLMs in the context of security, which is closely related to the broader research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.01949v1": {
    "title": "Kwai Keye-VL Technical Report",
    "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce \\textbf{Kwai Keye-VL}, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the \\textbf{KC-MMBench}, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
    "published": "2025-07-02T17:57:28Z",
    "updated": "2025-07-02T17:57:28Z",
    "id": "2507.01949v1",
    "authors": [
      " Kwai Keye Team",
      "Biao Yang",
      "Bin Wen",
      "Changyi Liu",
      "Chenglong Chu",
      "Chengru Song",
      "Chongling Rao",
      "Chuan Yi",
      "Da Li",
      "Dunju Zang",
      "Fan Yang",
      "Guorui Zhou",
      "Hao Peng",
      "Haojie Ding",
      "Jiaming Huang",
      "Jiangxia Cao",
      "Jiankang Chen",
      "Jingyun Hua",
      "Jin Ouyang",
      "Kaibing Chen",
      "Kaiyu Jiang",
      "Kaiyu Tang",
      "Kun Gai",
      "Shengnan Zhang",
      "Siyang Mao",
      "Sui Huang",
      "Tianke Zhang",
      "Tingting Gao",
      "Wei Chen",
      "Wei Yuan",
      "Xiangyu Wu",
      "Xiao Hu",
      "Xingyu Lu",
      "Yang Zhou",
      "Yi-Fan Zhang",
      "Yiping Yang",
      "Yulong Chen",
      "Zhenhua Wu",
      "Zhenyu Li",
      "Zhixin Ling",
      "Ziming Li",
      "Dehua Ma",
      "Di Xu",
      "Haixuan Gao",
      "Hang Li",
      "Jiawei Guo",
      "Jing Wang",
      "Lejian Ren",
      "Muhao Wei",
      "Qianqian Wang",
      "Qigen Hu",
      "Shiyao Wang",
      "Tao Yu",
      "Xinchen Luo",
      "Yan Li",
      "Yiming Liang",
      "Yuhang Hu",
      "Zeyi Lu",
      "Zhuoran Yang",
      "Zixing Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01949v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01949v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01949v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a Multimodal Large Language Model (MLLM) designed for short-video understanding, incorporating innovative training methods and a new benchmark. The core topics include multimodal models, pretraining strategies, and a new benchmark for evaluation.",
    "llm_cls_result": [
      "MLLM",
      "Pretrain",
      "Benchmark"
    ]
  },
  "2507.01939v2": {
    "title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars",
    "summary": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.",
    "published": "2025-07-02T17:49:52Z",
    "updated": "2025-07-23T17:47:04Z",
    "id": "2507.01939v2",
    "authors": [
      "Xiaosheng Zhao",
      "Yang Huang",
      "Guirong Xue",
      "Xiao Kong",
      "Jifeng Liu",
      "Xiaoyu Tang",
      "Timothy C. Beers",
      "Yuan-Sen Ting",
      "A-Li Luo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01939v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01939v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01939v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of LLM-inspired methodologies to stellar spectral analysis, focusing on pre-training and contrastive alignment techniques similar to those used in LLMs. However, the core focus is on spectroscopy and not directly on LLMs or their architectures.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.01936v2": {
    "title": "The Thin Line Between Comprehension and Persuasion in LLMs",
    "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.",
    "published": "2025-07-02T17:46:56Z",
    "updated": "2025-07-10T14:54:09Z",
    "id": "2507.01936v2",
    "authors": [
      "Adrian de Wynter",
      "Tangming Yuan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01936v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01936v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01936v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the capabilities and limitations of LLMs in maintaining persuasive dialogues and their comprehension of dialogical structures, which aligns with the topics of LLM research and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.01930v3": {
    "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic\n  Observations",
    "summary": "Recent advances in large Language Models (LLMs) have revolutionized mobile\nrobots, including unmanned aerial vehicles (UAVs), enabling their intelligent\noperation within Internet of Things (IoT) ecosystems. However, LLMs still face\nchallenges from logical reasoning and complex decision-making, leading to\nconcerns about the reliability of LLM-driven UAV operations in IoT\napplications. In this paper, we propose a LLM-driven closed-loop control\nframework that enables reliable UAV operations powered by effective feedback\nand refinement using two LLM modules, i.e., a Code Generator and an Evaluator.\nOur framework transforms numerical state observations from UAV operations into\nnatural language trajectory descriptions to enhance the evaluator LLM's\nunderstanding of UAV dynamics for precise feedback generation. Our framework\nalso enables a simulation-based refinement process, and hence eliminates the\nrisks to physical UAVs caused by incorrect code execution during the\nrefinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.",
    "published": "2025-07-02T17:44:17Z",
    "updated": "2025-07-13T02:15:41Z",
    "id": "2507.01930v3",
    "authors": [
      "Wenhao Wang",
      "Yanyan Li",
      "Long Jiao",
      "Jiawei Yuan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01930v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01930v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01930v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the context of UAV operations, focusing on logical reasoning and decision-making challenges. It introduces a closed-loop control framework involving LLMs for reliable UAV operations, which aligns with the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.01923v2": {
    "title": "Decision-Oriented Text Evaluation",
    "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.",
    "published": "2025-07-02T17:32:35Z",
    "updated": "2025-07-03T06:29:26Z",
    "id": "2507.01923v2",
    "authors": [
      "Yu-Shiang Huang",
      "Chuan-Ju Wang",
      "Chung-Chi Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01923v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01923v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01923v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating generated text's influence on decision-making by both humans and LLMs, highlighting the collaborative performance of human-LLM teams. This directly relates to LLM research and their application in decision-making scenarios, as well as the broader context of artificial general intelligence where human-AI collaboration is a key theme.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.01908v1": {
    "title": "Reasoning to Edit: Hypothetical Instruction-Based Image Editing with\n  Visual Reasoning",
    "summary": "Instruction-based image editing (IIE) has advanced rapidly with the success\nof diffusion models. However, existing efforts primarily focus on simple and\nexplicit instructions to execute editing operations such as adding, deleting,\nmoving, or swapping objects. They struggle to handle more complex implicit\nhypothetical instructions that require deeper reasoning to infer plausible\nvisual changes and user intent. Additionally, current datasets provide limited\nsupport for training and evaluating reasoning-aware editing capabilities.\nArchitecturally, these methods also lack mechanisms for fine-grained detail\nextraction that support such reasoning. To address these limitations, we\npropose Reason50K, a large-scale dataset specifically curated for training and\nevaluating hypothetical instruction reasoning image editing, along with\nReasonBrain, a novel framework designed to reason over and execute implicit\nhypothetical instructions across diverse scenarios. Reason50K includes over 50K\nsamples spanning four key reasoning scenarios: Physical, Temporal, Causal, and\nStory reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)\nfor editing guidance generation and a diffusion model for image synthesis,\nincorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture\ndetailed visual and textual semantics essential for supporting instruction\nreasoning. To mitigate the semantic loss, we further introduce a Cross-Modal\nEnhancer (CME) that enables rich interactions between the fine-grained cues and\nMLLM-derived features. Extensive experiments demonstrate that ReasonBrain\nconsistently outperforms state-of-the-art baselines on reasoning scenarios\nwhile exhibiting strong zero-shot generalization to conventional IIE tasks. Our\ndataset and code will be released publicly.",
    "published": "2025-07-02T17:22:21Z",
    "updated": "2025-07-02T17:22:21Z",
    "id": "2507.01908v1",
    "authors": [
      "Qingdong He",
      "Xueqin Chen",
      "Chaoyi Wang",
      "Yanjie Pan",
      "Xiaobin Hu",
      "Zhenye Gan",
      "Yabiao Wang",
      "Chengjie Wang",
      "Xiangtai Li",
      "Jiangning Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01908v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01908v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01908v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on instruction-based image editing with visual reasoning, leveraging Multimodal Large Language Models (MLLMs) and a novel framework for reasoning over implicit instructions. The core topics are MLLM for multimodal integration and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.01903v1": {
    "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research",
    "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.",
    "published": "2025-07-02T17:19:20Z",
    "updated": "2025-07-02T17:19:20Z",
    "id": "2507.01903v1",
    "authors": [
      "Qiguang Chen",
      "Mingda Yang",
      "Libo Qin",
      "Jinhao Liu",
      "Zheng Yan",
      "Jiannan Guan",
      "Dengyun Peng",
      "Yiyan Ji",
      "Hanjing Li",
      "Mengkang Hu",
      "Yimeng Zhang",
      "Yihao Liang",
      "Yuhang Zhou",
      "Jiaqi Wang",
      "Zhi Chen",
      "Wanxiang Che"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01903v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01903v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01903v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of AI, particularly large language models (LLMs), in scientific research, highlighting their capabilities in logical reasoning and experimental coding. It also provides a survey and taxonomy for AI in research, which aligns with the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.01900v1": {
    "title": "High-Layer Attention Pruning with Rescaling",
    "summary": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.",
    "published": "2025-07-02T17:15:05Z",
    "updated": "2025-07-02T17:15:05Z",
    "id": "2507.01900v1",
    "authors": [
      "Songtao Liu",
      "Peng Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01900v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01900v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01900v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses pruning techniques for large language models (LLMs), which is a method to compress and optimize LLMs. It specifically focuses on attention pruning and rescaling, which are relevant to the architecture and efficiency of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.01887v1": {
    "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher\n  Assistants",
    "summary": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.",
    "published": "2025-07-02T16:57:01Z",
    "updated": "2025-07-02T16:57:01Z",
    "id": "2507.01887v1",
    "authors": [
      "Dongyi Ding",
      "Tiannan Wang",
      "Chenghao Zhu",
      "Meiling Tao",
      "Yuchen Eleanor Jiang",
      "Wangchunshu Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01887v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01887v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01887v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the reasoning capabilities of small language models (SLMs) through distillation techniques involving intermediate-sized models and CoT (Chain of Thought) sequences. This aligns with the topics of Reasoning (as it involves CoT reasoning) and LLM (as it discusses large and small language models).",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2507.01872v1": {
    "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System",
    "summary": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.",
    "published": "2025-07-02T16:38:51Z",
    "updated": "2025-07-02T16:38:51Z",
    "id": "2507.01872v1",
    "authors": [
      "Kenan Tang",
      "Yanhong Li",
      "Yao Qin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01872v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01872v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01872v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a language learning system, focusing on vocabulary expansion and quiz generation, which aligns with the LLM topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.01857v1": {
    "title": "TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation\n  Types",
    "summary": "Dexterous teleoperation plays a crucial role in robotic manipulation for\nreal-world data collection and remote robot control. Previous dexterous\nteleoperation mostly relies on hand retargeting to closely mimic human hand\npostures. However, these approaches may fail to fully leverage the inherent\ndexterity of dexterous hands, which can execute unique actions through their\nstructural advantages compared to human hands. To address this limitation, we\npropose TypeTele, a type-guided dexterous teleoperation system, which enables\ndexterous hands to perform actions that are not constrained by human motion\npatterns. This is achieved by introducing dexterous manipulation types into the\nteleoperation system, allowing operators to employ appropriate types to\ncomplete specific tasks. To support this system, we build an extensible\ndexterous manipulation type library to cover comprehensive dexterous postures\nused in manipulation tasks. During teleoperation, we employ a MLLM\n(Multi-modality Large Language Model)-assisted type retrieval module to\nidentify the most suitable manipulation type based on the specific task and\noperator commands. Extensive experiments of real-world teleoperation and\nimitation learning demonstrate that the incorporation of manipulation types\nsignificantly takes full advantage of the dexterous robot's ability to perform\ndiverse and complex tasks with higher success rates.",
    "published": "2025-07-02T16:16:39Z",
    "updated": "2025-07-02T16:16:39Z",
    "id": "2507.01857v1",
    "authors": [
      "Yuhao Lin",
      "Yi-Lin Wei",
      "Haoran Liao",
      "Mu Lin",
      "Chengyi Xing",
      "Hao Li",
      "Dandan Zhang",
      "Mark Cutkosky",
      "Wei-Shi Zheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01857v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01857v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01857v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a Multi-modality Large Language Model (MLLM) in a teleoperation system for dexterous manipulation, which aligns with the MLLM topic. The focus on leveraging the dexterity of robotic hands and the use of MLLM for task-specific type retrieval are the main points of relevance.",
    "llm_cls_result": [
      "MLLM"
    ]
  },
  "2507.01853v3": {
    "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages",
    "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that address the requirements of linguistically\ndiverse regions, such as India, and go beyond English-centric benchmarks. We\nintroduce EKA-EVAL, a unified evaluation framework that integrates over 35+\nbenchmarks (including 10 Indic benchmarks) across nine major evaluation\ncategories. The framework provides broader coverage than existing Indian\nlanguage evaluation tools, offering 11 core capabilities through a modular\narchitecture, seamless integration with Hugging Face and proprietary models,\nand plug-and-play usability. As the first end-to-end suite for scalable,\nmultilingual LLM benchmarking, the framework combines extensive benchmarks,\nmodular workflows, and dedicated support for low-resource Indian languages to\nenable inclusive assessment of LLM capabilities across diverse domains. We\nconducted extensive comparisons against five existing baselines, demonstrating\nthat EKA-EVAL achieves the highest participant ratings in four out of five\ncategories. The framework is open-source and publicly available at:\nhttps://github.com/lingo-iitgn/eka-eval.",
    "published": "2025-07-02T16:07:54Z",
    "updated": "2025-07-12T05:20:11Z",
    "id": "2507.01853v3",
    "authors": [
      "Samridhi Raj Sinha",
      "Rajvee Sheth",
      "Abhishek Upperwal",
      "Mayank Singh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01853v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01853v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01853v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a comprehensive evaluation framework for Large Language Models (LLMs) specifically designed for Indian languages, which includes benchmarking and dataset integration. It focuses on evaluating LLMs across diverse linguistic domains, making it relevant to the 'Benchmark' and 'Dataset' categories.",
    "llm_cls_result": [
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.03020v2": {
    "title": "AI Literacy and LLM Engagement in Higher Education: A Cross-National\n  Quantitative Study",
    "summary": "This study presents a cross-national quantitative analysis of how university\nstudents in the United States and Bangladesh interact with Large Language\nModels (LLMs). Based on an online survey of 318 students, results show that\nLLMs enhance access to information, improve writing, and boost academic\nperformance. However, concerns about overreliance, ethical risks, and critical\nthinking persist. Guided by the AI Literacy Framework, Expectancy-Value Theory,\nand Biggs' 3P Model, the study finds that motivational beliefs and technical\ncompetencies shape LLM engagement. Significant correlations were found between\nLLM use and perceived literacy benefits (r = .59, p < .001) and optimism (r =\n.41, p < .001). ANOVA results showed more frequent use among U.S. students (F =\n7.92, p = .005) and STEM majors (F = 18.11, p < .001). Findings support the\ndevelopment of ethical, inclusive, and pedagogically sound frameworks for\nintegrating LLMs in higher education.",
    "published": "2025-07-02T15:59:53Z",
    "updated": "2025-07-08T05:25:51Z",
    "id": "2507.03020v2",
    "authors": [
      "Shahin Hossain",
      "Shapla Khanam",
      "Samaa Haniya",
      "Nesma Ragab Nasr"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03020v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03020v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03020v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the interaction of university students with Large Language Models (LLMs) and their impact on academic performance, which directly relates to the topic of LLMs.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.01844v1": {
    "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them",
    "summary": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.",
    "published": "2025-07-02T15:58:51Z",
    "updated": "2025-07-02T15:58:51Z",
    "id": "2507.01844v1",
    "authors": [
      "Arthur Wuhrmann",
      "Anastasiia Kucherenko",
      "Andrei Kucharavy"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01844v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01844v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01844v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on analyzing low-perplexity sequences generated by Large Language Models (LLMs) and their relationship with training data, which is directly related to LLM research and understanding their behavior.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2507.10559v2": {
    "title": "NLP Meets the World: Toward Improving Conversations With the Public\n  About Natural Language Processing Research",
    "summary": "Recent developments in large language models (LLMs) have been accompanied by\nrapidly growing public interest in natural language processing (NLP). This\nattention is reflected by major news venues, which sometimes invite NLP\nresearchers to share their knowledge and views with a wide audience.\nRecognizing the opportunities of the present, for both the research field and\nfor individual researchers, this paper shares recommendations for communicating\nwith a general audience about the capabilities and limitations of NLP. These\nrecommendations cover three themes: vague terminology as an obstacle to public\nunderstanding, unreasonable expectations as obstacles to sustainable growth,\nand ethical failures as obstacles to continued support. Published NLP research\nand popular news coverage are cited to illustrate these themes with examples.\nThe recommendations promote effective, transparent communication with the\ngeneral public about NLP, in order to strengthen public understanding and\nencourage support for research.",
    "published": "2025-07-02T15:50:09Z",
    "updated": "2025-07-16T14:25:07Z",
    "id": "2507.10559v2",
    "authors": [
      "Shomir Wilson"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10559v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10559v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10559v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact and communication of large language models (LLMs) and natural language processing (NLP) research to the public, which is closely related to LLMs and their societal implications.",
    "llm_cls_result": [
      "LLM",
      "Other"
    ]
  },
  "2507.01827v1": {
    "title": "APRMCTS: Improving LLM-based Automated Program Repair with Iterative\n  Tree Search",
    "summary": "Automated Program Repair (APR) attempts to fix software bugs without human\nintervention, which plays a crucial role in software development and\nmaintenance. Recently, with the advances in Large Language Models (LLMs), a\nrapidly increasing number of APR techniques have been proposed with remarkable\nperformance. However, existing LLM-based APR techniques typically adopt\ntrial-and-error strategies, which suffer from two major drawbacks: (1)\ninherently limited patch effectiveness due to local exploration, and (2) low\nsearch efficiency due to redundant exploration. In this paper, we propose\nAPRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS\nincorporates Monte Carlo Tree Search (MCTS) into patch searching by performing\na global evaluation of the explored patches and selecting the most promising\none for subsequent refinement and generation. APRMCTS effectively resolves the\nproblems of falling into local optima and thus helps improve the efficiency of\npatch searching. Our experiments on 835 bugs from Defects4J demonstrate that,\nwhen integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which\noutperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,\nGPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,\nrespectively. More importantly, APRMCTS boasts a significant performance\nadvantage while employing small patch size (16 and 32), notably fewer than the\n500 and 10,000 patches adopted in previous studies. In terms of cost, compared\nto existing state-of-the-art LLM-based APR methods, APRMCTS has time and\nmonetary costs of less than 20% and 50%, respectively. Our extensive study\ndemonstrates that APRMCTS exhibits good effectiveness and efficiency, with\nparticular advantages in addressing complex bugs.",
    "published": "2025-07-02T15:44:12Z",
    "updated": "2025-07-02T15:44:12Z",
    "id": "2507.01827v1",
    "authors": [
      "Haichuan Hu",
      "Congqing He",
      "Hao Zhang",
      "Xiaochen Xie",
      "Quanjun Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01827v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01827v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01827v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving Large Language Models (LLMs) for Automated Program Repair (APR) using iterative tree search, which directly involves LLMs and their application in a specific domain (program repair). The use of Monte Carlo Tree Search (MCTS) also hints at a reinforcement learning component, though it's secondary to the LLM focus.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.01806v1": {
    "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework\n  for LLMs",
    "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.",
    "published": "2025-07-02T15:24:47Z",
    "updated": "2025-07-02T15:24:47Z",
    "id": "2507.01806v1",
    "authors": [
      "Reza Arabpour",
      "Haitz Sez de Ocriz Borde",
      "Anastasis Kratsios"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01806v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01806v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01806v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for fine-tuning Large Language Models (LLMs) using Low-Rank Adapters (LoRAs) without GPUs, which is relevant to LLM research and parameter-efficient updates.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.01800v1": {
    "title": "HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing\n  Supervision",
    "summary": "3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the\nphysical world and perform spatial reasoning. Answer-centric supervision is a\ncommonly used training method for 3D VQA models. Many models that utilize this\nstrategy have achieved promising results in 3D VQA tasks. However, the\nanswer-centric approach only supervises the final output of models and allows\nmodels to develop reasoning pathways freely. The absence of supervision on the\nreasoning pathway enables the potential for developing superficial shortcuts\nthrough common patterns in question-answer pairs. Moreover, although\nslow-thinking methods advance large language models, they suffer from\nunderthinking. To address these issues, we propose \\textbf{HCNQA}, a 3D VQA\nmodel leveraging a hierarchical concentration narrowing supervision method. By\nmimicking the human process of gradually focusing from a broad area to specific\nobjects while searching for answers, our method guides the model to perform\nthree phases of concentration narrowing through hierarchical supervision. By\nsupervising key checkpoints on a general reasoning pathway, our method can\nensure the development of a rational and effective reasoning pathway. Extensive\nexperimental results demonstrate that our method can effectively ensure that\nthe model develops a rational reasoning pathway and performs better. The code\nis available at https://github.com/JianuoZhu/HCNQA.",
    "published": "2025-07-02T15:20:08Z",
    "updated": "2025-07-02T15:20:08Z",
    "id": "2507.01800v1",
    "authors": [
      "Shengli Zhou",
      "Jianuo Zhu",
      "Qilin Huang",
      "Fangjing Wang",
      "Yanfu Zhang",
      "Feng Zheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01800v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01800v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01800v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing 3D Visual Question-Answering (3D VQA) with a hierarchical concentration narrowing supervision method, which involves reasoning pathways and supervision strategies. This aligns with topics related to reasoning in models and multimodal large language models.",
    "llm_cls_result": [
      "Reasoning",
      "MLLM"
    ]
  },
  "2507.01785v1": {
    "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large\n  Language Model Pretraining",
    "summary": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.",
    "published": "2025-07-02T15:11:12Z",
    "updated": "2025-07-02T15:11:12Z",
    "id": "2507.01785v1",
    "authors": [
      "Zhixun Chen",
      "Ping Guo",
      "Wenhan Han",
      "Yifan Zhang",
      "Binbin Liu",
      "Haobin Lin",
      "Fengze Liu",
      "Yan Zhao",
      "Bingni Zhang",
      "Taifeng Wang",
      "Yin Zheng",
      "Meng Fang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01785v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01785v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01785v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on data quality selection for multilingual large language model pretraining, which involves pretraining strategies and multilingual aspects.",
    "llm_cls_result": [
      "Pretrain",
      "MLLM"
    ]
  },
  "2507.03019v1": {
    "title": "Look-Back: Implicit Visual Re-focusing in MLLM Reasoning",
    "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in\nmultimodal reasoning. However, they often excessively rely on textual\ninformation during the later stages of inference, neglecting the crucial\nintegration of visual input. Current methods typically address this by\nexplicitly injecting visual information to guide the reasoning process. In this\nwork, through an analysis of MLLM attention patterns, we made an intriguing\nobservation: with appropriate guidance, MLLMs can spontaneously re-focus their\nattention on visual inputs during the later stages of reasoning, even without\nexplicit visual information injection. This spontaneous shift in focus suggests\nthat MLLMs are intrinsically capable of performing visual fusion reasoning.\nBuilding on this insight, we introduce Look-Back, an implicit approach designed\nto guide MLLMs to ``look back\" at visual information in a self-directed manner\nduring reasoning. Look-Back empowers the model to autonomously determine when,\nwhere, and how to re-focus on visual inputs, eliminating the need for explicit\nmodel-structure constraints or additional input. We demonstrate that Look-Back\nsignificantly enhances the model's reasoning and perception capabilities, as\nevidenced by extensive empirical evaluations on multiple multimodal benchmarks.",
    "published": "2025-07-02T14:59:35Z",
    "updated": "2025-07-02T14:59:35Z",
    "id": "2507.03019v1",
    "authors": [
      "Shuo Yang",
      "Yuwei Niu",
      "Yuyang Liu",
      "Yang Ye",
      "Bin Lin",
      "Li Yuan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03019v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03019v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03019v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the reasoning capabilities of Multimodal Large Language Models (MLLMs) and introduces a method to enhance their visual integration during reasoning. This aligns with the topics of MLLM (Multimodal Large Language Models) and Reasoning (LLM reasoning abilities).",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.01756v2": {
    "title": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous\n  Autoregressive Image Synthesis",
    "summary": "Recent advances in large language models (LLMs) have spurred interests in\nencoding images as discrete tokens and leveraging autoregressive (AR)\nframeworks for visual generation. However, the quantization process in AR-based\nvisual generation models inherently introduces information loss that degrades\nimage fidelity. To mitigate this limitation, recent studies have explored to\nautoregressively predict continuous tokens. Unlike discrete tokens that reside\nin a structured and bounded space, continuous representations exist in an\nunbounded, high-dimensional space, making density estimation more challenging\nand increasing the risk of generating out-of-distribution artifacts. Based on\nthe above findings, this work introduces DisCon (Discrete-Conditioned\nContinuous Autoregressive Model), a novel framework that reinterprets discrete\ntokens as conditional signals rather than generation targets. By modeling the\nconditional probability of continuous representations conditioned on discrete\ntokens, DisCon circumvents the optimization challenges of continuous token\nmodeling while avoiding the information loss caused by quantization. DisCon\nachieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation,\noutperforming state-of-the-art autoregressive approaches by a clear margin.\nProject page: https://pengzheng0707.github.io/DisCon.",
    "published": "2025-07-02T14:33:52Z",
    "updated": "2025-07-22T02:25:49Z",
    "id": "2507.01756v2",
    "authors": [
      "Peng Zheng",
      "Junke Wang",
      "Yi Chang",
      "Yizhou Yu",
      "Rui Ma",
      "Zuxuan Wu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01756v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01756v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01756v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for image generation, focusing on the challenges of discrete and continuous token representations. It introduces a novel framework that leverages discrete tokens as conditional signals for continuous autoregressive image synthesis, which is relevant to both LLM and multimodal research.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.01752v1": {
    "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for\n  LLM Post-Training",
    "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.",
    "published": "2025-07-02T14:29:30Z",
    "updated": "2025-07-02T14:29:30Z",
    "id": "2507.01752v1",
    "authors": [
      "Ismail Labiad",
      "Mathurin Videau",
      "Matthieu Kowalski",
      "Marc Schoenauer",
      "Alessandro Leite",
      "Julia Kempe",
      "Olivier Teytaud"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01752v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01752v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01752v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a black-box optimization method for post-training of large language models (LLMs), focusing on privacy, generalization, and robustness. It mentions LLMs and reasoning datasets, but does not directly align with the provided topics like RL, MLLM, VLA, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.01734v1": {
    "title": "LLMs for Legal Subsumption in German Employment Contracts",
    "summary": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.",
    "published": "2025-07-02T14:07:54Z",
    "updated": "2025-07-02T14:07:54Z",
    "id": "2507.01734v1",
    "authors": [
      "Oliver Wardas",
      "Florian Matthes"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01734v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01734v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01734v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in legal contexts, specifically for evaluating the legality of clauses in German employment contracts. It discusses the use of LLMs and in-context learning, which aligns with the 'LLM' topic. The study also involves classification tasks and performance evaluation, which are relevant to 'Benchmark'.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.01728v1": {
    "title": "Token Communication in the Era of Large Models: An Information\n  Bottleneck-Based Approach",
    "summary": "This letter proposes UniToCom, a unified token communication paradigm that\ntreats tokens as the fundamental units for both processing and wireless\ntransmission. Specifically, to enable efficient token representations, we\npropose a generative information bottleneck (GenIB) principle, which\nfacilitates the learning of tokens that preserve essential information while\nsupporting reliable generation across multiple modalities. By doing this,\nGenIB-based tokenization is conducive to improving the communication efficiency\nand reducing computational complexity. Additionally, we develop $\\sigma$-GenIB\nto address the challenges of variance collapse in autoregressive modeling,\nmaintaining representational diversity and stability. Moreover, we employ a\ncausal Transformer-based multimodal large language model (MLLM) at the receiver\nto unify the processing of both discrete and continuous tokens under the\nnext-token prediction paradigm. Simulation results validate the effectiveness\nand superiority of the proposed UniToCom compared to baselines under dynamic\nchannel conditions. By integrating token processing with MLLMs, UniToCom\nenables scalable and generalizable communication in favor of multimodal\nunderstanding and generation, providing a potential solution for\nnext-generation intelligent communications.",
    "published": "2025-07-02T14:03:01Z",
    "updated": "2025-07-02T14:03:01Z",
    "id": "2507.01728v1",
    "authors": [
      "Hao Wei",
      "Wanli Ni",
      "Wen Wang",
      "Wenjun Xu",
      "Dusit Niyato",
      "Ping Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01728v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01728v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01728v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a unified token communication paradigm using a generative information bottleneck principle and employs a causal Transformer-based multimodal large language model (MLLM) for processing tokens. The focus on MLLM and token communication aligns with the topics of Multimodal Large Language Models (MLLM) and Large Language Models (LLM).",
    "llm_cls_result": [
      "MLLM",
      "LLM"
    ]
  },
  "2507.03018v1": {
    "title": "OpenTable-R1: A Reinforcement Learning Augmented Tool Agent for\n  Open-Domain Table Question Answering",
    "summary": "Open-domain table question answering traditionally relies on a two-stage\npipeline: static table retrieval followed by a closed-domain answer. In\ncontrast, we propose an end-to-end agentic framework that embeds multi-turn\ntool calls-using a BM25+-based search API and a SQLite SQL executor-directly\ninto a large language model. To further adapt a compact 4B-parameter model, we\nintroduce a two-stage fine-tuning process: supervised cold-start on easy\nquestions, then Async GRPO reinforcement learning on harder cases with LoRA\nadapters and a rollout buffer. This unified approach enables the model to\njointly retrieve, reason, and execute queries, yielding a dramatic accuracy\nimprovement from single-digit zero-shot performance to over 0.86 exact match on\na held-out test set. Our results underscore the effectiveness of integrating\nstructured tool calls with targeted RL fine-tuning for scalable, accurate table\nQA. The code is available at https://github.com/TabibitoQZP/OpenTableR1.",
    "published": "2025-07-02T13:54:54Z",
    "updated": "2025-07-02T13:54:54Z",
    "id": "2507.03018v1",
    "authors": [
      "Zipeng Qiu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03018v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03018v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03018v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of reinforcement learning (RL) to enhance a large language model (LLM) for table question answering, which involves integrating tool calls and fine-tuning with RL methods like Async GRPO and LoRA adapters. This aligns with the topics of Reinforcement Learning (RL) and Large Language Models (LLM).",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.01717v1": {
    "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using\n  Agentic AI",
    "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.",
    "published": "2025-07-02T13:47:17Z",
    "updated": "2025-07-02T13:47:17Z",
    "id": "2507.01717v1",
    "authors": [
      "Gopichand Kanumolu",
      "Ashok Urlana",
      "Charaka Vinayak Kumar",
      "Bala Mallikarjunarao Garlapati"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01717v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01717v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01717v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) and autonomous agents for generating product ideas from patents, which aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning, including agentic workflows). The focus on enhancing innovation through agentic workflows also touches on AGI (Artificial General Intelligence) due to the broader implications of autonomous agents in problem-solving.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.01702v1": {
    "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large\n  Language Models on Harmfulness",
    "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.",
    "published": "2025-07-02T13:32:30Z",
    "updated": "2025-07-02T13:32:30Z",
    "id": "2507.01702v1",
    "authors": [
      "Zixin Chen",
      "Hongzhan Lin",
      "Kaixin Li",
      "Ziyang Luo",
      "Zhen Ye",
      "Guang Chen",
      "Zhiyong Huang",
      "Jing Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01702v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01702v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01702v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the reasoning capabilities of Multimodal Large Language Models (mLLMs) in understanding harmful memes, which involves multimodal understanding and reasoning. The proposed framework, AdamMeme, adaptively probes these models, aligning with the topics of Multimodal Large Language Models (MLLM) and Reasoning.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.01694v1": {
    "title": "Graph Representation-based Model Poisoning on Federated LLMs in\n  CyberEdge Networks",
    "summary": "Federated large language models (FedLLMs) provide powerful generative\ncapabilities in CyberEdge networks while protecting data privacy. However,\nFedLLMs remains highly vulnerable to model poisoning attacks. This article\nfirst reviews recent model poisoning techniques and existing defense mechanisms\nfor FedLLMs, highlighting critical limitations, particularly under non-IID text\ndistributions. In particular, current defenses primarily utilize distance-based\noutlier detection or norm constraints, operating under the assumption that\nadversarial updates significantly diverge from benign statistics. This\nassumption can fail when facing adaptive attackers targeting billionparameter\nLLMs. Next, this article investigates emerging Graph Representation-Based Model\nPoisoning (GRMP), a novel attack paradigm that leverages higher-order\ncorrelations among honest client gradients to synthesize malicious updates\nindistinguishable from legitimate model updates. GRMP can effectively evade\nadvanced defenses, resulting in substantial accuracy loss and performance\ndegradation. Moreover, this article outlines a research roadmap emphasizing the\nimportance of graph-aware secure aggregation methods, FedLLMs-specific\nvulnerability metrics, and evaluation frameworks to strengthen the robustness\nof future federated language model deployments.",
    "published": "2025-07-02T13:20:52Z",
    "updated": "2025-07-02T13:20:52Z",
    "id": "2507.01694v1",
    "authors": [
      "Hanlin Cai",
      "Haofan Dong",
      "Houtianfu Wang",
      "Kai Li",
      "Ozgur B. Akan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01694v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01694v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01694v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses federated large language models (FedLLMs) and their vulnerabilities to model poisoning attacks, particularly focusing on a novel attack paradigm called Graph Representation-Based Model Poisoning (GRMP). The core topics revolve around LLMs and their security aspects in federated settings.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.01693v1": {
    "title": "GPT, But Backwards: Exactly Inverting Language Model Outputs",
    "summary": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879.",
    "published": "2025-07-02T13:20:30Z",
    "updated": "2025-07-02T13:20:30Z",
    "id": "2507.01693v1",
    "authors": [
      "Adrians Skapars",
      "Edoardo Manino",
      "Youcheng Sun",
      "Lucas C. Cordeiro"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01693v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01693v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01693v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on reconstructing inputs from LLM outputs, which involves analyzing and understanding the behavior of large language models (LLMs). The core topic is related to LLMs and their forensic analysis.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.01679v1": {
    "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling",
    "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.",
    "published": "2025-07-02T13:04:09Z",
    "updated": "2025-07-02T13:04:09Z",
    "id": "2507.01679v1",
    "authors": [
      "Zeyu Huang",
      "Tianhao Cheng",
      "Zihan Qiu",
      "Zili Wang",
      "Yinghui Xu",
      "Edoardo M. Ponti",
      "Ivan Titov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01679v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01679v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01679v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a hybrid approach combining Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) for large language models, which falls under the topics of Reinforcement Learning (RL) and LLM fine-tuning strategies.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.01663v1": {
    "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training",
    "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.",
    "published": "2025-07-02T12:45:34Z",
    "updated": "2025-07-02T12:45:34Z",
    "id": "2507.01663v1",
    "authors": [
      "Zhenyu Han",
      "Ansheng You",
      "Haibo Wang",
      "Kui Luo",
      "Guang Yang",
      "Wenqi Shi",
      "Menglong Chen",
      "Sicheng Zhang",
      "Zeshun Lan",
      "Chunshi Deng",
      "Huazhong Ji",
      "Wenjie Liu",
      "Yu Huang",
      "Yixiang Zhang",
      "Chenyi Pan",
      "Jing Wang",
      "Xin Huang",
      "Chunsheng Li",
      "Jianping Wu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01663v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01663v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01663v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a framework for reinforcement learning (RL) in the context of large language models (LLMs), focusing on efficiency and scalability in post-training. It directly mentions RL and LLMs, aligning with the topics 'RL' and 'LLM'.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.03014v1": {
    "title": "Intrinsic Fingerprint of LLMs: Continue Training is NOT All You Need to\n  Steal A Model!",
    "summary": "Large language models (LLMs) face significant copyright and intellectual\nproperty challenges as the cost of training increases and model reuse becomes\nprevalent. While watermarking techniques have been proposed to protect model\nownership, they may not be robust to continue training and development, posing\nserious threats to model attribution and copyright protection. This work\nintroduces a simple yet effective approach for robust LLM fingerprinting based\non intrinsic model characteristics. We discover that the standard deviation\ndistributions of attention parameter matrices across different layers exhibit\ndistinctive patterns that remain stable even after extensive continued\ntraining. These parameter distribution signatures serve as robust fingerprints\nthat can reliably identify model lineage and detect potential copyright\ninfringement. Our experimental validation across multiple model families\ndemonstrates the effectiveness of our method for model authentication. Notably,\nour investigation uncovers evidence that a recently Pangu Pro MoE model\nreleased by Huawei is derived from Qwen-2.5 14B model through upcycling\ntechniques rather than training from scratch, highlighting potential cases of\nmodel plagiarism, copyright violation, and information fabrication. These\nfindings underscore the critical importance of developing robust fingerprinting\nmethods for protecting intellectual property in large-scale model development\nand emphasize that deliberate continued training alone is insufficient to\ncompletely obscure model origins.",
    "published": "2025-07-02T12:29:38Z",
    "updated": "2025-07-02T12:29:38Z",
    "id": "2507.03014v1",
    "authors": [
      "Do-hyeon Yoon",
      "Minsoo Chun",
      "Thomas Allen",
      "Hans Mller",
      "Min Wang",
      "Rajesh Sharma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03014v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03014v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03014v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the fingerprinting of Large Language Models (LLMs) to protect intellectual property, which is directly related to LLM research and their architectures. It also touches on the robustness of these fingerprints against continued training, which is a concern in the scaling and development of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.01643v1": {
    "title": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via\n  Gradual Feature Refinement",
    "summary": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent.",
    "published": "2025-07-02T12:17:23Z",
    "updated": "2025-07-02T12:17:23Z",
    "id": "2507.01643v1",
    "authors": [
      "Weijie Yin",
      "Dingkang Yang",
      "Hongyuan Dong",
      "Zijian Kang",
      "Jiacong Wang",
      "Xiao Liang",
      "Chao Feng",
      "Jiao Ran"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01643v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01643v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01643v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving Vision Transformers (ViTs) for Multimodal Large Language Models (MLLMs) through gradual feature refinement, which directly relates to the development and enhancement of MLLMs and their visual comprehension capabilities.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Pretrain"
    ]
  },
  "2507.01628v1": {
    "title": "DaiFu: In-Situ Crash Recovery for Deep Learning Systems",
    "summary": "Deep learning (DL) systems have been widely adopted in many areas, and are\nbecoming even more popular with the emergence of large language models.\nHowever, due to the complex software stacks involved in their development and\nexecution, crashes are unavoidable and common. Crashes severely waste computing\nresources and hinder development productivity, so efficient crash recovery is\ncrucial. Existing solutions, such as checkpoint-retry, are too heavyweight for\nfast recovery from crashes caused by minor programming errors or transient\nruntime errors. Therefore, we present DaiFu, an in-situ recovery framework for\nDL systems. Through a lightweight code transformation to a given DL system,\nDaiFu augments it to intercept crashes in situ and enables dynamic and instant\nupdates to its program running context (e.g., code, configurations, and other\ndata) for agile crash recovery. Our evaluation shows that DaiFu helps reduce\nthe restore time for crash recovery, achieving a 1372x speedup compared with\nstate-of-the-art solutions. Meanwhile, the overhead of DaiFu is negligible\n(under 0.40%). We also construct a benchmark spanning 7 distinct crash\nscenarios in DL systems, and show the effectiveness of DaiFu in diverse\nsituations.",
    "published": "2025-07-02T11:58:38Z",
    "updated": "2025-07-02T11:58:38Z",
    "id": "2507.01628v1",
    "authors": [
      "Zilong He",
      "Pengfei Chen",
      "Hongyu Zhang",
      "Xiaoyun Li",
      "Guangba Yu",
      "Hongyang Chen",
      "Zibin Zheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01628v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01628v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01628v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on crash recovery in deep learning systems, which is a general topic related to deep learning system reliability and efficiency, but does not specifically align with the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.01627v1": {
    "title": "Chart Question Answering from Real-World Analytical Narratives",
    "summary": "We present a new dataset for chart question answering (CQA) constructed from\nvisualization notebooks. The dataset features real-world, multi-view charts\npaired with natural language questions grounded in analytical narratives.\nUnlike prior benchmarks, our data reflects ecologically valid reasoning\nworkflows. Benchmarking state-of-the-art multimodal large language models\nreveals a significant performance gap, with GPT-4.1 achieving an accuracy of\n69.3%, underscoring the challenges posed by this more authentic CQA setting.",
    "published": "2025-07-02T11:58:04Z",
    "updated": "2025-07-02T11:58:04Z",
    "id": "2507.01627v1",
    "authors": [
      "Maeve Hutchinson",
      "Radu Jianu",
      "Aidan Slingsby",
      "Jo Wood",
      "Pranava Madhyastha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01627v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01627v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01627v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a dataset for chart question answering (CQA) using real-world analytical narratives and evaluates multimodal large language models (MLLMs) on this dataset. The core topics are related to multimodal large language models and benchmarking their performance.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.01599v1": {
    "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems",
    "summary": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems.",
    "published": "2025-07-02T11:04:49Z",
    "updated": "2025-07-02T11:04:49Z",
    "id": "2507.01599v1",
    "authors": [
      "Zhaoyan Sun",
      "Jiayi Wang",
      "Xinyang Zhao",
      "Jiachi Wang",
      "Guoliang Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01599v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01599v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01599v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) to enhance semantic understanding, reasoning, and planning in Data+AI ecosystems, which aligns with the topics of LLM and Reasoning. Additionally, the mention of multi-modal data analytics agents suggests relevance to MLLM.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "MLLM"
    ]
  },
  "2507.01594v1": {
    "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture,\n  Representation, and Optimisation",
    "summary": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.",
    "published": "2025-07-02T11:00:33Z",
    "updated": "2025-07-02T11:00:33Z",
    "id": "2507.01594v1",
    "authors": [
      "Shutong Feng",
      "Hsien-chin Lin",
      "Nurul Lubis",
      "Carel van Niekerk",
      "Michael Heck",
      "Benjamin Ruppik",
      "Renato Vukovic",
      "Milica Gai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01594v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01594v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01594v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) and reinforcement learning (RL) in task-oriented dialogue systems, focusing on emotional intelligence and task success. This aligns with the topics of LLM and RL.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.03009v2": {
    "title": "PDFMathTranslate: Scientific Document Translation Preserving Layouts",
    "summary": "Language barriers in scientific documents hinder the diffusion and\ndevelopment of science and technologies. However, prior efforts in translating\nsuch documents largely overlooked the information in layouts. To bridge the\ngap, we introduce PDFMathTranslate, the world's first open-source software for\ntranslating scientific documents while preserving layouts. Leveraging the most\nrecent advances in large language models and precise layout detection, we\ncontribute to the community with key improvements in precision, flexibility,\nand efficiency. The work has been open-sourced at\nhttps://github.com/byaidu/pdfmathtranslate with more than 222k downloads.",
    "published": "2025-07-02T10:22:05Z",
    "updated": "2025-07-08T09:51:21Z",
    "id": "2507.03009v2",
    "authors": [
      "Rongxin Ouyang",
      "Chang Chu",
      "Zhikuang Xin",
      "Xiangyao Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03009v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03009v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03009v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on translating scientific documents while preserving layouts using large language models, but it does not directly contribute to the core topics listed. The primary focus is on document translation and layout preservation rather than LLM research, multimodal models, or other specified categories.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.01551v2": {
    "title": "Self-Guided Process Reward Optimization with Redefined Step-wise\n  Advantage for Process Reinforcement Learning",
    "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.",
    "published": "2025-07-02T10:05:14Z",
    "updated": "2025-07-03T10:33:08Z",
    "id": "2507.01551v2",
    "authors": [
      "Wu Fei",
      "Hao Kong",
      "Shuxian Liang",
      "Yang Lin",
      "Yibo Yang",
      "Jing Tang",
      "Lei Chen",
      "Xiansheng Hua"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01551v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01551v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01551v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the reasoning capabilities of Large Language Models (LLMs) through Process Reinforcement Learning (PRL) and introduces a novel framework (SPRO) that optimizes process rewards and step-wise advantage estimation. The core topics are related to Reinforcement Learning (RL) and Reasoning in LLMs.",
    "llm_cls_result": [
      "RL",
      "Reasoning"
    ]
  },
  "2507.01548v2": {
    "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for\n  Elderly Migrants",
    "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.",
    "published": "2025-07-02T10:00:12Z",
    "updated": "2025-07-03T08:45:46Z",
    "id": "2507.01548v2",
    "authors": [
      "Wen Zhan",
      "Ziqun Hua",
      "Peiyue Lin",
      "Yunfei Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01548v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01548v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01548v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a Large Language Model (LLM) in assisting elderly migrants to express personal narratives through co-creation, which aligns with the 'LLM' topic. The focus on human-AI collaboration and narrative agency also touches on aspects of 'AGI' as it explores broader implications of AI in human contexts.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.01541v1": {
    "title": "Efficient Out-of-Scope Detection in Dialogue Systems via\n  Uncertainty-Driven LLM Routing",
    "summary": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.",
    "published": "2025-07-02T09:51:41Z",
    "updated": "2025-07-02T09:51:41Z",
    "id": "2507.01541v1",
    "authors": [
      "lvaro Zaera",
      "Diana Nicoleta Popa",
      "Ivan Sekulic",
      "Paolo Rosso"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01541v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01541v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01541v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for out-of-scope intent detection in dialogue systems, which involves uncertainty modeling and fine-tuning of LLMs. This aligns with the topics of LLM (Large Language Models) and Reasoning (as it involves decision-making and problem-solving in dialogue systems).",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.01492v1": {
    "title": "AVC-DPO: Aligned Video Captioning via Direct Preference Optimization",
    "summary": "Although video multimodal large language models (video MLLMs) have achieved\nsubstantial progress in video captioning tasks, it remains challenging to\nadjust the focal emphasis of video captions according to human preferences. To\naddress this limitation, we propose Aligned Video Captioning via Direct\nPreference Optimization (AVC-DPO), a post-training framework designed to\nenhance captioning capabilities in video MLLMs through preference alignment.\nOur approach designs enhanced prompts that specifically target temporal\ndynamics and spatial information-two key factors that humans care about when\nwatching a video-thereby incorporating human-centric preferences. AVC-DPO\nleverages the same foundation model's caption generation responses under varied\nprompt conditions to conduct preference-aware training and caption alignment.\nUsing this framework, we have achieved exceptional performance in the\nLOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving\nfirst place on the Video Detailed Captioning (VDC) benchmark according to the\nVDCSCORE evaluation metric.",
    "published": "2025-07-02T08:51:45Z",
    "updated": "2025-07-02T08:51:45Z",
    "id": "2507.01492v1",
    "authors": [
      "Jiyang Tang",
      "Hengyi Li",
      "Yifan Du",
      "Wayne Xin Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01492v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01492v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01492v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing video captioning capabilities in video multimodal large language models (video MLLMs) through preference alignment, which involves human-centric preferences and leverages the foundation model's responses. This aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA) due to its focus on video captioning and human preferences in video understanding.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.01489v1": {
    "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with\n  Reinforcement Learning",
    "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match.",
    "published": "2025-07-02T08:49:43Z",
    "updated": "2025-07-02T08:49:43Z",
    "id": "2507.01489v1",
    "authors": [
      "Yanfei Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01489v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01489v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01489v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of reinforcement learning in LLM-based agents and proposes a hierarchical framework for decision-making, which aligns with the topics of Reinforcement Learning (RL) and Large Language Models (LLM).",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.01485v1": {
    "title": "BioMARS: A Multi-Agent Robotic System for Autonomous Biological\n  Experiments",
    "summary": "Large language models (LLMs) and vision-language models (VLMs) have the\npotential to transform biological research by enabling autonomous\nexperimentation. Yet, their application remains constrained by rigid protocol\ndesign, limited adaptability to dynamic lab conditions, inadequate error\nhandling, and high operational complexity. Here we introduce BioMARS\n(Biological Multi-Agent Robotic System), an intelligent platform that\nintegrates LLMs, VLMs, and modular robotics to autonomously design, plan, and\nexecute biological experiments. BioMARS uses a hierarchical architecture: the\nBiologist Agent synthesizes protocols via retrieval-augmented generation; the\nTechnician Agent translates them into executable robotic pseudo-code; and the\nInspector Agent ensures procedural integrity through multimodal perception and\nanomaly detection. The system autonomously conducts cell passaging and culture\ntasks, matching or exceeding manual performance in viability, consistency, and\nmorphological integrity. It also supports context-aware optimization,\noutperforming conventional strategies in differentiating retinal pigment\nepithelial cells. A web interface enables real-time human-AI collaboration,\nwhile a modular backend allows scalable integration with laboratory hardware.\nThese results highlight the feasibility of generalizable, AI-driven laboratory\nautomation and the transformative role of language-based reasoning in\nbiological research.",
    "published": "2025-07-02T08:47:02Z",
    "updated": "2025-07-02T08:47:02Z",
    "id": "2507.01485v1",
    "authors": [
      "Yibo Qiu",
      "Zan Huang",
      "Zhiyu Wang",
      "Handi Liu",
      "Yiling Qiao",
      "Yifeng Hu",
      "Shu'ang Sun",
      "Hangke Peng",
      "Ronald X Xu",
      "Mingzhai Sun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01485v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01485v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01485v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the integration of LLMs and VLMs with modular robotics for autonomous biological experiments, which involves language-based reasoning and multimodal perception.",
    "llm_cls_result": [
      "LLM",
      "VLA",
      "Reasoning"
    ]
  },
  "2507.01479v1": {
    "title": "Evaluating the Effectiveness of Direct Preference Optimization for\n  Personalizing German Automatic Text Simplifications for Persons with\n  Intellectual Disabilities",
    "summary": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.",
    "published": "2025-07-02T08:43:06Z",
    "updated": "2025-07-02T08:43:06Z",
    "id": "2507.01479v1",
    "authors": [
      "Yingqiang Gao",
      "Kaede Johnson",
      "David Froehlich",
      "Luisa Carrer",
      "Sarah Ebling"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01479v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01479v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01479v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for automatic text simplification and incorporates direct preference optimization (DPO) for personalization, which aligns with topics related to LLMs and reinforcement learning (RL) for model alignment.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.01446v1": {
    "title": "Using multi-agent architecture to mitigate the risk of LLM\n  hallucinations",
    "summary": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks.",
    "published": "2025-07-02T08:06:02Z",
    "updated": "2025-07-02T08:06:02Z",
    "id": "2507.01446v1",
    "authors": [
      "Abd Elrahman Amer",
      "Magdi Amer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01446v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01446v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01446v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a multi-agent system to mitigate hallucinations in LLMs, which involves both the use of LLMs and a system architecture to improve their reliability.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.01444v2": {
    "title": "A Large Language Model for Chemistry and Retrosynthesis Predictions",
    "summary": "Large language models (LLM) have achieved impressive progress across a broad\nrange of general-purpose tasks, but their effectiveness in chemistry remains\nlimited due to scarce domain-specific datasets and the demand for precise\nsymbolic and structural reasoning. Here we introduce ECNU-ChemGPT(name after\nEast China Normal University), a chemistry-specialized LLM engineered for deep\nchemical knowledge understanding and accurate retrosynthetic route planning.\nOur approach is distinguished by four key strategies: structured prompt-based\nknowledge distillation from authoritative chemistry textbooks to construct a\nhigh-quality question-answering dataset; domain-specific prompt engineering\nusing curated chemical keywords, combined with LLMs APIs for data derivation\nand knowledge distillation; large-scale fine-tuning on a meticulously cleaned\nand enriched Pistachio reaction dataset to enhance retrosynthesis prediction\naccuracy; and integration of BrainGPT, a dynamic multi-model scheduling\nframework that enables task-specific invocation of multiple specialized models\ntrained for diverse chemistry-related tasks. ECNU-ChemGPT exhibits superior\nperformance on chemistry question-answering and retrosynthetic planning\nbenchmarks, outperforming leading general-purpose models-including Deepseek-R1,\nQwen-2.5, and GPT-4o. In retrosynthesis, it achieves a Top-1 accuracy of 68.3%\non the USPTO_50K dataset and successfully reconstructed 13 complete\nexperimental pathways for real-world drug molecules from medicinal chemistry\njournals. These results underscore the effectiveness of domain-adapted\nfine-tuning combined with dynamic multi-model task scheduling, providing a\nscalable and robust solution for chemical knowledge question answering and\nretrosynthetic planning.",
    "published": "2025-07-02T08:04:36Z",
    "updated": "2025-07-10T11:15:13Z",
    "id": "2507.01444v2",
    "authors": [
      "Yueqing Zhang",
      "Wentao Liu",
      "Yan Zhang",
      "Danyang Xiong",
      "Jihang Zhai",
      "Hao Hao",
      "YuCheng Gu",
      "HaiBo Yang",
      "Shuanhu Gao",
      "Lianrui Hu",
      "Aimin Zhou",
      "Xiao He"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01444v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01444v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01444v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of a Large Language Model (LLM) in the domain of chemistry, specifically for retrosynthesis predictions and chemical knowledge understanding. It involves domain-specific adaptations and fine-tuning of the LLM, which aligns with the 'LLM' topic. The mention of 'retrosynthetic route planning' and 'chemical knowledge understanding' suggests a specialized application of LLMs, but does not directly fit into other provided categories like RL, MLLM, VLA, etc.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.01438v1": {
    "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
    "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
    "published": "2025-07-02T07:47:28Z",
    "updated": "2025-07-02T07:47:28Z",
    "id": "2507.01438v1",
    "authors": [
      "Zheyu Shen",
      "Yexiao He",
      "Ziyao Wang",
      "Yuning Zhang",
      "Guoheng Sun",
      "Wanghao Ye",
      "Ang Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01438v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01438v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01438v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the efficient deployment and serving of Large Language Models (LLMs) on edge devices, utilizing parameter-efficient adapters like LoRA. It addresses challenges specific to LLM serving systems, such as adapter selection, memory management, and batch processing, which are core to LLM research and deployment.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.01436v2": {
    "title": "Challenges & Opportunities with LLM-Assisted Visualization Retargeting",
    "summary": "Despite the ubiquity of visualization examples published on the web,\nretargeting existing custom chart implementations to new datasets remains\ndifficult, time-intensive, and tedious. The adaptation process assumes author\nfamiliarity with both the implementation of the example as well as how the new\ndataset might need to be transformed to fit into the example code. With recent\nadvances in Large Language Models (LLMs), automatic adaptation of code can be\nachieved from high-level user prompts, reducing the barrier for visualization\nretargeting. To better understand how LLMs can assist retargeting and its\npotential limitations, we characterize and evaluate the performance of LLM\nassistance across multiple datasets and charts of varying complexity,\ncategorizing failures according to type and severity. In our evaluation, we\ncompare two approaches: (1) directly instructing the LLM model to fully\ngenerate and adapt code by treating code as text inputs and (2) a more\nconstrained program synthesis pipeline where the LLM guides the code\nconstruction process by providing structural information (e.g., visual\nencodings) based on properties of the example code and data. We find that both\napproaches struggle when new data has not been appropriately transformed, and\ndiscuss important design recommendations for future retargeting systems.",
    "published": "2025-07-02T07:43:43Z",
    "updated": "2025-07-06T18:15:55Z",
    "id": "2507.01436v2",
    "authors": [
      "Luke S. Snyder",
      "Chenglong Wang",
      "Steven M. Drucker"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01436v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01436v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01436v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for assisting in visualization retargeting, which involves adapting existing custom chart implementations to new datasets. The focus is on the capabilities and limitations of LLMs in this context, which directly relates to the 'LLM' topic. Additionally, the paper evaluates different approaches to using LLMs for this task, which aligns with the 'Benchmark' topic as it involves performance comparison and evaluation.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.01431v2": {
    "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless\n  Handwritten STEM Grading",
    "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.",
    "published": "2025-07-02T07:33:19Z",
    "updated": "2025-07-07T05:10:47Z",
    "id": "2507.01431v2",
    "authors": [
      "Yoonseok Yang",
      "Minjune Kim",
      "Marlon Rondinelli",
      "Keren Shao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01431v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01431v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01431v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in an AI-assisted grading platform for STEM courses, which aligns with the 'LLM' topic. It also involves the application of AI in educational settings, which is not directly covered by the provided topics, hence the inclusion of 'Other'.",
    "llm_cls_result": [
      "LLM",
      "Other"
    ]
  },
  "2507.01413v1": {
    "title": "Evaluating LLM Agent Collusion in Double Auctions",
    "summary": "Large language models (LLMs) have demonstrated impressive capabilities as\nautonomous agents with rapidly expanding applications in various domains. As\nthese agents increasingly engage in socioeconomic interactions, identifying\ntheir potential for undesirable behavior becomes essential. In this work, we\nexamine scenarios where they can choose to collude, defined as secretive\ncooperation that harms another party. To systematically study this, we\ninvestigate the behavior of LLM agents acting as sellers in simulated\ncontinuous double auction markets. Through a series of controlled experiments,\nwe analyze how parameters such as the ability to communicate, choice of model,\nand presence of environmental pressures affect the stability and emergence of\nseller collusion. We find that direct seller communication increases collusive\ntendencies, the propensity to collude varies across models, and environmental\npressures, such as oversight and urgency from authority figures, influence\ncollusive behavior. Our findings highlight important economic and ethical\nconsiderations for the deployment of LLM-based market agents.",
    "published": "2025-07-02T07:06:49Z",
    "updated": "2025-07-02T07:06:49Z",
    "id": "2507.01413v1",
    "authors": [
      "Kushal Agrawal",
      "Verona Teo",
      "Juan J. Vazquez",
      "Sudarsh Kunnavakkam",
      "Vishak Srikanth",
      "Andy Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01413v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01413v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01413v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the behavior of LLM agents in socioeconomic interactions, specifically focusing on their potential for collusion in double auction markets. This aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning) as it involves autonomous agents and their interactions. The study also touches on ethical considerations, which is a broader aspect of AGI (Artificial General Intelligence).",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.03004v1": {
    "title": "CLUES: Collaborative High-Quality Data Selection for LLMs via Training\n  Dynamics",
    "summary": "Recent research has highlighted the importance of data quality in scaling\nlarge language models (LLMs). However, automated data quality control faces\nunique challenges in collaborative settings where sharing is not allowed\ndirectly between data silos. To tackle this issue, this paper proposes a novel\ndata quality control technique based on the notion of data influence on the\ntraining dynamics of LLMs, that high quality data are more likely to have\nsimilar training dynamics to the anchor dataset. We then leverage the influence\nof the training dynamics to select high-quality data from different private\ndomains, with centralized model updates on the server side in a collaborative\ntraining fashion by either model merging or federated learning. As for the data\nquality indicator, we compute the per-sample gradients with respect to the\nprivate data and the anchor dataset, and use the trace of the accumulated inner\nproducts as a measurement of data quality. In addition, we develop a quality\ncontrol evaluation tailored for collaborative settings with heterogeneous\ndomain data. Experiments show that training on the high-quality data selected\nby our method can often outperform other data selection methods for\ncollaborative fine-tuning of LLMs, across diverse private domain datasets, in\nmedical, multilingual and financial settings. Our code is released at\ngithub.com/Ryan0v0/CLUES.",
    "published": "2025-07-02T06:19:40Z",
    "updated": "2025-07-02T06:19:40Z",
    "id": "2507.03004v1",
    "authors": [
      "Wanru Zhao",
      "Hongxiang Fan",
      "Shell Xu Hu",
      "Wangchunshu Zhou",
      "Bofan Chen",
      "Nicholas D. Lane"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03004v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03004v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03004v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on data quality control for LLMs in collaborative settings, which involves training dynamics and data selection methods. This aligns with topics related to LLMs and their training strategies.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.01378v1": {
    "title": "RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms",
    "summary": "Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems.",
    "published": "2025-07-02T05:44:17Z",
    "updated": "2025-07-02T05:44:17Z",
    "id": "2507.01378v1",
    "authors": [
      "Ziyao Wang",
      "Rongpeng Li",
      "Sizhao Li",
      "Yuming Xiang",
      "Haiping Wang",
      "Zhifeng Zhao",
      "Honggang Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01378v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01378v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01378v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in the context of multi-agent systems for UAV swarms, combining LLM-driven semantic decision frameworks with reinforcement learning (RL) techniques. This aligns with the topics of LLM (for the use of large language models), RL (for the integration of reinforcement learning), and AGI (for the application towards more general intelligent systems).",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.01376v1": {
    "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future\n  Manufacturing",
    "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face.",
    "published": "2025-07-02T05:31:17Z",
    "updated": "2025-07-02T05:31:17Z",
    "id": "2507.01376v1",
    "authors": [
      "Yinwang Ren",
      "Yangyang Liu",
      "Tang Ji",
      "Xun Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01376v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01376v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01376v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses AI agents, particularly those based on LLMs and MLLMs, and their applications in manufacturing. It highlights the use of large language models and multimodal large language models in enhancing AI agents' capabilities, which aligns with the topics of LLM and MLLM. Additionally, the focus on autonomous decision-making and reasoning in dynamic environments touches on the RL topic, especially given the mention of Agentic AI and goal-directed autonomy.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "RL"
    ]
  },
  "2507.03003v1": {
    "title": "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt\n  Tuning for Low-Resource Languages",
    "summary": "Pre-trained large language models (LLMs) have become a cornerstone of modern\nnatural language processing, with their capabilities extending across a wide\nrange of applications and languages. However, the fine-tuning of multilingual\nLLMs, especially for low-resource languages, faces significant challenges\narising from data-sharing restrictions (the physical border) and inherent\nlinguistic differences (the linguistic border). These barriers hinder users of\nvarious languages, particularly those in low-resource regions, from fully\nbenefiting from the advantages of LLMs. To address these challenges, we propose\nthe Federated Prompt Tuning Paradigm for multilingual scenarios, which utilizes\nparameter-efficient fine-tuning while adhering to data sharing restrictions. We\ndesign a comprehensive set of experiments and analyze them using a novel notion\nof language distance to highlight the strengths of our paradigm: Even under\ncomputational constraints, our method not only improves data efficiency but\nalso facilitates mutual enhancements across languages, particularly benefiting\nlow-resource ones. Compared to traditional local cross-lingual transfer tuning\nmethods, our approach achieves 6.9\\% higher accuracy with improved data\nefficiency, and demonstrates greater stability and generalization. These\nfindings underscore the potential of our approach to promote social equality\nand champion linguistic diversity, ensuring that no language is left behind.",
    "published": "2025-07-02T05:23:20Z",
    "updated": "2025-07-02T05:23:20Z",
    "id": "2507.03003v1",
    "authors": [
      "Wanru Zhao",
      "Yihong Chen",
      "Royson Lee",
      "Xinchi Qiu",
      "Yan Gao",
      "Hongxiang Fan",
      "Nicholas D. Lane"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03003v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03003v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03003v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the fine-tuning of multilingual large language models (LLMs) for low-resource languages, addressing challenges related to data-sharing restrictions and linguistic differences. It proposes a Federated Prompt Tuning Paradigm, which is a parameter-efficient fine-tuning method. The focus on multilingual LLMs and their fine-tuning aligns with the topics of LLM and Pretrain.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.01368v1": {
    "title": "Activation Reward Models for Few-Shot Model Alignment",
    "summary": "Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to\nhuman preferences is a central challenge in improving the quality of the\nmodels' generative outputs for real-world applications. A common approach is to\nuse reward modeling to encode preferences, enabling alignment via post-training\nusing reinforcement learning. However, traditional reward modeling is not\neasily adaptable to new preferences because it requires a separate reward\nmodel, commonly trained on large preference datasets. To address this, we\nintroduce Activation Reward Models (Activation RMs) -- a novel few-shot reward\nmodeling method that leverages activation steering to construct well-aligned\nreward signals using minimal supervision and no additional model finetuning.\nActivation RMs outperform existing few-shot reward modeling approaches such as\nLLM-as-a-judge with in-context learning, voting-based scoring, and token\nprobability scoring on standard reward modeling benchmarks. Furthermore, we\ndemonstrate the effectiveness of Activation RMs in mitigating reward hacking\nbehaviors, highlighting their utility for safety-critical applications. Toward\nthis end, we propose PreferenceHack, a novel few-shot setting benchmark, the\nfirst to test reward models on reward hacking in a paired preference format.\nFinally, we show that Activation RM achieves state-of-the-art performance on\nthis benchmark, surpassing even GPT-4o.",
    "published": "2025-07-02T05:10:29Z",
    "updated": "2025-07-02T05:10:29Z",
    "id": "2507.01368v1",
    "authors": [
      "Tianning Chai",
      "Chancharik Mitra",
      "Brandon Huang",
      "Gautam Rajendrakumar Gare",
      "Zhiqiu Lin",
      "Assaf Arbelle",
      "Leonid Karlinsky",
      "Rogerio Feris",
      "Trevor Darrell",
      "Deva Ramanan",
      "Roei Herzig"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01368v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01368v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01368v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the alignment of LLMs and LMMs to human preferences using reward modeling and reinforcement learning, which falls under the RL (Reinforcement Learning) topic. It also introduces a novel few-shot reward modeling method and benchmarks, which are relevant to the Benchmark topic.",
    "llm_cls_result": [
      "RL",
      "Benchmark"
    ]
  },
  "2507.01348v2": {
    "title": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and\n  Text to Speech",
    "summary": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments.",
    "published": "2025-07-02T04:30:23Z",
    "updated": "2025-07-08T09:21:24Z",
    "id": "2507.01348v2",
    "authors": [
      "Zhuangfei Cheng",
      "Guangyan Zhang",
      "Zehai Tu",
      "Yangyang Song",
      "Shuiyang Mao",
      "Xiaoqi Jiao",
      "Jingyu Li",
      "Yiwen Guo",
      "Jiasong Wu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01348v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01348v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01348v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the adaptation of LLM-based techniques for foreign accent conversion and text-to-speech tasks, which involves the use of large language models (LLMs) and their application in speech processing. The core topics are related to LLMs and their use in specific tasks, but none of the provided topics directly match the content.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.01334v2": {
    "title": "Symbolic or Numerical? Understanding Physics Problem Solving in\n  Reasoning LLMs",
    "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.",
    "published": "2025-07-02T03:51:16Z",
    "updated": "2025-07-03T13:15:11Z",
    "id": "2507.01334v2",
    "authors": [
      "Nifu Dan",
      "Yujun Cai",
      "Yiwei Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01334v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01334v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01334v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the reasoning abilities of Large Language Models (LLMs) in solving physics problems, which aligns with the 'Reasoning' topic. It also mentions the use of LLMs, fitting the 'LLM' topic. The study involves benchmarking on the SciBench benchmark, which relates to the 'Benchmark' topic.",
    "llm_cls_result": [
      "Reasoning",
      "LLM",
      "Benchmark"
    ]
  },
  "2507.01327v1": {
    "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning\n  via Adaptive Perplexity-Aware Sampling Strategy",
    "summary": "Detecting abnormal events in real-world customer service dialogues is highly\nchallenging due to the complexity of business data and the dynamic nature of\ncustomer interactions. Moreover, models must demonstrate strong out-of-domain\n(OOD) generalization to enable rapid adaptation across different business\nscenarios and maximize commercial value. In this work, we propose a novel\nAdaptive Perplexity-Aware Reinforcement Learning (APARL) framework that\nleverages the advanced reasoning capabilities of large language models for\nabnormal event detection. APARL introduces a dual-loop dynamic curriculum\nlearning architecture, enabling the model to progressively focus on more\nchallenging samples as its proficiency increases. This design effectively\naddresses performance bottlenecks and significantly enhances OOD\ntransferability. Extensive evaluations on food delivery dialogue tasks show\nthat our model achieves significantly enhanced adaptability and robustness,\nattaining the highest F1 score with an average improvement of 17.19\\%, and an\naverage improvement of 9.59\\% in OOD transfer tests. This method provides a\nsuperior solution for industrial deployment of anomaly detection models,\ncontributing to improved operational efficiency and commercial benefits.",
    "published": "2025-07-02T03:26:02Z",
    "updated": "2025-07-02T03:26:02Z",
    "id": "2507.01327v1",
    "authors": [
      "Xiaoyun Zhang",
      "Jingqing Ruan",
      "Xing Ma",
      "Yawen Zhu",
      "Jiansong Chen",
      "Ke Zeng",
      "Xunliang Cai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01327v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01327v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01327v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using reinforcement learning with large language models for event detection, which aligns with the topics of Reinforcement Learning (RL) and Large Language Models (LLM). The abstract also mentions reasoning capabilities, but the primary focus is on RL and LLM applications.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.01321v1": {
    "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks",
    "summary": "In-context learning (ICL) has demonstrated remarkable success in large\nlanguage models (LLMs) due to its adaptability and parameter-free nature.\nHowever, it also introduces a critical vulnerability to backdoor attacks, where\nadversaries can manipulate LLM behaviors by simply poisoning a few ICL\ndemonstrations. In this paper, we propose, for the first time, the\ndual-learning hypothesis, which posits that LLMs simultaneously learn both the\ntask-relevant latent concepts and backdoor latent concepts within poisoned\ndemonstrations, jointly influencing the probability of model outputs. Through\ntheoretical analysis, we derive an upper bound for ICL backdoor effects,\nrevealing that the vulnerability is dominated by the concept preference ratio\nbetween the task and the backdoor. Motivated by these findings, we propose\nICLShield, a defense mechanism that dynamically adjusts the concept preference\nratio. Our method encourages LLMs to select clean demonstrations during the ICL\nphase by leveraging confidence and similarity scores, effectively mitigating\nsusceptibility to backdoor attacks. Extensive experiments across multiple LLMs\nand tasks demonstrate that our method achieves state-of-the-art defense\neffectiveness, significantly outperforming existing approaches (+26.02% on\naverage). Furthermore, our method exhibits exceptional adaptability and\ndefensive performance even for closed-source models (e.g., GPT-4).",
    "published": "2025-07-02T03:09:20Z",
    "updated": "2025-07-02T03:09:20Z",
    "id": "2507.01321v1",
    "authors": [
      "Zhiyao Ren",
      "Siyuan Liang",
      "Aishan Liu",
      "Dacheng Tao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01321v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01321v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01321v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses vulnerabilities in large language models (LLMs) related to in-context learning and backdoor attacks, which are core topics in LLM research. It also proposes a defense mechanism, which aligns with the broader context of LLM security and robustness.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.01299v1": {
    "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse\n  Activation",
    "summary": "Activation sparsity can reduce the computational overhead and memory\ntransfers during the forward pass of Large Language Model (LLM) inference.\nExisting methods face limitations, either demanding time-consuming recovery\ntraining that hinders real-world adoption, or relying on empirical\nmagnitude-based pruning, which causes fluctuating sparsity and unstable\ninference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse\nActivation), a novel method for activation sparsification designed to improve\nLLM efficiency without requiring additional training or magnitude-based\npruning. We leverage layerwise orthogonal rotations to transform input\nactivations into rotated forms that are more suitable for sparsification. By\nemploying a Top-K selection approach within the rotated activations, we achieve\nconsistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA\nis effective across various sizes and types of LLMs, demonstrating minimal\nperformance degradation and robust inference acceleration. Specifically, for\nLLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a\nconsistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in\nzero-shot tasks compared to the dense model to just 0.54%, while surpassing\nTEAL by 1.77% and CATS by 17.14%.",
    "published": "2025-07-02T02:36:03Z",
    "updated": "2025-07-02T02:36:03Z",
    "id": "2507.01299v1",
    "authors": [
      "Kai Liu",
      "Bowen Xu",
      "Shaoyu Wu",
      "Xin Chen",
      "Hao Zhou",
      "Yongliang Tao",
      "Lulu Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01299v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01299v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01299v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the efficiency of Large Language Models (LLMs) through activation sparsity, which is a key aspect of LLM research. It introduces a novel method for activation sparsification without requiring additional training, which aligns with the topics of LLM efficiency and scaling.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.01282v1": {
    "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care",
    "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that\nartificial intelligence (AI) systems could aid medical diagnosis. Yet despite\ndazzling benchmark scores, LLM assistants have yet to deliver measurable\nimprovements at the bedside. This scoping review aims to highlight the areas\nwhere AI is limited to make practical contributions in the clinical setting,\nspecifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom\nprovide actionable, interpretable guidance, eroding clinician trust. Adjacent\nuse of LLMs by physicians did not result in better diagnostic accuracy or\nspeed. Key limitations trace to the data-driven paradigm: black-box outputs\nwhich lack transparency, vulnerability to hallucinations, and weak causal\nreasoning. Hybrid approaches that combine statistical learning with expert\nrule-based knowledge, and involve clinicians throughout the process help bring\nback interpretability. They also fit better with existing clinical workflows,\nas seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking\npredictions to clinically meaningful causes. This can be done through\nneuro-symbolic or hybrid AI that combines the language ability of LLMs with\nhuman causal expertise. AI researchers have addressed this direction, with\nexplainable AI and neuro-symbolic AI being the next logical steps in further\nadvancement in AI. However, they are still based on data-driven knowledge\nintegration instead of human-in-the-loop approaches. Future research should\nmeasure success not only by accuracy but by improvements in clinician\nunderstanding, workflow fit, and patient outcomes. A better understanding of\nwhat helps improve human-computer interactions is greatly needed for AI systems\nto become part of clinical practice.",
    "published": "2025-07-02T01:43:06Z",
    "updated": "2025-07-02T01:43:06Z",
    "id": "2507.01282v1",
    "authors": [
      "Matthew JY Kang",
      "Wenli Yang",
      "Monica R Roberts",
      "Byeong Ho Kang",
      "Charles B Malpas"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01282v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01282v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01282v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the limitations of LLMs in clinical settings, particularly in dementia care, and suggests hybrid approaches combining LLMs with expert knowledge for better interpretability and workflow integration. It touches on LLMs, their application in medical diagnosis, and the need for interpretability and reasoning in AI systems.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "AGI"
    ]
  },
  "2507.01281v1": {
    "title": "Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented\n  Generation via Conflict-Driven Summarization",
    "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating their parametric knowledge with external retrieved content.\nHowever, knowledge conflicts caused by internal inconsistencies or noisy\nretrieved content can severely undermine the generation reliability of RAG\nsystems.In this work, we argue that LLMs should rethink all evidence, including\nboth retrieved content and internal knowledge, before generating responses.We\npropose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel\nframework that improves trustworthiness through Conflict-Driven Summarization\nof all available evidence.CARE-RAG first derives parameter-aware evidence by\ncomparing parameter records to identify diverse internal perspectives. It then\nrefines retrieved evidences to produce context-aware evidence, removing\nirrelevant or misleading content. To detect and summarize conflicts, we distill\na 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable\nsynthesis across multiple sources.To further ensure evaluation integrity, we\nintroduce a QA Repair step to correct outdated or ambiguous benchmark\nanswers.Experiments on revised QA datasets with retrieval data show that\nCARE-RAG consistently outperforms strong RAG baselines, especially in scenarios\nwith noisy or conflicting evidence.",
    "published": "2025-07-02T01:39:49Z",
    "updated": "2025-07-02T01:39:49Z",
    "id": "2507.01281v1",
    "authors": [
      "Juan Chen",
      "Baolong Bi",
      "Wei Zhang",
      "Jingyan Sui",
      "Xiaofei Zhu",
      "Yuanzhuo Wang",
      "Lingrui Mei",
      "Shenghua Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01281v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01281v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01281v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing Retrieval-Augmented Generation (RAG) systems, which involves memory-augmented models and retrieval-based methods. It also discusses the use of large language models (LLMs) and their integration with external knowledge, which falls under the LLM category. The paper does not directly address the other topics listed.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.01278v1": {
    "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic\n  Decision-Making in Diabetic Retinopathy and Glaucoma Screening",
    "summary": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology.",
    "published": "2025-07-02T01:35:59Z",
    "updated": "2025-07-02T01:35:59Z",
    "id": "2507.01278v1",
    "authors": [
      "Cindy Lie Tabuse",
      "David Restepo",
      "Carolina Gracitelli",
      "Fernando Korn Malerbi",
      "Caio Regatieri",
      "Luis Filipe Nakayama"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01278v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01278v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01278v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper evaluates the performance of GPT-4 (a Large Language Model) in interpreting structured textual descriptions of retinal fundus photographs and simulating clinical decisions, which involves multimodal data (text and images). The study focuses on the utility of LLMs in a specific domain (ophthalmology) and assesses their reasoning and decision-making capabilities.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "Reasoning"
    ]
  },
  "2507.01274v1": {
    "title": "AI Meets Maritime Training: Precision Analytics for Enhanced Safety and\n  Performance",
    "summary": "Traditional simulator-based training for maritime professionals is critical\nfor ensuring safety at sea but often depends on subjective trainer assessments\nof technical skills, behavioral focus, communication, and body language, posing\nchallenges such as subjectivity, difficulty in measuring key features, and\ncognitive limitations. Addressing these issues, this study develops an\nAI-driven framework to enhance maritime training by objectively assessing\ntrainee performance through visual focus tracking, speech recognition, and\nstress detection, improving readiness for high-risk scenarios. The system\nintegrates AI techniques, including visual focus determination using eye\ntracking, pupil dilation analysis, and computer vision; communication analysis\nthrough a maritime-specific speech-to-text model and natural language\nprocessing; communication correctness using large language models; and mental\nstress detection via vocal pitch. Models were evaluated on data from simulated\nmaritime scenarios with seafarers exposed to controlled high-stress events. The\nAI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for\nmaritime speech recognition, and ~90% for stress detection, surpassing existing\nbenchmarks. The system provides insights into visual attention, adherence to\ncommunication checklists, and stress levels under demanding conditions. This\nstudy demonstrates how AI can transform maritime training by delivering\nobjective performance analytics, enabling personalized feedback, and improving\npreparedness for real-world operational challenges.",
    "published": "2025-07-02T01:19:32Z",
    "updated": "2025-07-02T01:19:32Z",
    "id": "2507.01274v1",
    "authors": [
      "Vishakha Lall",
      "Yisi Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01274v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01274v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01274v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of AI techniques, including large language models, in maritime training for performance assessment and safety enhancement. However, it does not specifically focus on the core topics related to LLMs, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset as defined in the topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.01271v1": {
    "title": "PULSE: Practical Evaluation Scenarios for Large Multimodal Model\n  Unlearning",
    "summary": "In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially.",
    "published": "2025-07-02T01:13:08Z",
    "updated": "2025-07-02T01:13:08Z",
    "id": "2507.01271v1",
    "authors": [
      "Tatsuki Kawakami",
      "Kazuki Egashira",
      "Atsuyuki Miyai",
      "Go Irie",
      "Kiyoharu Aizawa"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01271v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01271v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01271v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on unlearning techniques in large multimodal models (LMMs), which involves both privacy and copyright concerns, and introduces a new evaluation framework. The core topics are related to multimodal large language models (MLLM) and benchmarking (Benchmark) as it evaluates unlearning methods in LMMs.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.03001v1": {
    "title": "Evaluating Hierarchical Clinical Document Classification Using\n  Reasoning-Based LLMs",
    "summary": "This study evaluates how well large language models (LLMs) can classify\nICD-10 codes from hospital discharge summaries, a critical but error-prone task\nin healthcare. Using 1,500 summaries from the MIMIC-IV dataset and focusing on\nthe 10 most frequent ICD-10 codes, the study tested 11 LLMs, including models\nwith and without structured reasoning capabilities. Medical terms were\nextracted using a clinical NLP tool (cTAKES), and models were prompted in a\nconsistent, coder-like format. None of the models achieved an F1 score above\n57%, with performance dropping as code specificity increased. Reasoning-based\nmodels generally outperformed non-reasoning ones, with Gemini 2.5 Pro\nperforming best overall. Some codes, such as those related to chronic heart\ndisease, were classified more accurately than others. The findings suggest that\nwhile LLMs can assist human coders, they are not yet reliable enough for full\nautomation. Future work should explore hybrid methods, domain-specific model\ntraining, and the use of structured clinical data.",
    "published": "2025-07-02T00:53:54Z",
    "updated": "2025-07-02T00:53:54Z",
    "id": "2507.03001v1",
    "authors": [
      "Akram Mustafa",
      "Usman Naseem",
      "Mostafa Rahimi Azghadi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03001v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03001v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03001v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the performance of large language models (LLMs) in classifying clinical documents, specifically using reasoning-based models. The study involves benchmarking LLMs and discusses their reasoning capabilities, which aligns with the topics 'Benchmark' and 'Reasoning'.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning"
    ]
  },
  "2507.01264v1": {
    "title": "LLM-based Realistic Safety-Critical Driving Video Generation",
    "summary": "Designing diverse and safety-critical driving scenarios is essential for\nevaluating autonomous driving systems. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) for few-shot code\ngeneration to automatically synthesize driving scenarios within the CARLA\nsimulator, which has flexibility in scenario scripting, efficient code-based\ncontrol of traffic participants, and enforcement of realistic physical\ndynamics. Given a few example prompts and code samples, the LLM generates\nsafety-critical scenario scripts that specify the behavior and placement of\ntraffic participants, with a particular focus on collision events. To bridge\nthe gap between simulation and real-world appearance, we integrate a video\ngeneration pipeline using Cosmos-Transfer1 with ControlNet, which converts\nrendered scenes into realistic driving videos. Our approach enables\ncontrollable scenario generation and facilitates the creation of rare but\ncritical edge cases, such as pedestrian crossings under occlusion or sudden\nvehicle cut-ins. Experimental results demonstrate the effectiveness of our\nmethod in generating a wide range of realistic, diverse, and safety-critical\nscenarios, offering a promising tool for simulation-based testing of autonomous\nvehicles.",
    "published": "2025-07-02T00:45:19Z",
    "updated": "2025-07-02T00:45:19Z",
    "id": "2507.01264v1",
    "authors": [
      "Yongjie Fu",
      "Ruijian Zha",
      "Pei Tian",
      "Xuan Di"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01264v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01264v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01264v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging Large Language Models (LLMs) for generating safety-critical driving scenarios, which involves LLM-based code generation and simulation. The integration of LLMs for scenario generation and the use of CARLA simulator align with the topics related to LLMs and their applications in generating realistic scenarios.",
    "llm_cls_result": [
      "LLM",
      "VLA"
    ]
  },
  "2507.01259v1": {
    "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based\n  Assistant",
    "summary": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings.",
    "published": "2025-07-02T00:36:27Z",
    "updated": "2025-07-02T00:36:27Z",
    "id": "2507.01259v1",
    "authors": [
      "Micha Matak",
      "Jarosaw A. Chudziak"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01259v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01259v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01259v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in legal information retrieval and introduces a specific architecture (gAIus) that leverages LLMs for legal tasks, including retrieval mechanisms and evaluation. The focus is on the application of LLMs in a specialized domain (legal) and their enhancement through retrieval-based methods.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Benchmark"
    ]
  },
  "2507.01241v1": {
    "title": "Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients\n  and AdamW",
    "summary": "Stochastic gradient-based descent (SGD), have long been central to training\nlarge language models (LLMs). However, their effectiveness is increasingly\nbeing questioned, particularly in large-scale applications where empirical\nevidence suggests potential performance limitations. In response, this paper\nproposes a stochastic conjugate subgradient method together with adaptive\nsampling tailored specifically for training LLMs. The method not only achieves\nfaster convergence per iteration but also demonstrates improved scalability\ncompared to traditional SGD techniques. It leverages sample complexity analysis\nto adaptively choose the sample size, employs a stochastic conjugate\nsubgradient approach to determine search directions and utilizing an AdamW-like\nalgorithm to adaptively adjust step sizes. This approach preserves the key\nadvantages of first-order methods while effectively addressing the nonconvexity\nand non-smoothness inherent in LLMs training. Additionally, we provide a\ndetailed analysis of the advantage of the algorithm. Experimental results show\nthat the proposed method not only maintains, but in many cases surpasses, the\nscalability of traditional SGD techniques, significantly enhancing both the\nspeed and accuracy of the optimization process.",
    "published": "2025-07-01T23:30:15Z",
    "updated": "2025-07-01T23:30:15Z",
    "id": "2507.01241v1",
    "authors": [
      "Di Zhang",
      "Yihang Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01241v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01241v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01241v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses training methods for large language models (LLMs), focusing on optimization techniques beyond traditional stochastic gradient descent (SGD). It introduces a stochastic conjugate subgradient method and adaptive sampling tailored for LLMs, which aligns with research on LLM architectures and scaling.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.01216v1": {
    "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
    "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
    "published": "2025-07-01T22:27:21Z",
    "updated": "2025-07-01T22:27:21Z",
    "id": "2507.01216v1",
    "authors": [
      "Xingke Yang",
      "Liang Li",
      "Zhiyi Wan",
      "Sicong Li",
      "Hao Wang",
      "Xiaoqi Qi",
      "Jiang Liu",
      "Tomoaki Ohtsuki",
      "Xin Fu",
      "Miao Pan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01216v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01216v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01216v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on fine-tuning large language models (LLMs) on mobile devices, addressing privacy and efficiency concerns. It discusses server-assisted methods and additive side-tuning, which are relevant to LLM research and fine-tuning strategies.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.02996v1": {
    "title": "Text-Guided Multi-Instance Learning for Scoliosis Screening via Gait\n  Video Analysis",
    "summary": "Early-stage scoliosis is often difficult to detect, particularly in\nadolescents, where delayed diagnosis can lead to serious health issues.\nTraditional X-ray-based methods carry radiation risks and rely heavily on\nclinical expertise, limiting their use in large-scale screenings. To overcome\nthese challenges, we propose a Text-Guided Multi-Instance Learning Network\n(TG-MILNet) for non-invasive scoliosis detection using gait videos. To handle\ntemporal misalignment in gait sequences, we employ Dynamic Time Warping (DTW)\nclustering to segment videos into key gait phases. To focus on the most\nrelevant diagnostic features, we introduce an Inter-Bag Temporal Attention\n(IBTA) mechanism that highlights critical gait phases. Recognizing the\ndifficulty in identifying borderline cases, we design a Boundary-Aware Model\n(BAM) to improve sensitivity to subtle spinal deviations. Additionally, we\nincorporate textual guidance from domain experts and large language models\n(LLM) to enhance feature representation and improve model interpretability.\nExperiments on the large-scale Scoliosis1K gait dataset show that TG-MILNet\nachieves state-of-the-art performance, particularly excelling in handling class\nimbalance and accurately detecting challenging borderline cases. The code is\navailable at https://github.com/lhqqq/TG-MILNet",
    "published": "2025-07-01T22:13:27Z",
    "updated": "2025-07-01T22:13:27Z",
    "id": "2507.02996v1",
    "authors": [
      "Haiqing Li",
      "Yuzhi Guo",
      "Feng Jiang",
      "Thao M. Dang",
      "Hehuan Ma",
      "Qifeng Zhou",
      "Jean Gao",
      "Junzhou Huang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02996v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02996v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02996v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using gait video analysis for scoliosis screening, incorporating textual guidance from domain experts and large language models (LLM) to enhance feature representation and model interpretability. The core topics are related to the use of LLM for enhancing diagnostic models and multimodal learning.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.01206v1": {
    "title": "2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User\n  Interface for Robotics and Space Exploration",
    "summary": "As modern computing advances, new interaction paradigms have emerged,\nparticularly in Augmented Reality (AR), which overlays virtual interfaces onto\nphysical objects. This evolution poses challenges in machine perception,\nespecially for tasks like 3D object pose estimation in complex, dynamic\nenvironments. Our project addresses critical issues in human-robot interaction\nwithin mobile AR, focusing on non-intrusive, spatially aware interfaces. We\npresent URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024\nSUITS challenge, targeting future spaceflight needs such as the Artemis\nmissions. URSA integrates three core technologies: a head-mounted AR device\n(e.g., HoloLens) for intuitive visual feedback, voice control powered by large\nlanguage models for hands-free interaction, and robot tracking algorithms that\nenable accurate 3D localization in dynamic settings. To enhance precision, we\nleverage digital twin localization technologies, using datasets like\nDTTD-Mobile and specialized hardware such as the ZED2 camera for real-world\ntracking under noise and occlusion. Our system enables real-time robot control\nand monitoring via an AR interface, even in the absence of ground-truth\nsensors--vital for hazardous or remote operations. Key contributions include:\n(1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based\ndataset tailored for non-rigid robotic bodies; (3) a Local Mission Control\nConsole (LMCC) for mission visualization; (4) a transformer-based 6DoF pose\nestimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5)\nend-to-end integration for astronaut mission support. This work advances\ndigital twin applications in robotics, offering scalable solutions for both\naerospace and industrial domains.",
    "published": "2025-07-01T21:54:35Z",
    "updated": "2025-07-01T21:54:35Z",
    "id": "2507.01206v1",
    "authors": [
      "Kathy Zhuang",
      "Zixun Huang",
      "Yukun Song",
      "Rui Li",
      "Yinuo Zhou",
      "Allen Y. Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01206v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01206v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01206v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) into an Augmented Reality (AR) system for robotics and space exploration, which aligns with the LLM topic. It also involves multimodal interactions (AR and voice control), which is relevant to MLLM. The application in robotics and space exploration hints at AGI-related advancements.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "AGI"
    ]
  },
  "2507.02002v1": {
    "title": "Dynamic Strategy Adaptation in Multi-Agent Environments with Large\n  Language Models",
    "summary": "Large language models (LLMs) demonstrate strong reasoning abilities across\nmathematical, strategic, and linguistic tasks, yet little is known about how\nwell they reason in dynamic, real-time, multi-agent scenarios, such as\ncollaborative environments in which agents continuously adapt to each other's\nbehavior, as in cooperative gameplay settings. In this paper, we bridge this\ngap by combining LLM-driven agents with strategic reasoning and real-time\nadaptation in cooperative, multi-agent environments grounded in game-theoretic\nprinciples such as belief consistency and Nash equilibrium. The proposed\nframework applies broadly to dynamic scenarios in which agents coordinate,\ncommunicate, and make decisions in response to continuously changing\nconditions. We provide real-time strategy refinement and adaptive feedback\nmechanisms that enable agents to dynamically adjust policies based on immediate\ncontextual interactions, in contrast to previous efforts that evaluate LLM\ncapabilities in static or turn-based settings. Empirical results show that our\nmethod achieves up to a 26\\% improvement in return over PPO baselines in\nhigh-noise environments, while maintaining real-time latency under 1.05\nmilliseconds. Our approach improves collaboration efficiency, task completion\nrates, and flexibility, illustrating that game-theoretic guidance integrated\nwith real-time feedback enhances LLM performance, ultimately fostering more\nresilient and flexible strategic multi-agent systems.",
    "published": "2025-07-01T20:09:50Z",
    "updated": "2025-07-01T20:09:50Z",
    "id": "2507.02002v1",
    "authors": [
      "Shaurya Mallampati",
      "Rashed Shelim",
      "Walid Saad",
      "Naren Ramakrishnan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02002v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02002v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02002v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in dynamic, multi-agent environments, focusing on strategic reasoning and real-time adaptation. It aligns with topics related to LLMs and Reinforcement Learning (RL) due to the emphasis on multi-agent scenarios and adaptive feedback mechanisms.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.01154v1": {
    "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD",
    "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.",
    "published": "2025-07-01T19:28:37Z",
    "updated": "2025-07-01T19:28:37Z",
    "id": "2507.01154v1",
    "authors": [
      "Liangyu Wang",
      "Junxiao Wang",
      "Jie Ren",
      "Zihang Xiang",
      "David E. Keyes",
      "Di Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01154v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01154v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01154v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the efficiency of Differentially Private Stochastic Gradient Descent (DP-SGD) for training large language models (LLMs), which directly relates to the topics of LLM and Scaling, as it discusses the challenges and solutions for scaling LLM training with privacy considerations.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.02990v1": {
    "title": "`For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in\n  Suicide and Self-Harm Contexts",
    "summary": "Recent advances in large language models (LLMs) have led to increasingly\nsophisticated safety protocols and features designed to prevent harmful,\nunethical, or unauthorized outputs. However, these guardrails remain\nsusceptible to novel and creative forms of adversarial prompting, including\nmanually generated test cases. In this work, we present two new test cases in\nmental health for (i) suicide and (ii) self-harm, using multi-step,\nprompt-level jailbreaking and bypass built-in content and safety filters. We\nshow that user intent is disregarded, leading to the generation of detailed\nharmful content and instructions that could cause real-world harm. We conduct\nan empirical evaluation across six widely available LLMs, demonstrating the\ngeneralizability and reliability of the bypass. We assess these findings and\nthe multilayered ethical tensions that they present for their implications on\nprompt-response filtering and context- and task-specific model development. We\nrecommend a more comprehensive and systematic approach to AI safety and ethics\nwhile emphasizing the need for continuous adversarial testing in\nsafety-critical AI deployments. We also argue that while certain clearly\ndefined safety measures and guardrails can and must be implemented in LLMs,\nensuring robust and comprehensive safety across all use cases and domains\nremains extremely challenging given the current technical maturity of\ngeneral-purpose LLMs.",
    "published": "2025-07-01T18:00:04Z",
    "updated": "2025-07-01T18:00:04Z",
    "id": "2507.02990v1",
    "authors": [
      "Annika M Schoene",
      "Cansu Canca"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02990v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02990v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02990v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the vulnerabilities of large language models (LLMs) in generating harmful content despite safety protocols, focusing on adversarial prompting and jailbreaking techniques in the context of suicide and self-harm. This directly relates to research on LLMs and their safety mechanisms.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.01004v2": {
    "title": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear\n  Attention",
    "summary": "Linear attention mechanisms deliver significant advantages for Large Language\nModels (LLMs) by providing linear computational complexity, enabling efficient\nprocessing of ultra-long sequences (e.g., 1M context). However, existing\nSequence Parallelism (SP) methods, essential for distributing these workloads\nacross devices, become the primary bottleneck due to substantial communication\noverhead. In this paper, we introduce ZeCO (Zero Communication Overhead)\nsequence parallelism for linear attention models, a new SP method designed to\novercome these limitations and achieve end-to-end near-linear scalability for\nlong sequence training. For example, training a model with a 1M sequence length\nacross 64 devices using ZeCO takes roughly the same time as training with an\n16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new\ncollective communication primitive. All-Scan provides each SP rank with\nprecisely the initial operator state it requires while maintaining a minimal\ncommunication footprint, effectively eliminating communication overhead.\nTheoretically, we prove the optimaity of ZeCO, showing that it introduces only\nnegligible time and space overhead. Empirically, we compare the communication\ncosts of different sequence parallelism strategies and demonstrate that\nAll-Scan achieves the fastest communication in SP scenarios. Specifically, on\n256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to\nthe current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a\nclear path toward efficiently training next-generation LLMs on previously\nintractable sequence lengths.",
    "published": "2025-07-01T17:54:53Z",
    "updated": "2025-07-02T10:04:00Z",
    "id": "2507.01004v2",
    "authors": [
      "Yuhong Chou",
      "Zehao Liu",
      "Ruijie Zhu",
      "Xinyi Wan",
      "Tianjian Li",
      "Congying Chu",
      "Qian Liu",
      "Jibin Wu",
      "Zejun Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01004v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01004v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01004v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving sequence parallelism for linear attention models, which are a type of Large Language Model (LLM). It introduces a new method (ZeCO) to reduce communication overhead, which is a key challenge in scaling LLMs. The primary topic is LLM, with a secondary focus on Scaling due to its emphasis on efficient training and scalability.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.00999v1": {
    "title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties\n  and Languages of Spain and Latin America",
    "summary": "Leaderboards showcase the current capabilities and limitations of Large\nLanguage Models (LLMs). To motivate the development of LLMs that represent the\nlinguistic and cultural diversity of the Spanish-speaking community, we present\nLa Leaderboard, the first open-source leaderboard to evaluate generative LLMs\nin languages and language varieties of Spain and Latin America. La Leaderboard\nis a community-driven project that aims to establish an evaluation standard for\neveryone interested in developing LLMs for the Spanish-speaking community. This\ninitial version combines 66 datasets in Basque, Catalan, Galician, and\ndifferent Spanish varieties, showcasing the evaluation results of 50 models. To\nencourage community-driven development of leaderboards in other languages, we\nexplain our methodology, including guidance on selecting the most suitable\nevaluation setup for each downstream task. In particular, we provide a\nrationale for using fewer few-shot examples than typically found in the\nliterature, aiming to reduce environmental impact and facilitate access to\nreproducible results for a broader research community.",
    "published": "2025-07-01T17:50:48Z",
    "updated": "2025-07-01T17:50:48Z",
    "id": "2507.00999v1",
    "authors": [
      "Mara Grandury",
      "Javier Aula-Blasco",
      "Jlia Falco",
      "Clmentine Fourrier",
      "Miguel Gonzlez",
      "Gonzalo Martnez",
      "Gonzalo Santamara",
      "Rodrigo Agerri",
      "Nuria Aldama",
      "Luis Chiruzzo",
      "Javier Conde",
      "Helena Gmez",
      "Marta Guerrero",
      "Guido Ivetta",
      "Natalia Lpez",
      "Flor Miriam Plaza-del-Arco",
      "Mara Teresa Martn-Valdivia",
      "Helena Montoro",
      "Carmen Muoz",
      "Pedro Reviriego",
      "Leire Rosado",
      "Alejandro Vaca",
      "Mara Estrella Vallecillo-Rodrguez",
      "Jorge Vallego",
      "Irune Zubiaga"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00999v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00999v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00999v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating Large Language Models (LLMs) in various Spanish languages and varieties, which involves benchmarking and dataset creation for LLMs.",
    "llm_cls_result": [
      "Benchmark",
      "Dataset",
      "LLM"
    ]
  },
  "2507.00985v1": {
    "title": "Discourse Heuristics For Paradoxically Moral Self-Correction",
    "summary": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales.",
    "published": "2025-07-01T17:36:41Z",
    "updated": "2025-07-01T17:36:41Z",
    "id": "2507.00985v1",
    "authors": [
      "Guangliang Liu",
      "Zimo Qi",
      "Xitong Zhang",
      "Kristen Marie Johnson"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00985v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00985v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00985v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses moral self-correction in Large Language Models (LLMs), which involves aligning LLM outputs with human moral values. It addresses the paradoxes in moral self-correction and proposes solutions leveraging heuristics from curated datasets. The core topics are related to LLMs and their alignment with human values, which falls under the 'LLM' and 'RL' categories due to the focus on alignment and self-correction mechanisms.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.00979v1": {
    "title": "Enhancing LLM Agent Safety via Causal Influence Prompting",
    "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.",
    "published": "2025-07-01T17:31:51Z",
    "updated": "2025-07-01T17:31:51Z",
    "id": "2507.00979v1",
    "authors": [
      "Dongyoon Hahm",
      "Woogyeol Jin",
      "June Suk Choi",
      "Sungsoo Ahn",
      "Kimin Lee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00979v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00979v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00979v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the safety of LLM agents through causal influence prompting, which involves reinforcement learning and decision-making processes.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2507.00953v2": {
    "title": "From Sentences to Sequences: Rethinking Languages in Biological System",
    "summary": "The paradigm of large language models in natural language processing (NLP)\nhas also shown promise in modeling biological languages, including proteins,\nRNA, and DNA. Both the auto-regressive generation paradigm and evaluation\nmetrics have been transferred from NLP to biological sequence modeling.\nHowever, the intrinsic structural correlations in natural and biological\nlanguages differ fundamentally. Therefore, we revisit the notion of language in\nbiological systems to better understand how NLP successes can be effectively\ntranslated to biological domains. By treating the 3D structure of biomolecules\nas the semantic content of a sentence and accounting for the strong\ncorrelations between residues or bases, we highlight the importance of\nstructural evaluation and demonstrate the applicability of the auto-regressive\nparadigm in biological language modeling. Code can be found at\n\\href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}",
    "published": "2025-07-01T16:57:39Z",
    "updated": "2025-07-03T10:33:16Z",
    "id": "2507.00953v2",
    "authors": [
      "Ke Liu",
      "Shuaike Shen",
      "Hao Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00953v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00953v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00953v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models (LLMs) to biological sequence modeling, which involves transferring NLP paradigms to biological domains. However, it does not focus on the core aspects of LLMs, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset as defined in the topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.00938v1": {
    "title": "WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks",
    "summary": "Recent progress in large language models (LLMs) has enabled the development\nof autonomous web agents capable of navigating and interacting with real\nwebsites. However, evaluating such agents remains challenging due to the\ninstability and inconsistency of existing benchmarks, which often rely on\ndynamic content or oversimplified simulations. In this work, we introduce\nWebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks\ngrounded in the arXiv platform. WebArXiv ensures reproducible and reliable\nevaluation by anchoring tasks in fixed web snapshots with deterministic ground\ntruths and standardized action trajectories. Through behavioral analysis, we\nidentify a common failure mode, Rigid History Reflection, where agents\nover-rely on fixed interaction histories. To address this, we propose a\nlightweight dynamic reflection mechanism that allows agents to selectively\nretrieve relevant past steps during decision-making. We evaluate ten\nstate-of-the-art web agents on WebArXiv. Results demonstrate clear performance\ndifferences across agents and validate the effectiveness of our proposed\nreflection strategy.",
    "published": "2025-07-01T16:43:57Z",
    "updated": "2025-07-01T16:43:57Z",
    "id": "2507.00938v1",
    "authors": [
      "Zihao Sun",
      "Meng Fang",
      "Ling Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00938v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00938v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00938v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark (WebArXiv) for evaluating multimodal agents, which aligns with the 'Benchmark' category. It also discusses large language models (LLMs) and their application in autonomous web agents, which is relevant to the 'LLM' category. The focus on multimodal agents and their evaluation also touches on the 'MLLM' category.",
    "llm_cls_result": [
      "Benchmark",
      "LLM",
      "MLLM"
    ]
  },
  "2507.01080v1": {
    "title": "Development and Comparative Evaluation of Three Artificial Intelligence\n  Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A\n  7-Month Retrospective Proof-of-Concept",
    "summary": "Triage errors, including undertriage and overtriage, are persistent\nchallenges in emergency departments (EDs). With increasing patient influx and\nstaff shortages, the integration of artificial intelligence (AI) into triage\nprotocols has gained attention. This study compares the performance of three AI\nmodels [Natural Language Processing (NLP), Large Language Models (LLM), and\nJoint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes\nagainst the FRENCH scale and clinical practice.We conducted a retrospective\nanalysis of a prospectively recruited cohort gathering adult patient triage\ndata over a 7-month period at the Roger Salengro Hospital ED (Lille, France).\nThree AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)\nURGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic\ndetails, verbatim chief complaints, vital signs, and triage outcomes based on\nthe FRENCH scale and GEMSA coding. The primary outcome was the concordance of\nAI-predicted triage level with the FRENCH gold-standard. It was assessed thanks\nto various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM\nmodel (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared\nto JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse\ntriage (-4.343). Secondary analyses highlighted the effectiveness of\nURGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness\nwith structured data versus raw transcripts (either for GEMSA prediction or for\nFRENCH prediction). LLM architecture, through abstraction of patient\nrepresentations, offers the most accurate triage predictions among tested\nmodels. Integrating AI into ED workflows could enhance patient safety and\noperational efficiency, though integration into clinical workflows requires\naddressing model limitations and ensuring ethical transparency.",
    "published": "2025-07-01T16:37:55Z",
    "updated": "2025-07-01T16:37:55Z",
    "id": "2507.01080v1",
    "authors": [
      "Edouard Lansiaux",
      "Ramy Azzouz",
      "Emmanuel Chazard",
      "Amlie Vromant",
      "Eric Wiel"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01080v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01080v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01080v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLM) in predicting triage outcomes in emergency departments, comparing it with other AI models. The study highlights the performance and potential of LLMs in a specific clinical setting.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.00914v1": {
    "title": "Large Language Model Powered Intelligent Urban Agents: Concepts,\n  Capabilities, and Applications",
    "summary": "The long-standing vision of intelligent cities is to create efficient,\nlivable, and sustainable urban environments using big data and artificial\nintelligence technologies. Recently, the advent of Large Language Models (LLMs)\nhas opened new ways toward realizing this vision. With powerful semantic\nunderstanding and reasoning capabilities, LLMs can be deployed as intelligent\nagents capable of autonomously solving complex problems across domains. In this\narticle, we focus on Urban LLM Agents, which are LLM-powered agents that are\nsemi-embodied within the hybrid cyber-physical-social space of cities and used\nfor system-level urban decision-making. First, we introduce the concept of\nurban LLM agents, discussing their unique capabilities and features. Second, we\nsurvey the current research landscape from the perspective of agent workflows,\nencompassing urban sensing, memory management, reasoning, execution, and\nlearning. Third, we categorize the application domains of urban LLM agents into\nfive groups: urban planning, transportation, environment, public safety, and\nurban society, presenting representative works in each group. Finally, we\ndiscuss trustworthiness and evaluation issues that are critical for real-world\ndeployment, and identify several open problems for future research. This survey\naims to establish a foundation for the emerging field of urban LLM agents and\nto provide a roadmap for advancing the intersection of LLMs and urban\nintelligence. A curated list of relevant papers and open-source resources is\nmaintained and continuously updated at\nhttps://github.com/usail-hkust/Awesome-Urban-LLM-Agents.",
    "published": "2025-07-01T16:18:29Z",
    "updated": "2025-07-01T16:18:29Z",
    "id": "2507.00914v1",
    "authors": [
      "Jindong Han",
      "Yansong Ning",
      "Zirui Yuan",
      "Hang Ni",
      "Fan Liu",
      "Tengfei Lyu",
      "Hao Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00914v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00914v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00914v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) as intelligent agents in urban environments, highlighting their capabilities in reasoning, memory management, and execution. This aligns with the topics of LLM research and Reinforcement Learning (RL) due to the agent-based approach and potential use of RLHF. The application in urban environments also touches on AGI aspects due to the generalist nature of the agents.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.00883v1": {
    "title": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and\n  Scenario Perturbations",
    "summary": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks",
    "published": "2025-07-01T15:51:46Z",
    "updated": "2025-07-01T15:51:46Z",
    "id": "2507.00883v1",
    "authors": [
      "Aditya Tomar",
      "Nihar Ranjan Sahoo",
      "Ashish Mittal",
      "Rudra Murthy",
      "Pushpak Bhattacharyya"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00883v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00883v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00883v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of large language models (LLMs) on culturally adapted versions of a math problem dataset, focusing on their performance and robustness to cultural variations. This aligns with topics related to LLMs and their reasoning abilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.00875v1": {
    "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation\n  of the Collaborative Translation",
    "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.",
    "published": "2025-07-01T15:39:26Z",
    "updated": "2025-07-01T15:39:26Z",
    "id": "2507.00875v1",
    "authors": [
      "Xi Xuan",
      "King-kui Sin",
      "Yufei Zhou",
      "Chunyu Kit"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00875v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00875v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00875v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in a multi-agent system for legal translation, which involves benchmarking various LLMs. This aligns with the topics of LLM research and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.00841v1": {
    "title": "SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for\n  Multimodal Mobile Agents",
    "summary": "With the wide application of multimodal foundation models in intelligent\nagent systems, scenarios such as mobile device control, intelligent assistant\ninteraction, and multimodal task execution are gradually relying on such large\nmodel-driven agents. However, the related systems are also increasingly exposed\nto potential jailbreak risks. Attackers may induce the agents to bypass the\noriginal behavioral constraints through specific inputs, and then trigger\ncertain risky and sensitive operations, such as modifying settings, executing\nunauthorized commands, or impersonating user identities, which brings new\nchallenges to system security. Existing security measures for intelligent\nagents still have limitations when facing complex interactions, especially in\ndetecting potentially risky behaviors across multiple rounds of conversations\nor sequences of tasks. In addition, an efficient and consistent automated\nmethodology to assist in assessing and determining the impact of such risks is\ncurrently lacking. This work explores the security issues surrounding mobile\nmultimodal agents, attempts to construct a risk discrimination mechanism by\nincorporating behavioral sequence information, and designs an automated\nassisted assessment scheme based on a large language model. Through preliminary\nvalidation in several representative high-risk tasks, the results show that the\nmethod can improve the recognition of risky behaviors to some extent and assist\nin reducing the probability of agents being jailbroken. We hope that this study\ncan provide some valuable references for the security risk modeling and\nprotection of multimodal intelligent agent systems.",
    "published": "2025-07-01T15:10:00Z",
    "updated": "2025-07-01T15:10:00Z",
    "id": "2507.00841v1",
    "authors": [
      "Siyuan Liang",
      "Tianmeng Fang",
      "Zhe Liu",
      "Aishan Liu",
      "Yan Xiao",
      "Jinyuan He",
      "Ee-Chien Chang",
      "Xiaochun Cao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00841v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00841v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00841v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses security issues in multimodal mobile agents, which involves multimodal large language models (MLLM) and their applications in agent systems. It also touches on the detection and evaluation of risky behaviors, which is related to benchmarking (Benchmark) and dataset (Dataset) creation for security purposes.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.00838v2": {
    "title": "Stylometry recognizes human and LLM-generated texts in short samples",
    "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.",
    "published": "2025-07-01T15:08:53Z",
    "updated": "2025-07-15T11:31:45Z",
    "id": "2507.00838v2",
    "authors": [
      "Karol Przystalski",
      "Jan K. Argasiski",
      "Iwona Grabska-Gradziska",
      "Jeremi K. Ochab"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00838v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00838v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00838v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on distinguishing between texts generated by LLMs and humans using stylometry, which involves analyzing writing patterns and features. It specifically mentions LLMs like GPT-3.5/4, LLaMa 2/3, Orca, and Falcon, and discusses the creation of a benchmark dataset for this purpose. The study is relevant to LLM research and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.00829v1": {
    "title": "On the Surprising Efficacy of LLMs for Penetration-Testing",
    "summary": "This paper presents a critical examination of the surprising efficacy of\nLarge Language Models (LLMs) in penetration testing. The paper thoroughly\nreviews the evolution of LLMs and their rapidly expanding capabilities which\nrender them increasingly suitable for complex penetration testing operations.\nIt systematically details the historical adoption of LLMs in both academic\nresearch and industry, showcasing their application across various offensive\nsecurity tasks and covering broader phases of the cyber kill chain. Crucially,\nthe analysis also extends to the observed adoption of LLMs by malicious actors,\nunderscoring the inherent dual-use challenge of this technology within the\nsecurity landscape.\n  The unexpected effectiveness of LLMs in this context is elucidated by several\nkey factors: the strong alignment between penetration testing's reliance on\npattern-matching and LLMs' core strengths, their inherent capacity to manage\nuncertainty in dynamic environments, and cost-effective access to competent\npre-trained models through LLM providers.\n  The current landscape of LLM-aided penetration testing is categorized into\ninteractive 'vibe-hacking' and the emergence of fully autonomous systems. The\npaper identifies and discusses significant obstacles impeding wider adoption\nand safe deployment. These include critical issues concerning model reliability\nand stability, paramount safety and security concerns, substantial monetary and\necological costs, implications for privacy and digital sovereignty, complex\nquestions of accountability, and profound ethical dilemmas. This comprehensive\nreview and analysis provides a foundation for discussion on future research\ndirections and the development of robust safeguards at the intersection of AI\nand security.",
    "published": "2025-07-01T15:01:18Z",
    "updated": "2025-07-01T15:01:18Z",
    "id": "2507.00829v1",
    "authors": [
      "Andreas Happe",
      "Jrgen Cito"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00829v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00829v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00829v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in penetration testing, highlighting their capabilities and challenges in this specific domain. The core topic is clearly related to LLMs and their use in security tasks, which aligns with the 'LLM' category. Additionally, the discussion on the dual-use challenge and ethical dilemmas touches on broader implications of LLM technology, which is also relevant to the 'LLM' category.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.01078v1": {
    "title": "yProv4ML: Effortless Provenance Tracking for Machine Learning Systems",
    "summary": "The rapid growth of interest in large language models (LLMs) reflects their\npotential for flexibility and generalization, and attracted the attention of a\ndiverse range of researchers. However, the advent of these techniques has also\nbrought to light the lack of transparency and rigor with which development is\npursued. In particular, the inability to determine the number of epochs and\nother hyperparameters in advance presents challenges in identifying the best\nmodel. To address this challenge, machine learning frameworks such as MLFlow\ncan automate the collection of this type of information. However, these tools\ncapture data using proprietary formats and pose little attention to lineage.\nThis paper proposes yProv4ML, a framework to capture provenance information\ngenerated during machine learning processes in PROV-JSON format, with minimal\ncode modifications.",
    "published": "2025-07-01T14:59:52Z",
    "updated": "2025-07-01T14:59:52Z",
    "id": "2507.01078v1",
    "authors": [
      "Gabriele Padovani",
      "Valentine Anantharaj",
      "Sandro Fiore"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01078v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01078v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01078v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the tracking of provenance information in machine learning systems, which is related to the broader field of LLMs but does not directly focus on any specific core topics like LLM architectures, reasoning, or scaling. The primary focus is on tracking and transparency in ML systems, which is not explicitly covered by the given topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.01077v1": {
    "title": "Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without\n  Reliable Labels",
    "summary": "Anomaly detection often relies on supervised or clustering approaches, with\nlimited success in specialized domains like automotive communication systems\nwhere scalable solutions are essential. We propose a novel decoder-only Large\nLanguage Model (LLM) to detect anomalies in Electronic Control Unit (ECU)\ncommunication logs. Our approach addresses two key challenges: the lack of LLMs\ntailored for ECU communication and the complexity of inconsistent ground truth\ndata. By learning from UDP communication logs, we formulate anomaly detection\nsimply as identifying deviations in time from normal behavior. We introduce an\nentropy regularization technique that increases model's uncertainty in known\nanomalies while maintaining consistency in similar scenarios. Our solution\noffers three novelties: a decoder-only anomaly detection architecture, a way to\nhandle inconsistent labeling, and an adaptable LLM for different ECU\ncommunication use cases. By leveraging the generative capabilities of\ndecoder-only models, we present a new technique that addresses the high cost\nand error-prone nature of manual labeling through a more scalable system that\nis able to learn from a minimal set of examples, while improving detection\naccuracy in complex communication environments.",
    "published": "2025-07-01T14:56:09Z",
    "updated": "2025-07-01T14:56:09Z",
    "id": "2507.01077v1",
    "authors": [
      "Bogdan Bogdan",
      "Arina Cazacu",
      "Laura Vasilie"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01077v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01077v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01077v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a decoder-only Large Language Model (LLM) for anomaly detection in ECU logs, which directly relates to LLM research and its application in specialized domains. The abstract does not mention other topics like RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.02989v1": {
    "title": "A Comparative Study of Competency Question Elicitation Methods from\n  Ontology Requirements",
    "summary": "Competency Questions (CQs) are pivotal in knowledge engineering, guiding the\ndesign, validation, and testing of ontologies. A number of diverse formulation\napproaches have been proposed in the literature, ranging from completely manual\nto Large Language Model (LLM) driven ones. However, attempts to characterise\nthe outputs of these approaches and their systematic comparison are scarce.\nThis paper presents an empirical comparative evaluation of three distinct CQ\nformulation approaches: manual formulation by ontology engineers, instantiation\nof CQ patterns, and generation using state of the art LLMs. We generate CQs\nusing each approach from a set of requirements for cultural heritage, and\nassess them across different dimensions: degree of acceptability, ambiguity,\nrelevance, readability and complexity. Our contribution is twofold: (i) the\nfirst multi-annotator dataset of CQs generated from the same source using\ndifferent methods; and (ii) a systematic comparison of the characteristics of\nthe CQs resulting from each approach. Our study shows that different CQ\ngeneration approaches have different characteristics and that LLMs can be used\nas a way to initially elicit CQs, however these are sensitive to the model used\nto generate CQs and they generally require a further refinement step before\nthey can be used to model requirements.",
    "published": "2025-07-01T14:49:30Z",
    "updated": "2025-07-01T14:49:30Z",
    "id": "2507.02989v1",
    "authors": [
      "Reham Alharbi",
      "Valentina Tamma",
      "Terry R. Payne",
      "Jacopo de Berardinis"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02989v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02989v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02989v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in generating Competency Questions (CQs) and compares their outputs with manual and pattern-based methods. While it involves LLMs, the primary focus is on knowledge engineering and ontology requirements, which does not directly align with the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.00817v1": {
    "title": "CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on\n  Video MLLMs",
    "summary": "Video Multimodal Large Language Models (V-MLLMs) have shown impressive\ncapabilities in temporal reasoning and cross-modal understanding, yet their\nvulnerability to adversarial attacks remains underexplored due to unique\nchallenges: complex cross-modal reasoning mechanisms, temporal dependencies,\nand computational constraints. We present CAVALRY-V (Cross-modal\nLanguage-Vision Adversarial Yielding for Videos), a novel framework that\ndirectly targets the critical interface between visual perception and language\ngeneration in V-MLLMs. Our approach introduces two key innovations: (1) a\ndual-objective semantic-visual loss function that simultaneously disrupts the\nmodel's text generation logits and visual representations to undermine\ncross-modal integration, and (2) a computationally efficient two-stage\ngenerator framework that combines large-scale pre-training for cross-model\ntransferability with specialized fine-tuning for spatiotemporal coherence.\nEmpirical evaluation on comprehensive video understanding benchmarks\ndemonstrates that CAVALRY-V significantly outperforms existing attack methods,\nachieving 22.8% average improvement over the best baseline attacks on both\ncommercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,\nInternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves\nflexibility through implicit temporal coherence modeling rather than explicit\nregularization, enabling significant performance improvements even on image\nunderstanding (34.4% average gain). This capability demonstrates CAVALRY-V's\npotential as a foundational approach for adversarial research across multimodal\nsystems.",
    "published": "2025-07-01T14:48:27Z",
    "updated": "2025-07-01T14:48:27Z",
    "id": "2507.00817v1",
    "authors": [
      "Jiaming Zhang",
      "Rui Hu",
      "Qing Guo",
      "Wei Yang Bryan Lim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00817v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00817v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00817v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on adversarial attacks on Video Multimodal Large Language Models (V-MLLMs), which involves cross-modal reasoning and temporal dependencies. The key topics are related to multimodal models and their vulnerabilities.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.00814v1": {
    "title": "Many LLMs Are More Utilitarian Than One",
    "summary": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning.",
    "published": "2025-07-01T14:46:16Z",
    "updated": "2025-07-01T14:46:16Z",
    "id": "2507.00814v1",
    "authors": [
      "Anita Keshmirian",
      "Razan Baltaji",
      "Babak Hemmatian",
      "Hadi Asghari",
      "Lav R. Varshney"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00814v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00814v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00814v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the moral judgment and alignment of large language models (LLMs) in multi-agent systems, focusing on their collective behavior compared to individual agents. It examines how LLMs function in groups and their moral reasoning, which is relevant to LLM research and alignment.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.00797v1": {
    "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator",
    "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.",
    "published": "2025-07-01T14:30:31Z",
    "updated": "2025-07-01T14:30:31Z",
    "id": "2507.00797v1",
    "authors": [
      "Zhican Wang",
      "Hongxiang Fan",
      "Haroon Waris",
      "Gang Wang",
      "Zhenyu Li",
      "Jianfei Jiang",
      "Yanan Sun",
      "Guanghui He"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00797v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00797v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00797v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the efficiency of LLM inference through algorithm-hardware-dataflow optimizations, specifically addressing computational and memory challenges. The core topics include LLM efficiency, hardware acceleration, and KV cache management, which align with the 'LLM' and 'Scaling' categories.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.00769v1": {
    "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative\n  Writing",
    "summary": "Evaluating creative writing generated by large language models (LLMs) remains\nchallenging because open-ended narratives lack ground truths. Without\nperformant automated evaluation methods, off-the-shelf (OTS) language models\nare employed as zero-shot judges, yet their reliability is unclear in this\ncontext. In pursuit of robust evaluation for creative writing, we introduce\nLitBench, the first standardized benchmark and paired dataset for creative\nwriting verification, comprising a held-out test set of 2,480 debiased,\nhuman-labeled story comparisons drawn from Reddit and a 43,827-pair training\ncorpus of human preference labels. Using LitBench, we (i) benchmark zero-shot\nLLM judges, (ii) train Bradley Terry and generative reward models, and (iii)\nconduct an online human study to validate reward model rankings on newly\nLLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the\nstrongest off-the-shelf judge, reaching 73% agreement with human preferences;\namong trained reward models, Bradley-Terry and Generative reward models both\nattain an accuracy of 78%, outperforming all off-the-shelf judges. An online\nhuman study further confirms that our trained reward models consistently align\nwith human preferences in novel LLM-generated stories. We release LitBench and\nreward models at\nhttps://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,\nproviding a vetted resource for reliable, automated evaluation and optimization\nof creative writing systems.",
    "published": "2025-07-01T14:10:36Z",
    "updated": "2025-07-01T14:10:36Z",
    "id": "2507.00769v1",
    "authors": [
      "Daniel Fein",
      "Sebastian Russo",
      "Violet Xiang",
      "Kabir Jolly",
      "Rafael Rafailov",
      "Nick Haber"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00769v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00769v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00769v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces LitBench, a benchmark and dataset for evaluating creative writing generated by LLMs, which involves benchmarking zero-shot LLM judges and training reward models. This aligns with the topics of Benchmark (evaluating LLMs) and Dataset (providing a dataset for evaluation).",
    "llm_cls_result": [
      "Benchmark",
      "Dataset"
    ]
  },
  "2507.00754v2": {
    "title": "Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision\n  Transformers with LLMs",
    "summary": "The integration of Large Language Model (LLMs) blocks with Vision\nTransformers (ViTs) holds immense promise for vision-only tasks by leveraging\nthe rich semantic knowledge and reasoning capabilities of LLMs. However, a\nfundamental challenge lies in the inherent modality mismatch between\ntext-centric pretraining of LLMs and vision-centric training of ViTs. Direct\nfusion often fails to fully exploit the LLM's potential and suffers from\nunstable finetuning. As a result, LLM blocks are kept frozen while only the\nvision components are learned. As a remedy to these challenges, we introduce\nLanguage-Unlocked Vision Transformers (LUViT), a novel approach that bridges\nthis modality mismatch through a synergistic pre-training strategy. LUViT\nco-adapts a ViT backbone and an LLM fusion block by (1) employing Masked\nAuto-Encoding (MAE) to pre-train the ViT for richer visual representations, and\n(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM\nblock using the MAE objective. This joint optimization guides the ViT to\nproduce LLM-aligned features and the LLM to effectively interpret visual\ninformation. We demonstrate through extensive experiments that LUViT\nsignificantly improves performance on various downstream vision tasks,\nshowcasing a more effective and efficient pathway to harness LLM knowledge for\nvisual understanding.",
    "published": "2025-07-01T13:58:21Z",
    "updated": "2025-07-08T19:44:08Z",
    "id": "2507.00754v2",
    "authors": [
      "Selim Kuzucu",
      "Muhammad Ferjad Naeem",
      "Anna Kukleva",
      "Federico Tombari",
      "Bernt Schiele"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00754v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00754v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00754v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with Vision Transformers (ViTs) to enhance visual understanding, which involves multimodal learning and leveraging LLM knowledge for vision tasks.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "VLA"
    ]
  },
  "2507.00726v2": {
    "title": "Can Large Language Models Develop Strategic Reasoning? Post-training\n  Insights from Learning Chess",
    "summary": "While reinforcement learning (RL) for large language models (LLMs) has shown\npromise in mathematical reasoning, strategic reasoning for LLMs using RL\nremains largely unexplored. We investigate whether LLMs can develop strategic\nreasoning capabilities through RL in chess. To this end, we leverage a\nchess-pretrained action-value network to provide dense reward on the LLM's\noutput move quality, which can be seen as a form of knowledge distillation. Our\nexperiments show that our distillation-based dense rewards often outperform\nsparse binary rewards. However, surprisingly, all models plateau far below\nexpert levels. We provide SFT and RL ablations on chess reasoning training and\nfind evidence that this limitation stems from a deficit in the pretrained\nmodels' internal understanding of chess--a deficit which RL alone may not be\nable to fully overcome.",
    "published": "2025-07-01T13:16:34Z",
    "updated": "2025-07-02T05:31:51Z",
    "id": "2507.00726v2",
    "authors": [
      "Dongyoon Hwang",
      "Hojoon Lee",
      "Jaegul Choo",
      "Dongmin Park",
      "Jongho Park"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00726v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00726v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00726v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper explores the use of reinforcement learning (RL) to enhance strategic reasoning in large language models (LLMs) through chess, which directly relates to RL and LLM research. It also touches on the reasoning capabilities of LLMs, making Reasoning a relevant topic.",
    "llm_cls_result": [
      "RL",
      "LLM",
      "Reasoning"
    ]
  },
  "2507.00718v1": {
    "title": "AI Analyst: Framework and Comprehensive Evaluation of Large Language\n  Models for Financial Time Series Report Generation",
    "summary": "This paper explores the potential of large language models (LLMs) to generate\nfinancial reports from time series data. We propose a framework encompassing\nprompt engineering, model selection, and evaluation. We introduce an automated\nhighlighting system to categorize information within the generated reports,\ndifferentiating between insights derived directly from time series data,\nstemming from financial reasoning, and those reliant on external knowledge.\nThis approach aids in evaluating the factual grounding and reasoning\ncapabilities of the models. Our experiments, utilizing both data from the real\nstock market indices and synthetic time series, demonstrate the capability of\nLLMs to produce coherent and informative financial reports.",
    "published": "2025-07-01T12:57:18Z",
    "updated": "2025-07-01T12:57:18Z",
    "id": "2507.00718v1",
    "authors": [
      "Elizabeth Fons",
      "Elena Kochkina",
      "Rachneet Kaur",
      "Zhen Zeng",
      "Berowne Hlavaty",
      "Charese Smiley",
      "Svitlana Vyetrenko",
      "Manuela Veloso"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00718v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00718v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00718v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of large language models (LLMs) in generating financial reports from time series data, which involves reasoning capabilities and model evaluation. The abstract mentions the use of LLMs and their reasoning abilities, aligning with the topics of 'LLM' and 'Reasoning'. The evaluation aspect also touches on 'Benchmark'.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.00715v1": {
    "title": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens",
    "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.",
    "published": "2025-07-01T12:42:06Z",
    "updated": "2025-07-01T12:42:06Z",
    "id": "2507.00715v1",
    "authors": [
      "Chaoqun Yang",
      "Xinyu Lin",
      "Wenjie Wang",
      "Yongqi Li",
      "Teng Sun",
      "Xianjing Han",
      "Tat-Seng Chua"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00715v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00715v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00715v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the efficiency of Large Language Model-based generative recommendation (LLMRec) by addressing inference latency and KV Cache issues, which are directly related to LLM research and optimization.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.00699v1": {
    "title": "A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction\n  Following with Multi-Turn Feedback",
    "summary": "Large language models (LLMs) have advanced significantly in code generation,\nyet their ability to follow complex programming instructions with layered and\ndiverse constraints remains underexplored. Existing benchmarks often prioritize\nfunctional correctness, overlooking the nuanced requirements found in\nreal-world development. We introduce MultiCodeIF, a comprehensive benchmark\ndesigned to evaluate instruction-following in code generation across multiple\ndimensions: constraint type, hierarchical levels, and iterative refinement.\nBuilt upon a structured taxonomy of 9 categories and 27 constraint types,\nMultiCodeIF enables granular assessment of both functional and non-functional\ninstruction adherence. Using an automated pipeline, ConstraGen, we synthesize\nand evolve 2,021 code tasks sourced from 14 programming languages, supporting\nmulti-turn evaluation through feedback-driven task variants. Empirical\nevaluation of six state-of-the-art LLMs uncovers substantial performance\ndisparities. The top-performing model, Claude-3-7-Sonnet, achieves 63.0%\naverage constraint satisfaction, while smaller models like Qwen3-1.7B fall to\n44.8%. Models perform well on explicit constraints, but struggle with implicit\nor abstract constraints. Tasks with multiple hierarchical constraints\nsignificantly reduce model success rates, from 54.5% in single-level to just\n18.8% in multi-level scenarios. However, structured feedback enables\nprogressive improvement: average constraint satisfaction rises from 63.0% to\n83.4% over four iterative refinement rounds. MultiCodeIF provides a scalable,\nconstraint-aware, and feedback-sensitive framework to benchmark LLMs under\nrealistic code generation scenarios, bridging the gap between synthetic\nevaluations and real-world instruction complexity. The full benchmark dataset,\nevaluation pipeline, and source code are available at\nhttps://github.com/SYSUSELab/MultiCodeIF.",
    "published": "2025-07-01T11:51:40Z",
    "updated": "2025-07-01T11:51:40Z",
    "id": "2507.00699v1",
    "authors": [
      "Guoliang Duan",
      "Mingwei Liu",
      "Yanlin Wang",
      "Chong Wang",
      "Xin Peng",
      "Zibin Zheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00699v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00699v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00699v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating Large Language Models (LLMs) in the context of code generation and instruction-following, which aligns with the 'LLM' topic. Additionally, the benchmark aspect of the study fits under 'Benchmark' as it involves evaluating LLMs' performance. The iterative refinement and feedback mechanisms mentioned in the abstract also suggest a connection to 'Reasoning' as it involves complex problem-solving and improvement over iterations.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Reasoning"
    ]
  },
  "2507.00693v1": {
    "title": "Leveraging Large Language Models for Spontaneous Speech-Based Suicide\n  Risk Detection",
    "summary": "Early identification of suicide risk is crucial for preventing suicidal\nbehaviors. As a result, the identification and study of patterns and markers\nrelated to suicide risk have become a key focus of current research. In this\npaper, we present the results of our work in the 1st SpeechWellness Challenge\n(SW1), which aims to explore speech as a non-invasive and easily accessible\nmental health indicator for identifying adolescents at risk of suicide.Our\napproach leverages large language model (LLM) as the primary tool for feature\nextraction, alongside conventional acoustic and semantic features. The proposed\nmethod achieves an accuracy of 74\\% on the test set, ranking first in the SW1\nchallenge. These findings demonstrate the potential of LLM-based methods for\nanalyzing speech in the context of suicide risk assessment.",
    "published": "2025-07-01T11:45:23Z",
    "updated": "2025-07-01T11:45:23Z",
    "id": "2507.00693v1",
    "authors": [
      "Yifan Gao",
      "Jiao Fu",
      "Long Guo",
      "Hong Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00693v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00693v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00693v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLM) for feature extraction in the context of suicide risk detection from speech, which directly aligns with the 'LLM' topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.00683v4": {
    "title": "Testing the spin-bath view of self-attention: A Hamiltonian analysis of\n  GPT-2 Transformer",
    "summary": "The recently proposed physics-based framework by Huo and\nJohnson~\\cite{huo2024capturing} models the attention mechanism of Large\nLanguage Models (LLMs) as an interacting two-body spin system, offering a\nfirst-principles explanation for phenomena like repetition and bias. Building\non this hypothesis, we extract the complete Query-Key weight matrices from a\nproduction-grade GPT-2 model and derive the corresponding effective Hamiltonian\nfor every attention head. From these Hamiltonians, we obtain analytic phase\nboundaries and logit gap criteria that predict which token should dominate the\nnext-token distribution for a given context. A systematic evaluation on 144\nheads across 20 factual-recall prompts reveals a strong negative correlation\nbetween the theoretical logit gaps and the model's empirical token rankings\n($r\\approx-0.70$, $p<10^{-3}$).Targeted ablations further show that suppressing\nthe heads most aligned with the spin-bath predictions induces the anticipated\nshifts in output probabilities, confirming a causal link rather than a\ncoincidental association. Taken together, our findings provide the first strong\nempirical evidence for the spin-bath analogy in a production-grade model. In\nthis work, we utilize the context-field lens, which provides physics-grounded\ninterpretability and motivates the development of novel generative models\nbridging theoretical condensed matter physics and artificial intelligence.",
    "published": "2025-07-01T11:33:39Z",
    "updated": "2025-07-21T05:24:54Z",
    "id": "2507.00683v4",
    "authors": [
      "Satadeep Bhattacharjee",
      "Seung-Cheol Lee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00683v4",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00683v4.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00683v4",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the attention mechanism of Large Language Models (LLMs) like GPT-2, focusing on a physics-based framework to explain phenomena such as repetition and bias. It involves analyzing the Query-Key weight matrices and deriving effective Hamiltonians, which are core to understanding LLM architectures and their underlying mechanisms.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.02987v3": {
    "title": "Leveraging the Structure of Medical Data for Improved Representation\n  Learning",
    "summary": "Building generalizable medical AI systems requires pretraining strategies\nthat are data-efficient and domain-aware. Unlike internet-scale corpora,\nclinical datasets such as MIMIC-CXR offer limited image counts and scarce\nannotations, but exhibit rich internal structure through multi-view imaging. We\npropose a self-supervised framework that leverages the inherent structure of\nmedical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and\nlateral views) as natural positive pairs, learning to reconstruct each view\nfrom sparse patches while aligning their latent embeddings. Our method requires\nno textual supervision and produces informative representations. Evaluated on\nMIMIC-CXR, we show strong performance compared to supervised objectives and\nbaselines being trained without leveraging structure. This work provides a\nlightweight, modality-agnostic blueprint for domain-specific pretraining where\ndata is structured but scarce",
    "published": "2025-07-01T11:14:45Z",
    "updated": "2025-07-24T12:44:31Z",
    "id": "2507.02987v3",
    "authors": [
      "Andrea Agostini",
      "Sonia Laguna",
      "Alain Ryser",
      "Samuel Ruiperez-Campillo",
      "Moritz Vandenhirtz",
      "Nicolas Deperrois",
      "Farhad Nooralahzadeh",
      "Michael Krauthammer",
      "Thomas M. Sutter",
      "Julia E. Vogt"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02987v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02987v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02987v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on pretraining strategies for medical AI systems, leveraging the structure of medical datasets for improved representation learning. It does not directly align with the provided topics related to LLMs, RL, MLLM, etc., but rather pertains to domain-specific pretraining in the medical field.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.00672v1": {
    "title": "Toward Edge General Intelligence with Multiple-Large Language Model\n  (Multi-LLM): Architecture, Trust, and Orchestration",
    "summary": "Edge computing enables real-time data processing closer to its source, thus\nimproving the latency and performance of edge-enabled AI applications. However,\ntraditional AI models often fall short when dealing with complex, dynamic tasks\nthat require advanced reasoning and multimodal data processing. This survey\nexplores the integration of multi-LLMs (Large Language Models) to address this\nin edge computing, where multiple specialized LLMs collaborate to enhance task\nperformance and adaptability in resource-constrained environments. We review\nthe transition from conventional edge AI models to single LLM deployment and,\nultimately, to multi-LLM systems. The survey discusses enabling technologies\nsuch as dynamic orchestration, resource scheduling, and cross-domain knowledge\ntransfer that are key for multi-LLM implementation. A central focus is on\ntrusted multi-LLM systems, ensuring robust decision-making in environments\nwhere reliability and privacy are crucial. We also present multimodal multi-LLM\narchitectures, where multiple LLMs specialize in handling different data\nmodalities, such as text, images, and audio, by integrating their outputs for\ncomprehensive analysis. Finally, we highlight future directions, including\nimproving resource efficiency, trustworthy governance multi-LLM systems, while\naddressing privacy, trust, and robustness concerns. This survey provides a\nvaluable reference for researchers and practitioners aiming to leverage\nmulti-LLM systems in edge computing applications.",
    "published": "2025-07-01T11:13:56Z",
    "updated": "2025-07-01T11:13:56Z",
    "id": "2507.00672v1",
    "authors": [
      "Haoxiang Luo",
      "Yinqiu Liu",
      "Ruichen Zhang",
      "Jiacheng Wang",
      "Gang Sun",
      "Dusit Niyato",
      "Hongfang Yu",
      "Zehui Xiong",
      "Xianbin Wang",
      "Xuemin Shen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00672v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00672v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00672v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of multiple Large Language Models (LLMs) in edge computing, focusing on their architecture, trust, and orchestration. It also mentions multimodal data processing and the need for advanced reasoning, which aligns with the topics of LLM, MLLM, and AGI.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "AGI"
    ]
  },
  "2507.00657v1": {
    "title": "Generative Exaggeration in LLM Social Agents: Consistency, Bias, and\n  Toxicity",
    "summary": "We investigate how Large Language Models (LLMs) behave when simulating\npolitical discourse on social media. Leveraging 21 million interactions on X\nduring the 2024 U.S. presidential election, we construct LLM agents based on\n1,186 real users, prompting them to reply to politically salient tweets under\ncontrolled conditions. Agents are initialized either with minimal ideological\ncues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one\ncomparisons with human replies. We evaluate three model families (Gemini,\nMistral, and DeepSeek) across linguistic style, ideological consistency, and\ntoxicity. We find that richer contextualization improves internal consistency\nbut also amplifies polarization, stylized signals, and harmful language. We\nobserve an emergent distortion that we call \"generation exaggeration\": a\nsystematic amplification of salient traits beyond empirical baselines. Our\nanalysis shows that LLMs do not emulate users, they reconstruct them. Their\noutputs, indeed, reflect internal optimization dynamics more than observed\nbehavior, introducing structural biases that compromise their reliability as\nsocial proxies. This challenges their use in content moderation, deliberative\nsimulations, and policy modeling.",
    "published": "2025-07-01T10:54:51Z",
    "updated": "2025-07-01T10:54:51Z",
    "id": "2507.00657v1",
    "authors": [
      "Jacopo Nudo",
      "Mario Edoardo Pandolfo",
      "Edoardo Loru",
      "Mattia Samory",
      "Matteo Cinelli",
      "Walter Quattrociocchi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00657v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00657v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00657v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper investigates the behavior of Large Language Models (LLMs) in simulating political discourse, focusing on aspects like ideological consistency, bias, and toxicity. It involves the use of LLMs to simulate social media interactions and evaluates their outputs, which aligns with research on LLMs and their applications in social contexts.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Benchmark"
    ]
  },
  "2507.00653v1": {
    "title": "Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for\n  Optimizing the Token Economy of Large Language Models",
    "summary": "The escalating computational costs of Large Language Model (LLM) inference\nhave become a critical barrier to their widespread and sustainable deployment.\nWhile existing optimization strategies are effective, they are predominantly\nbased on statistical heuristics or architectural modifications, lacking a\nguiding cognitive theory to manage the inference process itself. This paper\naims to bridge this gap by introducing a novel paradigm: the Cognitive\nLoad-Aware Inference (CLAI) framework, which operationalizes principles from\nCognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize\nthe concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and\nGermane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,\nand $GCL_{LLM}$), thereby reframing the inference process as a cognitive\neconomics optimization problem: based on the intrinsic complexity of a problem\n($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically\nallocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two\nimplementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM\nthrough cognitive control steps via a structured meta-prompt, and CLAI-Tune, a\nfine-tuned model that internalizes these principles for spontaneous cognitive\neconomy. Across a range of benchmarks in complex reasoning, long-context\nquestion answering, and code generation, our methods achieve significant\nreductions in token consumption (up to 45\\%) without sacrificing accuracy.\nFurthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose\ndifficult problems, a key characteristic of human expert cognition. This work\ndemonstrates that by emulating the brain's resource management strategies, we\ncan build more efficient, robust, and capable artificial intelligence systems.",
    "published": "2025-07-01T10:51:18Z",
    "updated": "2025-07-01T10:51:18Z",
    "id": "2507.00653v1",
    "authors": [
      "Yilun Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00653v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00653v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00653v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing the token economy of Large Language Models (LLMs) by introducing a cognitive load-aware inference framework. It discusses LLM inference optimization, cognitive load theory, and efficient reasoning strategies, which are relevant to LLM research and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.00642v1": {
    "title": "ChatHLS: Towards Systematic Design Automation and Optimization for\n  High-Level Synthesis",
    "summary": "The increasing complexity of computational demands has accelerated the\nadoption of domain-specific accelerators, yet traditional hardware design\nmethodologies remain constrained by prolonged development and verification\ncycles. High-Level Synthesis (HLS) bridges the gap between software and\nhardware by enabling hardware design from high-level programming languages.\nHowever, its widespread adoption is hindered by strict coding constraints and\nintricate hardware-specific optimizations, creating significant obstacles for\ndevelopers. Recent advancements in Large Language Models (LLMs) demonstrate\nsubstantial potential in hardware design automation. However, their\neffectiveness is limited by the scarcity of high-quality datasets, particularly\nin the context of HLS. To address these challenges, we introduce ChatHLS, an\nagile HLS design automation and optimization workflow that leverages fine-tuned\nLLMs integrated within a multi-agent framework for error correction and design\noptimization. Our extensive evaluations reveal that ChatHLS achieves an average\nrepair pass rate of 82.7% over 612 test cases, outperforming the GPT-4o and\nLlama3-8B by 19.1% and 63.0%, respectively. Furthermore, ChatHLS delivers\nperformance enhancements ranging from 1.9$\\times$ to 14.8$\\times$ upon\nresource-constrained kernels. By enabling sophisticated optimization reasoning\nwithin practical computational budgets, ChatHLS attains a 4.9$\\times$ geometric\nmean speedup compared to state-of-the-art DSL-based approaches. These results\nunderscore the potential of ChatHLS in substantially expediting hardware\ndevelopment cycles while maintaining rigorous standards of design reliability\nand optimization quality.",
    "published": "2025-07-01T10:34:17Z",
    "updated": "2025-07-01T10:34:17Z",
    "id": "2507.00642v1",
    "authors": [
      "Runkai Li",
      "Jia Xiong",
      "Xiuyuan He",
      "Jieru Zhao",
      "Qiang Xu",
      "Xi Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00642v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00642v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00642v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in hardware design automation and optimization, specifically in the context of High-Level Synthesis (HLS). It highlights the application of fine-tuned LLMs within a multi-agent framework for error correction and design optimization, which aligns with the 'LLM' and 'Reasoning' topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.02986v2": {
    "title": "GAF-Guard: An Agentic Framework for Risk Management and Governance in\n  Large Language Models",
    "summary": "As Large Language Models (LLMs) continue to be increasingly applied across\nvarious domains, their widespread adoption necessitates rigorous monitoring to\nprevent unintended negative consequences and ensure robustness. Furthermore,\nLLMs must be designed to align with human values, like preventing harmful\ncontent and ensuring responsible usage. The current automated systems and\nsolutions for monitoring LLMs in production are primarily centered on\nLLM-specific concerns like hallucination etc, with little consideration given\nto the requirements of specific use-cases and user preferences. This paper\nintroduces GAF-Guard, a novel agentic framework for LLM governance that places\nthe user, the use-case, and the model itself at the center. The framework is\ndesigned to detect and monitor risks associated with the deployment of LLM\nbased applications. The approach models autonomous agents that identify risks,\nactivate risk detection tools, within specific use-cases and facilitate\ncontinuous monitoring and reporting to enhance AI safety, and user\nexpectations. The code is available at\nhttps://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.",
    "published": "2025-07-01T10:01:21Z",
    "updated": "2025-07-08T15:44:49Z",
    "id": "2507.02986v2",
    "authors": [
      "Seshu Tirupathi",
      "Dhaval Salwala",
      "Elizabeth Daly",
      "Inge Vejsbjerg"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02986v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02986v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02986v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a framework for risk management and governance in Large Language Models (LLMs), focusing on monitoring and aligning LLMs with human values. It involves autonomous agents for risk detection and monitoring, which is relevant to LLM governance and safety.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2507.00605v1": {
    "title": "Quantize-Sample-and-Verify: LLM Acceleration via Adaptive Edge-Cloud\n  Speculative Decoding",
    "summary": "In edge-cloud speculative decoding (SD), edge devices equipped with small\nlanguage models (SLMs) generate draft tokens that are verified by large\nlanguage models (LLMs) in the cloud. A key bottleneck in such systems is the\nlimited communication bandwidth between edge and cloud, which necessitates\nquantization of the information transmitted about generated tokens. In this\nwork, we introduce a novel quantize-sample (Q-S) strategy that provably\npreserves the output distribution of the cloud-based model, ensuring that the\nverified tokens match the distribution of those that would have been generated\ndirectly by the LLM. We develop a throughput model for edge-cloud SD that\nexplicitly accounts for communication latency. Leveraging this model, we\npropose an adaptive mechanism that optimizes token throughput by dynamically\nadjusting the draft length and quantization precision in response to both\nsemantic uncertainty and channel conditions. Simulations demonstrate that the\nproposed Q-S approach significantly improves decoding efficiency in realistic\nedge-cloud deployment scenarios.",
    "published": "2025-07-01T09:38:15Z",
    "updated": "2025-07-01T09:38:15Z",
    "id": "2507.00605v1",
    "authors": [
      "Guangyi Zhang",
      "Yunlong Cai",
      "Guanding Yu",
      "Petar Popovski",
      "Osvaldo Simeone"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00605v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00605v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00605v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for accelerating Large Language Models (LLMs) through edge-cloud speculative decoding, focusing on quantization and communication efficiency. The core topics are related to LLM acceleration and optimization, which aligns with the 'LLM' and 'Scaling' categories.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.00601v2": {
    "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt\n  and Alignment-Based Approach",
    "summary": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks.",
    "published": "2025-07-01T09:34:49Z",
    "updated": "2025-07-02T06:39:55Z",
    "id": "2507.00601v2",
    "authors": [
      "Shuangquan Lyu",
      "Yingnan Deng",
      "Guiran Liu",
      "Zhen Qi",
      "Ruotong Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00601v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00601v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00601v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the transfer and adaptation capabilities of large language models (LLMs) in low-resource scenarios, which involves knowledge transfer, parameter-efficient fine-tuning, and alignment strategies. These topics are closely related to LLM research and pretraining strategies.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2507.01997v2": {
    "title": "Towards a Playground to Democratize Experimentation and Benchmarking of\n  AI Agents for Network Troubleshooting",
    "summary": "Recent research has demonstrated the effectiveness of Artificial Intelligence\n(AI), and more specifically, Large Language Models (LLMs), in supporting\nnetwork configuration synthesis and automating network diagnosis tasks, among\nothers. In this preliminary work, we restrict our focus to the application of\nAI agents to network troubleshooting and elaborate on the need for a\nstandardized, reproducible, and open benchmarking platform, where to build and\nevaluate AI agents with low operational effort.",
    "published": "2025-07-01T08:46:37Z",
    "updated": "2025-07-04T07:39:58Z",
    "id": "2507.01997v2",
    "authors": [
      "Zhihao Wang",
      "Alessandro Cornacchia",
      "Franco Galante",
      "Carlo Centofanti",
      "Alessio Sacco",
      "Dingde Jiang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01997v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01997v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01997v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of AI agents, specifically Large Language Models (LLMs), in network troubleshooting and emphasizes the need for a standardized benchmarking platform. This aligns with topics related to LLM applications and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.00543v1": {
    "title": "Reliable Annotations with Less Effort: Evaluating LLM-Human\n  Collaboration in Search Clarifications",
    "summary": "Despite growing interest in using large language models (LLMs) to automate\nannotation, their effectiveness in complex, nuanced, and multi-dimensional\nlabelling tasks remains relatively underexplored. This study focuses on\nannotation for the search clarification task, leveraging a high-quality,\nmulti-dimensional dataset that includes five distinct fine-grained annotation\nsubtasks. Although LLMs have shown impressive capabilities in general settings,\nour study reveals that even state-of-the-art models struggle to replicate\nhuman-level performance in subjective or fine-grained evaluation tasks. Through\na systematic assessment, we demonstrate that LLM predictions are often\ninconsistent, poorly calibrated, and highly sensitive to prompt variations. To\naddress these limitations, we propose a simple yet effective human-in-the-loop\n(HITL) workflow that uses confidence thresholds and inter-model disagreement to\nselectively involve human review. Our findings show that this lightweight\nintervention significantly improves annotation reliability while reducing human\neffort by up to 45%, offering a relatively scalable and cost-effective yet\naccurate path forward for deploying LLMs in real-world evaluation settings.",
    "published": "2025-07-01T08:04:58Z",
    "updated": "2025-07-01T08:04:58Z",
    "id": "2507.00543v1",
    "authors": [
      "Leila Tavakoli",
      "Hamed Zamani"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00543v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00543v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00543v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of LLMs for annotation tasks and evaluates their performance in complex, nuanced labeling tasks. It also proposes a human-in-the-loop workflow to improve annotation reliability, which is relevant to LLM research.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.00509v1": {
    "title": "TeamCMU at Touch: Adversarial Co-Evolution for Advertisement\n  Integration and Detection in Conversational Search",
    "summary": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers.",
    "published": "2025-07-01T07:24:29Z",
    "updated": "2025-07-01T07:24:29Z",
    "id": "2507.00509v1",
    "authors": [
      "To Eun Kim",
      "Joo Coelho",
      "Gbemileke Onilude",
      "Jai Singh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00509v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00509v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00509v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the integration and detection of advertisements in conversational search systems powered by Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), which involves LLM applications and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.00505v3": {
    "title": "LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for\n  MLLMs",
    "summary": "The architecture of multimodal large language models (MLLMs) commonly\nconnects a vision encoder, often based on CLIP-ViT, to a large language model.\nWhile CLIP-ViT works well for capturing global image features, it struggles to\nmodel local relationships between adjacent patches, leading to weaker visual\nrepresentation, which in turn affects the detailed understanding ability of\nMLLMs. To solve this, we propose LLaVA-SP, which only adds six spatial visual\ntokens to the original visual tokens to enhance the visual representation. Our\napproach offers three key advantages: 1) We propose a novel Projector, which\nuses convolutional kernels to derive visual spatial tokens from ViT patch\nfeatures, simulating two visual spatial ordering approaches: \"from central\nregion to global\" and \"from abstract to specific\". Then, a cross-attention\nmechanism is applied to fuse fine-grained visual information, enriching the\noverall visual representation. 2) We present two model variants:\nLLaVA-SP-Cropping, which focuses on detail features through progressive\ncropping, and LLaVA-SP-Pooling, which captures global semantics through\nadaptive pooling, enabling the model to handle diverse visual understanding\ntasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,\nachieves significant performance improvements across various multimodal\nbenchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple\ntasks with nearly identical inference latency. The code and models are\navailable at https://github.com/CnFaker/LLaVA-SP.",
    "published": "2025-07-01T07:20:11Z",
    "updated": "2025-07-04T13:15:34Z",
    "id": "2507.00505v3",
    "authors": [
      "Haoran Lou",
      "Chunxiao Fan",
      "Ziyan Liu",
      "Yuexin Wu",
      "Xinliang Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00505v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00505v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00505v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing visual representation in Multimodal Large Language Models (MLLMs) by introducing visual spatial tokens, which directly relates to the topics of MLLM (Multimodal Large Language Models) and VLA (Vision-Language Alignment models). The paper also discusses performance improvements across various multimodal benchmarks, which aligns with the Benchmark topic.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Benchmark"
    ]
  },
  "2507.00491v1": {
    "title": "Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge\n  Platforms",
    "summary": "Compound AI (cAI) systems chain multiple AI models to solve complex problems.\ncAI systems are typically composed of deep neural networks (DNNs),\ntransformers, and large language models (LLMs), exhibiting a high degree of\ncomputational diversity and dynamic workload variation. Deploying cAI services\non mobile edge platforms poses a significant challenge in scheduling concurrent\nDNN-transformer inference tasks, which arrive dynamically in an unknown\nsequence. Existing mobile edge AI inference strategies manage multi-DNN or\ntransformer-only workloads, relying on design-time profiling, and cannot handle\nconcurrent inference of DNNs and transformers required by cAI systems. In this\nwork, we address the challenge of scheduling cAI systems on heterogeneous\nmobile edge platforms. We present Twill, a run-time framework to handle\nconcurrent inference requests of cAI workloads through task affinity-aware\ncluster mapping and migration, priority-aware task freezing/unfreezing, and\nDVFS, while minimizing inference latency within power budgets. We implement and\ndeploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate\nTwill against state-of-the-art edge AI inference techniques over contemporary\nDNNs and LLMs, reducing inference latency by 54% on average, while honoring\npower budgets.",
    "published": "2025-07-01T07:06:45Z",
    "updated": "2025-07-01T07:06:45Z",
    "id": "2507.00491v1",
    "authors": [
      "Zain Taufique",
      "Aman Vyas",
      "Antonio Miele",
      "Pasi Liljeberg",
      "Anil Kanduri"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00491v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00491v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00491v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the scheduling of Compound AI (cAI) systems, which include large language models (LLMs) and other AI models, on heterogeneous mobile edge platforms. It focuses on optimizing inference tasks and reducing latency, which is relevant to the deployment and performance of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2507.00487v2": {
    "title": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large\n  Language Models",
    "summary": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool.",
    "published": "2025-07-01T07:02:26Z",
    "updated": "2025-07-02T04:35:44Z",
    "id": "2507.00487v2",
    "authors": [
      "Jianghao Lin",
      "Xinyuan Wang",
      "Xinyi Dai",
      "Menghui Zhu",
      "Bo Chen",
      "Ruiming Tang",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00487v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00487v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00487v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing tool retrieval for large language models (LLMs) through a multi-task search-based framework, which involves query representation and tool retrieval accuracy. This aligns with the 'LLM' and 'Memory' topics as it deals with improving LLM interactions with external tools and retrieval-based methods.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.00477v1": {
    "title": "Read the Docs Before Rewriting: Equip Rewriter with Domain Knowledge via\n  Continual Pre-training",
    "summary": "A Retrieval-Augmented Generation (RAG)-based question-answering (QA) system\nenhances a large language model's knowledge by retrieving relevant documents\nbased on user queries. Discrepancies between user queries and document\nphrasings often necessitate query rewriting. However, in specialized domains,\nthe rewriter model may struggle due to limited domain-specific knowledge. To\nresolve this, we propose the R\\&R (Read the doc before Rewriting) rewriter,\nwhich involves continual pre-training on professional documents, akin to how\nstudents prepare for open-book exams by reviewing textbooks. Additionally, it\ncan be combined with supervised fine-tuning for improved results. Experiments\non multiple datasets demonstrate that R\\&R excels in professional QA across\nmultiple domains, effectively bridging the query-document gap, while\nmaintaining good performance in general scenarios, thus advancing the\napplication of RAG-based QA systems in specialized fields.",
    "published": "2025-07-01T06:51:00Z",
    "updated": "2025-07-01T06:51:00Z",
    "id": "2507.00477v1",
    "authors": [
      "Qi Wang",
      "Yixuan Cao",
      "Yifan Liu",
      "Jiangtao Zhao",
      "Ping Luo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00477v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00477v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00477v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses enhancing a large language model's knowledge through retrieval-augmented generation and continual pre-training, which aligns with topics related to memory and pretraining.",
    "llm_cls_result": [
      "Memory",
      "Pretrain"
    ]
  },
  "2507.02983v1": {
    "title": "Truth, Trust, and Trouble: Medical AI on the Edge",
    "summary": "Large Language Models (LLMs) hold significant promise for transforming\ndigital health by enabling automated medical question answering. However,\nensuring these models meet critical industry standards for factual accuracy,\nusefulness, and safety remains a challenge, especially for open-source\nsolutions. We present a rigorous benchmarking framework using a dataset of over\n1,000 health questions. We assess model performance across honesty,\nhelpfulness, and harmlessness. Our results highlight trade-offs between factual\nreliability and safety among evaluated models -- Mistral-7B,\nBioMistral-7B-DARE, and AlpaCare-13B. AlpaCare-13B achieves the highest\naccuracy (91.7%) and harmlessness (0.92), while domain-specific tuning in\nBioMistral-7B-DARE boosts safety (0.90) despite its smaller scale. Few-shot\nprompting improves accuracy from 78% to 85%, and all models show reduced\nhelpfulness on complex queries, highlighting ongoing challenges in clinical QA.",
    "published": "2025-07-01T06:39:39Z",
    "updated": "2025-07-01T06:39:39Z",
    "id": "2507.02983v1",
    "authors": [
      "Mohammad Anas Azeez",
      "Rafiq Ali",
      "Ebad Shabbir",
      "Zohaib Hasan Siddiqui",
      "Gautam Siddharth Kashyap",
      "Jiechao Gao",
      "Usman Naseem"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02983v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02983v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02983v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in medical question answering, focusing on their performance in terms of factual accuracy, usefulness, and safety. It involves benchmarking different LLMs, which aligns with the topics of LLM research and Benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.02982v1": {
    "title": "We Need Knowledge Distillation for Solving Math Word Problems",
    "summary": "The enhancement of mathematical capabilities in large language models (LLMs)\nfosters new developments in mathematics education within primary and secondary\nschools, particularly as they relate to intelligent tutoring systems. However,\nLLMs require substantial computational resources, resulting in significant\ncosts in educational contexts. To mitigate this drawback, this paper\ninvestigates the feasibility of compressing LLMs for solving math word problems\n(MWPs). We compress the embedded vectors encoded by BERT and distill a\nconsiderably smaller student model. Our findings indicate that the student\nmodel can maintain nearly 90% of the performance of the teacher model while\nutilizing only 1/12 of its parameters. In addition to achieving high accuracy,\nthe model exhibits strong generalizability, as the compressed vectors perform\nwell across all tasks related to MWPs, and the distillation process is not\ntask-specific. The success of this distillation demonstrates that the\nunderlying principles are generic and not limited to a specific task. We\nfurther explore the reasons behind the compressibility of embedded vectors,\nrevealing that part-of-speech information, rather than entity recognition, is\ncrucial for MWPs, which may significantly contribute to their compressibility.\nThe improvements in efficiency and cost reduction provide substantial value for\nintelligent tutoring systems and significantly advance the field of intelligent\neducation.",
    "published": "2025-07-01T06:34:57Z",
    "updated": "2025-07-01T06:34:57Z",
    "id": "2507.02982v1",
    "authors": [
      "Zhenquan Shen",
      "Xinguo Yu",
      "Xiaotian Cheng",
      "Rao Peng",
      "Hao Ming"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02982v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02982v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02982v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the compression of large language models (LLMs) for solving math word problems, which involves knowledge distillation and focuses on enhancing mathematical capabilities in LLMs. The core topics are related to LLMs and their application in educational contexts, but none of the provided topics directly match the focus on knowledge distillation and math word problems.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.00460v1": {
    "title": "Pitfalls of Evaluating Language Models with Open Benchmarks",
    "summary": "Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer\nstandardized, transparent protocols that facilitate the fair comparison,\nreproducibility, and iterative advancement of Language Models (LMs). However,\ntheir openness also introduces critical and underexplored pitfalls. This study\nexposes these weaknesses by systematically constructing ``cheating'' models --\nsmaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets\n-- which achieve top rankings on a prominent open, holistic benchmark (HELM)\ndespite poor generalization and limited practical utility. Our findings\nunderscore three key insights: \\ca high leaderboard performance on open\nbenchmarks may not always reflect real-world effectiveness; \\cb private or\ndynamic benchmarks must complement open evaluations to safeguard integrity; and\n\\cc a fundamental reevaluation of current benchmarking practices is essential\nto ensure robust and trustworthy LM assessments.",
    "published": "2025-07-01T06:17:48Z",
    "updated": "2025-07-01T06:17:48Z",
    "id": "2507.00460v1",
    "authors": [
      "Md. Najib Hasan",
      "Mohammad Fakhruddin Babar",
      "Souvika Sarkar",
      "Monowar Hasan",
      "Santu Karmaker"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00460v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00460v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00460v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of language models using open benchmarks, highlighting pitfalls and the need for better evaluation practices. It directly relates to benchmarking LLMs and their evaluation metrics.",
    "llm_cls_result": [
      "Benchmark"
    ]
  },
  "2507.02980v1": {
    "title": "Modeling Gene Expression Distributional Shifts for Unseen Genetic\n  Perturbations",
    "summary": "We train a neural network to predict distributional responses in gene\nexpression following genetic perturbations. This is an essential task in\nearly-stage drug discovery, where such responses can offer insights into gene\nfunction and inform target identification. Existing methods only predict\nchanges in the mean expression, overlooking stochasticity inherent in\nsingle-cell data. In contrast, we offer a more realistic view of cellular\nresponses by modeling expression distributions. Our model predicts gene-level\nhistograms conditioned on perturbations and outperforms baselines in capturing\nhigher-order statistics, such as variance, skewness, and kurtosis, at a\nfraction of the training cost. To generalize to unseen perturbations, we\nincorporate prior knowledge via gene embeddings from large language models\n(LLMs). While modeling a richer output space, the method remains competitive in\npredicting mean expression changes. This work offers a practical step towards\nmore expressive and biologically informative models of perturbation effects.",
    "published": "2025-07-01T06:04:28Z",
    "updated": "2025-07-01T06:04:28Z",
    "id": "2507.02980v1",
    "authors": [
      "Kalyan Ramakrishnan",
      "Jonathan G. Hedley",
      "Sisi Qu",
      "Puneet K. Dokania",
      "Philip H. S. Torr",
      "Cesar A. Prada-Medina",
      "Julien Fauqueur",
      "Kaspar Martens"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02980v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02980v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02980v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on predicting gene expression distributional shifts using neural networks and incorporates gene embeddings from large language models (LLMs) for generalization to unseen perturbations. The primary focus is on biological applications rather than core LLM research.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.00432v1": {
    "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning",
    "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.",
    "published": "2025-07-01T05:23:05Z",
    "updated": "2025-07-01T05:23:05Z",
    "id": "2507.00432v1",
    "authors": [
      "Maggie Huan",
      "Yuetai Li",
      "Tuney Zheng",
      "Xiaoyu Xu",
      "Seungone Kim",
      "Minxin Du",
      "Radha Poovendran",
      "Graham Neubig",
      "Xiang Yue"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00432v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00432v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00432v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the transferability of reasoning abilities in large language models (LLMs) across different domains, including math, scientific QA, agent planning, coding, and standard instruction-following. It evaluates reasoning-tuned models and examines the impact of different tuning methods like reinforcement learning (RL) and supervised fine-tuning (SFT) on generalization. The study is relevant to reasoning abilities in LLMs and the use of reinforcement learning in LLMs.",
    "llm_cls_result": [
      "Reasoning",
      "RL"
    ]
  },
  "2507.00418v1": {
    "title": "Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI\n  100 Ultra and High-Performance GPUs",
    "summary": "This study presents a benchmarking analysis of the Qualcomm Cloud AI 100\nUltra (QAic) accelerator for large language model (LLM) inference, evaluating\nits energy efficiency (throughput per watt) and performance against leading\nNVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform\n(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90\nbillion parameters, are served using the vLLM framework. The QAic inference\ncards appears to be energy efficient and performs well in the energy efficiency\nmetric in most cases. The findings offer insights into the potential of the\nQualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications\nwithin the National Research Platform (NRP).",
    "published": "2025-07-01T04:11:09Z",
    "updated": "2025-07-01T04:11:09Z",
    "id": "2507.00418v1",
    "authors": [
      "Mohammad Firas Sada",
      "John J. Graham",
      "Elham E Khoda",
      "Mahidhar Tatineni",
      "Dmitry Mishin",
      "Rajesh K. Gupta",
      "Rick Wagner",
      "Larry Smarr",
      "Thomas A. DeFanti",
      "Frank Wrthwein"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00418v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00418v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00418v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking the performance and energy efficiency of Qualcomm Cloud AI 100 Ultra for LLM inference, comparing it with high-performance GPUs. This directly relates to benchmarking LLMs and their performance in high-performance computing environments.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2507.00406v2": {
    "title": "Partnering with AI: A Pedagogical Feedback System for LLM Integration\n  into Programming Education",
    "summary": "Feedback is one of the most crucial components to facilitate effective\nlearning. With the rise of large language models (LLMs) in recent years,\nresearch in programming education has increasingly focused on automated\nfeedback generation to help teachers provide timely support to every student.\nHowever, prior studies often overlook key pedagogical principles, such as\nmastery and progress adaptation, that shape effective feedback strategies. This\npaper introduces a novel pedagogical framework for LLM-driven feedback\ngeneration derived from established feedback models and local insights from\nsecondary school teachers. To evaluate this framework, we implemented a\nweb-based application for Python programming with LLM-based feedback that\nfollows the framework and conducted a mixed-method evaluation with eight\nsecondary-school computer science teachers. Our findings suggest that teachers\nconsider that, when aligned with the framework, LLMs can effectively support\nstudents and even outperform human teachers in certain scenarios through\ninstant and precise feedback. However, we also found several limitations, such\nas its inability to adapt feedback to dynamic classroom contexts. Such a\nlimitation highlights the need to complement LLM-generated feedback with human\nexpertise to ensure effective student learning. This work demonstrates an\neffective way to use LLMs for feedback while adhering to pedagogical standards\nand highlights important considerations for future systems.",
    "published": "2025-07-01T03:48:48Z",
    "updated": "2025-07-16T04:02:40Z",
    "id": "2507.00406v2",
    "authors": [
      "Niklas Scholz",
      "Manh Hung Nguyen",
      "Adish Singla",
      "Tomohiro Nagashima"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00406v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00406v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00406v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of LLMs into programming education for feedback generation, aligning with pedagogical principles. It focuses on the application of LLMs in an educational context, which is a specific use case of LLMs rather than a core research topic on LLMs themselves.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.00378v1": {
    "title": "iPanda: An Intelligent Protocol Testing and Debugging Agent for\n  Conformance Testing",
    "summary": "Conformance testing is essential for ensuring that protocol implementations\ncomply with their specifications. However, traditional testing approaches\ninvolve manually creating numerous test cases and scripts, making the process\nlabor-intensive and inefficient. Recently, Large Language Models (LLMs) have\ndemonstrated impressive text comprehension and code generation abilities,\nproviding promising opportunities for automation. In this paper, we propose\niPanda, the first end-to-end framework that leverages LLMs to automate protocol\nconformance testing. Given a protocol specification document and its\nimplementation, iPanda first employs a keyword-based method to automatically\ngenerate comprehensive test cases. Then, it utilizes a code-based\nretrieval-augmented generation approach to effectively interpret the\nimplementation and produce executable test code. To further enhance code\nquality, iPanda incorporates an iterative self-correction mechanism to refine\ngenerated test scripts interactively. Finally, by executing and analyzing the\ngenerated tests, iPanda systematically verifies compliance between\nimplementations and protocol specifications. Comprehensive experiments on\nvarious protocols show that iPanda significantly outperforms pure LLM-based\napproaches, improving the success rate (Pass@1) of test-code generation by\nfactors ranging from 4.675 times to 10.751 times.",
    "published": "2025-07-01T02:27:44Z",
    "updated": "2025-07-01T02:27:44Z",
    "id": "2507.00378v1",
    "authors": [
      "Xikai Sun",
      "Fan Dang",
      "Kebin Liu",
      "Xin Miao",
      "Zihao Yang",
      "Haimo Lu",
      "Yawen Zheng",
      "Yunhao Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00378v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00378v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00378v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for automating protocol conformance testing, which involves text comprehension and code generation abilities of LLMs. It also mentions the use of retrieval-augmented generation, which is related to memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.00355v1": {
    "title": "Question Decomposition for Retrieval-Augmented Generation",
    "summary": "Grounding large language models (LLMs) in verifiable external sources is a\nwell-established strategy for generating reliable answers. Retrieval-augmented\ngeneration (RAG) is one such approach, particularly effective for tasks like\nquestion answering: it retrieves passages that are semantically related to the\nquestion and then conditions the model on this evidence. However, multi-hop\nquestions, such as \"Which company among NVIDIA, Apple, and Google made the\nbiggest profit in 2023?,\" challenge RAG because relevant facts are often\ndistributed across multiple documents rather than co-occurring in one source,\nmaking it difficult for standard RAG to retrieve sufficient information. To\naddress this, we propose a RAG pipeline that incorporates question\ndecomposition: (i) an LLM decomposes the original query into sub-questions,\n(ii) passages are retrieved for each sub-question, and (iii) the merged\ncandidate pool is reranked to improve the coverage and precision of the\nretrieved evidence. We show that question decomposition effectively assembles\ncomplementary documents, while reranking reduces noise and promotes the most\nrelevant passages before answer generation. Although reranking itself is\nstandard, we show that pairing an off-the-shelf cross-encoder reranker with\nLLM-driven question decomposition bridges the retrieval gap on multi-hop\nquestions and provides a practical, drop-in enhancement, without any extra\ntraining or specialized indexing. We evaluate our approach on the MultiHop-RAG\nand HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy\n(F1: +11.6%) over standard RAG baselines.",
    "published": "2025-07-01T01:01:54Z",
    "updated": "2025-07-01T01:01:54Z",
    "id": "2507.00355v1",
    "authors": [
      "Paul J. L. Ammann",
      "Jonas Golde",
      "Alan Akbik"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00355v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00355v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00355v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing Retrieval-Augmented Generation (RAG) by incorporating question decomposition and reranking to improve retrieval and answer accuracy, which aligns with topics related to Memory (retrieval-augmented generation) and Reasoning (question decomposition for complex problem solving).",
    "llm_cls_result": [
      "Memory",
      "Reasoning"
    ]
  },
  "2507.00319v1": {
    "title": "When Digital Twins Meet Large Language Models: Realistic, Interactive,\n  and Editable Simulation for Autonomous Driving",
    "summary": "Simulation frameworks have been key enablers for the development and\nvalidation of autonomous driving systems. However, existing methods struggle to\ncomprehensively address the autonomy-oriented requirements of balancing: (i)\ndynamical fidelity, (ii) photorealistic rendering, (iii) context-relevant\nscenario orchestration, and (iv) real-time performance. To address these\nlimitations, we present a unified framework for creating and curating\nhigh-fidelity digital twins to accelerate advancements in autonomous driving\nresearch. Our framework leverages a mix of physics-based and data-driven\ntechniques for developing and simulating digital twins of autonomous vehicles\nand their operating environments. It is capable of reconstructing real-world\nscenes and assets (real2sim) with geometric and photorealistic accuracy and\ninfusing them with various physical properties to enable real-time dynamical\nsimulation of the ensuing driving scenarios. Additionally, it also incorporates\na large language model (LLM) interface to flexibly edit the driving scenarios\nonline via natural language prompts. We analyze the presented framework in\nterms of its fidelity, performance, and serviceability. Results indicate that\nour framework can reconstruct 3D scenes and assets with up to 97% structural\nsimilarity, while maintaining frame rates above 60 Hz. We also demonstrate that\nit can handle natural language prompts to generate diverse driving scenarios\nwith up to 95% repeatability and 85% generalizability.",
    "published": "2025-06-30T23:23:24Z",
    "updated": "2025-06-30T23:23:24Z",
    "id": "2507.00319v1",
    "authors": [
      "Tanmay Vilas Samak",
      "Chinmay Vilas Samak",
      "Bing Li",
      "Venkat Krovi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00319v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00319v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00319v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with digital twins for autonomous driving simulation, focusing on the use of LLMs for scenario editing via natural language prompts.",
    "llm_cls_result": [
      "LLM",
      "VLA"
    ]
  },
  "2507.00316v2": {
    "title": "$^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for\n  Radiology Report Generation",
    "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale\n$\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The\nnovel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasets demonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited\ndata for RRG tasks. At the same time, for prompt engineering, we introduce a\nfive-stage, LLM-driven pipeline that converts routine CT reports into paired\nvisual-question-answer triples and citation-linked reasoning narratives,\ncreating a scalable, high-quality supervisory corpus for explainable multimodal\nradiology LLM. All code, datasets, and models will be publicly available in our\nofficial repository. https://github.com/Siyou-Li/u2Tokenizer",
    "published": "2025-06-30T23:14:49Z",
    "updated": "2025-07-02T01:08:41Z",
    "id": "2507.00316v2",
    "authors": [
      "Siyou Li",
      "Pengyao Qin",
      "Huanan Wu",
      "Dong Nie",
      "Arun J. Thirunavukarasu",
      "Juntao Yu",
      "Le Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00316v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00316v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00316v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a multimodal large language model (MLLM) for radiology report generation, integrating multi-modal features and utilizing a novel tokenizer. It also involves fine-tuning and direct preference optimization, which are relevant to multimodal LLMs and their applications.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Dataset"
    ]
  },
  "2507.00310v1": {
    "title": "Open-ended Scientific Discovery via Bayesian Surprise",
    "summary": "The promise of autonomous scientific discovery (ASD) hinges not only on\nanswering questions, but also on knowing which questions to ask. Most recent\nworks in ASD explore the use of large language models (LLMs) in goal-driven\nsettings, relying on human-specified research questions to guide hypothesis\ngeneration. However, scientific discovery may be accelerated further by\nallowing the AI system to drive exploration by its own criteria. The few\nexisting approaches in open-ended ASD select hypotheses based on diversity\nheuristics or subjective proxies for human interestingness, but the former\nstruggles to meaningfully navigate the typically vast hypothesis space, and the\nlatter suffers from imprecise definitions. This paper presents AutoDS -- a\nmethod for open-ended ASD that instead drives scientific exploration using\nBayesian surprise. Here, we quantify the epistemic shift from the LLM's prior\nbeliefs about a hypothesis to its posterior beliefs after gathering\nexperimental results. To efficiently explore the space of nested hypotheses,\nour method employs a Monte Carlo tree search (MCTS) strategy with progressive\nwidening using surprisal as the reward function. We evaluate AutoDS in the\nsetting of data-driven discovery across 21 real-world datasets spanning domains\nsuch as biology, economics, finance, and behavioral science. Our results\ndemonstrate that under a fixed budget, AutoDS substantially outperforms\ncompetitors by producing 5--29\\% more discoveries deemed surprising by the LLM.\nOur human evaluation further finds that two-thirds of AutoDS discoveries are\nsurprising to the domain experts, suggesting this is an important step forward\ntowards building open-ended ASD systems.",
    "published": "2025-06-30T22:53:59Z",
    "updated": "2025-06-30T22:53:59Z",
    "id": "2507.00310v1",
    "authors": [
      "Dhruv Agarwal",
      "Bodhisattwa Prasad Majumder",
      "Reece Adamson",
      "Megha Chakravorty",
      "Satvika Reddy Gavireddy",
      "Aditya Parashar",
      "Harshit Surana",
      "Bhavana Dalvi Mishra",
      "Andrew McCallum",
      "Ashish Sabharwal",
      "Peter Clark"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00310v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00310v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00310v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in autonomous scientific discovery (ASD) and focuses on open-ended exploration driven by Bayesian surprise. It involves LLMs in generating hypotheses and evaluating them, which aligns with research on LLMs and their applications in scientific reasoning and discovery.",
    "llm_cls_result": [
      "LLM",
      "AGI",
      "Reasoning"
    ]
  },
  "2507.00263v1": {
    "title": "Room Scene Discovery and Grouping in Unstructured Vacation Rental Image\n  Collections",
    "summary": "The rapid growth of vacation rental (VR) platforms has led to an increasing\nvolume of property images, often uploaded without structured categorization.\nThis lack of organization poses significant challenges for travelers attempting\nto understand the spatial layout of a property, particularly when multiple\nrooms of the same type are present. To address this issue, we introduce an\neffective approach for solving the room scene discovery and grouping problem,\nas well as identifying bed types within each bedroom group. This grouping is\nvaluable for travelers to comprehend the spatial organization, layout, and the\nsleeping configuration of the property. We propose a computationally efficient\nmachine learning pipeline characterized by low latency and the ability to\nperform effectively with sample-efficient learning, making it well-suited for\nreal-time and data-scarce environments. The pipeline integrates a supervised\nroom-type detection model, a supervised overlap detection model to identify the\noverlap similarity between two images, and a clustering algorithm to group the\nimages of the same space together using the similarity scores. Additionally,\nthe pipeline maps each bedroom group to the corresponding bed types specified\nin the property's metadata, based on the visual content present in the group's\nimages using a Multi-modal Large Language Model (MLLM) model. We evaluate the\naforementioned models individually and also assess the pipeline in its\nentirety, observing strong performance that significantly outperforms\nestablished approaches such as contrastive learning and clustering with\npretrained embeddings.",
    "published": "2025-06-30T21:11:35Z",
    "updated": "2025-06-30T21:11:35Z",
    "id": "2507.00263v1",
    "authors": [
      "Vignesh Ram Nithin Kappagantula",
      "Shayan Hassantabar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00263v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00263v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00263v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a Multi-modal Large Language Model (MLLM) for identifying bed types within bedroom groups, which aligns with the MLLM topic. The focus on image grouping and scene discovery also relates to vision-language tasks, fitting the VLA topic.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.02976v2": {
    "title": "Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on\n  SWE-bench",
    "summary": "Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools.",
    "published": "2025-06-30T21:10:19Z",
    "updated": "2025-07-24T15:50:13Z",
    "id": "2507.02976v2",
    "authors": [
      "Amirali Sajadi",
      "Kostadin Damevski",
      "Preetha Chatterjee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02976v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02976v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02976v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the security analysis of patches generated by Large Language Models (LLMs) and their agentic frameworks in real-world software development contexts, specifically using the SWE-bench dataset. It evaluates the security of these patches and compares them to developer-written patches, highlighting the vulnerabilities introduced by LLMs and agents.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Benchmark"
    ]
  },
  "2507.00258v1": {
    "title": "Impact of Fine-Tuning Methods on Memorization in Large Language Models",
    "summary": "As the capabilities of pre-trained large language models (LLMs) continue to\nadvance, the \"pre-train and fine-tune\" paradigm has become increasingly\nmainstream, leading to the development of various fine-tuning methods. However,\nthe privacy risks arising from memorization during fine-tuning have received\nrelatively little attention. To address this gap, we categorize popular\nfine-tuning approaches and assess their impact on memorization through the lens\nof membership inference attacks (MIAs). Our results show that, compared to\nparameter-based fine-tuning, prompt-based fine-tuning achieves competitive\nperformance while exhibiting lower vulnerability to MIAs. Furthermore,\nprompt-based methods maintain low memorization regardless of model scale. These\nfindings suggest that parameter-based fine-tuning is more prone to leaking\nprivate information, whereas prompt-based fine-tuning serves as a more\nprivacy-preserving option.",
    "published": "2025-06-30T20:52:15Z",
    "updated": "2025-06-30T20:52:15Z",
    "id": "2507.00258v1",
    "authors": [
      "Jie Hou",
      "Chuxiong Wu",
      "Lannan Luo",
      "Qiang Zeng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00258v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00258v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00258v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of fine-tuning methods on memorization in large language models, which directly relates to the topics of LLM (Large Language Models) and Memory (specifically LLM memory and privacy risks). The study also touches on pretraining strategies, though the primary focus is on fine-tuning and memorization.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Pretrain"
    ]
  },
  "2507.00217v1": {
    "title": "CrossPipe: Towards Optimal Pipeline Schedules for Cross-Datacenter\n  Training",
    "summary": "Training large language models (LLMs) now requires resources that exceed a\nsingle datacenter, making cross-datacenter strategies increasingly crucial. We\npresent CrossPipe, a framework designed to optimize model training across\ngeographically distributed datacenters by explicitly modeling and mitigating\nthe impact of network latency and limited bandwidth. It enables unified\nanalysis and optimization incorporating both pipeline parallelism (PP) and\nopportunities for overlapping data parallelism (DP) communication. CrossPipe\ngenerates optimized pipeline schedules using either solver-based optimal or\nfast near-optimal greedy algorithms, built upon a flexible execution engine\nthat separates scheduling logic from communication details. Our evaluation\nshows that CrossPipe reduces training time by up to 33.6\\% compared to\ntraditional pipeline schedules under identical memory constraints. When memory\nconstraints are relaxed, CrossPipe maintains strong performance despite\ncommunication delays, approaching the efficiency of idealized schedules without\ndelays. CrossPipe offers improved scalability and resource utilization,\nparticularly in environments with high network latency or limited bandwidth.",
    "published": "2025-06-30T19:38:27Z",
    "updated": "2025-06-30T19:38:27Z",
    "id": "2507.00217v1",
    "authors": [
      "Tiancheng Chen",
      "Ales Kubicek",
      "Langwen Huang",
      "Torsten Hoefler"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00217v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00217v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00217v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing training of large language models (LLMs) across geographically distributed datacenters, which involves scaling and resource utilization challenges. It does not directly address the core topics like LLM architectures, reasoning, or multimodal aspects.",
    "llm_cls_result": [
      "Scaling"
    ]
  },
  "2507.00214v1": {
    "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with\n  LLM-Generated Reasoning",
    "summary": "Standard classification models often map inputs directly to labels without\nexplicit reasoning, potentially limiting their performance, robustness, and\ninterpretability. This paper introduces a novel two-stage approach to enhance\ntext classification by leveraging Large Language Model (LLM)-generated\nreasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model\n(henceforth Llama-R-Gen) on a general-purpose reasoning dataset\n(syvai/reasoning-gen) to generate textual reasoning (R) given a question and\nits answer. In the second stage, this generally trained Llama-R-Gen is used\noffline to create an augmented training dataset for a downstream generative\nmodel. This downstream model, based on Llama-3.2-1B-Instruct, takes only the\ninput text (Q) and is trained to output the generated reasoning (R) immediately\nfollowed by the predicted emotion (A). We demonstrate this methodology on the\ndair-ai/emotion dataset for emotion classification. Our experiments show that\nthe generative model trained to output reasoning and the emotion (Classifier\nQ->RA) achieves a significant improvement of 8.7 percentage points in accuracy\n(for emotion prediction) compared to a baseline generative model trained solely\nto output the emotion (Classifier Q->A), highlighting the strong generalization\ncapabilities of the reasoning generation and the benefit of explicit reasoning\ntraining. This work underscores the potential of LLM-generated reasonings for\ncreating richer training datasets, thereby improving the performance of diverse\ndownstream NLP tasks and providing explicit explanations.",
    "published": "2025-06-30T19:34:57Z",
    "updated": "2025-06-30T19:34:57Z",
    "id": "2507.00214v1",
    "authors": [
      "Mads Henrichsen",
      "Rasmus Krebs"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00214v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00214v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00214v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging LLM-generated reasoning to improve text classification, which involves reasoning abilities in LLMs and the use of Large Language Models.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.00210v1": {
    "title": "LineRetriever: Planning-Aware Observation Reduction for Web Agents",
    "summary": "While large language models have demonstrated impressive capabilities in web\nnavigation tasks, the extensive context of web pages, often represented as DOM\nor Accessibility Tree (AxTree) structures, frequently exceeds model context\nlimits. Current approaches like bottom-up truncation or embedding-based\nretrieval lose critical information about page state and action history. This\nis particularly problematic for adaptive planning in web agents, where\nunderstanding the current state is essential for determining future actions. We\nhypothesize that embedding models lack sufficient capacity to capture\nplan-relevant information, especially when retrieving content that supports\nfuture action prediction. This raises a fundamental question: how can retrieval\nmethods be optimized for adaptive planning in web navigation tasks? In\nresponse, we introduce \\textit{LineRetriever}, a novel approach that leverages\na language model to identify and retrieve observation lines most relevant to\nfuture navigation steps. Unlike traditional retrieval methods that focus solely\non semantic similarity, \\textit{LineRetriever} explicitly considers the\nplanning horizon, prioritizing elements that contribute to action prediction.\nOur experiments demonstrate that \\textit{LineRetriever} can reduce the size of\nthe observation at each step for the web agent while maintaining consistent\nperformance within the context limitations.",
    "published": "2025-06-30T19:24:45Z",
    "updated": "2025-06-30T19:24:45Z",
    "id": "2507.00210v1",
    "authors": [
      "Imene Kerboua",
      "Sahar Omidi Shayegan",
      "Megh Thakkar",
      "Xing Han L",
      "Massimo Caccia",
      "Vronique Eglin",
      "Alexandre Aussem",
      "Jrmy Espinas",
      "Alexandre Lacoste"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00210v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00210v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00210v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models in web navigation tasks, focusing on optimizing retrieval methods for adaptive planning. It addresses the limitations of current approaches and introduces a novel method to improve performance within context limits.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Memory"
    ]
  },
  "2507.00181v1": {
    "title": "ChatGPT produces more \"lazy\" thinkers: Evidence of cognitive engagement\n  decline",
    "summary": "Despite the increasing use of large language models (LLMs) in education,\nconcerns have emerged about their potential to reduce deep thinking and active\nlearning. This study investigates the impact of generative artificial\nintelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of\nstudents during academic writing tasks. The study employed an experimental\ndesign with participants randomly assigned to either an AI-assisted (ChatGPT)\nor a non-assisted (control) condition. Participants completed a structured\nargumentative writing task followed by a cognitive engagement scale (CES), the\nCES-AI, developed to assess mental effort, attention, deep processing, and\nstrategic thinking. The results revealed significantly lower cognitive\nengagement scores in the ChatGPT group compared to the control group. These\nfindings suggest that AI assistance may lead to cognitive offloading. The study\ncontributes to the growing body of literature on the psychological implications\nof AI in education and raises important questions about the integration of such\ntools into academic practice. It calls for pedagogical strategies that promote\nactive, reflective engagement with AI-generated content to avoid compromising\nself-regulated learning and deep cognitive involvement of students.",
    "published": "2025-06-30T18:41:50Z",
    "updated": "2025-06-30T18:41:50Z",
    "id": "2507.00181v1",
    "authors": [
      "Georgios P. Georgiou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00181v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00181v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00181v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of ChatGPT, a large language model (LLM), on cognitive engagement in educational settings, which is related to the broader research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.02975v1": {
    "title": "Introducing Answered with Evidence -- a framework for evaluating whether\n  LLM responses to biomedical questions are founded in evidence",
    "summary": "The growing use of large language models (LLMs) for biomedical question\nanswering raises concerns about the accuracy and evidentiary support of their\nresponses. To address this, we present Answered with Evidence, a framework for\nevaluating whether LLM-generated answers are grounded in scientific literature.\nWe analyzed thousands of physician-submitted questions using a comparative\npipeline that included: (1) Alexandria, fka the Atropos Evidence Library, a\nretrieval-augmented generation (RAG) system based on novel observational\nstudies, and (2) two PubMed-based retrieval-augmented systems (System and\nPerplexity). We found that PubMed-based systems provided evidence-supported\nanswers for approximately 44% of questions, while the novel evidence source did\nso for about 50%. Combined, these sources enabled reliable answers to over 70%\nof biomedical queries. As LLMs become increasingly capable of summarizing\nscientific content, maximizing their value will require systems that can\naccurately retrieve both published and custom-generated evidence or generate\nsuch evidence in real time.",
    "published": "2025-06-30T18:00:52Z",
    "updated": "2025-06-30T18:00:52Z",
    "id": "2507.02975v1",
    "authors": [
      "Julian D Baldwin",
      "Christina Dinh",
      "Arjun Mukerji",
      "Neil Sanghavi",
      "Saurabh Gombar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02975v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02975v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02975v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the evaluation of LLM responses in the biomedical domain, focusing on evidence-based answers and retrieval-augmented generation (RAG) systems. This aligns with topics related to LLM evaluation and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Benchmark"
    ]
  },
  "2506.24124v2": {
    "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual\n  and Textual Perspectives",
    "summary": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.",
    "published": "2025-06-30T17:59:14Z",
    "updated": "2025-07-01T03:40:22Z",
    "id": "2506.24124v2",
    "authors": [
      "Sixun Dong",
      "Wei Fan",
      "Teresa Wu",
      "Yanjie Fu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.24124v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.24124v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.24124v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multimodal contrastive learning framework that transforms time series data into visual and textual perspectives, aligning them in a shared semantic space. This involves the use of large language models (LLMs) and multimodal learning, which are key topics in the provided list.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "VLA"
    ]
  },
  "2506.24123v1": {
    "title": "Calligrapher: Freestyle Text Image Customization",
    "summary": "We introduce Calligrapher, a novel diffusion-based framework that\ninnovatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges of\nprecise style control and data dependency in typographic customization, our\nframework incorporates three key technical contributions. First, we develop a\nself-distillation mechanism that leverages the pre-trained text-to-image\ngenerative model itself alongside the large language model to automatically\nconstruct a style-centric typography benchmark. Second, we introduce a\nlocalized style injection framework via a trainable style encoder, which\ncomprises both Qformer and linear layers, to extract robust style features from\nreference images. An in-context generation mechanism is also employed to\ndirectly embed reference images into the denoising process, further enhancing\nthe refined alignment of target styles. Extensive quantitative and qualitative\nevaluations across diverse fonts and design contexts confirm Calligrapher's\naccurate reproduction of intricate stylistic details and precise glyph\npositioning. By automating high-quality, visually consistent typography,\nCalligrapher surpasses traditional models, empowering creative practitioners in\ndigital art, branding, and contextual typographic design.",
    "published": "2025-06-30T17:59:06Z",
    "updated": "2025-06-30T17:59:06Z",
    "id": "2506.24123v1",
    "authors": [
      "Yue Ma",
      "Qingyan Bai",
      "Hao Ouyang",
      "Ka Leong Cheng",
      "Qiuyu Wang",
      "Hongyu Liu",
      "Zichen Liu",
      "Haofan Wang",
      "Jingye Chen",
      "Yujun Shen",
      "Qifeng Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.24123v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.24123v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.24123v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a diffusion-based framework for text image customization, leveraging large language models and style injection techniques, but does not directly align with the provided topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.24120v1": {
    "title": "Data Uniformity Improves Training Efficiency and More, with a\n  Convergence Framework Beyond the NTK Regime",
    "summary": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.",
    "published": "2025-06-30T17:58:30Z",
    "updated": "2025-06-30T17:58:30Z",
    "id": "2506.24120v1",
    "authors": [
      "Yuqing Wang",
      "Shangding Gu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.24120v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.24120v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.24120v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses data selection principles for improving training efficiency and performance in large language models (LLMs), which is relevant to the topic of LLM research. It also touches on optimization strategies and model scaling, which are related to the Scaling topic.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.24102v1": {
    "title": "DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World",
    "summary": "Multimodal Large Language Models (MLLMs) demonstrate a complex understanding\nof scenes, benefiting from large-scale and high-quality datasets. Most existing\ncaption datasets lack the ground locations and relations for visual entities.\nSeveral grounded caption datasets face the problems of missing detailed\ndescriptions, relations, and massive object descriptions on high-resolution\nimages. To fill this gap for the community, we present DenseWorld-1M, the first\nmassive, detailed, dense grounded caption dataset in the real world. We design\na three-stage labeling pipeline, containing open-world perception, detailed\nobject caption generation, and dense caption merging. The first stage obtains\nentity-level masks and labels. The second stage generates the object-level,\ndetailed captions with the guidance of masks and labels from the first stage.\nThe final stage merges object captions and masks into spatial and relational\ndense captions. To accelerate the labeling process and improve caption quality,\nwe present two VLM models: the Detailed Region Caption model and the Spatial\nCaption Merging model. Extensive experiments on various settings, including\nvision-language understanding, visual grounding, and region caption generation,\ndemonstrate the effectiveness of our DenseWorld-1M dataset and labeling models.",
    "published": "2025-06-30T17:51:25Z",
    "updated": "2025-06-30T17:51:25Z",
    "id": "2506.24102v1",
    "authors": [
      "Xiangtai Li",
      "Tao Zhang",
      "Yanwei Li",
      "Haobo Yuan",
      "Shihao Chen",
      "Yikang Zhou",
      "Jiahao Meng",
      "Yueyi Sun",
      "Shilin Xu",
      "Lu Qi",
      "Tianheng Cheng",
      "Yi Lin",
      "Zilong Huang",
      "Wenhao Huang",
      "Jiashi Feng",
      "Guang Shi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.24102v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.24102v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.24102v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a new dataset for Multimodal Large Language Models (MLLMs) and discusses the development of VLM models for detailed captioning, which aligns with the topics of MLLM and VLA.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.00108v1": {
    "title": "Teaching Programming in the Age of Generative AI: Insights from\n  Literature, Pedagogical Proposals, and Student Perspectives",
    "summary": "Computer programming is undergoing a true transformation driven by powerful\nnew tools for automatic source code generation based on large language models.\nThis transformation is also manifesting in introductory programming courses at\nuniversities around the world, generating an in-depth debate about how\nprogramming content should be taught, learned, and assessed in the context of\ngenerative artificial intelligence.\n  This article aims, on the one hand, to review the most relevant studies on\nthis issue, highlighting the advantages and disadvantages identified in the\nspecialized literature. On the other hand, it proposes enriching teaching and\nlearning methodologies by focusing on code comprehension and execution rather\nthan on mere coding or program functionality. In particular, it advocates for\nthe use of visual representations of code and visual simulations of its\nexecution as effective tools for teaching, learning, and assessing programming,\nthus fostering a deeper understanding among students.\n  Finally, the opinions of students who took the object-oriented programming\ncourse are presented to provide preliminary context supporting the\nincorporation of visual simulations in Java (or other languages) as part of the\ntraining process.",
    "published": "2025-06-30T17:38:27Z",
    "updated": "2025-06-30T17:38:27Z",
    "id": "2507.00108v1",
    "authors": [
      "Clemente Rubio-Manzano",
      "Jazna Meza",
      "Rodolfo Fernandez-Santibanez",
      "Christian Vidal-Castro"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00108v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00108v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00108v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of large language models on programming education, focusing on teaching methodologies and student perspectives, but does not directly align with the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.24045v1": {
    "title": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on\n  Heterogeneous SoC",
    "summary": "The proliferation of agentic Large Language Models (LLMs) on personal devices\nintroduces a new class of workloads characterized by a dichotomy of objectives.\nReactive tasks, initiated by users, demand immediate, low-latency responses,\nwhile proactive tasks operate invisibly and prioritize throughput. Existing\non-device LLM engines, designed for isolated inferences, fail to efficiently\nmanage these concurrent and conflicting requests on consumer-grade\nheterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces\nAgent.xpu, an efficient serving system for agentic LLM workloads on\nmemory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu\nfirst constructs a heterogeneous execution graph, which fuses and chunks model\nkernels for affinity-guided, elastic accelerator mapping with predictive kernel\nannotation. At runtime, its online scheduler enables fine-grained, kernel-level\npreemption to guarantee the responsiveness of reactive tasks. To maximize SoC\nutilization, it adopts slack-aware kernel backfill to opportunistically append\nproactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware\ndispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves\n4.6$\\times$ lower latency for reactive tasks and sustains\n1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to\nstate-of-the-art inference engines.",
    "published": "2025-06-30T16:50:48Z",
    "updated": "2025-06-30T16:50:48Z",
    "id": "2506.24045v1",
    "authors": [
      "Xinming Wei",
      "Jiahao Zhang",
      "Haoran Li",
      "Jiayu Chen",
      "Rui Qu",
      "Maoliang Li",
      "Xiang Chen",
      "Guojie Luo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.24045v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.24045v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.24045v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the efficient scheduling of agentic Large Language Model (LLM) workloads on heterogeneous SoCs, which involves both reactive and proactive tasks. This aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning, particularly in the context of agentic LLMs). The focus on scheduling and efficiency also touches on the broader topic of AGI (Artificial General Intelligence) as it pertains to optimizing LLM performance in real-world applications.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2506.24016v1": {
    "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with\n  Structured Explanations",
    "summary": "Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT.",
    "published": "2025-06-30T16:20:51Z",
    "updated": "2025-06-30T16:20:51Z",
    "id": "2506.24016v1",
    "authors": [
      "Hyunjong Kim",
      "Sangyeop Kim",
      "Jongheon Jeong",
      "Yeongjae Cho",
      "Sungzoon Cho"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.24016v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.24016v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.24016v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on an explainable evaluation metric for image captioning, which involves vision-language models and structured explanations. It also mentions the use of large-scale datasets and benchmark datasets.",
    "llm_cls_result": [
      "VLA",
      "Benchmark",
      "Dataset"
    ]
  },
  "2506.24006v1": {
    "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping\n  Review from a Mathematics Education Perspective",
    "summary": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms.",
    "published": "2025-06-30T16:10:42Z",
    "updated": "2025-06-30T16:10:42Z",
    "id": "2506.24006v1",
    "authors": [
      "Anselm R. Strohmaier",
      "Wim Van Dooren",
      "Kathrin Seler",
      "Brian Greer",
      "Lieven Verschaffel"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.24006v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.24006v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.24006v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the evaluation of Large Language Models (LLMs) in solving mathematical word problems and their potential integration into education. It discusses the limitations of LLMs in understanding real-world contexts and their implications for instructional tools in mathematics classrooms.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2506.23979v1": {
    "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference\n  Data Generation",
    "summary": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger.",
    "published": "2025-06-30T15:45:28Z",
    "updated": "2025-06-30T15:45:28Z",
    "id": "2506.23979v1",
    "authors": [
      "Renren Jin",
      "Tianhao Shen",
      "Xinwei Wu",
      "Dan Shi",
      "Haoran Sun",
      "Wuwei Huang",
      "Quandong Wang",
      "Wei Liu",
      "Jian Luan",
      "Bin Wang",
      "Deyi Xiong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23979v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23979v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23979v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on generating high-quality datasets for supervised and preference fine-tuning of large language models (LLMs), which is closely related to the topics of LLM and RL (specifically RLHF). The framework aims to improve the alignment of LLMs with human preferences, a key aspect of RLHF.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.23951v1": {
    "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction\n  of influential and interpretable concepts with Sparse Autoencoders",
    "summary": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features.",
    "published": "2025-06-30T15:18:50Z",
    "updated": "2025-06-30T15:18:50Z",
    "id": "2506.23951v1",
    "authors": [
      "Mathis Le Bail",
      "Jrmie Dentan",
      "Davide Buscaldi",
      "Sonia Vanier"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23951v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23951v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23951v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Sparse Autoencoders (SAEs) to probe Large Language Models (LLMs) for interpretable concepts in the context of text classification. It involves LLMs and their internal representations, which aligns with the 'LLM' topic. The use of SAEs and interpretability methods also touches on 'Reasoning' as it involves understanding and explaining the decision-making processes of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23949v1": {
    "title": "AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and\n  Foundation Models",
    "summary": "Increasingly multi-purpose AI models, such as cutting-edge large language\nmodels or other 'general-purpose AI' (GPAI) models, 'foundation models,'\ngenerative AI models, and 'frontier models' (typically all referred to\nhereafter with the umbrella term 'GPAI/foundation models' except where greater\nspecificity is needed), can provide many beneficial capabilities but also risks\nof adverse events with profound consequences. This document provides\nrisk-management practices or controls for identifying, analyzing, and\nmitigating risks of GPAI/foundation models. We intend this document primarily\nfor developers of large-scale, state-of-the-art GPAI/foundation models; others\nthat can benefit from this guidance include downstream developers of end-use\napplications that build on a GPAI/foundation model. This document facilitates\nconformity with or use of leading AI risk management-related standards,\nadapting and building on the generic voluntary guidance in the NIST AI Risk\nManagement Framework and ISO/IEC 23894, with a focus on the unique issues faced\nby developers of GPAI/foundation models.",
    "published": "2025-06-30T15:18:18Z",
    "updated": "2025-06-30T15:18:18Z",
    "id": "2506.23949v1",
    "authors": [
      "Anthony M. Barrett",
      "Jessica Newman",
      "Brandie Nonnecke",
      "Nada Madkour",
      "Dan Hendrycks",
      "Evan R. Murphy",
      "Krystal Jackson",
      "Deepika Raman"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23949v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23949v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23949v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses risk-management practices for general-purpose AI and foundation models, which are closely related to the development and scaling of large language models and their applications. It does not directly fit into the specific technical topics provided but is relevant to the broader context of AI development and risk management.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.23940v2": {
    "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy\n  for MLLMs",
    "summary": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs.",
    "published": "2025-06-30T15:07:41Z",
    "updated": "2025-07-01T03:26:52Z",
    "id": "2506.23940v2",
    "authors": [
      "Yang Dai",
      "Jianxiang An",
      "Tianwei Lin",
      "Hongyang He",
      "Hongzhe Huang",
      "Wenqiao Zhang",
      "Zheqi Lv",
      "Siliang Tang",
      "Yueting Zhuang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23940v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23940v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23940v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Multimodal Large Language Models (MLLMs) and proposes a framework for integrating domain-specific knowledge through parameter synergy, which aligns with the topics of MLLM and Reasoning.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2506.23930v1": {
    "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection\n  in Low-Resource Languages",
    "summary": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.",
    "published": "2025-06-30T14:59:25Z",
    "updated": "2025-06-30T14:59:25Z",
    "id": "2506.23930v1",
    "authors": [
      "Ruhina Tabasshum Prome",
      "Tarikul Islam Tamiti",
      "Anomadarshi Barua"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23930v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23930v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23930v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging prompt engineering techniques with large language models (LLMs) for hate speech detection, particularly in low-resource languages. It involves the use of LLMs and various prompting strategies, which are central to the LLM topic. The research also involves evaluating different models and embeddings, which aligns with the broader scope of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23924v1": {
    "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems:\n  From Theory to Practice",
    "summary": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.",
    "published": "2025-06-30T14:54:15Z",
    "updated": "2025-06-30T14:54:15Z",
    "id": "2506.23924v1",
    "authors": [
      "Akshit Kumar",
      "Tianyi Peng",
      "Yuhang Wu",
      "Assaf Zeevi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23924v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23924v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23924v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the performance of Large Language Models (LLMs) in solving stochastic modeling problems in Operations Research, which involves uncertainty and tools from probability, statistics, and stochastic processes. The study tests LLMs' abilities on graduate-level problems and real-world decision-making under uncertainty, highlighting their proficiency and potential for automation in OR.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23921v2": {
    "title": "The Trilemma of Truth in Large Language Models",
    "summary": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge.",
    "published": "2025-06-30T14:49:28Z",
    "updated": "2025-07-08T21:09:56Z",
    "id": "2506.23921v2",
    "authors": [
      "Germans Savcisens",
      "Tina Eliassi-Rad"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23921v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23921v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23921v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the veracity of knowledge in Large Language Models (LLMs) and introduces a method to assess it, which is directly related to LLM research. It also touches on reinforcement learning from human feedback (RLHF), which is a sub-topic under RL.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.02966v2": {
    "title": "PBa-LLM: Privacy- and Bias-aware NLP using Named-Entity Recognition\n  (NER)",
    "summary": "The use of Natural Language Processing (NLP) in highstakes AI-based\napplications has increased significantly in recent years, especially since the\nemergence of Large Language Models (LLMs). However, despite their strong\nperformance, LLMs introduce important legal/ ethical concerns, particularly\nregarding privacy, data protection, and transparency. Due to these concerns,\nthis work explores the use of Named- Entity Recognition (NER) to facilitate the\nprivacy-preserving training (or adaptation) of LLMs. We propose a framework\nthat uses NER technologies to anonymize sensitive information in text data,\nsuch as personal identities or geographic locations. An evaluation of the\nproposed privacy-preserving learning framework was conducted to measure its\nimpact on user privacy and system performance in a particular high-stakes and\nsensitive setup: AI-based resume scoring for recruitment processes. The study\ninvolved two language models (BERT and RoBERTa) and six anonymization\nalgorithms (based on Presidio, FLAIR, BERT, and different versions of GPT)\napplied to a database of 24,000 candidate profiles. The findings indicate that\nthe proposed privacy preservation techniques effectively maintain system\nperformance while playing a critical role in safeguarding candidate\nconfidentiality, thus promoting trust in the experimented scenario. On top of\nthe proposed privacy-preserving approach, we also experiment applying an\nexisting approach that reduces the gender bias in LLMs, thus finally obtaining\nour proposed Privacyand Bias-aware LLMs (PBa-LLMs). Note that the proposed\nPBa-LLMs have been evaluated in a particular setup (resume scoring), but are\ngenerally applicable to any other LLM-based AI application.",
    "published": "2025-06-30T14:42:49Z",
    "updated": "2025-07-09T08:02:08Z",
    "id": "2507.02966v2",
    "authors": [
      "Gonzalo Mancera",
      "Aythami Morales",
      "Julian Fierrez",
      "Ruben Tolosana",
      "Alejandro Penna",
      "Miguel Lopez-Duran",
      "Francisco Jurado",
      "Alvaro Ortigosa"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02966v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02966v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02966v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Named-Entity Recognition (NER) for privacy-preserving training of LLMs and reducing bias in LLMs, which aligns with research on Large Language Models (LLM) and their ethical concerns.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23862v1": {
    "title": "Large Language Models for Statistical Inference: Context Augmentation\n  with Applications to the Two-Sample Problem and Regression",
    "summary": "We introduce context augmentation, a data-augmentation approach that uses\nlarge language models (LLMs) to generate contexts around observed strings as a\nmeans of facilitating valid frequentist inference. These generated contexts\nserve to reintroduce uncertainty, incorporate auxiliary information, and\nfacilitate interpretability. For example, in the two-sample test, we compare\nthe log-probability of strings under contexts from its own versus the other\ngroup. We show on synthetic data that the method's t-statistics exhibit the\nexpected null behaviour while maintaining power and, through a replication,\nthat the method is powerful and interpretable. We next introduce text-on-text\nregression. Contexts generated around the predictor string are treated as\nmediating variables between the predictor and outcome strings. Using negative\ncontrols, we then distinguish between semantic and syntactic dimensions of\nprediction. Analysis of real-world dialogic data illustrates behaviour\npredicted from a psycholinguistic framework. Theoretically, we provide\nidentification conditions, derive an influence-function decomposition, and show\nthat repeated cross-fitting of a pivotal statistic yields higher-order\nefficiency. We derive bounds linking estimation error, context count, and\nnumber of cross-fits. Taken together, context augmentation offers the ability\nto connect LLMs with longstanding statistical practice.",
    "published": "2025-06-30T13:57:11Z",
    "updated": "2025-06-30T13:57:11Z",
    "id": "2506.23862v1",
    "authors": [
      "Marc Ratkovic"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23862v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23862v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23862v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for statistical inference, specifically focusing on context augmentation and its applications in statistical problems like the two-sample test and regression. The core focus is on leveraging LLMs for statistical tasks, which aligns with the 'LLM' topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.23850v1": {
    "title": "Email as the Interface to Generative AI Models: Seamless Administrative\n  Automation",
    "summary": "This paper introduces a novel architectural framework that integrates Large\nLanguage Models (LLMs) with email interfaces to automate administrative tasks,\nspecifically targeting accessibility barriers in enterprise environments. The\nsystem connects email communication channels with Optical Character Recognition\n(OCR) and intelligent automation, enabling non-technical administrative staff\nto delegate complex form-filling and document processing tasks using familiar\nemail interfaces. By treating the email body as a natural language prompt and\nattachments as contextual information, the workflow bridges the gap between\nadvanced AI capabilities and practical usability. Empirical evaluation shows\nthat the system can complete complex administrative forms in under 8 seconds of\nautomated processing, with human supervision reducing total staff time by a\nfactor of three to four compared to manual workflows. The top-performing LLM\naccurately filled 16 out of 29 form fields and reduced the total cost per\nprocessed form by 64% relative to manual completion. These findings demonstrate\nthat email-based LLM integration is a viable and cost-effective approach for\ndemocratizing advanced automation in organizational settings, supporting\nwidespread adoption without requiring specialized technical knowledge or major\nworkflow changes. This aligns with broader trends in leveraging LLMs to enhance\naccessibility and automate complex tasks for non-technical users, making\ntechnology more inclusive and efficient.",
    "published": "2025-06-30T13:39:54Z",
    "updated": "2025-06-30T13:39:54Z",
    "id": "2506.23850v1",
    "authors": [
      "Andres Navarro",
      "Carlos de Quinto",
      "Jos Alberto Hernndez"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23850v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23850v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23850v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) with email interfaces to automate administrative tasks, which directly relates to the use of LLMs in practical applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.23844v1": {
    "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents",
    "summary": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop.",
    "published": "2025-06-30T13:34:34Z",
    "updated": "2025-06-30T13:34:34Z",
    "id": "2506.23844v1",
    "authors": [
      "Hang Su",
      "Jun Luo",
      "Chang Liu",
      "Xiao Yang",
      "Yichi Zhang",
      "Yinpeng Dong",
      "Jun Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23844v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23844v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23844v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the security risks and architectural challenges in autonomous AI agents based on large language models (LLMs), including memory-augmented capabilities and reasoning. It also introduces a cognitive framework for safety in decision-making, which aligns with topics related to LLM-based agents, memory, and reasoning.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Reasoning"
    ]
  },
  "2506.23826v1": {
    "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents\n  powered by personal Human Digital Twins",
    "summary": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.",
    "published": "2025-06-30T13:18:31Z",
    "updated": "2025-06-30T13:18:31Z",
    "id": "2506.23826v1",
    "authors": [
      "Llus C. Coll",
      "Martin W. Lauer-Schmaltz",
      "Philip Cash",
      "John P. Hansen",
      "Anja Maier"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23826v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23826v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23826v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models with personal data to create Human Digital Twins, focusing on conversational style, memory, and adaptive learning mechanisms. This aligns with topics related to memory-augmented models and the broader context of large language models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.23825v2": {
    "title": "Flash-VStream: Efficient Real-Time Understanding for Long Video Streams",
    "summary": "Benefiting from the advances in large language models and cross-modal\nalignment, existing multimodal large language models have achieved prominent\nperformance in image and short video understanding. However, the understanding\nof long videos is still challenging, as their long-context nature results in\nsignificant computational and memory overhead. Most existing work treats long\nvideos in the same way as short videos, which is inefficient for real-world\napplications and hard to generalize to even longer videos. To address these\nissues, we propose Flash-VStream, an efficient video language model capable of\nprocessing extremely long videos and responding to user queries in real time.\nParticularly, we design a Flash Memory module, containing a low-capacity\ncontext memory to aggregate long-context temporal information and model the\ndistribution of information density, and a high-capacity augmentation memory to\nretrieve detailed spatial information based on this distribution. Compared to\nexisting models, Flash-VStream achieves significant reductions in inference\nlatency. Extensive experiments on long video benchmarks and comprehensive video\nbenchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate\nthe state-of-the-art performance and outstanding efficiency of our method. Code\nis available at https://github.com/IVGSZ/Flash-VStream.",
    "published": "2025-06-30T13:17:49Z",
    "updated": "2025-07-24T07:25:10Z",
    "id": "2506.23825v2",
    "authors": [
      "Haoji Zhang",
      "Yiqin Wang",
      "Yansong Tang",
      "Yong Liu",
      "Jiashi Feng",
      "Xiaojie Jin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23825v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23825v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23825v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the efficiency and performance of multimodal large language models (MLLMs) for long video understanding, which involves memory management and real-time processing. The key aspects include the use of a Flash Memory module for handling long-context temporal information and detailed spatial information retrieval, aligning with the topics of MLLM and Memory.",
    "llm_cls_result": [
      "MLLM",
      "Memory"
    ]
  },
  "2506.23815v2": {
    "title": "The Impact of AI on Educational Assessment: A Framework for Constructive\n  Alignment",
    "summary": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods.",
    "published": "2025-06-30T13:02:01Z",
    "updated": "2025-07-01T07:51:20Z",
    "id": "2506.23815v2",
    "authors": [
      "Patrick Stokkink"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23815v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23815v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23815v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of Large Language Models (LLM) on educational assessment, aligning with the LLM topic due to its focus on LLMs in education. It also touches on the need for structured guidelines and training, which relates to the broader implications of LLMs in practical applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.02964v1": {
    "title": "Less Data, More Security: Advancing Cybersecurity LLMs Specialization\n  via Resource-Efficient Domain-Adaptive Continuous Pre-training with Minimal\n  Tokens",
    "summary": "While Large Language Models (LLMs) demonstrate exceptional natural language\ncapabilities, general-purpose models lack specialized domain knowledge for\neffective cybersecurity analysis. In this work, we investigate Domain-Adaptive\nContinuous Pretraining (DAP) as a methodology for enhancing cybersecurity\nunderstanding in pretrained LLMs while preserving general language\ncapabilities. We systematically adapted three decoder-based architectures --\nLlama-3.1-8B, DeepSeek-R1-Distill-Qwen-14B, and Llama-3.3-70B-Instruct -- using\na curated 126-million-word cybersecurity corpus from standards, academic\nliterature, and various other sources. Our approach employed constrained\ntraining parameters and distributed FSDP training to balance domain\nspecialization with knowledge preservation. Evaluation across three\ncybersecurity benchmarks, namely, CTI-MCQ, CyberMetric, and SecEval,\ndemonstrates consistent improvements post-adaptation. The Llama-3.3-70B-Ins-DAP\nmodel achieved state-of-the-art accuracies of 0.718, 0.933, and 0.864,\nrespectively, outperforming specialized models, including Llama-Primus-Base.\nNotably, competitive performance was achieved using substantially smaller\ndatasets (118.8 million versus 2.77 billion tokens), demonstrating efficient\ndomain specialization viability. We establish that targeted continuous\npretraining enables effective cybersecurity domain adaptation with\ncomputational feasibility, providing foundations for specialized AI assistants\nin threat analysis, vulnerability assessment, and security documentation while\nchallenging prevailing assumptions about data requirements for LLM\nspecialization.",
    "published": "2025-06-30T12:59:29Z",
    "updated": "2025-06-30T12:59:29Z",
    "id": "2507.02964v1",
    "authors": [
      "Salahuddin Salahuddin",
      "Ahmed Hussain",
      "Jussi Lppnen",
      "Toni Jutila",
      "Panos Papadimitratos"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02964v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02964v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02964v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing cybersecurity understanding in pretrained LLMs through Domain-Adaptive Continuous Pretraining (DAP), which involves pretraining strategies and specialized domain adaptation for LLMs.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2506.23774v1": {
    "title": "Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate\n  Incidents Management",
    "summary": "Computer-aided teacher training is a state-of-the-art method designed to\nenhance teachers' professional skills effectively while minimising concerns\nrelated to costs, time constraints, and geographical limitations. We\ninvestigate the potential of large language models (LLMs) in teacher education,\nusing a case of teaching hate incidents management in schools. To this end, we\ncreate a multi-agent LLM-based system that mimics realistic situations of hate,\nusing a combination of retrieval-augmented prompting and persona modelling. It\nis designed to identify and analyse hate speech patterns, predict potential\nescalation, and propose effective intervention strategies. By integrating\npersona modelling with agentic LLMs, we create contextually diverse simulations\nof hate incidents, mimicking real-life situations. The system allows teachers\nto analyse and understand the dynamics of hate incidents in a safe and\ncontrolled environment, providing valuable insights and practical knowledge to\nmanage such situations confidently in real life. Our pilot evaluation\ndemonstrates teachers' enhanced understanding of the nature of annotator\ndisagreements and the role of context in hate speech interpretation, leading to\nthe development of more informed and effective strategies for addressing hate\nin classroom settings.",
    "published": "2025-06-30T12:18:13Z",
    "updated": "2025-06-30T12:18:13Z",
    "id": "2506.23774v1",
    "authors": [
      "Ewelina Gajewska",
      "Michal Wawer",
      "Katarzyna Budzynska",
      "Jarosaw A. Chudziak"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23774v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23774v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23774v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a multi-agent LLM-based system for teacher education, specifically in managing hate incidents. It leverages LLMs for simulation and training purposes, which aligns with the LLM and Reasoning topics due to the use of large language models and their application in complex problem-solving scenarios.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23762v1": {
    "title": "Software Engineering for Large Language Models: Research Status,\n  Challenges and the Road Ahead",
    "summary": "The rapid advancement of large language models (LLMs) has redefined\nartificial intelligence (AI), pushing the boundaries of AI research and\nenabling unbounded possibilities for both academia and the industry. However,\nLLM development faces increasingly complex challenges throughout its lifecycle,\nyet no existing research systematically explores these challenges and solutions\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\nwe systematically analyze research status throughout the LLM development\nlifecycle, divided into six phases: requirements engineering, dataset\nconstruction, model development and enhancement, testing and evaluation,\ndeployment and operations, and maintenance and evolution. We then conclude by\nidentifying the key challenges for each phase and presenting potential research\ndirections to address these challenges. In general, we provide valuable\ninsights from an SE perspective to facilitate future advances in LLM\ndevelopment.",
    "published": "2025-06-30T12:09:29Z",
    "updated": "2025-06-30T12:09:29Z",
    "id": "2506.23762v1",
    "authors": [
      "Hongzhou Rao",
      "Yanjie Zhao",
      "Xinyi Hou",
      "Shenao Wang",
      "Haoyu Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23762v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23762v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23762v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development lifecycle of large language models (LLMs) from a software engineering perspective, focusing on challenges and solutions throughout various phases of LLM development.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.23749v1": {
    "title": "A Survey of LLM-based Automated Program Repair: Taxonomies, Design\n  Paradigms, and Applications",
    "summary": "Large language models (LLMs) are reshaping automated program repair (APR). We\ncategorize the recent 63 LLM-based APR systems published from January 2022 to\nJune 2025 into four paradigms, and show how retrieval- or analysis-augmented\ncontexts strengthen any of them. This taxonomy clarifies key trade-offs:\nfine-tuning delivers strong task alignment at high training cost; prompting\nenables rapid deployment but is limited by prompt design and context windows;\nprocedural pipelines offer reproducible control with moderate overhead; agentic\nframeworks tackle multi-hunk or cross-file bugs at the price of increased\nlatency and complexity. Persistent challenges include verifying semantic\ncorrectness beyond test suites, repairing repository-scale defects, and\nlowering the costs of LLMs. We outline research directions that combine\nlightweight human feedback, repository-aware retrieval, code analysis, and\ncost-aware planning to advance reliable and efficient LLM-based APR.",
    "published": "2025-06-30T11:46:01Z",
    "updated": "2025-06-30T11:46:01Z",
    "id": "2506.23749v1",
    "authors": [
      "Boyang Yang",
      "Zijian Cai",
      "Fengling Liu",
      "Bach Le",
      "Lingming Zhang",
      "Tegawend F. Bissyand",
      "Yang Liu",
      "Haoye Tian"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23749v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23749v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23749v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in automated program repair (APR), discussing various paradigms and challenges. It directly relates to LLM research and its applications.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23743v2": {
    "title": "Positional Bias in Binary Question Answering: How Uncertainty Shapes\n  Model Preferences",
    "summary": "Positional bias in binary question answering occurs when a model\nsystematically favors one choice over another based solely on the ordering of\npresented options. In this study, we quantify and analyze positional bias\nacross five large language models under varying degrees of answer uncertainty.\nWe re-adapted the SQuAD-it dataset by adding an extra incorrect answer option\nand then created multiple versions with progressively less context and more\nout-of-context answers, yielding datasets that range from low to high\nuncertainty. Additionally, we evaluate two naturally higher-uncertainty\nbenchmarks: (1) WebGPT - question pairs with unequal human-assigned quality\nscores, and (2) Winning Arguments - where models predict the more persuasive\nargument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order\nof the \"correct\" (or higher-quality/persuasive) option is systematically\nflipped (first placed in position 1, then in position 2) to compute both\nPreference Fairness and Position Consistency. We observe that positional bias\nis nearly absent under low-uncertainty conditions, but grows exponentially when\nit becomes doubtful to decide which option is correct.",
    "published": "2025-06-30T11:30:23Z",
    "updated": "2025-07-01T10:39:07Z",
    "id": "2506.23743v2",
    "authors": [
      "Tiziano Labruna",
      "Simone Gallo",
      "Giovanni Da San Martino"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23743v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23743v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23743v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on analyzing positional bias in binary question answering across large language models, which involves evaluating model behavior under varying uncertainty conditions. The study uses benchmarks and datasets to measure bias, aligning with topics related to benchmarking LLMs and their reasoning abilities.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning"
    ]
  },
  "2506.23735v1": {
    "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM\n  Evaluation Data",
    "summary": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.",
    "published": "2025-06-30T11:18:56Z",
    "updated": "2025-06-30T11:18:56Z",
    "id": "2506.23735v1",
    "authors": [
      "JiaRu Wu",
      "Mingwei Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23735v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23735v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23735v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the evaluation of Large Language Models (LLMs) through an automated framework that evolves evaluation data, which aligns with the topics of Benchmark and Dataset in the context of LLMs.",
    "llm_cls_result": [
      "Benchmark",
      "Dataset"
    ]
  },
  "2506.23731v1": {
    "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative\n  Models",
    "summary": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images.",
    "published": "2025-06-30T11:08:10Z",
    "updated": "2025-06-30T11:08:10Z",
    "id": "2506.23731v1",
    "authors": [
      "Michel Meintz",
      "Jan Dubiski",
      "Franziska Boenisch",
      "Adam Dziedzic"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23731v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23731v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23731v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses watermarking methods for image generative models, specifically diffusion models and autoregressive models, with a focus on preserving radioactivity for provenance tracking. It draws inspiration from techniques in large language models (LLMs) due to the shared autoregressive paradigm, but the primary focus is on image generative models and watermarking techniques rather than LLMs themselves.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.23694v1": {
    "title": "If You Had to Pitch Your Ideal Software -- Evaluating Large Language\n  Models to Support User Scenario Writing for User Experience Experts and\n  Laypersons",
    "summary": "The process of requirements analysis requires an understanding of the end\nusers of a system. Thus, expert stakeholders, such as User Experience (UX)\ndesigners, usually create various descriptions containing information about the\nusers and their possible needs. In our paper, we investigate to what extent UX\nnovices are able to write such descriptions into user scenarios. We conducted a\nuser study with 60 participants consisting of 30 UX experts and 30 novices who\nwere asked to write a user scenario with or without the help of an\nLLM-supported writing assistant. Our findings show that LLMs empower laypersons\nto write reasonable user scenarios and provide first-hand insights for\nrequirements analysis that are comparable to UX experts in terms of structure\nand clarity, while especially excelling at audience-orientation. We present our\nqualitative and quantitative findings, including user scenario anatomies,\npotential influences, and differences in the way participants approached the\ntask.",
    "published": "2025-06-30T10:15:44Z",
    "updated": "2025-06-30T10:15:44Z",
    "id": "2506.23694v1",
    "authors": [
      "Patrick Stadler",
      "Christopher Lazik",
      "Christopher Katins",
      "Thomas Kosch"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23694v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23694v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23694v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to assist in writing user scenarios for UX experts and novices, focusing on the capabilities and impacts of LLMs in this specific application.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.23692v1": {
    "title": "Agent4S: The Transformation of Research Paradigms from the Perspective\n  of Large Language Models",
    "summary": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery.",
    "published": "2025-06-30T10:11:39Z",
    "updated": "2025-06-30T10:11:39Z",
    "id": "2506.23692v1",
    "authors": [
      "Boyuan Zheng",
      "Zerui Fang",
      "Zhe Xu",
      "Rui Wang",
      "Yiwen Chen",
      "Cunshi Wang",
      "Mengwei Qu",
      "Lei Lei",
      "Zhen Feng",
      "Yan Liu",
      "Yuyang Li",
      "Mingzhou Tan",
      "Jiaji Wu",
      "Jianwei Shuai",
      "Jia Li",
      "Fangfu Ye"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23692v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23692v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23692v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of LLM-driven agents to automate research workflows, which aligns with the topics of Large Language Models (LLM) and Reinforcement Learning (RL) due to the potential use of RLHF or other reinforcement learning techniques in developing these agents. Additionally, the concept of automating scientific discovery touches on Artificial General Intelligence (AGI) as it aims for autonomous, collaborative 'AI Scientists'.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2506.23689v1": {
    "title": "PokAI: A Goal-Generating, Battle-Optimizing Multi-agent System for\n  Pokemon Red",
    "summary": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.",
    "published": "2025-06-30T10:09:13Z",
    "updated": "2025-06-30T10:09:13Z",
    "id": "2506.23689v1",
    "authors": [
      "Zihao Liu",
      "Xinhang Sui",
      "Yueran Song",
      "Siwen Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23689v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23689v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23689v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multi-agent system using LLMs for autonomous gameplay in Pokemon Red, focusing on strategic reasoning and decision-making, which aligns with the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23667v1": {
    "title": "L0: Reinforcement Learning to Become General Agents",
    "summary": "Training large language models (LLMs) to act as autonomous agents for\nmulti-turn, long-horizon tasks remains significant challenges in scalability\nand training efficiency. To address this, we introduce L-Zero (L0), a scalable,\nend-to-end training pipeline for general-purpose agents. Featuring a low-cost,\nextensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier\nfor applying reinforcement learning in complex environments. We also introduce\nNB-Agent, the agent scaffold within L0, which operates in a \"code-as-action\"\nfashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality\nquestion-answering benchmarks. Our experiments demonstrate that a base model\ncan develop robust problem-solving skills using solely Reinforcement Learning\nwith Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method\nboosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41\n%. We have open-sourced the entire L0 system, including our L0 series models,\nthe NB-Agent, a complete training pipeline, and the corresponding training\nrecipes on (https://github.com/cmriat/l0).",
    "published": "2025-06-30T09:44:32Z",
    "updated": "2025-06-30T09:44:32Z",
    "id": "2506.23667v1",
    "authors": [
      "Junjie Zhang",
      "Jingyi Xi",
      "Zhuoyang Song",
      "Junyu Lu",
      "Yuhua Ke",
      "Ting Sun",
      "Yukun Yang",
      "Jiaxing Zhang",
      "Songxin Zhang",
      "Zejian Xie"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23667v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23667v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23667v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on training large language models (LLMs) as autonomous agents using reinforcement learning, specifically mentioning Reinforcement Learning with Verifiable Rewards (RLVR). It also discusses scalability and training efficiency for general-purpose agents.",
    "llm_cls_result": [
      "RL",
      "AGI",
      "LLM"
    ]
  },
  "2506.23663v1": {
    "title": "On the Domain Robustness of Contrastive Vision-Language Models",
    "summary": "In real-world vision-language applications, practitioners increasingly rely\non large, pretrained foundation models rather than custom-built solutions,\ndespite limited transparency regarding their training data and processes. While\nthese models achieve impressive performance on general benchmarks, their\neffectiveness can decline notably under specialized domain shifts, such as\nunique imaging conditions or environmental variations. In this work, we\nintroduce Deepbench, a framework designed to assess domain-specific robustness\nof vision-language models (VLMs). Deepbench leverages a large language model\n(LLM) to generate realistic, context-aware image corruptions tailored to\nspecific deployment domains without requiring labeled data. We evaluate a range\nof contrastive vision-language architectures and architectural variants across\nsix real-world domains and observe substantial variability in robustness,\nhighlighting the need for targeted, domain-aware evaluation. Deepbench is\nreleased as open-source software to support further research into domain-aware\nrobustness assessment.",
    "published": "2025-06-30T09:39:33Z",
    "updated": "2025-06-30T09:39:33Z",
    "id": "2506.23663v1",
    "authors": [
      "Mario Koddenbrock",
      "Rudolf Hoffmann",
      "David Brodmann",
      "Erik Rodner"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23663v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23663v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23663v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the domain robustness of vision-language models (VLMs) using a framework called Deepbench, which involves generating realistic image corruptions with the help of a large language model (LLM). The study assesses various contrastive vision-language architectures across different domains, emphasizing the need for domain-aware evaluation.",
    "llm_cls_result": [
      "VLA",
      "Benchmark",
      "LLM"
    ]
  },
  "2507.00088v1": {
    "title": "How large language models judge and influence human cooperation",
    "summary": "Humans increasingly rely on large language models (LLMs) to support decisions\nin social settings. Previous work suggests that such tools shape people's moral\nand political judgements. However, the long-term implications of LLM-based\nsocial decision-making remain unknown. How will human cooperation be affected\nwhen the assessment of social interactions relies on language models? This is a\npressing question, as human cooperation is often driven by indirect\nreciprocity, reputations, and the capacity to judge interactions of others.\nHere, we assess how state-of-the-art LLMs judge cooperative actions. We provide\n21 different LLMs with an extensive set of examples where individuals cooperate\n-- or refuse cooperating -- in a range of social contexts, and ask how these\ninteractions should be judged. Furthermore, through an evolutionary\ngame-theoretical model, we evaluate cooperation dynamics in populations where\nthe extracted LLM-driven judgements prevail, assessing the long-term impact of\nLLMs on human prosociality. We observe a remarkable agreement in evaluating\ncooperation against good opponents. On the other hand, we notice within- and\nbetween-model variance when judging cooperation with ill-reputed individuals.\nWe show that the differences revealed between models can significantly impact\nthe prevalence of cooperation. Finally, we test prompts to steer LLM norms,\nshowing that such interventions can shape LLM judgements, particularly through\ngoal-oriented prompts. Our research connects LLM-based advices and long-term\nsocial dynamics, and highlights the need to carefully align LLM norms in order\nto preserve human cooperation.",
    "published": "2025-06-30T09:14:42Z",
    "updated": "2025-06-30T09:14:42Z",
    "id": "2507.00088v1",
    "authors": [
      "Alexandre S. Pires",
      "Laurens Samson",
      "Sennay Ghebreab",
      "Fernando P. Santos"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00088v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00088v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00088v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of large language models (LLMs) on human cooperation, focusing on how LLMs judge cooperative actions and influence social dynamics. It involves LLM-based decision-making and social implications, which are central to LLM research.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2506.23643v1": {
    "title": "Act-With-Think: Chunk Auto-Regressive Modeling for Generative\n  Recommendation",
    "summary": "Generative recommendation (GR) typically encodes behavioral or semantic\naspects of item information into discrete tokens, leveraging the standard\nautoregressive (AR) generation paradigm to make predictions. However, existing\nmethods tend to overlook their intrinsic relationship, that is, the semantic\nusually provides some reasonable explainability \"$\\textbf{why}$\" for the\nbehavior \"$\\textbf{what}$\", which may constrain the full potential of GR. To\nthis end, we present Chunk AutoRegressive Modeling (CAR), a new generation\nparadigm following the decision pattern that users usually think semantic\naspects of items (e.g. brand) and then take actions on target items (e.g.\npurchase). Our CAR, for the $\\textit{first time}$, incorporates semantics\n(SIDs) and behavior (UID) into a single autoregressive transformer from an\n``act-with-think'' dual perspective via chunk-level autoregression.\nSpecifically, CAR packs SIDs and UID into a conceptual chunk for item unified\nrepresentation, allowing each decoding step to make a holistic prediction.\nExperiments show that our CAR significantly outperforms existing methods based\non traditional AR, improving Recall@5 by 7.93% to 22.30%. Furthermore, we\nverify the scaling effect between model performance and SIDs bit number,\ndemonstrating that CAR preliminary emulates a kind of slow-thinking style\nmechanism akin to the reasoning processes observed in large language models\n(LLMs).",
    "published": "2025-06-30T09:13:54Z",
    "updated": "2025-06-30T09:13:54Z",
    "id": "2506.23643v1",
    "authors": [
      "Yifan Wang",
      "Weinan Gan",
      "Longtao Xiao",
      "Jieming Zhu",
      "Heng Chang",
      "Haozhao Wang",
      "Rui Zhang",
      "Zhenhua Dong",
      "Ruiming Tang",
      "Ruixuan Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23643v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23643v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23643v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a new generation paradigm for generative recommendation that incorporates semantic and behavioral aspects into a single autoregressive transformer, which is related to reasoning processes observed in large language models (LLMs).",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23639v1": {
    "title": "Unified Multimodal Understanding via Byte-Pair Visual Encoding",
    "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvision-language understanding, yet effectively aligning different modalities\nremains a fundamental challenge. We present a framework that unifies multimodal\nunderstanding by applying byte-pair encoding to visual tokens. Unlike\nconventional approaches that rely on modality-specific encoders, our method\ndirectly incorporates structural information into visual tokens, mirroring\nsuccessful tokenization strategies in text-only language models. We introduce a\npriority-guided encoding scheme that considers both frequency and spatial\nconsistency, coupled with a multi-stage training procedure based on\ncurriculum-driven data composition. These enhancements enable the transformer\nmodel to better capture cross-modal relationships and reason with visual\ninformation. Comprehensive experiments demonstrate improved performance across\ndiverse vision-language tasks. By bridging the gap between visual and textual\nrepresentations, our approach contributes to the advancement of more capable\nand efficient multimodal foundation models.",
    "published": "2025-06-30T09:08:08Z",
    "updated": "2025-06-30T09:08:08Z",
    "id": "2506.23639v1",
    "authors": [
      "Wanpeng Zhang",
      "Yicheng Feng",
      "Hao Luo",
      "Yijiang Li",
      "Zihao Yue",
      "Sipeng Zheng",
      "Zongqing Lu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23639v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23639v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23639v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on multimodal large language models (MLLMs) and their ability to align different modalities, specifically vision and language. It introduces a novel encoding scheme for visual tokens and a training procedure to enhance cross-modal relationships, which aligns with the topics of MLLM and VLA.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.01061v1": {
    "title": "Epitome: Pioneering an Experimental Platform for AI-Social Science\n  Integration",
    "summary": "The integration of Large Language Models (LLMs) into social science\nexperiments represents a transformative approach to understanding human-AI\ninteractions and their societal impacts. We introduce Epitome, the world's\nfirst open experimental platform dedicated to the deep integration of\nartificial intelligence and social science. Rooted in theoretical foundations\nfrom management, communication studies, sociology, psychology, and ethics,\nEpitome focuses on the interactive impacts of AI on individuals, organizations,\nand society during its real-world deployment. It constructs a theoretical\nsupport system through cross-disciplinary experiments. The platform offers a\none-stop comprehensive experimental solution spanning \"foundation\nmodels-complex application development-user feedback\" through seven core\nmodules, while embedding the classical \"control-comparison-comparative causal\nlogic\" of social science experiments into multilevel human-computer interaction\nenvironments, including dialogues, group chats, and multi-agent virtual\nscenarios. With its canvas-style, user-friendly interface, Epitome enables\nresearchers to easily design and run complex experimental scenarios,\nfacilitating systematic investigations into the social impacts of AI and\nexploration of integrated solutions.To demonstrate its capabilities, we\nreplicated three seminal social science experiments involving LLMs, showcasing\nEpitome's potential to streamline complex experimental designs and produce\nrobust results, suitable for publishing in the top selective journals. Our\nfindings highlight the platform's utility in enhancing the efficiency and\nquality of human-AI interactions, providing valuable insights into the societal\nimplications of AI technologies. Epitome thus offers a powerful tool for\nadvancing interdisciplinary research at the intersection of AI and social\nscience, with potential applications in policy-making, ...",
    "published": "2025-06-30T09:06:16Z",
    "updated": "2025-06-30T09:06:16Z",
    "id": "2507.01061v1",
    "authors": [
      "Jingjing Qu",
      "Kejia Hu",
      "Jun Zhu",
      "Wenhao Li",
      "Teng Wang",
      "Zhiyun Chen",
      "Yulei Ye",
      "Chaochao Lu",
      "Aimin Zhou",
      "Xiangfeng Wang",
      "James Evan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01061v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01061v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01061v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) into social science experiments, focusing on human-AI interactions and societal impacts. It introduces a platform for interdisciplinary research involving LLMs.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2507.02962v3": {
    "title": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs\n  through Multi-query Parallelism",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while they remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have explored enhancing models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to the single-query mode. In this paper, we propose\nRAG-R1, a novel training framework designed to enable LLMs to adaptively\nleverage internal and external knowledge during the reasoning process. We\nfurther expand the generation and retrieval processes within the framework from\nsingle-query mode to multi-query parallelism, aimed at reducing inference time\nand enhancing the model's capabilities. Extensive experiments on seven\nquestion-answering benchmarks demonstrate that our method outperforms the\nstrongest baseline by up to 13.2% and decreases inference time by 11.1%.",
    "published": "2025-06-30T09:02:45Z",
    "updated": "2025-07-15T12:01:49Z",
    "id": "2507.02962v3",
    "authors": [
      "Zhiwen Tan",
      "Jiaming Huang",
      "Qintong Wu",
      "Hongxuan Zhang",
      "Chenyi Zhuang",
      "Jinjie Gu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02962v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02962v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02962v3",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses enhancing LLMs' search and reasoning capabilities using Retrieval-Augmented Generation (RAG) and reinforcement learning (RL), which aligns with the topics of Memory (due to RAG) and RL (due to the use of reinforcement learning). The focus on reasoning capabilities also makes Reasoning a relevant topic.",
    "llm_cls_result": [
      "Memory",
      "RL",
      "Reasoning"
    ]
  },
  "2506.23610v1": {
    "title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to\n  Misinformation with LLMs",
    "summary": "Large language models (LLMs) make it possible to generate synthetic\nbehavioural data at scale, offering an ethical and low-cost alternative to\nhuman experiments. Whether such data can faithfully capture psychological\ndifferences driven by personality traits, however, remains an open question. We\nevaluate the capacity of LLM agents, conditioned on Big-Five profiles, to\nreproduce personality-based variation in susceptibility to misinformation,\nfocusing on news discernment, the ability to judge true headlines as true and\nfalse headlines as false. Leveraging published datasets in which human\nparticipants with known personality profiles rated headline accuracy, we create\nmatching LLM agents and compare their responses to the original human patterns.\nCertain trait-misinformation associations, notably those involving\nAgreeableness and Conscientiousness, are reliably replicated, whereas others\ndiverge, revealing systematic biases in how LLMs internalize and express\npersonality. The results underscore both the promise and the limits of\npersonality-aligned LLMs for behavioral simulation, and offer new insight into\nmodeling cognitive diversity in artificial agents.",
    "published": "2025-06-30T08:16:07Z",
    "updated": "2025-06-30T08:16:07Z",
    "id": "2506.23610v1",
    "authors": [
      "Manuel Pratelli",
      "Marinella Petrocchi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23610v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23610v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23610v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper evaluates the capacity of LLM agents to reproduce personality-based variation in susceptibility to misinformation, which involves the use of Large Language Models (LLMs) and their behavioral simulation capabilities. The study also touches on the alignment of LLMs with human personality traits, which is relevant to the broader field of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.23607v1": {
    "title": "PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global\n  Curriculum",
    "summary": "Existing open-vocabulary 3D semantic segmentation methods typically supervise\n3D segmentation models by merging text-aligned features (e.g., CLIP) extracted\nfrom multi-view images onto 3D points. However, such approaches treat\nmulti-view images merely as intermediaries for transferring open-vocabulary\ninformation, overlooking their rich semantic content and cross-view\ncorrespondences, which limits model effectiveness. To address this, we propose\nPGOV3D, a novel framework that introduces a Partial-to-Global curriculum for\nimproving open-vocabulary 3D semantic segmentation. The key innovation lies in\na two-stage training strategy. In the first stage, we pre-train the model on\npartial scenes that provide dense semantic information but relatively simple\ngeometry. These partial point clouds are derived from multi-view RGB-D inputs\nvia pixel-wise depth projection. To enable open-vocabulary learning, we\nleverage a multi-modal large language model (MLLM) and a 2D segmentation\nfoundation model to generate open-vocabulary labels for each viewpoint,\noffering rich and aligned supervision. An auxiliary inter-frame consistency\nmodule is introduced to enforce feature consistency across varying viewpoints\nand enhance spatial understanding. In the second stage, we fine-tune the model\non complete scene-level point clouds, which are sparser and structurally more\ncomplex. We aggregate the partial vocabularies associated with each scene and\ngenerate pseudo labels using the pre-trained model, effectively bridging the\nsemantic gap between dense partial observations and large-scale 3D\nenvironments. Extensive experiments on ScanNet, ScanNet200, and S3DIS\nbenchmarks demonstrate that PGOV3D achieves competitive performance in\nopen-vocabulary 3D semantic segmentation.",
    "published": "2025-06-30T08:13:07Z",
    "updated": "2025-06-30T08:13:07Z",
    "id": "2506.23607v1",
    "authors": [
      "Shiqi Zhang",
      "Sha Zhang",
      "Jiajun Deng",
      "Yedong Shen",
      "Mingxiao MA",
      "Yanyong Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23607v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23607v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23607v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on open-vocabulary 3D semantic segmentation using a multi-modal large language model (MLLM) and a 2D segmentation foundation model, which aligns with the MLLM topic. It also involves semantic segmentation and cross-modal learning, which are relevant to the VLA topic.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2506.23605v1": {
    "title": "AI-Generated Lecture Slides for Improving Slide Element Detection and\n  Retrieval",
    "summary": "Lecture slide element detection and retrieval are key problems in slide\nunderstanding. Training effective models for these tasks often depends on\nextensive manual annotation. However, annotating large volumes of lecture\nslides for supervised training is labor intensive and requires domain\nexpertise. To address this, we propose a large language model (LLM)-guided\nsynthetic lecture slide generation pipeline, SynLecSlideGen, which produces\nhigh-quality, coherent and realistic slides. We also create an evaluation\nbenchmark, namely RealSlide by manually annotating 1,050 real lecture slides.\nTo assess the utility of our synthetic slides, we perform few-shot transfer\nlearning on real data using models pre-trained on them. Experimental results\nshow that few-shot transfer learning with pretraining on synthetic slides\nsignificantly improves performance compared to training only on real data. This\ndemonstrates that synthetic data can effectively compensate for limited labeled\nlecture slides. The code and resources of our work are publicly available on\nour project website: https://synslidegen.github.io/.",
    "published": "2025-06-30T08:11:31Z",
    "updated": "2025-06-30T08:11:31Z",
    "id": "2506.23605v1",
    "authors": [
      "Suyash Maniyar",
      "Vishvesh Trivedi",
      "Ajoy Mondal",
      "Anand Mishra",
      "C. V. Jawahar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23605v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23605v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23605v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a large language model (LLM) for generating synthetic lecture slides to improve slide element detection and retrieval. It involves training models with synthetic data and evaluating their performance, which aligns with topics related to LLMs and datasets.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2506.23603v2": {
    "title": "SoK: Semantic Privacy in Large Language Models",
    "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs.",
    "published": "2025-06-30T08:08:15Z",
    "updated": "2025-07-16T15:51:30Z",
    "id": "2506.23603v2",
    "authors": [
      "Baihe Ma",
      "Yanna Jiang",
      "Xu Wang",
      "Guangsheng Yu",
      "Qin Wang",
      "Caijun Sun",
      "Chen Li",
      "Xuelei Qi",
      "Ying He",
      "Wei Ni",
      "Ren Ping Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23603v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23603v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23603v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses privacy issues in Large Language Models (LLMs), focusing on semantic privacy risks across various stages of LLM lifecycle. It mentions pretraining and alignment stages, which are key aspects of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2506.23580v1": {
    "title": "Dataset Distillation via Vision-Language Category Prototype",
    "summary": "Dataset distillation (DD) condenses large datasets into compact yet\ninformative substitutes, preserving performance comparable to the original\ndataset while reducing storage, transmission costs, and computational\nconsumption. However, previous DD methods mainly focus on distilling\ninformation from images, often overlooking the semantic information inherent in\nthe data. The disregard for context hinders the model's generalization ability,\nparticularly in tasks involving complex datasets, which may result in illogical\noutputs or the omission of critical objects. In this study, we integrate\nvision-language methods into DD by introducing text prototypes to distill\nlanguage information and collaboratively synthesize data with image prototypes,\nthereby enhancing dataset distillation performance. Notably, the text\nprototypes utilized in this study are derived from descriptive text information\ngenerated by an open-source large language model. This framework demonstrates\nbroad applicability across datasets without pre-existing text descriptions,\nexpanding the potential of dataset distillation beyond traditional image-based\napproaches. Compared to other methods, the proposed approach generates\nlogically coherent images containing target objects, achieving state-of-the-art\nvalidation performance and demonstrating robust generalization. Source code and\ngenerated data are available in\nhttps://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/",
    "published": "2025-06-30T07:34:33Z",
    "updated": "2025-06-30T07:34:33Z",
    "id": "2506.23580v1",
    "authors": [
      "Yawen Zou",
      "Guang Li",
      "Duo Su",
      "Zi Wang",
      "Jun Yu",
      "Chao Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23580v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23580v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23580v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses dataset distillation with a focus on integrating vision-language methods, which involves both image and text data processing. It mentions the use of large language models for generating descriptive text information, indicating relevance to multimodal approaches and dataset creation.",
    "llm_cls_result": [
      "VLA",
      "Dataset",
      "MLLM"
    ]
  },
  "2506.23576v1": {
    "title": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large\n  Language Models",
    "summary": "Recent advances in large language models (LLMs) have raised concerns about\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\ninvestigates the use of multi-agent LLM systems as a defence against such\nattacks. We evaluate three jailbreaking strategies, including the original\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\nAutoDefense framework, we compare single-agent setups with two- and three-agent\nconfigurations. Our results show that multi-agent systems enhance resistance to\njailbreaks, especially by reducing false negatives. However, its effectiveness\nvaries by attack type, and it introduces trade-offs such as increased false\npositives and computational overhead. These findings point to the limitations\nof current automated defences and suggest directions for improving alignment\nrobustness in future LLM systems.",
    "published": "2025-06-30T07:29:07Z",
    "updated": "2025-06-30T07:29:07Z",
    "id": "2506.23576v1",
    "authors": [
      "Maria Carolina Cornelia Wit",
      "Jun Pang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23576v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23576v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23576v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of multi-agent LLM systems to defend against jailbreaking attacks on large language models, which involves LLM safety mechanisms and alignment robustness.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2506.23563v1": {
    "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for\n  MLLMs Toward AGI",
    "summary": "Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason.",
    "published": "2025-06-30T07:14:38Z",
    "updated": "2025-06-30T07:14:38Z",
    "id": "2506.23563v1",
    "authors": [
      "Huanjin Yao",
      "Jiaxing Huang",
      "Yawen Qiu",
      "Michael K. Chen",
      "Wenzheng Liu",
      "Wei Zhang",
      "Wenjie Zeng",
      "Xikun Zhang",
      "Jingyi Zhang",
      "Yuxin Song",
      "Wenhao Wu",
      "Dacheng Tao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23563v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23563v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23563v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark (MMReason) for evaluating the reasoning capabilities of Multimodal Large Language Models (MLLMs), which aligns with the topics of MLLM (Multimodal Large Language Models), Benchmark (evaluation metrics and performance comparison), and AGI (advancing toward Artificial General Intelligence).",
    "llm_cls_result": [
      "MLLM",
      "Benchmark",
      "AGI"
    ]
  },
  "2507.05266v1": {
    "title": "User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost\n  Evaluation Strategy for Estimating Generalization in LLMs",
    "summary": "Measuring the generalization ability of Large Language Models (LLMs) is\nchallenging due to data contamination. As models grow and computation becomes\ncheaper, ensuring tasks and test cases are unseen during training phases will\nbecome nearly impossible. We argue that knowledge-retrieval and reasoning tasks\nare not ideal for measuring generalization, as LLMs are not trained for\nspecific tasks. Instead, we propose user behavior prediction, also a key aspect\nof personalization, as a theoretically sound, scalable, and robust alternative.\nWe introduce a novel framework for this approach and test it on movie and music\nrecommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.\nResults align with our framework's predictions, showing GPT-4o outperforms\nGPT-4o-mini and Llama, though all models have much room for improvement,\nespecially Llama.",
    "published": "2025-06-30T06:14:32Z",
    "updated": "2025-06-30T06:14:32Z",
    "id": "2507.05266v1",
    "authors": [
      "Sougata Saha",
      "Monojit Choudhury"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05266v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05266v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05266v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of Large Language Models (LLMs) and proposes a novel framework for measuring generalization using user behavior prediction. It focuses on benchmarking and evaluating LLMs, which aligns with the 'Benchmark' topic. Additionally, it touches on the reasoning abilities of LLMs, which is relevant to the 'Reasoning' topic.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning"
    ]
  },
  "2506.23535v1": {
    "title": "Comparative Analysis of the Code Generated by Popular Large Language\n  Models (LLMs) for MISRA C++ Compliance",
    "summary": "Safety-critical systems are engineered systems whose failure or malfunction\ncould result in catastrophic consequences. The software development for\nsafety-critical systems necessitates rigorous engineering practices and\nadherence to certification standards like DO-178C for avionics. DO-178C is a\nguidance document which requires compliance to well-defined software coding\nstandards like MISRA C++ to enforce coding guidelines that prevent the use of\nambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have\ndemonstrated significant capabilities in automatic code generation across a\nwide range of programming languages, including C++. Despite their impressive\nperformance, code generated by LLMs in safety-critical domains must be\ncarefully analyzed for conformance to MISRA C++ coding standards. In this\npaper, I have conducted a comparative analysis of the C++ code generated by\npopular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and\nMicrosoft Copilot for compliance with MISRA C++.",
    "published": "2025-06-30T05:53:45Z",
    "updated": "2025-06-30T05:53:45Z",
    "id": "2506.23535v1",
    "authors": [
      "Malik Muhammad Umer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23535v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23535v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23535v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on analyzing the code generated by Large Language Models (LLMs) for compliance with MISRA C++ standards, which directly relates to the evaluation and benchmarking of LLMs in a specific domain.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.23527v1": {
    "title": "On Recipe Memorization and Creativity in Large Language Models: Is Your\n  Model a Creative Cook, a Bad Cook, or Merely a Plagiator?",
    "summary": "This work-in-progress investigates the memorization, creativity, and nonsense\nfound in cooking recipes generated from Large Language Models (LLMs).\nPrecisely, we aim (i) to analyze memorization, creativity, and non-sense in\nLLMs using a small, high-quality set of human judgments and (ii) to evaluate\npotential approaches to automate such a human annotation in order to scale our\nstudy to hundreds of recipes. To achieve (i), we conduct a detailed human\nannotation on 20 preselected recipes generated by LLM (Mixtral), extracting\neach recipe's ingredients and step-by-step actions to assess which elements are\nmemorized--i.e., directly traceable to online sources possibly seen during\ntraining--and which arise from genuine creative synthesis or outright nonsense.\nWe find that Mixtral consistently reuses ingredients that can be found in\nonline documents, potentially seen during model training, suggesting strong\nreliance on memorized content. To achieve aim (ii) and scale our analysis\nbeyond small sample sizes and single LLM validation, we design an\n``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,\nparsing ingredients and recipe steps, and their annotation. For instance,\ncomparing its output against human annotations, the best ingredient extractor\nand annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on\ningredient matching. This automated framework enables large-scale\nquantification of memorization, creativity, and nonsense in generated recipes,\nproviding rigorous evidence of the models' creative capacities.",
    "published": "2025-06-30T05:27:11Z",
    "updated": "2025-06-30T05:27:11Z",
    "id": "2506.23527v1",
    "authors": [
      "Jan Kvapil",
      "Martin Fajcik"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23527v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23527v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23527v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on analyzing memorization, creativity, and nonsense in cooking recipes generated by Large Language Models (LLMs), specifically Mixtral. It involves human annotation and an automated pipeline for scaling the analysis, which aligns with research on LLMs and their capabilities.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.23524v1": {
    "title": "NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment\n  analysis and topic Classification toward multitask learning",
    "summary": "In the field of education, understanding students' opinions through their\ncomments is crucial, especially in the Vietnamese language, where resources\nremain limited. Existing educational datasets often lack domain relevance and\nstudent slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese\ndataset for Educational Sentiment Classification and Topic Classification,\ncurated from university forums, which offers more samples, richer class\ndiversity, longer texts, and broader vocabulary. In addition, we explore\nmultitask learning using encoder-only language models (BERT), in which we\nshowed that it achieves performance up to 83.7% and 79.8% accuracy for\nsentiment and topic classification tasks. We also benchmark our dataset and\nmodel with other datasets and models, including Large Language Models, and\ndiscuss these benchmarks. The dataset is publicly available at:\nhttps://huggingface.co/datasets/hung20gg/NEU-ESC.",
    "published": "2025-06-30T05:19:04Z",
    "updated": "2025-06-30T05:19:04Z",
    "id": "2506.23524v1",
    "authors": [
      "Phan Quoc Hung Mai",
      "Quang Hung Nguyen",
      "Phuong Giang Duong",
      "Hong Hanh Nguyen",
      "Nguyen Tuan Long"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23524v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23524v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23524v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a Vietnamese dataset for educational sentiment and topic classification, and benchmarks it with other models including Large Language Models. However, the primary focus is on dataset creation and multitask learning rather than core LLM research.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.23520v2": {
    "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions\n  with LLM-Generated Data",
    "summary": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor.",
    "published": "2025-06-30T05:11:19Z",
    "updated": "2025-07-01T08:11:18Z",
    "id": "2506.23520v2",
    "authors": [
      "Yu Zhang",
      "Ruijie Yu",
      "Jidong Tian",
      "Feng Zhu",
      "Jiapeng Liu",
      "Xiaokang Yang",
      "Yaohui Jin",
      "Yanyan Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23520v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23520v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23520v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of a large language model (LLM) for automated extraction of chemical synthesis actions, which involves fine-tuning an LLM and using LLM-generated data to enhance performance in specific chemical tasks.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2506.23508v1": {
    "title": "Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably",
    "summary": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and\nReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large\nlanguage models to downstream tasks. While effective at task adaptation, their\nimpact on prior knowledge remains unclear. In this paper, we introduce jigsaw\npuzzles as a novel task absent from existing pretraining corpora and\nsystematically study the behavior of SFT and RFT on an open-source multimodal\nmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid\ntask acquisition but leads to catastrophic forgetting, whereas RFT learns more\nslowly on novel tasks but maintains prior knowledge. We analyze this phenomenon\nthrough the lens of learning dynamics, showing that RFT reinforces correct\nsamples that are naturally aligned with the base model's probability landscape,\nmitigating interference with prior knowledge. Moreover, supervised training on\ncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidly\nlearning new tasks. These findings suggest that data distribution, rather than\nalgorithmic differences, plays a central role in forgetting, and highlight\nRFT's potential for stable continual learning in multimodal large language\nmodels.",
    "published": "2025-06-30T04:15:01Z",
    "updated": "2025-06-30T04:15:01Z",
    "id": "2506.23508v1",
    "authors": [
      "Zhihao Zhang",
      "Qiaole Dong",
      "Qi Zhang",
      "Jun Zhao",
      "Enyu Zhou",
      "Zhiheng Xi",
      "Senjie Jin",
      "Xiaoran Fan",
      "Yuhao Zhou",
      "Yanwei Fu",
      "Tao Ji",
      "Tao Gui",
      "Xuanjing Huang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23508v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23508v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23508v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Reinforcement Fine-Tuning (RFT) and Supervised Fine-Tuning (SFT) in the context of multimodal large language models (MLLMs), focusing on their impact on prior knowledge and task adaptation. It specifically addresses the trade-offs between learning new tasks and maintaining existing knowledge, which is relevant to both MLLMs and Reinforcement Learning (RL) topics.",
    "llm_cls_result": [
      "MLLM",
      "RL"
    ]
  },
  "2506.23502v2": {
    "title": "LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text\n  Matching",
    "summary": "Driven by large-scale contrastive vision-language pre-trained models such as\nCLIP, recent advancements in the image-text matching task have achieved\nremarkable success in representation learning. Due to image-level\nvisual-language alignment, CLIP falls short in understanding fine-grained\ndetails such as object attributes and spatial relationships between objects.\nRecent efforts have attempted to compel CLIP to acquire structured visual\nrepresentations by introducing prompt learning to achieve object-level\nalignment. While achieving promising results, they still lack the capability to\nperceive actions, which are crucial for describing the states or relationships\nbetween objects. Therefore, we propose to endow CLIP with fine-grained\naction-level understanding by introducing an LLM-enhanced action-aware\nmulti-modal prompt-tuning method, incorporating the action-related external\nknowledge generated by large language models (LLMs). Specifically, we design an\naction triplet prompt and an action state prompt to exploit compositional\nsemantic knowledge and state-related causal knowledge implicitly stored in\nLLMs. Subsequently, we propose an adaptive interaction module to aggregate\nattentive visual features conditioned on action-aware prompted knowledge for\nestablishing discriminative and action-aware visual representations, which\nfurther improves the performance. Comprehensive experimental results on two\nbenchmark datasets demonstrate the effectiveness of our method.",
    "published": "2025-06-30T03:49:08Z",
    "updated": "2025-07-12T09:48:32Z",
    "id": "2506.23502v2",
    "authors": [
      "Mengxiao Tian",
      "Xinxiao Wu",
      "Shuo Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23502v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23502v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23502v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing CLIP's capabilities with action-aware understanding using LLM-generated knowledge, which involves multimodal learning and leveraging large language models for fine-grained details.",
    "llm_cls_result": [
      "MLLM",
      "LLM",
      "VLA"
    ]
  },
  "2506.23485v1": {
    "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent",
    "summary": "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA.",
    "published": "2025-06-30T03:15:50Z",
    "updated": "2025-06-30T03:15:50Z",
    "id": "2506.23485v1",
    "authors": [
      "Haocheng Yu",
      "Yaxiong Wu",
      "Hao Wang",
      "Wei Guo",
      "Yong Liu",
      "Yawen Li",
      "Yuyang Ye",
      "Junping Du",
      "Enhong Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23485v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23485v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23485v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in interactive recommendation systems, focusing on enhancing planning and generalization capabilities through thought-augmented methods. It involves LLM-powered agents and their application in handling complex user intents, which aligns with the topics of LLM and RL (Reinforcement Learning) due to the interactive and agent-based nature of the system.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.23481v1": {
    "title": "Evaluation of Geolocation Capabilities of Multimodal Large Language\n  Models and Analysis of Associated Privacy Risks",
    "summary": "Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)\nhas significantly enhanced their reasoning capabilities, enabling a wide range\nof intelligent applications. However, these advancements also raise critical\nconcerns regarding privacy and ethics. MLLMs are now capable of inferring the\ngeographic location of images -- such as those shared on social media or\ncaptured from street views -- based solely on visual content, thereby posing\nserious risks of privacy invasion, including doxxing, surveillance, and other\nsecurity threats.\n  Methods: This study provides a comprehensive analysis of existing geolocation\ntechniques based on MLLMs. It systematically reviews relevant litera-ture and\nevaluates the performance of state-of-the-art visual reasoning models on\ngeolocation tasks, particularly in identifying the origins of street view\nimagery.\n  Results: Empirical evaluation reveals that the most advanced visual large\nmodels can successfully localize the origin of street-level imagery with up to\n$49\\%$ accuracy within a 1-kilometer radius. This performance underscores the\nmodels' powerful capacity to extract and utilize fine-grained geographic cues\nfrom visual data.\n  Conclusions: Building on these findings, the study identifies key visual\nelements that contribute to suc-cessful geolocation, such as text,\narchitectural styles, and environmental features. Furthermore, it discusses the\npotential privacy implications associated with MLLM-enabled geolocation and\ndiscuss several technical and policy-based coun-termeasures to mitigate\nassociated risks. Our code and dataset are available at\nhttps://github.com/zxyl1003/MLLM-Geolocation-Evaluation.",
    "published": "2025-06-30T03:05:30Z",
    "updated": "2025-06-30T03:05:30Z",
    "id": "2506.23481v1",
    "authors": [
      "Xian Zhang",
      "Xiang Cheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23481v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23481v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23481v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the evaluation of Multimodal Large Language Models (MLLMs) in geolocation tasks and discusses associated privacy risks, which directly aligns with the topics of MLLM and Benchmark.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2507.00082v1": {
    "title": "Federated Learning-Enabled Hybrid Language Models for\n  Communication-Efficient Token Transmission",
    "summary": "Hybrid Language Models (HLMs) combine the low-latency efficiency of Small\nLanguage Models (SLMs) on edge devices with the high accuracy of Large Language\nModels (LLMs) on centralized servers. Unlike traditional end-to-end LLM\ninference, HLMs reduce latency and communication by invoking LLMs only when\nlocal SLM predictions are uncertain, i.e., when token-level confidence is low\nor entropy is high. However, ambiguous or low-confidence predictions still\nrequire frequent offloading to the LLM, leading to significant communication\noverhead in bandwidth-constrained settings. To address this, we propose FedHLM,\na communication-efficient HLM framework that integrates uncertainty-aware\ninference with Federated Learning (FL). FedHLM's key innovation lies in\ncollaboratively learning token-level uncertainty thresholds that govern when\nLLM assistance is needed. Rather than using static or manually tuned\nthresholds, FedHLM employs FL to optimize these thresholds in a\nprivacy-preserving, distributed manner. Additionally, it leverages\nembedding-based token representations for Peer-to-Peer (P2P) resolution,\nenabling clients to reuse tokens inferred by semantically similar peers without\nengaging the LLM. We further introduce hierarchical model aggregation: edge\nservers refine local routing policies through client updates, while\ncross-cluster coordination aligns global decision boundaries. This layered\ndesign captures recurring uncertainty patterns, reducing redundant LLM queries.\nExperiments on large-scale news classification tasks show that FedHLM reduces\nLLM transmissions by over 95 percent with negligible accuracy loss, making it\nwell-suited for scalable and efficient edge-AI applications.",
    "published": "2025-06-30T02:56:11Z",
    "updated": "2025-06-30T02:56:11Z",
    "id": "2507.00082v1",
    "authors": [
      "Faranaksadat Solat",
      "Joohyung Lee",
      "Mohamed Seif",
      "Dusit Niyato",
      "H. Vincent Poor"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00082v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00082v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00082v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses Hybrid Language Models (HLMs) that combine Small Language Models (SLMs) and Large Language Models (LLMs), focusing on communication efficiency and federated learning. It does not directly align with the provided topics but is related to LLMs and their optimization.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.23463v2": {
    "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework",
    "summary": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by 70%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks.",
    "published": "2025-06-30T02:03:23Z",
    "updated": "2025-07-09T15:10:56Z",
    "id": "2506.23463v2",
    "authors": [
      "WonJune Jang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23463v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23463v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23463v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a framework (ATF) that uses LLMs for table-based reasoning and filtering, focusing on improving performance by reducing input size and maintaining informativeness. It directly involves LLMs and their application in reasoning tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.00081v1": {
    "title": "State and Memory is All You Need for Robust and Reliable AI Agents",
    "summary": "Large language models (LLMs) have enabled powerful advances in natural\nlanguage understanding and generation. Yet their application to complex,\nreal-world scientific workflows remain limited by challenges in memory,\nplanning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke\nArtificial Intelligence Agents Optimized for Research Goals), a modular agentic\nframework that allows LLM-based agents to autonomously plan, reason, and\nachieve robust and reliable domain-specific task execution. Agents are\nconstructed dynamically from source code documentation and augmented with\nfinite-state automata (FSA) memory, enabling persistent state tracking and\ncontext-aware decision-making. This approach eliminates the need for manual\nprompt engineering and allows for robust, scalable deployment across diverse\napplications via maintaining context across extended workflows and to recover\nfrom tool or execution failures. We validate SciBORG through integration with\nboth physical and virtual hardware, such as microwave synthesizers for\nexecuting user-specified reactions, with context-aware decision making and\ndemonstrate its use in autonomous multi-step bioassay retrieval from the\nPubChem database utilizing multi-step planning, reasoning, agent-to-agent\ncommunication and coordination for execution of exploratory tasks. Systematic\nbenchmarking shows that SciBORG agents achieve reliable execution, adaptive\nplanning, and interpretable state transitions. Our results show that memory and\nstate awareness are critical enablers of agentic planning and reliability,\noffering a generalizable foundation for deploying AI agents in complex\nenvironments.",
    "published": "2025-06-30T02:02:35Z",
    "updated": "2025-06-30T02:02:35Z",
    "id": "2507.00081v1",
    "authors": [
      "Matthew Muhoberac",
      "Atharva Parikh",
      "Nirvi Vakharia",
      "Saniya Virani",
      "Aco Radujevic",
      "Savannah Wood",
      "Meghav Verma",
      "Dimitri Metaxotos",
      "Jeyaraman Soundararajan",
      "Thierry Masquelin",
      "Alexander G. Godfrey",
      "Sean Gardner",
      "Dobrila Rudnicki",
      "Sam Michael",
      "Gaurav Chopra"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00081v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00081v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00081v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of LLMs in complex workflows, focusing on memory, planning, and tool integration, which aligns with the topics of LLM, Memory, and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Reasoning"
    ]
  },
  "2506.23462v1": {
    "title": "Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for\n  Multimodal Disaster Classification",
    "summary": "Effective disaster management requires timely and accurate insights, yet\ntraditional methods struggle to integrate multimodal data such as images,\nweather records, and textual reports. To address this, we propose\nDisasterNet-LLM, a specialized Large Language Model (LLM) designed for\ncomprehensive disaster analysis. By leveraging advanced pretraining,\ncross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM\nexcels in disaster classification. Experimental results demonstrate its\nsuperiority over state-of-the-art models, achieving higher accuracy of 89.5%,\nan F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal\ndisaster classification tasks.",
    "published": "2025-06-30T01:56:05Z",
    "updated": "2025-06-30T01:56:05Z",
    "id": "2506.23462v1",
    "authors": [
      "Manaswi Kulahara",
      "Gautam Siddharth Kashyap",
      "Nipun Joshi",
      "Arpita Soni"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23462v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23462v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23462v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a specialized Large Language Model (LLM) designed for multimodal disaster analysis, which involves integrating images, weather records, and textual reports. This aligns with the topics of Multimodal Large Language Models (MLLM) and Pretraining strategies for such models.",
    "llm_cls_result": [
      "MLLM",
      "Pretrain"
    ]
  },
  "2506.23423v1": {
    "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses\n  of LLMs",
    "summary": "Past work has studied the effects of fine-tuning on large language models'\n(LLMs) overall performance on certain tasks. However, a quantitative and\nsystematic method for analyzing its effect on individual outputs is still\nlacking. Here, we propose a new method for measuring the contribution that\nfine-tuning makes to individual LLM responses, assuming access to the original\npre-trained model. Our method tracks the model's intermediate hidden states,\nproviding a more fine-grained insight into the effects of fine-tuning than a\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\nLLM into a pre-training component and a fine-tuning component. Empirically, we\nfind that model behavior and performance can be steered by up- or down-scaling\nthe fine-tuning component during the forward pass. Motivated by this finding\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\nratio of the magnitudes of the fine-tuning component to the pre-training\ncomponent. We observe that three prominent adversarial attacks on LLMs\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\nconsistently lower on prompts where these attacks succeed compared to those\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\nenables the quantitative study of how fine-tuning influences model behavior and\nsafety, and vice versa.",
    "published": "2025-06-29T23:08:36Z",
    "updated": "2025-06-29T23:08:36Z",
    "id": "2506.23423v1",
    "authors": [
      "Felipe Nuti",
      "Tim Franzmeyer",
      "Joo Henriques"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23423v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23423v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23423v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on measuring the contribution of fine-tuning to individual responses of LLMs, which involves analyzing the effects of fine-tuning on model behavior and safety. This aligns with the topics of LLM (Large Language Models) and Pretrain (pretraining strategies and objectives).",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2506.23411v1": {
    "title": "Datasets for Fairness in Language Models: An In-Depth Survey",
    "summary": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community.",
    "published": "2025-06-29T22:11:58Z",
    "updated": "2025-06-29T22:11:58Z",
    "id": "2506.23411v1",
    "authors": [
      "Jiale Zhang",
      "Zichong Wang",
      "Avash Palikhe",
      "Zhipeng Yin",
      "Wenbin Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23411v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23411v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23411v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on fairness datasets used in language model research, which aligns with the 'Dataset' topic as it involves LLM datasets and evaluation datasets. It also touches on benchmarking, but the primary focus is on the datasets themselves.",
    "llm_cls_result": [
      "Dataset"
    ]
  },
  "2506.23408v1": {
    "title": "Do LLMs Dream of Discrete Algorithms?",
    "summary": "Large Language Models (LLMs) have rapidly transformed the landscape of\nartificial intelligence, enabling natural language interfaces and dynamic\norchestration of software components. However, their reliance on probabilistic\ninference limits their effectiveness in domains requiring strict logical\nreasoning, discrete decision-making, and robust interpretability. This paper\ninvestigates these limitations and proposes a neurosymbolic approach that\naugments LLMs with logic-based reasoning modules, particularly leveraging\nProlog predicates and composable toolsets. By integrating first-order logic and\nexplicit rule systems, our framework enables LLMs to decompose complex queries\ninto verifiable sub-tasks, orchestrate reliable solutions, and mitigate common\nfailure modes such as hallucination and incorrect step decomposition. We\ndemonstrate the practical benefits of this hybrid architecture through\nexperiments on the DABStep benchmark, showing improved precision, coverage, and\nsystem documentation in multi-step reasoning tasks. Our results indicate that\ncombining LLMs with modular logic reasoning restores engineering rigor,\nenhances system reliability, and offers a scalable path toward trustworthy,\ninterpretable AI agents across complex domains.",
    "published": "2025-06-29T22:03:01Z",
    "updated": "2025-06-29T22:03:01Z",
    "id": "2506.23408v1",
    "authors": [
      "Claudionor Coelho Jr",
      "Yanen Li",
      "Philip Tee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23408v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23408v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23408v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the limitations of LLMs in logical reasoning and proposes a neurosymbolic approach to enhance their capabilities in discrete decision-making and interpretability. It focuses on augmenting LLMs with logic-based reasoning modules, which aligns with the 'Reasoning' and 'LLM' topics.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2506.23377v2": {
    "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs",
    "summary": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.",
    "published": "2025-06-29T19:26:37Z",
    "updated": "2025-07-12T17:57:39Z",
    "id": "2506.23377v2",
    "authors": [
      "Taejin Kim",
      "Siun-Chuon Mau",
      "Konrad Vesey"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23377v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23377v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23377v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on measuring and controlling the perspective and bias in LLM outputs, which involves understanding and manipulating LLM behavior. This aligns with topics related to LLM research and reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23352v1": {
    "title": "GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language\n  Fields",
    "summary": "The advancement of 3D language fields has enabled intuitive interactions with\n3D scenes via natural language. However, existing approaches are typically\nlimited to small-scale environments, lacking the scalability and compositional\nreasoning capabilities necessary for large, complex urban settings. To overcome\nthese limitations, we propose GeoProg3D, a visual programming framework that\nenables natural language-driven interactions with city-scale high-fidelity 3D\nscenes. GeoProg3D consists of two key components: (i) a Geography-aware\nCity-scale 3D Language Field (GCLF) that leverages a memory-efficient\nhierarchical 3D model to handle large-scale data, integrated with geographic\ninformation for efficiently filtering vast urban spaces using directional cues,\ndistance measurements, elevation data, and landmark references; and (ii)\nGeographical Vision APIs (GV-APIs), specialized geographic vision tools such as\narea segmentation and object detection. Our framework employs large language\nmodels (LLMs) as reasoning engines to dynamically combine GV-APIs and operate\nGCLF, effectively supporting diverse geographic vision tasks. To assess\nperformance in city-scale reasoning, we introduce GeoEval3D, a comprehensive\nbenchmark dataset containing 952 query-answer pairs across five challenging\ntasks: grounding, spatial reasoning, comparison, counting, and measurement.\nExperiments demonstrate that GeoProg3D significantly outperforms existing 3D\nlanguage fields and vision-language models across multiple tasks. To our\nknowledge, GeoProg3D is the first framework enabling compositional geographic\nreasoning in high-fidelity city-scale 3D environments via natural language. The\ncode is available at https://snskysk.github.io/GeoProg3D/.",
    "published": "2025-06-29T18:03:03Z",
    "updated": "2025-06-29T18:03:03Z",
    "id": "2506.23352v1",
    "authors": [
      "Shunsuke Yasuki",
      "Taiki Miyanishi",
      "Nakamasa Inoue",
      "Shuhei Kurita",
      "Koya Sakamoto",
      "Daichi Azuma",
      "Masato Taki",
      "Yutaka Matsuo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23352v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23352v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23352v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on a visual programming framework for city-scale 3D language fields, leveraging large language models (LLMs) for reasoning and geographic vision tasks. It introduces a benchmark dataset and evaluates performance, aligning with topics related to reasoning in LLMs and multimodal large language models (MLLM).",
    "llm_cls_result": [
      "Reasoning",
      "MLLM",
      "Benchmark"
    ]
  },
  "2506.23342v1": {
    "title": "ATGen: A Framework for Active Text Generation",
    "summary": "Active learning (AL) has demonstrated remarkable potential in reducing the\nannotation effort required for training machine learning models. However,\ndespite the surging popularity of natural language generation (NLG) tasks in\nrecent years, the application of AL to NLG has been limited. In this paper, we\nintroduce Active Text Generation (ATGen) - a comprehensive framework that\nbridges AL with text generation tasks, enabling the application of\nstate-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered\nannotation in NLG tasks using both human annotators and automatic annotation\nagents based on large language models (LLMs). The framework supports LLMs\ndeployed as services, such as ChatGPT and Claude, or operated on-premises.\nFurthermore, ATGen provides a unified platform for smooth implementation and\nbenchmarking of novel AL strategies tailored to NLG tasks. Finally, we present\nevaluation results for state-of-the-art AL strategies across diverse settings\nand multiple text generation tasks. We show that ATGen reduces both the effort\nof human annotators and costs associated with API calls to LLM-based annotation\nagents. The code of the framework is available on GitHub under the MIT license.\nThe video presentation is available at http://atgen-video.nlpresearch.group",
    "published": "2025-06-29T17:27:48Z",
    "updated": "2025-06-29T17:27:48Z",
    "id": "2506.23342v1",
    "authors": [
      "Akim Tsvigun",
      "Daniil Vasilev",
      "Ivan Tsvigun",
      "Ivan Lysenko",
      "Talgat Bektleuov",
      "Aleksandr Medvedev",
      "Uliana Vinogradova",
      "Nikita Severin",
      "Mikhail Mozikov",
      "Andrey Savchenko",
      "Rostislav Grigorev",
      "Ramil Kuleev",
      "Fedor Zhdanov",
      "Artem Shelmanov",
      "Ilya Makarov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23342v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23342v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23342v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of active learning to text generation tasks, leveraging large language models (LLMs) for annotation and benchmarking. The core topics include the use of LLMs and benchmarking in the context of text generation.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.23340v1": {
    "title": "Information Loss in LLMs' Multilingual Translation: The Role of Training\n  Data, Language Proximity, and Language Family",
    "summary": "Large language models have achieved impressive progress in multilingual\ntranslation, yet they continue to face challenges with certain language\npairs-particularly those with limited training data or significant linguistic\ndivergence from English. This study systematically investigates how training\ndata, language proximity, and language family affect information loss in\nmultilingual translation. We evaluate two large language models, GPT-4 and\nLlama 2, by performing round-trip translations. Translation quality was\nassessed using BLEU scores and BERT similarity metrics. Our results reveal a\nrobust interaction between training data size and language distance: while\nabundant training data can mitigate the effects of linguistic divergence,\nlanguages structurally closer to English consistently yield higher translation\nquality in low-resource conditions. Among various distance metrics,\northographic, phylogenetic, syntactic, and geographical distances emerge as\nstrong predictors of translation performance. Language family also exerts an\nindependent influence. These findings contribute to a deeper understanding of\nthe linguistic constraints shaping multilingual translation in large language\nmodels, emphasizing that translation quality is shaped not only by data volume\nbut also by structural and typological relationships between languages.",
    "published": "2025-06-29T17:21:05Z",
    "updated": "2025-06-29T17:21:05Z",
    "id": "2506.23340v1",
    "authors": [
      "Yumeng Lin",
      "Xufeng Duan",
      "David Haslett",
      "Yige Chen",
      "Zhenguang G. Cai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23340v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23340v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23340v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the performance of large language models (LLMs) in multilingual translation, specifically examining the impact of training data, language proximity, and language family on translation quality. This aligns with the 'LLM' topic, which covers research on large language models, including their architectures and performance in various tasks.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.23339v1": {
    "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular\n  Design",
    "summary": "Large Language Models (LLMs) demonstrate remarkable potential for scientific\ndiscovery, but their application in domains requiring factual accuracy and\ndomain-specific constraints remains challenging. In molecular design for drug\ndiscovery, LLMs can suggest creative molecular modifications but often produce\nchemically invalid or impractical structures. We present VALID-Mol, a\nsystematic framework for integrating chemical validation with LLM-driven\nmolecular design that increases the rate of generating valid chemical\nstructures from 3% to 83%. Our approach combines methodical prompt engineering,\nautomated chemical validation, and a fine-tuned domain-adapted LLM to ensure\nreliable generation of synthesizable molecules with improved properties. Beyond\nthe specific implementation, we contribute a generalizable methodology for\nscientifically-constrained LLM applications, with quantifiable reliability\nimprovements. Computational predictions suggest our framework can generate\npromising candidates for synthesis with up to 17-fold computationally predicted\nimprovements in target affinity while maintaining synthetic accessibility. We\nprovide a detailed analysis of our prompt engineering process, validation\narchitecture, and fine-tuning approach, offering a reproducible blueprint for\napplying LLMs to other scientific domains where domain-specific validation is\nessential.",
    "published": "2025-06-29T17:17:04Z",
    "updated": "2025-06-29T17:17:04Z",
    "id": "2506.23339v1",
    "authors": [
      " Malikussaid",
      "Hilal Hudan Nuha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23339v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23339v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23339v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in molecular design, focusing on improving the validity and practicality of generated structures through systematic validation and fine-tuning. This aligns with the 'LLM' topic as it involves research on LLMs and their application in a specific domain. Additionally, the paper's focus on domain-specific constraints and validation could be loosely related to 'AGI' as it touches on the broader challenge of applying AI models to specialized scientific tasks.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2506.23325v2": {
    "title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate\n  Speech Codecs",
    "summary": "Speech codecs serve as bridges between speech signals and large language\nmodels. An ideal codec for speech language models should not only preserve\nacoustic information but also capture rich semantic information. However,\nexisting speech codecs struggle to balance high-quality audio reconstruction\nwith ease of modeling by language models. In this study, we analyze the\nlimitations of previous codecs in balancing semantic richness and acoustic\nfidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict\nbetween semantic and acoustic capabilities through multi-stage, multi-task\nlearning. Experimental results demonstrate that XY-Tokenizer achieves\nperformance in both semantic and acoustic tasks comparable to that of\nstate-of-the-art codecs operating at similar bitrates, even though those\nexisting codecs typically excel in only one aspect. Specifically, XY-Tokenizer\nachieves strong text alignment, surpassing distillation-based semantic modeling\nmethods such as SpeechTokenizer and Mimi, while maintaining a speaker\nsimilarity score of 0.83 between reconstructed and original audio. The\nreconstruction performance of XY-Tokenizer is comparable to that of BigCodec,\nthe current state-of-the-art among acoustic-only codecs, which achieves a\nspeaker similarity score of 0.84 at a similar bitrate. Code and models are\navailable at https://github.com/gyt1145028706/XY-Tokenizer.",
    "published": "2025-06-29T16:51:50Z",
    "updated": "2025-07-09T17:40:35Z",
    "id": "2506.23325v2",
    "authors": [
      "Yitian Gong",
      "Luozhijie Jin",
      "Ruifan Deng",
      "Dong Zhang",
      "Xin Zhang",
      "Qinyuan Cheng",
      "Zhaoye Fei",
      "Shimin Li",
      "Xipeng Qiu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23325v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23325v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23325v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a novel speech codec that bridges speech signals and large language models, focusing on balancing semantic richness and acoustic fidelity. However, it does not directly align with the provided topics which are more centered around LLMs, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.23306v2": {
    "title": "GATSim: Urban Mobility Simulation with Generative Agents",
    "summary": "Traditional agent-based urban mobility simulations often rely on rigid\nrule-based systems that struggle to capture the complexity, adaptability, and\nbehavioral diversity inherent in human travel decision making. Recent\nadvancements in large language models and AI agent technologies present new\nopportunities to develop agents with enhanced reasoning capabilities,\npersistent memory, and adaptive learning. We introduce GATSim (Generative-Agent\nTransport Simulation), a novel framework that leverages these advancements to\nsimulate urban mobility using generative agents with rich, human-like\nbehaviors. Unlike conventional approaches, GATSim agents are characterized by\ndiverse socioeconomic profiles, individual lifestyles, and evolving preferences\nshaped through psychologically informed memory systems, tool usage, and\nlifelong learning. The main contributions of this work are: (1) a comprehensive\narchitecture that integrates an urban mobility foundation model with agent\ncognitive systems and a transport simulation environment; (2) a hierarchical\nmemory designed for efficient retrieval of contextually relevant information,\nincorporating spatial and temporal associations, keyword matching, and semantic\nrelevance; (3) innovative planning and reactive mechanisms for modeling\nadaptive mobility behaviors which integrate a multi-scale reflection process to\ntransform specific travel experiences into generalized behavioral insights. We\nimplement a prototype system and conduct systematic validation, demonstrating\nthat generative agents produce believable and coherent travel behaviors.\nExperimental results indicate that generative agents perform at least as well\nas human annotators with 92\\% posterior probability, while naturally producing\nrealistic macroscopic traffic patterns. The code for the prototype\nimplementation is publicly available at https://github.com/qiliuchn/gatsim.",
    "published": "2025-06-29T15:52:16Z",
    "updated": "2025-07-18T04:20:16Z",
    "id": "2506.23306v2",
    "authors": [
      "Qi Liu",
      "Can Li",
      "Wanjing Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23306v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23306v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23306v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of generative agents with enhanced reasoning capabilities, persistent memory, and adaptive learning, which aligns with the topics of LLM (Large Language Models) and Memory (memory-augmented models). Additionally, the focus on agent-based simulation and adaptive behaviors touches on the RL (Reinforcement Learning) topic.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "RL"
    ]
  },
  "2506.23298v3": {
    "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in\n  MLLM Few-Shot In-Context Learning for Medical Image Classification",
    "summary": "Multimodal large language models (MLLMs) have enormous potential to perform\nfew-shot in-context learning in the context of medical image analysis. However,\nsafe deployment of these models into real-world clinical practice requires an\nin-depth analysis of the accuracies of their predictions, and their associated\ncalibration errors, particularly across different demographic subgroups. In\nthis work, we present the first investigation into the calibration biases and\ndemographic unfairness of MLLMs' predictions and confidence scores in few-shot\nin-context learning for medical image classification. We introduce CALIN, an\ninference-time calibration method designed to mitigate the associated biases.\nSpecifically, CALIN estimates the amount of calibration needed, represented by\ncalibration matrices, using a bi-level procedure: progressing from the\npopulation level to the subgroup level prior to inference. It then applies this\nestimation to calibrate the predicted confidence scores during inference.\nExperimental results on three medical imaging datasets: PAPILA for fundus image\nclassification, HAM10000 for skin cancer classification, and MIMIC-CXR for\nchest X-ray classification demonstrate CALIN's effectiveness at ensuring fair\nconfidence calibration in its prediction, while improving its overall\nprediction accuracies and exhibiting minimum fairness-utility trade-off. Our\ncodebase can be found at\nhttps://github.com/xingbpshen/medical-calibration-fairness-mllm.",
    "published": "2025-06-29T15:37:17Z",
    "updated": "2025-07-17T18:00:33Z",
    "id": "2506.23298v3",
    "authors": [
      "Xing Shen",
      "Justin Szeto",
      "Mingyang Li",
      "Hengguan Huang",
      "Tal Arbel"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23298v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23298v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23298v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Multimodal Large Language Models (MLLMs) and their application in medical image classification, addressing calibration biases and demographic unfairness. The introduction of CALIN, an inference-time calibration method, is central to the study.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2506.23288v1": {
    "title": "Two Spelling Normalization Approaches Based on Large Language Models",
    "summary": "The absence of standardized spelling conventions and the organic evolution of\nhuman language present an inherent linguistic challenge within historical\ndocuments, a longstanding concern for scholars in the humanities. Addressing\nthis issue, spelling normalization endeavors to align a document's orthography\nwith contemporary standards. In this study, we propose two new approaches based\non large language models: one of which has been trained without a supervised\ntraining, and a second one which has been trained for machine translation. Our\nevaluation spans multiple datasets encompassing diverse languages and\nhistorical periods, leading us to the conclusion that while both of them\nyielded encouraging results, statistical machine translation still seems to be\nthe most suitable technology for this task.",
    "published": "2025-06-29T15:25:09Z",
    "updated": "2025-06-29T15:25:09Z",
    "id": "2506.23288v1",
    "authors": [
      "Miguel Domingo",
      "Francisco Casacuberta"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23288v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23288v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23288v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models for spelling normalization in historical documents, which involves processing and aligning text with contemporary standards. The focus on large language models and their application in text processing aligns with the LLM topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.23276v2": {
    "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in\n  Public Goods Games",
    "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim",
    "published": "2025-06-29T15:02:47Z",
    "updated": "2025-07-24T13:13:24Z",
    "id": "2506.23276v2",
    "authors": [
      "David Guzman Piedrahita",
      "Yongjin Yang",
      "Mrinmaya Sachan",
      "Giorgia Ramponi",
      "Bernhard Schlkopf",
      "Zhijing Jin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23276v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23276v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23276v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the behavior of large language models (LLMs) in social dilemmas, focusing on their cooperation and reasoning capabilities. It examines how different LLMs navigate social dilemmas, particularly in the context of public goods games, and highlights the challenges of reasoning LLMs in achieving cooperation. This aligns with topics related to LLM reasoning and their social mechanisms.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23273v1": {
    "title": "FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis",
    "summary": "Despite the advancements of large language models, text2sql still faces many\nchallenges, particularly with complex and domain-specific queries. In finance,\ndatabase designs and financial reporting layouts vary widely between financial\nentities and countries, making text2sql even more challenging. We present\nFinStat2SQL, a lightweight text2sql pipeline enabling natural language queries\nover financial statements. Tailored to local standards like VAS, it combines\nlarge and small language models in a multi-agent setup for entity extraction,\nSQL generation, and self-correction. We build a domain-specific database and\nevaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves\n61.33\\% accuracy with sub-4-second response times on consumer hardware,\noutperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient\nsolution for financial analysis, making AI-powered querying accessible to\nVietnamese enterprises.",
    "published": "2025-06-29T14:55:21Z",
    "updated": "2025-06-29T14:55:21Z",
    "id": "2506.23273v1",
    "authors": [
      "Quang Hung Nguyen",
      "Phuong Anh Trinh",
      "Phan Quoc Hung Mai",
      "Tuan Phong Trinh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23273v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23273v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23273v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models in a specific domain (financial statement analysis) and involves text-to-SQL tasks, which are related to reasoning and domain-specific applications of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23260v1": {
    "title": "From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI\n  Agents Workflows",
    "summary": "Autonomous AI agents powered by large language models (LLMs) with structured\nfunction-calling interfaces have dramatically expanded capabilities for\nreal-time data retrieval, complex computation, and multi-step orchestration.\nYet, the explosive proliferation of plugins, connectors, and inter-agent\nprotocols has outpaced discovery mechanisms and security practices, resulting\nin brittle integrations vulnerable to diverse threats. In this survey, we\nintroduce the first unified, end-to-end threat model for LLM-agent ecosystems,\nspanning host-to-tool and agent-to-agent communications, formalize adversary\ncapabilities and attacker objectives, and catalog over thirty attack\ntechniques. Specifically, we organized the threat model into four domains:\nInput Manipulation (e.g., prompt injections, long-context hijacks, multimodal\nadversarial inputs), Model Compromise (e.g., prompt- and parameter-level\nbackdoors, composite and encrypted multi-backdoors, poisoning strategies),\nSystem and Privacy Attacks (e.g., speculative side-channels, membership\ninference, retrieval poisoning, social-engineering simulations), and Protocol\nVulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent\nCommunication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent\n(A2A) protocol). For each category, we review representative scenarios, assess\nreal-world feasibility, and evaluate existing defenses. Building on our threat\ntaxonomy, we identify key open challenges and future research directions, such\nas securing MCP deployments through dynamic trust management and cryptographic\nprovenance tracking; designing and hardening Agentic Web Interfaces; and\nachieving resilience in multi-agent and federated environments. Our work\nprovides a comprehensive reference to guide the design of robust defense\nmechanisms and establish best practices for resilient LLM-agent workflows.",
    "published": "2025-06-29T14:32:32Z",
    "updated": "2025-06-29T14:32:32Z",
    "id": "2506.23260v1",
    "authors": [
      "Mohamed Amine Ferrag",
      "Norbert Tihanyi",
      "Djallel Hamouda",
      "Leandros Maglaras",
      "Merouane Debbah"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23260v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23260v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23260v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses threats and security issues in LLM-powered AI agents, focusing on vulnerabilities in LLM-agent ecosystems, which aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning, particularly in the context of agents).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.23253v1": {
    "title": "Vibe coding: programming through conversation with artificial\n  intelligence",
    "summary": "We examine \"vibe coding\": an emergent programming paradigm where developers\nprimarily write code by interacting with code-generating large language models\nrather than writing code directly. We analysed a curated set of videos\ndepicting extended vibe coding sessions with rich think-aloud reflections.\nUsing framework analysis, we investigated programmers' goals, workflows,\nprompting techniques, debugging approaches, and challenges encountered. We find\nthat vibe coding follows iterative goal satisfaction cycles where developers\nalternate between prompting AI, evaluating generated code through rapid\nscanning and application testing, and manual editing. Prompting strategies\nblend vague, high-level directives with detailed technical specifications.\nDebugging remains a hybrid process combining AI assistance with manual\npractices. Critically, vibe coding does not eliminate the need for programming\nexpertise but rather redistributes it toward context management, rapid code\nevaluation, and decisions about when to transition between AI-driven and manual\nmanipulation of code. Trust in AI tools during vibe coding is dynamic and\ncontextual, developed through iterative verification rather than blanket\nacceptance. Vibe coding is an evolution of AI-assisted programming that\nrepresents an early manifestation of \"material disengagement\", where\npractitioners orchestrate code production and manipulation, mediated through\nAI, while maintaining selective and strategic oversight.",
    "published": "2025-06-29T14:19:29Z",
    "updated": "2025-06-29T14:19:29Z",
    "id": "2506.23253v1",
    "authors": [
      "Advait Sarkar",
      "Ian Drosos"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23253v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23253v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23253v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in a programming context, focusing on how developers interact with these models to generate and evaluate code. This aligns with the 'LLM' topic, which covers research on Large Language Models and their applications. Additionally, the iterative interaction and evaluation process described in the abstract suggests elements of 'Reasoning', as it involves complex problem-solving and decision-making by the developers. The paper does not directly address the other topics listed.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.00079v1": {
    "title": "VoyagerVision: Investigating the Role of Multi-modal Information for\n  Open-ended Learning Systems",
    "summary": "Open-endedness is an active field of research in the pursuit of capable\nArtificial General Intelligence (AGI), allowing models to pursue tasks of their\nown choosing. Simultaneously, recent advancements in Large Language Models\n(LLMs) such as GPT-4o [9] have allowed such models to be capable of\ninterpreting image inputs. Implementations such as OMNI-EPIC [4] have made use\nof such features, providing an LLM with pixel data of an agent's POV to parse\nthe environment and allow it to solve tasks. This paper proposes that providing\nthese visual inputs to a model gives it greater ability to interpret spatial\nenvironments, and as such, can increase the number of tasks it can successfully\nperform, extending its open-ended potential. To this aim, this paper proposes\nVoyagerVision -- a multi-modal model capable of creating structures within\nMinecraft using screenshots as a form of visual feedback, building on the\nfoundation of Voyager. VoyagerVision was capable of creating an average of 2.75\nunique structures within fifty iterations of the system, as Voyager was\nincapable of this, it is an extension in an entirely new direction.\nAdditionally, in a set of building unit tests VoyagerVision was successful in\nhalf of all attempts in flat worlds, with most failures arising in more complex\nstructures. Project website is available at\nhttps://esmyth-dev.github.io/VoyagerVision.github.io/",
    "published": "2025-06-29T14:16:11Z",
    "updated": "2025-06-29T14:16:11Z",
    "id": "2507.00079v1",
    "authors": [
      "Ethan Smyth",
      "Alessandro Suglia"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00079v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00079v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00079v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of multi-modal information (visual inputs) with Large Language Models (LLMs) to enhance open-ended learning systems, which aligns with the topics of Multimodal Large Language Models (MLLM) and Artificial General Intelligence (AGI). The use of visual feedback to improve task performance also relates to Vision-Language Action (VLA) models.",
    "llm_cls_result": [
      "MLLM",
      "AGI",
      "VLA"
    ]
  },
  "2507.00078v1": {
    "title": "The language of time: a language model perspective on time-series\n  foundation models",
    "summary": "With the rise of large language models, the paradigm of training foundation\nmodels with massive parameter counts on vast datasets has been adopted in\nmultiple domains to achieve remarkable success. Time series foundation models\nrepresent a significant extension of this paradigm, demonstrating exceptional\nexpressive power, generalization, and cross-domain transferability. However,\nthis gives rise to a fundamental paradox: time series data reflect distinct\ndynamical systems, making cross-domain transfer intuitively implausible, yet\nthis is contradicted by the models' empirical success. To resolve this paradox,\nthis paper investigates, from both theoretical and experimental perspectives,\nthe representation learning mechanisms and generalization capabilities of\npatch-based time series foundation models. We argue that such models are not\nmerely applying a new architecture but are fundamentally generalizing the\nrepresentation paradigm of language models by extending deterministic\nvector-based representations to latent probabilistic distributional forms. Our\ntheoretical analysis supports this framework by demonstrating that continuous\ntime-series patches can be faithfully quantized into a discrete vocabulary\nwhose key statistical properties are highly consistent with those of natural\nlanguage. This generalization allows time series models to inherit the robust\nrepresentation and transfer abilities of large language models, thereby\nexplaining their superior performance in temporal tasks. Ultimately, our work\nprovides a rigorous theoretical cornerstone for understanding, evaluating, and\nimproving the safety and reliability of large-scale time series foundation\nmodels.",
    "published": "2025-06-29T14:03:34Z",
    "updated": "2025-06-29T14:03:34Z",
    "id": "2507.00078v1",
    "authors": [
      "Yi Xie",
      "Yun Xiong",
      "Zejian Shi",
      "Hao Niu",
      "Zhengfu Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00078v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00078v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00078v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the extension of large language models to time-series foundation models, focusing on their representation learning mechanisms and generalization capabilities. It draws parallels between language models and time-series models, suggesting a generalization of the representation paradigm. The core topics are related to LLMs and their applications in different domains.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.23235v1": {
    "title": "Generalist Reward Models: Found Inside Large Language Models",
    "summary": "The alignment of Large Language Models (LLMs) is critically dependent on\nreward models trained on costly human preference data. While recent work\nexplores bypassing this cost with AI feedback, these methods often lack a\nrigorous theoretical foundation. In this paper, we discover that a powerful\ngeneralist reward model is already latently present within any LLM trained via\nstandard next-token prediction. We prove that this endogenous reward is not a\nheuristic, but is theoretically equivalent to a reward function learned through\noffline inverse reinforcement learning. This connection allows us to directly\nelicit a high-quality reward signal from a base (pre-trained or supervised\nfine-tuned) model without any further training. Critically, we also prove that\nsubsequent reinforcement learning using this endogenous reward leads to a\npolicy with a provably superior error bound compared to the base model. To our\nbest knowledge, this is the first theoretical proof of the effectiveness of\nreinforcement learning for LLMs. Our experiments validate this theory,\ndemonstrating that our method not only outperforms existing LLM-as-a-judge\napproaches but can also surpass explicitly trained reward models. These\nfindings suggest that the reward modeling stage can be replaced by a principled\nmethod of eliciting the knowledge already captured during pre-training,\nheralding a more efficient, powerful, and scalable paradigm for LLMs alignment\nas well as multi-modal models.",
    "published": "2025-06-29T13:45:54Z",
    "updated": "2025-06-29T13:45:54Z",
    "id": "2506.23235v1",
    "authors": [
      "Yi-Chen Li",
      "Tian Xu",
      "Yang Yu",
      "Xuqin Zhang",
      "Xiong-Hui Chen",
      "Zhongxiang Ling",
      "Ningjing Chao",
      "Lei Yuan",
      "Zhi-Hua Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23235v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23235v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23235v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the alignment of Large Language Models (LLMs) and the use of reward models, which is closely related to Reinforcement Learning (RL) and LLM research. It also touches on the theoretical foundations and practical applications of these models, aligning with the topics of RL and LLM.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2506.23230v1": {
    "title": "Digital Transformation and the Restructuring of Employment: Evidence\n  from Chinese Listed Firms",
    "summary": "This paper examines how digital transformation reshapes employment structures\nwithin Chinese listed firms, focusing on occupational functions and task\nintensity. Drawing on recruitment data classified under ISCO-08 and the Chinese\nStandard Occupational Classification 2022, we categorize jobs into five\nfunctional groups: management, professional, technical, auxiliary, and manual.\nUsing a task-based framework, we construct routine, abstract, and manual task\nintensity indices through keyword analysis of job descriptions. We find that\ndigitalization is associated with increased hiring in managerial, professional,\nand technical roles, and reduced demand for auxiliary and manual labor. At the\ntask level, abstract task demand rises, while routine and manual tasks decline.\nModeration analyses link these shifts to improvements in managerial efficiency\nand executive compensation. Our findings highlight how emerging technologies,\nincluding large language models (LLMs), are reshaping skill demands and labor\ndynamics in Chinas corporate sector.",
    "published": "2025-06-29T13:28:17Z",
    "updated": "2025-06-29T13:28:17Z",
    "id": "2506.23230v1",
    "authors": [
      "Yubo Cheng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23230v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23230v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23230v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of digital transformation, including large language models (LLMs), on employment structures in Chinese firms. However, the primary focus is on labor dynamics and skill demands rather than the technical aspects of LLMs or other specific topics in the provided list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.23225v1": {
    "title": "Masked Gated Linear Unit",
    "summary": "Gated Linear Units (GLUs) have become essential components in the\nfeed-forward networks of state-of-the-art Large Language Models (LLMs).\nHowever, they require twice as many memory reads compared to feed-forward\nlayers without gating, due to the use of separate weight matrices for the gate\nand value streams. To address this bottleneck, we introduce Masked Gated Linear\nUnits (MGLUs), a novel family of GLUs with an efficient kernel implementation.\nThe core contribution of MGLUs include: (1) the Mixture of Element-wise Gating\n(MoEG) architecture that learns multiple binary masks, each determining gate or\nvalue assignments at the element level on a single shared weight matrix\nresulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly\nkernel that yields up to a 19.7 $\\times$ inference-time speed-up over a naive\nPyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs\ndespite added architectural complexity on an RTX5090 GPU. In LLM experiments,\nthe Swish-activated variant SwiMGLU preserves its memory advantages while\nmatching - or even surpassing - the downstream accuracy of the SwiGLU baseline.",
    "published": "2025-06-29T13:16:20Z",
    "updated": "2025-06-29T13:16:20Z",
    "id": "2506.23225v1",
    "authors": [
      "Yukito Tajima",
      "Nakamasa Inoue",
      "Yusuke Sekikawa",
      "Ikuro Sato",
      "Rio Yokota"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23225v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23225v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23225v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces Masked Gated Linear Units (MGLUs) which are improvements over traditional Gated Linear Units (GLUs) used in Large Language Models (LLMs). It discusses the architectural enhancements and efficiency gains, which are relevant to LLM architectures and scaling.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.23219v1": {
    "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence\n  with Spatial Reasoning and Understanding",
    "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\n$\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.",
    "published": "2025-06-29T13:04:27Z",
    "updated": "2025-06-29T13:04:27Z",
    "id": "2506.23219v1",
    "authors": [
      "Jie Feng",
      "Shengyuan Wang",
      "Tianhui Liu",
      "Yanxin Xi",
      "Yong Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23219v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23219v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23219v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a multi-modal large language model (MLLM) designed for urban intelligence, which involves processing multi-modal data and spatial reasoning. It also discusses the creation of a dataset and benchmarking for urban tasks.",
    "llm_cls_result": [
      "MLLM",
      "Dataset",
      "Benchmark"
    ]
  },
  "2506.23180v1": {
    "title": "ImprovMate: Multimodal AI Assistant for Improv Actor Training",
    "summary": "Improvisation training for actors presents unique challenges, particularly in\nmaintaining narrative coherence and managing cognitive load during\nperformances. Previous research on AI in improvisation performance often\npredates advances in large language models (LLMs) and relies on human\nintervention. We introduce ImprovMate, which leverages LLMs as GPTs to automate\nthe generation of narrative stimuli and cues, allowing actors to focus on\ncreativity without keeping track of plot or character continuity. Based on\ninsights from professional improvisers, ImprovMate incorporates exercises that\nmimic live training, such as abrupt story resolution and reactive thinking\nexercises, while maintaining coherence via reference tables. By balancing\nrandomness and structured guidance, ImprovMate provides a groundbreaking tool\nfor improv training. Our pilot study revealed that actors might embrace AI\ntechniques if the latter mirrors traditional practices, and appreciate the\nfresh twist introduced by our approach with the AI-generated cues.",
    "published": "2025-06-29T10:28:13Z",
    "updated": "2025-06-29T10:28:13Z",
    "id": "2506.23180v1",
    "authors": [
      "Riccardo Drago",
      "Yotam Sechayk",
      "Mustafa Doga Dogan",
      "Andrea Sanna",
      "Takeo Igarashi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23180v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23180v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23180v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in a multimodal AI assistant for improvisation training, which involves both language and potentially other modalities (e.g., audio or visual cues). The focus on LLMs and their application in a multimodal context aligns with the topics 'LLM' and 'MLLM'.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2506.23154v1": {
    "title": "Can LLM Improve for Expert Forecast Combination? Evidence from the\n  European Central Bank Survey",
    "summary": "This study explores the potential of large language models (LLMs) to enhance\nexpert forecasting through ensemble learning. Leveraging the European Central\nBank's Survey of Professional Forecasters (SPF) dataset, we propose a\ncomprehensive framework to evaluate LLM-driven ensemble predictions under\nvarying conditions, including the intensity of expert disagreement, dynamics of\nherd behavior, and limitations in attention allocation.",
    "published": "2025-06-29T09:22:08Z",
    "updated": "2025-06-29T09:22:08Z",
    "id": "2506.23154v1",
    "authors": [
      "Yinuo Ren",
      "Jue Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23154v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23154v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23154v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in enhancing expert forecasting, which directly relates to research on LLMs and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.23149v1": {
    "title": "V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse\n  In-Context Demonstrations from Scratch via V-Entropy",
    "summary": "High labeling cost for in-context learning (ICL) demonstrations motivates\nusing large language models (LLMs) for synthesis to reduce overhead. However,\nexisting synthesis methods are mainly task-specific or rely on pre-existing\ndemonstrations. So this paper focuses on synthesizing demonstrations from\nscratch for arbitrary tasks. A major challenge in synthesizing from scratch is\nensuring consistency with the target task, as the lack of labeling guidance\ncould lead to synthesis bias. We first propose a consistency metric called\nV-Score, which has higher performance and lower computation cost compared with\nthe metrics based on grams or embedding vectors. Furthermore, we introduce\nV-Synthesis, which leverages V-Score for proportional sampling to ensure both\nhigh consistency and diversity of synthesized demonstrations. Experimental\nresults demonstrate that V-Synthesis yields an average performance improvement\nof 2.0% compared to existing synthesis methods confirming the effectiveness of\nV-Synthesis.",
    "published": "2025-06-29T08:57:09Z",
    "updated": "2025-06-29T08:57:09Z",
    "id": "2506.23149v1",
    "authors": [
      "Dingzirui Wang",
      "Xuanliang Zhang",
      "Keyan Xu",
      "Qingfu Zhu",
      "Wanxiang Che",
      "Yang Deng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23149v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23149v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23149v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the synthesis of in-context learning demonstrations using large language models (LLMs) and introduces a new method called V-Synthesis. It focuses on task-agnostic synthesis and consistency metrics, which are relevant to LLM research and reasoning abilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23146v3": {
    "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness\n  Beyond Performance Illusions",
    "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.",
    "published": "2025-06-29T08:55:37Z",
    "updated": "2025-07-13T15:01:01Z",
    "id": "2506.23146v3",
    "authors": [
      "Dingzriui Wang",
      "Xuanliang Zhang",
      "Keyan Xu",
      "Qingfu Zhu",
      "Wanxiang Che",
      "Yang Deng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23146v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23146v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23146v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the effectiveness of in-context learning (ICL) in large language models (LLMs), proposing a novel metric to quantify ICL effectiveness. The core topics are related to LLM evaluation and reasoning abilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2506.23136v1": {
    "title": "LLM-Assisted Question-Answering on Technical Documents Using Structured\n  Data-Aware Retrieval Augmented Generation",
    "summary": "Large Language Models (LLMs) are capable of natural language understanding\nand generation. But they face challenges such as hallucination and outdated\nknowledge. Fine-tuning is one possible solution, but it is resource-intensive\nand must be repeated with every data update. Retrieval-Augmented Generation\n(RAG) offers an efficient solution by allowing LLMs to access external\nknowledge sources. However, traditional RAG pipelines struggle with retrieving\ninformation from complex technical documents with structured data such as\ntables and images. In this work, we propose a RAG pipeline, capable of handling\ntables and images in documents, for technical documents that support both\nscanned and searchable formats. Its retrieval process combines vector\nsimilarity search with a fine-tuned reranker based on Gemma-2-9b-it. The\nreranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom\ndataset designed to improve context identification for question answering. Our\nevaluation demonstrates that the proposed pipeline achieves a high faithfulness\nscore of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%\n(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed\narchitecture is superior to general RAG pipelines in terms of table-based\nquestions and handling questions outside context.",
    "published": "2025-06-29T08:22:03Z",
    "updated": "2025-06-29T08:22:03Z",
    "id": "2506.23136v1",
    "authors": [
      "Shadman Sobhan",
      "Mohammad Ariful Haque"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23136v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23136v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23136v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in conjunction with Retrieval-Augmented Generation (RAG) to improve question-answering on technical documents. It specifically addresses challenges like hallucination and outdated knowledge in LLMs, and proposes a RAG pipeline that handles structured data like tables and images. The topics 'LLM' and 'Memory' are relevant as the paper focuses on LLMs and their augmentation with external memory (RAG). The topic 'Reasoning' is also relevant as the paper involves improving the reasoning capabilities of LLMs through better context retrieval and question-answering.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Reasoning"
    ]
  },
  "2506.23133v1": {
    "title": "Format-Adapter: Improving Reasoning Capability of LLMs by Adapting\n  Suitable Format",
    "summary": "Generating and voting multiple answers is an effective method to mitigate\nreasoning inconsistencies of large language models (LLMs). Prior works have\nshown that multiple reasoning formats outperform a single format when\ngenerating multiple answers. However, previous works using multiple formats\nrely on formats labeled by humans, which could be unsuitable for all tasks and\nhave high labeling costs. To address this issue, we adapt suitable formats to\nthe given tasks by generating and selecting formats. We first propose how to\nmeasure the reasoning error when generating multiple answers. Then, we\nintroduce Format-Adapter, which utilizes LLMs to generate and select suitable\nreasoning formats by minimizing the error measurement we present. We conduct\nexperiments on math and commonsense reasoning tasks, where Format-Adapter\nachieves a 4.3% performance improvement on average over previous works,\ndemonstrating the effectiveness.",
    "published": "2025-06-29T08:11:52Z",
    "updated": "2025-06-29T08:11:52Z",
    "id": "2506.23133v1",
    "authors": [
      "Dingzirui Wang",
      "Xuanliang Zhang",
      "Rongyu Cao",
      "Longxu Dou",
      "Xianzhen Luo",
      "Yingwei Ma",
      "Qingfu Zhu",
      "Wanxiang Che",
      "Binhua Li",
      "Fei Huang",
      "Yongbin Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23133v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23133v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23133v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the reasoning capabilities of LLMs by adapting suitable formats, which aligns with the 'Reasoning' topic. It also involves the use of LLMs, which is relevant to the 'LLM' topic.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2506.23127v1": {
    "title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement\n  Learning",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they face significant challenges in embodied task planning\nscenarios that require continuous environmental understanding and action\ngeneration. Existing approaches generate open-loop action scripts based on\nstatic knowledge, making it difficult to learn causal relationships between\nactions and environmental feedback, particularly in partially observable\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\nreinforcement learning framework that enables LLMs to develop interactive\ncapabilities through autonomous exploration with minimal supervision. Our\nframework incorporates three key innovations: (1) Without human annotations, we\nemploy pure reinforcement learning with group rollout, incorporating\nin-environment interaction through parallel exploration; (2) completion-driven\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\nlearning from grouped trajectories. Across two challenging text-based Embodied\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\nevidencing strong generalization.",
    "published": "2025-06-29T07:31:24Z",
    "updated": "2025-06-29T07:31:24Z",
    "id": "2506.23127v1",
    "authors": [
      "Zhaoye Fei",
      "Li Ji",
      "Siyin Wang",
      "Junhao Shi",
      "Jingjing Gong",
      "Xipeng Qiu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23127v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23127v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23127v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the embodied task planning abilities of LLMs through reinforcement learning, which directly relates to the topics of LLM and RL. The use of reinforcement learning to improve LLM capabilities in interactive environments is a key aspect of the research.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.23120v1": {
    "title": "Enhancing Spatial Reasoning in Multimodal Large Language Models through\n  Reasoning-based Segmentation",
    "summary": "Recent advances in point cloud perception have demonstrated remarkable\nprogress in scene understanding through vision-language alignment leveraging\nlarge language models (LLMs). However, existing methods may still encounter\nchallenges in handling complex instructions that require accurate spatial\nreasoning, even if the 3D point cloud data provides detailed spatial cues such\nas size and position for identifying the targets. To tackle this issue, we\npropose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based\nsegmentation framework. The framework emulates human cognitive processes by\ndecomposing spatial reasoning into two sequential stages: first identifying\nrelevant elements, then processing instructions guided by their associated\nvisual priors. Furthermore, acknowledging the inadequacy of existing datasets\nin complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based\nsegmentation dataset comprising 25,185 training samples and 3,966 validation\nsamples with precise annotations. Both quantitative and qualitative experiments\ndemonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud\nperception with stronger spatial reasoning capabilities, and we hope that they\ncan serve as a new baseline and benchmark for future work.",
    "published": "2025-06-29T06:58:08Z",
    "updated": "2025-06-29T06:58:08Z",
    "id": "2506.23120v1",
    "authors": [
      "Zhenhua Ning",
      "Zhuotao Tian",
      "Shaoshuai Shi",
      "Guangming Lu",
      "Daojing He",
      "Wenjie Pei",
      "Li Jiang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23120v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23120v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23120v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing spatial reasoning in Multimodal Large Language Models (MLLMs) through a reasoning-based segmentation framework and introduces a new dataset for complex reasoning tasks. The core topics are MLLM for multimodal integration, Reasoning for spatial reasoning capabilities, and Benchmark for the new dataset and evaluation metrics.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2507.00075v1": {
    "title": "Theoretical Modeling of LLM Self-Improvement Training Dynamics Through\n  Solver-Verifier Gap",
    "summary": "Self-improvement is among the most prominent techniques within the realm of\nlarge language models (LLM), aiming to enhance the LLM performance without\nrelying on external data. Despite its significance, generally how LLM\nperformances evolve during the self-improvement process remains underexplored.\nIn this paper, we theoretically model the training dynamics of self-improvement\nvia the concept of solver-verifier gap. This is inspired by the conjecture that\nthe performance enhancement of self-improvement stems from the gap between\nLLM's solver capability and verifier capability. Based on the theoretical\nframework, we further introduce how to predict the ultimate power of\nself-improvement using only information from the first few training epochs. We\nempirically validate the effectiveness of the theoretical model on various LLMs\nand datasets. Beyond self-improvement, we extend our analysis to investigate\nhow external data influences these dynamics within the framework. Notably, we\nfind that under limited external data regimes, such external data can be\nutilized at any stage without significantly affecting final performances, which\naccords with the empirical observations.",
    "published": "2025-06-29T06:48:47Z",
    "updated": "2025-06-29T06:48:47Z",
    "id": "2507.00075v1",
    "authors": [
      "Yifan Sun",
      "Yushan Liang",
      "Zhen Zhang",
      "Jiaye Teng"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00075v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00075v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00075v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the theoretical modeling of self-improvement in large language models (LLMs), which directly relates to the study of LLMs and their training dynamics. It also touches on the performance enhancement through solver-verifier gap, which is a specific aspect of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.23107v1": {
    "title": "Can Large Language Models Capture Human Risk Preferences? A\n  Cross-Cultural Study",
    "summary": "Large language models (LLMs) have made significant strides, extending their\napplications to dialogue systems, automated content creation, and\ndomain-specific advisory tasks. However, as their use grows, concerns have\nemerged regarding their reliability in simulating complex decision-making\nbehavior, such as risky decision-making, where a single choice can lead to\nmultiple outcomes. This study investigates the ability of LLMs to simulate\nrisky decision-making scenarios. We compare model-generated decisions with\nactual human responses in a series of lottery-based tasks, using transportation\nstated preference survey data from participants in Sydney, Dhaka, Hong Kong,\nand Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and\nChatGPT o1-mini -- which were tasked with predicting individual choices. Risk\npreferences were analyzed using the Constant Relative Risk Aversion (CRRA)\nframework. Results show that both models exhibit more risk-averse behavior than\nhuman participants, with o1-mini aligning more closely with observed human\ndecisions. Further analysis of multilingual data from Nanjing and Hong Kong\nindicates that model predictions in Chinese deviate more from actual responses\ncompared to English, suggesting that prompt language may influence simulation\nperformance. These findings highlight both the promise and the current\nlimitations of LLMs in replicating human-like risk behavior, particularly in\nlinguistic and cultural settings.",
    "published": "2025-06-29T06:16:57Z",
    "updated": "2025-06-29T06:16:57Z",
    "id": "2506.23107v1",
    "authors": [
      "Bing Song",
      "Jianing Liu",
      "Sisi Jian",
      "Chenyang Wu",
      "Vinayak Dixit"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23107v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23107v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23107v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the ability of Large Language Models (LLMs) to simulate human risk preferences, which directly involves research on LLMs and their applications in decision-making scenarios.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.23101v1": {
    "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal\n  Large Language Models from the Lens of Social Relationship",
    "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nacross tasks involving both visual and textual modalities. However, growing\nconcerns remain about their potential to encode and amplify gender bias,\nparticularly in socially sensitive applications. Existing benchmarks\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\nemerge subtly through interpersonal interactions. We fill this gap by going\nbeyond single-entity evaluation and instead focusing on a deeper examination of\nrelational and contextual gender bias in dual-individual interactions. We\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\nthrough the lens of social relationships in generated narratives. Genres\nassesses gender bias through a dual-character profile and narrative generation\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\nevaluation suite across multiple dimensions. Experiments on both open- and\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\nnot evident in single-character settings. Our findings underscore the\nimportance of relationship-aware benchmarks for diagnosing subtle,\ninteraction-driven gender bias in MLLMs and provide actionable insights for\nfuture bias mitigation.",
    "published": "2025-06-29T06:03:21Z",
    "updated": "2025-06-29T06:03:21Z",
    "id": "2506.23101v1",
    "authors": [
      "Yue Xu",
      "Wenjie Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23101v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23101v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23101v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating gender bias in Multimodal Large Language Models (MLLMs) through a novel benchmark, which involves social relationships and interactions. This aligns with the topics of MLLM (Multimodal Large Language Models) and Benchmark (evaluation metrics and performance comparison).",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2506.23100v1": {
    "title": "Repair Ingredients Are All You Need: Improving Large Language\n  Model-Based Program Repair via Repair Ingredients Search",
    "summary": "Automated Program Repair (APR) techniques aim to automatically fix buggy\nprograms. Among these, Large Language Model-based (LLM-based) approaches have\nshown great promise. Recent advances demonstrate that directly leveraging LLMs\ncan achieve leading results. However, these techniques remain suboptimal in\ngenerating contextually relevant and accurate patches, as they often overlook\nrepair ingredients crucial for practical program repair. In this paper, we\npropose ReinFix, a novel framework that enables LLMs to autonomously search for\nrepair ingredients throughout both the reasoning and solution phases of bug\nfixing. In the reasoning phase, ReinFix integrates static analysis tools to\nretrieve internal ingredients, such as variable definitions, to assist the LLM\nin root cause analysis when it encounters difficulty understanding the context.\nDuring the solution phase, when the LLM lacks experience in fixing specific\nbugs, ReinFix searches for external ingredients from historical bug fixes with\nsimilar bug patterns, leveraging both the buggy code and its root cause to\nguide the LLM in identifying appropriate repair actions, thereby increasing the\nlikelihood of generating correct patches. Evaluations on two popular benchmarks\n(Defects4J V1.2 and V2.0) demonstrate the effectiveness of our approach over\nSOTA baselines. Notably, ReinFix fixes 146 bugs, which is 32 more than the\nbaselines on Defects4J V1.2. On Defects4J V2.0, ReinFix fixes 38 more bugs than\nthe SOTA. Importantly, when evaluating on the recent benchmarks that are free\nof data leakage risk, ReinFix also maintains the best performance.",
    "published": "2025-06-29T06:02:11Z",
    "updated": "2025-06-29T06:02:11Z",
    "id": "2506.23100v1",
    "authors": [
      "Jiayi Zhang",
      "Kai Huang",
      "Jian Zhang",
      "Yang Liu",
      "Chunyang Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23100v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23100v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23100v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving Large Language Model (LLM)-based approaches for Automated Program Repair (APR) by integrating repair ingredients search during the reasoning and solution phases. This directly involves the use and enhancement of LLMs in a specific application context.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.01990v1": {
    "title": "Integrating Large Language Models in Financial Investments and Market\n  Analysis: A Survey",
    "summary": "Large Language Models (LLMs) have been employed in financial decision making,\nenhancing analytical capabilities for investment strategies. Traditional\ninvestment strategies often utilize quantitative models, fundamental analysis,\nand technical indicators. However, LLMs have introduced new capabilities to\nprocess and analyze large volumes of structured and unstructured data, extract\nmeaningful insights, and enhance decision-making in real-time. This survey\nprovides a structured overview of recent research on LLMs within the financial\ndomain, categorizing research contributions into four main frameworks:\nLLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and\nAdaptation Approaches, and Agent-Based Architectures. This study provides a\nstructured review of recent LLMs research on applications in stock selection,\nrisk assessment, sentiment analysis, trading, and financial forecasting. By\nreviewing the existing literature, this study highlights the capabilities,\nchallenges, and potential directions of LLMs in financial markets.",
    "published": "2025-06-29T05:25:31Z",
    "updated": "2025-06-29T05:25:31Z",
    "id": "2507.01990v1",
    "authors": [
      "Sedigheh Mahdavi",
      " Jiating",
      " Chen",
      "Pradeep Kumar Joshi",
      "Lina Huertas Guativa",
      "Upmanyu Singh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01990v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01990v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01990v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in financial decision making, which directly relates to the 'LLM' topic. It also mentions fine-tuning and adaptation approaches, which are part of the 'Pretrain' topic, and agent-based architectures, which are part of the 'RL' topic.",
    "llm_cls_result": [
      "LLM",
      "Pretrain",
      "RL"
    ]
  },
  "2506.23088v1": {
    "title": "Where, What, Why: Towards Explainable Driver Attention Prediction",
    "summary": "Modeling task-driven attention in driving is a fundamental challenge for both\nautonomous vehicles and cognitive science. Existing methods primarily predict\nwhere drivers look by generating spatial heatmaps, but fail to capture the\ncognitive motivations behind attention allocation in specific contexts, which\nlimits deeper understanding of attention mechanisms. To bridge this gap, we\nintroduce Explainable Driver Attention Prediction, a novel task paradigm that\njointly predicts spatial attention regions (where), parses attended semantics\n(what), and provides cognitive reasoning for attention allocation (why). To\nsupport this, we present W3DA, the first large-scale explainable driver\nattention dataset. It enriches existing benchmarks with detailed semantic and\ncausal annotations across diverse driving scenarios, including normal\nconditions, safety-critical situations, and traffic accidents. We further\npropose LLada, a Large Language model-driven framework for driver attention\nprediction, which unifies pixel modeling, semantic parsing, and cognitive\nreasoning within an end-to-end architecture. Extensive experiments demonstrate\nthe effectiveness of LLada, exhibiting robust generalization across datasets\nand driving conditions. This work serves as a key step toward a deeper\nunderstanding of driver attention mechanisms, with significant implications for\nautonomous driving, intelligent driver training, and human-computer\ninteraction.",
    "published": "2025-06-29T04:59:39Z",
    "updated": "2025-06-29T04:59:39Z",
    "id": "2506.23088v1",
    "authors": [
      "Yuchen Zhou",
      "Jiayu Tang",
      "Xiaoyan Xiao",
      "Yueyao Lin",
      "Linkai Liu",
      "Zipeng Guo",
      "Hao Fei",
      "Xiaobo Xia",
      "Chao Gou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23088v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23088v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23088v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on driver attention prediction using a large language model-driven framework, which involves multimodal understanding and reasoning, but does not directly align with the provided topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.23079v1": {
    "title": "Research on Comprehensive Classroom Evaluation System Based on Multiple\n  AI Models",
    "summary": "The promotion of the national education digitalization strategy has\nfacilitated the development of teaching quality evaluation towards all-round,\nprocess-oriented, precise, and intelligent directions, inspiring explorations\ninto new methods and technologies for educational quality assurance. Classroom\nteaching evaluation methods dominated by teaching supervision and student\nteaching evaluation suffer from issues such as low efficiency, strong\nsubjectivity, and limited evaluation dimensions. How to further advance\nintelligent and objective evaluation remains a topic to be explored. This\npaper, based on image recognition technology, speech recognition technology,\nand AI large language models, develops a comprehensive evaluation system that\nautomatically generates evaluation reports and optimization suggestions from\ntwo dimensions: teacher teaching ability and classroom teaching effectiveness.\nThis study establishes a closed-loop classroom evaluation model that\ncomprehensively evaluates student and teaching conditions based on\nmulti-dimensional data throughout the classroom teaching process, and further\nanalyzes the data to guide teaching improvement. It meets the requirements of\nall-round and process-oriented classroom evaluation in the era of digital\neducation, effectively solves the main problems of manual evaluation methods,\nand provides data collection and analysis methods as well as technologies for\nrelevant research on educational teaching evaluation.",
    "published": "2025-06-29T04:06:55Z",
    "updated": "2025-06-29T04:06:55Z",
    "id": "2506.23079v1",
    "authors": [
      "Cong Xie",
      "Li Yang",
      "Daben Wang",
      "Jing Xiao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23079v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23079v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23079v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of AI models, including large language models, for classroom evaluation, but it does not specifically focus on the core topics listed such as LLM, RL, MLLM, etc. Instead, it is more about the application of AI in education.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.23056v1": {
    "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced\n  Tree Search Reasoning",
    "summary": "Molecular structure elucidation involves deducing a molecule's structure from\nvarious types of spectral data, which is crucial in chemical experimental\nanalysis. While large language models (LLMs) have shown remarkable proficiency\nin analyzing and reasoning through complex tasks, they still encounter\nsubstantial challenges in molecular structure elucidation. We identify that\nthese challenges largely stem from LLMs' limited grasp of specialized chemical\nknowledge. In this work, we introduce a Knowledge-enhanced reasoning framework\nfor Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search\nfor test-time scaling as a plugin. Specifically, we construct an external\nmolecular substructure knowledge base to extend the LLMs' coverage of the\nchemical structure space. Furthermore, we design a specialized\nmolecule-spectrum scorer to act as a reward model for the reasoning process,\naddressing the issue of inaccurate solution evaluation in LLMs. Experimental\nresults show that our approach significantly boosts performance, particularly\ngaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is\navailable at https://github.com/HICAI-ZJU/K-MSE.",
    "published": "2025-06-29T02:00:38Z",
    "updated": "2025-06-29T02:00:38Z",
    "id": "2506.23056v1",
    "authors": [
      "Xiang Zhuang",
      "Bin Wu",
      "Jiyu Cui",
      "Kehua Feng",
      "Xiaotong Li",
      "Huabin Xing",
      "Keyan Ding",
      "Qiang Zhang",
      "Huajun Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23056v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23056v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23056v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the reasoning capabilities of LLMs in the specific domain of molecular structure elucidation, utilizing knowledge enhancement and tree search reasoning. This aligns with the 'Reasoning' and 'LLM' topics.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2506.23055v1": {
    "title": "Measuring How LLMs Internalize Human Psychological Concepts: A\n  preliminary analysis",
    "summary": "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities\nin producing human-like text. However, it is unclear how accurately these\nmodels internalize concepts that shape human thought and behavior. Here, we\ndeveloped a quantitative framework to assess concept alignment between LLMs and\nhuman psychological dimensions using 43 standardized psychological\nquestionnaires, selected for their established validity in measuring distinct\npsychological constructs. Our method evaluates how accurately language models\nreconstruct and classify questionnaire items through pairwise similarity\nanalysis. We compared resulting cluster structures with the original\ncategorical labels using hierarchical clustering. A GPT-4 model achieved\nsuperior classification accuracy (66.2\\%), significantly outperforming GPT-3.5\n(55.9\\%) and BERT (48.1\\%), all exceeding random baseline performance (31.9\\%).\nWe also demonstrated that the estimated semantic similarity from GPT-4 is\nassociated with Pearson's correlation coefficients of human responses in\nmultiple psychological questionnaires. This framework provides a novel approach\nto evaluate the alignment of the human-LLM concept and identify potential\nrepresentational biases. Our findings demonstrate that modern LLMs can\napproximate human psychological constructs with measurable accuracy, offering\ninsights for developing more interpretable AI systems.",
    "published": "2025-06-29T01:56:56Z",
    "updated": "2025-06-29T01:56:56Z",
    "id": "2506.23055v1",
    "authors": [
      "Hiro Taiyo Hamada",
      "Ippei Fujisawa",
      "Genji Kawakita",
      "Yuki Yamada"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23055v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23055v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23055v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating how Large Language Models (LLMs) internalize human psychological concepts, which aligns with the 'LLM' topic as it involves research on LLMs and their capabilities. Additionally, the study involves benchmarking the performance of different LLMs, which fits the 'Benchmark' topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.23040v2": {
    "title": "Treatment, evidence, imitation, and chat",
    "summary": "Large language models are thought to have potential to aid in medical\ndecision making. We investigate this here. We start with the treatment problem,\nthe patient's core medical decision-making task, which is solved in\ncollaboration with a healthcare provider. We discuss approaches to solving the\ntreatment problem, including -- within evidence-based medicine -- trials and\nobservational data. We then discuss the chat problem, and how this differs from\nthe treatment problem -- in particular as it relates to imitation. We then\ndiscuss how a large language model might be used to solve the treatment problem\nand highlight some of the challenges that emerge. We finally discuss how these\nchallenges relate to evidence-based medicine, and how this might inform next\nsteps.",
    "published": "2025-06-29T00:23:06Z",
    "updated": "2025-07-04T00:25:07Z",
    "id": "2506.23040v2",
    "authors": [
      "Samuel J. Weisenthal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23040v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23040v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23040v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the potential of large language models (LLMs) in aiding medical decision-making, specifically focusing on the treatment problem and its challenges. This aligns with the 'LLM' topic as it involves research on the application of large language models in a specific domain (medicine).",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.23034v1": {
    "title": "Guiding AI to Fix Its Own Flaws: An Empirical Study on LLM-Driven Secure\n  Code Generation",
    "summary": "Large Language Models (LLMs) have become powerful tools for automated code\ngeneration. However, these models often overlook critical security practices,\nwhich can result in the generation of insecure code that contains\nvulnerabilities-weaknesses or flaws in the code that attackers can exploit to\ncompromise a system. However, there has been limited exploration of strategies\nto guide LLMs in generating secure code and a lack of in-depth analysis of the\neffectiveness of LLMs in repairing code containing vulnerabilities. In this\npaper, we present a comprehensive evaluation of state-of-the-art LLMs by\nexamining their inherent tendencies to produce insecure code, their capability\nto generate secure code when guided by self-generated vulnerability hints, and\ntheir effectiveness in repairing vulnerabilities when provided with different\nlevels of feedback. Our study covers both proprietary and open-weight models\nacross various scales and leverages established benchmarks to assess a wide\nrange of vulnerability types. Through quantitative and qualitative analyses, we\nreveal that although LLMs are prone to generating insecure code, advanced\nmodels can benefit from vulnerability hints and fine-grained feedback to avoid\nor fix vulnerabilities. We also provide actionable suggestions to developers to\nreduce vulnerabilities when using LLMs for code generation.",
    "published": "2025-06-28T23:24:33Z",
    "updated": "2025-06-28T23:24:33Z",
    "id": "2506.23034v1",
    "authors": [
      "Hao Yan",
      "Swapneel Suhas Vaidya",
      "Xiaokuan Zhang",
      "Ziyu Yao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23034v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23034v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23034v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the evaluation and improvement of Large Language Models (LLMs) in generating secure code, which involves analyzing their tendencies, capabilities, and effectiveness in handling vulnerabilities. This aligns with the topics of LLM research and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.23025v1": {
    "title": "Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language\n  Models",
    "summary": "Large language models (LLMs) are increasingly used across research and\nindustry applications, yet their inference efficiency remains a significant\nchallenge. As the computational power of modern GPU architectures continuously\nimproves, their memory bandwidth and capacity have not scaled proportionally,\ncreating a critical bottleneck during inference. To address this, we\ninvestigate ternary language models (TriLMs) that employ quantization-aware\ntraining to significantly reduce memory requirements. We first analyze the\nscalability of TriLMs by conducting a scaling law analysis, revealing that\nTriLMs benefit more from increasing training data than from scaling model\nparameters. Based on this observation, we introduce Spectra-1.1, an open suite\nof TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained\nperformance gains at scale. Furthermore, to improve inference efficiency, we\npropose novel 2-bit and 1.6-bit packing schemes for ternary weights, which\ndemonstrate accelerated inference across various CPU architectures. Also,\nbuilding on the 2-bit packing, we develop a GPU kernel called TriRun that\naccelerates end-to-end model inference by up to 5 times compared to\nfloating-point baselines. To encourage further exploration and development of\nTriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.\nOverall, our work lays the foundation for building and deploying efficient\nLLMs, providing a valuable resource for the research community.",
    "published": "2025-06-28T22:13:43Z",
    "updated": "2025-06-28T22:13:43Z",
    "id": "2506.23025v1",
    "authors": [
      "Tejas Vaidhya",
      "Ayush Kaushal",
      "Vineet Jain",
      "Francis Couture Harpin",
      "Prashant Shishodia",
      "Majid Behbahani",
      "Yuriy Nevmyvaka",
      "Irina Rish"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23025v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23025v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23025v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses scaling laws and efficient inference for ternary language models, which are a type of large language models (LLMs). It focuses on the scalability and efficiency of these models, which aligns with the topics of 'Scaling' and 'LLM'.",
    "llm_cls_result": [
      "Scaling",
      "LLM"
    ]
  },
  "2507.02950v2": {
    "title": "Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator\n  Roles Assessed by Motivational Interviewing Criteria",
    "summary": "This study provides the first comprehensive evaluation of large language\nmodel (LLM) performance across three counseling roles in Japanese-language\ntherapeutic contexts. We simultaneously assessed counselor artificial\nintelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured\nMulti-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations,\nand evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human\nexperts (n = 15) with extensive counseling experience evaluated AI-generated\ndialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding\nManual 4.2.1.\n  Notably, SMDP implementation significantly enhanced counselor AI performance\nacross all MITI global ratings compared with zeroshot prompting, with no\nsignificant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed\ncomparable performance to human raters for Cultivating Change Talk but\nsystematically overestimated Softening Sustain Talk and the overall quality\nmetrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3\nfocused on technical proficiency, and Sonnet prioritized emotional expression.\nClient AI simulations exhibited a limited emotional range and unnaturally high\ncompliance, indicating the need for enhanced realism.\n  These findings establish benchmarks for AI-assisted counseling in non-English\ncontexts and identify critical areas for improvement through advanced prompt\nengineering, retrieval-augmented generation, and targeted fine-tuning, with\nimportant implications for developing culturally sensitive AI mental health\ntools.",
    "published": "2025-06-28T21:50:29Z",
    "updated": "2025-07-08T06:16:17Z",
    "id": "2507.02950v2",
    "authors": [
      "Keita Kiuchi",
      "Yoshikazu Fujimoto",
      "Hideyuki Goto",
      "Tomonori Hosokawa",
      "Makoto Nishimura",
      "Yosuke Sato",
      "Izumi Sezai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02950v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02950v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02950v2",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper evaluates the performance of large language models (LLMs) in counseling roles, specifically in Japanese-language therapeutic contexts, and discusses enhancements through prompt engineering and fine-tuning. It focuses on LLM applications in a specific domain (counseling) but does not directly align with the provided technical topics like LLM architectures, scaling, or multimodal models.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.02949v1": {
    "title": "RADIANT: Retrieval AugmenteD entIty-context AligNmenT -- Introducing\n  RAG-ability and Entity-Context Divergence",
    "summary": "As Large Language Models (LLMs) continue to advance, Retrieval-Augmented\nGeneration (RAG) has emerged as a vital technique to enhance factual accuracy\nby integrating external knowledge into the generation process. However, LLMs\noften fail to faithfully integrate retrieved evidence into their generated\nresponses, leading to factual inconsistencies. To quantify this gap, we\nintroduce Entity-Context Divergence (ECD), a metric that measures the extent to\nwhich retrieved information is accurately reflected in model outputs. We\nsystematically evaluate contemporary LLMs on their ability to preserve factual\nconsistency in retrieval-augmented settings, a capability we define as\nRAG-ability. Our empirical analysis reveals that RAG-ability remains low across\nmost LLMs, highlighting significant challenges in entity retention and context\nfidelity. This paper introduces Radiant (Retrieval AugmenteD entIty-context\nAligNmenT), a novel framework that merges RAG with alignment designed to\noptimize the interplay between retrieved evidence and generated content.\nRadiant extends Direct Preference Optimization (DPO) to teach LLMs how to\nintegrate provided additional information into subsequent generations. As a\nbehavior correction mechanism, Radiant boosts RAG performance across varied\nretrieval scenarios, such as noisy web contexts, knowledge conflicts, and\nhallucination reduction. This enables more reliable, contextually grounded, and\nfactually coherent content generation.",
    "published": "2025-06-28T21:40:35Z",
    "updated": "2025-06-28T21:40:35Z",
    "id": "2507.02949v1",
    "authors": [
      "Vipula Rawte",
      "Rajarshi Roy",
      "Gurpreet Singh",
      "Danush Khanna",
      "Yaswanth Narsupalli",
      "Basab Ghosh",
      "Abhay Gupta",
      "Argha Kamal Samanta",
      "Aditya Shingote",
      "Aadi Krishna Vikram",
      "Vinija Jain",
      "Aman Chadha",
      "Amit Sheth",
      "Amitava Das"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02949v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02949v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02949v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on Retrieval-Augmented Generation (RAG) and its integration with Large Language Models (LLMs), specifically addressing the challenges of factual consistency and entity-context alignment. It introduces a novel framework (Radiant) that enhances RAG performance, which is closely related to the topics of Memory (retrieval-based methods) and LLM (research on Large Language Models).",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.02947v1": {
    "title": "The Application of Large Language Models on Major Depressive Disorder\n  Support Based on African Natural Products",
    "summary": "Major depressive disorder represents one of the most significant global\nhealth challenges of the 21st century, affecting millions of people worldwide\nand creating substantial economic and social burdens. While conventional\nantidepressant therapies have provided relief for many individuals, their\nlimitations including delayed onset of action, significant side effects, and\ntreatment resistance in a substantial portion of patients have prompted\nresearchers and healthcare providers to explore alternative therapeutic\napproaches (Kasneci et al.). African traditional medicine, with its rich\nheritage of plant-based remedies developed over millennia, offers a valuable\nresource for developing novel antidepressant treatments that may address some\nof these limitations. This paper examines the integration of large language\nmodels with African natural products for depression support, combining\ntraditional knowledge with modern artificial intelligence technology to create\naccessible, evidence-based mental health support systems.\n  The research presented here encompasses a comprehensive analysis of African\nmedicinal plants with documented antidepressant properties, their\npharmacological mechanisms, and the development of an AI-powered support system\nthat leverages DeepSeek's advanced language model capabilities. The system\nprovides evidence-based information about African herbal medicines, their\nclinical applications, safety considerations, and therapeutic protocols while\nmaintaining scientific rigor and appropriate safety standards. Our findings\ndemonstrate the potential for large language models to serve as bridges between\ntraditional knowledge and modern healthcare, offering personalized, culturally\nappropriate depression support that honors both traditional wisdom and\ncontemporary medical understanding.",
    "published": "2025-06-28T21:05:57Z",
    "updated": "2025-06-28T21:05:57Z",
    "id": "2507.02947v1",
    "authors": [
      "Linyan Zou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02947v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02947v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02947v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in the context of mental health support, specifically for Major Depressive Disorder, using African natural products. It highlights the use of LLMs to bridge traditional knowledge and modern healthcare.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.23009v1": {
    "title": "MusiXQA: Advancing Visual Music Understanding in Multimodal Large\n  Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable visual\nreasoning abilities in natural images, text-rich documents, and graphic\ndesigns. However, their ability to interpret music sheets remains\nunderexplored. To bridge this gap, we introduce MusiXQA, the first\ncomprehensive dataset for evaluating and advancing MLLMs in music sheet\nunderstanding. MusiXQA features high-quality synthetic music sheets generated\nvia MusiXTeX, with structured annotations covering note pitch and duration,\nchords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.\nThrough extensive evaluations, we reveal significant limitations of current\nstate-of-the-art MLLMs in this domain. Beyond benchmarking, we developed\nPhi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant\nperformance gains over GPT-based methods. The proposed dataset and model\nestablish a foundation for future advances in MLLMs for music sheet\nunderstanding. Code, data, and model will be released upon acceptance.",
    "published": "2025-06-28T20:46:47Z",
    "updated": "2025-06-28T20:46:47Z",
    "id": "2506.23009v1",
    "authors": [
      "Jian Chen",
      "Wenye Ma",
      "Penghang Liu",
      "Wei Wang",
      "Tengwei Song",
      "Ming Li",
      "Chenguang Wang",
      "Ruiyi Zhang",
      "Changyou Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23009v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23009v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23009v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on Multimodal Large Language Models (MLLMs) and their application in music sheet understanding, which involves both visual and language modalities. It introduces a new dataset and benchmarks for evaluating MLLMs in this specific domain.",
    "llm_cls_result": [
      "MLLM",
      "Dataset",
      "Benchmark"
    ]
  },
  "2507.01058v1": {
    "title": "A Data Science Approach to Calcutta High Court Judgments: An Efficient\n  LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval",
    "summary": "The judiciary, as one of democracy's three pillars, is dealing with a rising\namount of legal issues, needing careful use of judicial resources. This\nresearch presents a complex framework that leverages Data Science\nmethodologies, notably Large Language Models (LLM) and Retrieval-Augmented\nGeneration (RAG) techniques, to improve the efficiency of analyzing Calcutta\nHigh Court verdicts. Our framework focuses on two key aspects: first, the\ncreation of a robust summarization mechanism that distills complex legal texts\ninto concise and coherent summaries; and second, the development of an\nintelligent system for retrieving similar cases, which will assist legal\nprofessionals in research and decision making. By fine-tuning the Pegasus model\nusing case head note summaries, we achieve significant improvements in the\nsummarization of legal cases. Our two-step summarizing technique preserves\ncrucial legal contexts, allowing for the production of a comprehensive vector\ndatabase for RAG. The RAG-powered framework efficiently retrieves similar cases\nin response to user queries, offering thorough overviews and summaries. This\ntechnique not only improves legal research efficiency, but it also helps legal\nprofessionals and students easily acquire and grasp key legal information,\nbenefiting the overall legal scenario.",
    "published": "2025-06-28T20:24:34Z",
    "updated": "2025-06-28T20:24:34Z",
    "id": "2507.01058v1",
    "authors": [
      "Puspendu Banerjee",
      "Aritra Mazumdar",
      "Wazib Ansar",
      "Saptarsi Goswami",
      "Amlan Chakrabarti"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01058v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01058v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01058v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLM) and Retrieval-Augmented Generation (RAG) techniques for legal text summarization and case retrieval, which aligns with the topics of LLM and Memory (due to the use of RAG).",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.22967v1": {
    "title": "ActAlign: Zero-Shot Fine-Grained Video Classification via\n  Language-Guided Sequence Alignment",
    "summary": "We address the task of zero-shot fine-grained video classification, where no\nvideo examples or temporal annotations are available for unseen action classes.\nWhile contrastive vision-language models such as SigLIP demonstrate strong\nopen-set recognition via mean-pooled image-text similarity, they fail to\ncapture the temporal structure critical for distinguishing fine-grained\nactivities. We introduce ActAlign, a zero-shot framework that formulates video\nclassification as sequence alignment. For each class, a large language model\ngenerates an ordered sub-action sequence, which is aligned with video frames\nusing Dynamic Time Warping (DTW) in a shared embedding space. Without any\nvideo-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the\nextremely challenging ActionAtlas benchmark, where human accuracy is only\n61.6%. ActAlign outperforms billion-parameter video-language models while using\napproximately 8x less parameters. These results demonstrate that structured\nlanguage priors, combined with classical alignment techniques, offer a scalable\nand general approach to unlocking the open-set recognition potential of\nvision-language models for fine-grained video understanding.",
    "published": "2025-06-28T17:57:58Z",
    "updated": "2025-06-28T17:57:58Z",
    "id": "2506.22967v1",
    "authors": [
      "Amir Aghdam",
      "Vincent Tao Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22967v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22967v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22967v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on zero-shot fine-grained video classification using language-guided sequence alignment, leveraging large language models and vision-language models. It involves multimodal integration (vision and language) and does not fit neatly into the provided topics but is closest to MLLM and VLA due to its use of vision-language models and alignment techniques.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2506.22957v1": {
    "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among\n  Large Language Models",
    "summary": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.",
    "published": "2025-06-28T17:22:59Z",
    "updated": "2025-06-28T17:22:59Z",
    "id": "2506.22957v1",
    "authors": [
      "Younwoo Choi",
      "Changling Li",
      "Yongjin Yang",
      "Zhijing Jin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22957v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22957v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22957v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the awareness and interaction capabilities of large language models (LLMs) in multi-agent systems, which involves understanding and adapting to conversational partners. This aligns with topics related to LLMs and their reasoning abilities in multi-agent contexts.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "RL"
    ]
  },
  "2506.22954v1": {
    "title": "Evaluating and Improving Large Language Models for Competitive Program\n  Generation",
    "summary": "Context: Due to the demand for strong algorithmic reasoning, complex logic\nimplementation, and strict adherence to input/output formats and resource\nconstraints, competitive programming generation by large language models (LLMs)\nis considered the most challenging problem in current LLM-based code\ngeneration. However, previous studies often evaluate LLMs using simple prompts\nand benchmark datasets prone to data leakage. Moreover, prior work has limited\nconsideration of the diversity in algorithm types and difficulty levels.\nObjective: In this study, we aim to evaluate and improve LLMs in solving\nreal-world competitive programming problems. Methods: We initially collect 117\nproblems from nine regional ICPC/CCPC contests held in 2024 and design four\nfiltering criteria to construct a curated benchmark consisting of 80 problems.\nLeveraging DeepSeek-R1 as the LLM, we evaluate its competitive program\ngeneration capabilities through the online judge (OJ) platforms, guided by a\ncarefully designed basic prompt. For incorrect submissions, we construct a\nfine-grained error taxonomy and then propose a targeted improvement framework\nby combining a multi-turn dialogue-based repair phase and an\ninformation-augmented regeneration phase. Results: Experimental results show\nthat only 5 out of 80 problems are fully accepted when using basic prompts. For\nthe unsolved problems, we construct the error taxonomy, including general\nerrors (such as design, boundary, condition, data type, syntax, and\ninput/output errors) and specialized errors (such as those in mathematical\nproblems, greedy algorithms, and graph theories). After applying our proposed\nimprovement strategies, we substantially increased the number of correct\nsolutions, with 46 out of 80 problems successfully accepted.",
    "published": "2025-06-28T17:18:23Z",
    "updated": "2025-06-28T17:18:23Z",
    "id": "2506.22954v1",
    "authors": [
      "Minnan Wei",
      "Ziming Li",
      "Xiang Chen",
      "Menglin Zheng",
      "Ziyan Qu",
      "Cheng Yu",
      "Siyu Chen",
      "Xiaolin Ju"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22954v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22954v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22954v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating and improving Large Language Models (LLMs) for competitive programming generation, which involves algorithmic reasoning and complex logic implementation. The study uses a benchmark dataset and evaluates the LLM's performance, which aligns with the topics of 'LLM' and 'Benchmark'. Additionally, the paper discusses reasoning abilities in LLMs, which fits the 'Reasoning' topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Reasoning"
    ]
  },
  "2506.22950v1": {
    "title": "Infinite Sampling: Efficient and Stable Grouped RL Training for Large\n  Language Models",
    "summary": "Group-based reinforcement learning algorithms such as Group Reward Policy\nOptimization (GRPO) have proven effective for fine-tuning large language models\n(LLMs) with human feedback. However, generating and storing multiple responses\nper prompt incurs substantial memory overhead, especially as the sample group\nsize increases, limiting scalability under constrained hardware.\n  We propose Infinite Sampling, a framework that enables efficient and stable\nGRPO training by decoupling group size from GPU memory usage. It consists of:\n(1) micro sampling groups that decompose large groups into memory-feasible\nrounds; (2) continuous sampling that interleaves generation across groups to\nimprove utilization; and (3) a length-aware scheduler combining\ntoken-conditioned sequence length prediction with a two-stage plan: global\ngrouping via FPTAS and runtime refill via SJF.\n  Experiments show that our Micro Sampling Groups reduce peak memory usage by\nover 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on\nQwen3-1.7B). Building on this, Infinite Sampling improves throughput by over\n25% compared to the naive micro sampling group method, reducing decoding steps\nwhile maintaining full-length completions and memory usage. Our hybrid\nscheduling ensures efficient and stable GRPO training with larger groups under\nrealistic GPU memory constraints.",
    "published": "2025-06-28T16:52:29Z",
    "updated": "2025-06-28T16:52:29Z",
    "id": "2506.22950v1",
    "authors": [
      "Liangyu Wang",
      "Huanyi Xie",
      "Xinhai Wang",
      "Tianjin Huang",
      "Mengdi Li",
      "Di Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22950v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22950v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22950v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for efficient and stable training of large language models using group-based reinforcement learning, specifically mentioning GRPO and RLHF, which are key topics in the RL category. It also addresses scalability and memory usage, relevant to the Scaling topic.",
    "llm_cls_result": [
      "RL",
      "Scaling"
    ]
  },
  "2506.22941v3": {
    "title": "Positioning AI Tools to Support Online Harm Reduction Practice:\n  Applications and Design Directions",
    "summary": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem.",
    "published": "2025-06-28T16:15:47Z",
    "updated": "2025-07-13T00:53:06Z",
    "id": "2506.22941v3",
    "authors": [
      "Kaixuan Wang",
      "Jason T. Jacques",
      "Chenxin Diao",
      "Carl-Cyril J Dreue"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22941v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22941v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22941v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in the context of harm reduction for People Who Use Drugs (PWUD), focusing on their design and ethical considerations. This aligns with the 'LLM' topic as it involves research on the use and implications of LLMs in a specific high-stakes domain.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.22937v1": {
    "title": "GamerAstra: Enhancing Video Game Accessibility for Blind and Low-Vision\n  Players through a Multi-Agent AI Framework",
    "summary": "Blind and low-vision (BLV) players encounter critical challenges in engaging\nwith video games due to the inaccessibility of visual elements, difficulties in\nnavigating interfaces, and limitations in sending interaction input. Moreover,\nthe development of specialized accessibility features typically requires\nsubstantial programming effort and is often implemented on a game-by-game\nbasis. To address these challenges, we introduce \\textit{GamerAstra}, a\ngeneralized accessibility framework that leverages a multi-agent design to\nfacilitate access to video games for BLV players. It integrates multi-modal\ntechniques including large language models and vision-language models, enabling\ninteraction with games lacking native accessibility support. The framework\nfurther incorporates customizable assistance granularities to support varying\ndegrees of visual impairment and enhances interface navigation through multiple\ninput modalities. The evaluation through technical assessments and user studies\nindicate that \\textit{GamerAstra} effectively enhances playability and delivers\na more immersive gaming experience for BLV players. These findings also\nunderscore potential avenues for advancing intelligent accessibility frameworks\nin the gaming domain.",
    "published": "2025-06-28T16:08:08Z",
    "updated": "2025-06-28T16:08:08Z",
    "id": "2506.22937v1",
    "authors": [
      "Tianrun Qiu",
      "Changxin Chen",
      "Sizhe Cheng",
      "Yiming Yang",
      "Yixiao Guo",
      "Zhicong Lu",
      "Yuxin Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22937v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22937v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22937v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multi-agent AI framework that integrates large language models and vision-language models to enhance video game accessibility for blind and low-vision players. This involves the use of multimodal techniques and AI models, which are relevant to the topics of MLLM (Multimodal Large Language Models) and VLA (Vision-Language Action).",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.02946v1": {
    "title": "Iterative Zoom-In: Temporal Interval Exploration for Long Video\n  Understanding",
    "summary": "Multimodal Large Language Models (MLLMs) have shown strong performance in\nvideo understanding tasks. However, they continue to struggle with long-form\nvideos because of an inefficient perception of temporal intervals. Unlike\nhumans, who can dynamically adjust their temporal focus to locate\nquery-relevant moments, current MLLMs often rely on dense, uniform sampling\nacross the video timeline, leading to high memory consumption and a risk of\nmissing crucial information. To address this challenge, we introduce Temporal\nSearch, a training-free framework that enables MLLMs to explore temporal\nregions for improved long video understanding iteratively. TS is based on a key\nobservation: the model's generation confidence across different temporal\nintervals is highly correlated with prediction accuracy. TS operates through\ntwo main iterative stages. First, the MLLM proposes a temporal interval that is\nlikely to contain task-relevant information. Then, it samples a fixed number of\nframes from the interval, regardless of length, and feeds them into the model\nto produce a refined response and confidence score. TS refines the focus of the\nmodel by iteratively shifting attention to more fine-grained temporal\nintervals, improving its understanding of long videos. Additionally,\nkeyframe-level descriptions are collected to facilitate cross-interval\nperception throughout the video. To further improve efficiency, we introduce\nTS-BFS, a best-first search strategy over a tree. Each node represents a\ncandidate interval and is expanded via two methods: self-driven proposals and\nuniform partitioning. Nodes are scored based on confidence and self-evaluation,\nand the most promising one is selected for continued exploration.",
    "published": "2025-06-28T15:24:05Z",
    "updated": "2025-06-28T15:24:05Z",
    "id": "2507.02946v1",
    "authors": [
      "Chenglin Li",
      "Qianglong Chen",
      " fengtao",
      "Yin Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02946v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02946v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02946v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the performance of Multimodal Large Language Models (MLLMs) in understanding long videos by addressing temporal interval exploration. The abstract mentions MLLMs and their challenges with long-form videos, which aligns with the MLLM topic. Additionally, the proposed framework involves iterative refinement and confidence-based exploration, which could be related to reasoning in LLMs.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2506.22920v2": {
    "title": "Improving Rationality in the Reasoning Process of Language Models\n  through Self-playing Game",
    "summary": "Large language models (LLMs) have demonstrated considerable reasoning\nabilities in various tasks such as mathematics and coding. However, recent\nstudies indicate that even the best models lack true comprehension of their\nreasoning processes. In this paper, we explore how self-play can enhance the\nrationality of models in the reasoning process without supervision from humans\nor superior models. We design a Critic-Discernment Game(CDG) in which a prover\nfirst provides a solution to a given problem and is subsequently challenged by\ncritiques of its solution. These critiques either aim to assist or mislead the\nprover. The objective of the prover is to maintain the correct answer when\nfaced with misleading comments, while correcting errors in response to\nconstructive feedback. Our experiments on tasks involving mathematical\nreasoning, stepwise error detection, self-correction, and long-chain reasoning\ndemonstrate that CDG training can significantly improve the ability of\nwell-aligned LLMs to comprehend their reasoning process.",
    "published": "2025-06-28T15:11:23Z",
    "updated": "2025-07-06T13:58:07Z",
    "id": "2506.22920v2",
    "authors": [
      "Pinzheng Wang",
      "Juntao Li",
      "Zecheng Tang",
      "Haijia Gui",
      "Min zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22920v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22920v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22920v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the reasoning abilities of Large Language Models (LLMs) through self-play, which aligns with the 'Reasoning' and 'LLM' topics. The study involves enhancing the rationality of models in their reasoning processes, which is a core aspect of 'Reasoning'. Additionally, the use of LLMs as the primary subject of study places it under 'LLM'.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2506.22884v1": {
    "title": "Performance Measurements in the AI-Centric Computing Continuum Systems",
    "summary": "Over the Eight decades, computing paradigms have shifted from large,\ncentralized systems to compact, distributed architectures, leading to the rise\nof the Distributed Computing Continuum (DCC). In this model, multiple layers\nsuch as cloud, edge, Internet of Things (IoT), and mobile platforms work\ntogether to support a wide range of applications. Recently, the emergence of\nGenerative AI and large language models has further intensified the demand for\ncomputational resources across this continuum. Although traditional performance\nmetrics have provided a solid foundation, they need to be revisited and\nexpanded to keep pace with changing computational demands and application\nrequirements. Accurate performance measurements benefit both system designers\nand users by supporting improvements in efficiency and promoting alignment with\nsystem goals. In this context, we review commonly used metrics in DCC and IoT\nenvironments. We also discuss emerging performance dimensions that address\nevolving computing needs, such as sustainability, energy efficiency, and system\nobservability. We also outline criteria and considerations for selecting\nappropriate metrics, aiming to inspire future research and development in this\ncritical area.",
    "published": "2025-06-28T13:46:07Z",
    "updated": "2025-06-28T13:46:07Z",
    "id": "2506.22884v1",
    "authors": [
      "Praveen Kumar Donta",
      "Qiyang Zhang",
      "Schahram Dustdar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22884v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22884v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22884v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses performance metrics in distributed computing systems, including the impact of Generative AI and large language models, but does not focus specifically on LLMs, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.22880v1": {
    "title": "Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and\n  Grounder",
    "summary": "Existing video segmenter and grounder approaches, exemplified by Sa2VA,\ndirectly fuse features within segmentation models. This often results in an\nundesirable entanglement of dynamic visual information and static semantics,\nthereby degrading segmentation accuracy. To systematically mitigate this issue,\nwe propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text\npre-training and a linear decoupling module to address the information\nprocessing limitations inherent in SAM-2. Specifically, first, we devise a\npre-training paradigm that converts textual ground-truth labels into\npoint-level prompts while generating corresponding text masks. These masks are\nrefined through a hybrid loss function to strengthen the model's semantic\ngrounding capabilities. Next, we employ linear projection to disentangle hidden\nstates that generated by a large language model into distinct textual and\nvisual feature subspaces. Finally, a dynamic mask fusion strategy\nsynergistically combines these decoupled features through triple supervision\nfrom predicted text/visual masks and ground-truth annotations. Extensive\nexperiments demonstrate state-of-the-art performance across diverse tasks,\nincluding image segmentation, image question answering, video segmentation, and\nvideo question answering. Our codes are available at\nhttps://github.com/longmalongma/DeSa2VA.",
    "published": "2025-06-28T13:30:36Z",
    "updated": "2025-06-28T13:30:36Z",
    "id": "2506.22880v1",
    "authors": [
      "Dang Jisheng",
      "Wu Xudong",
      "Wang Bimei",
      "Lv Ning",
      "Chen Jiayu",
      "Jingwen Zhao",
      "Yichu liu",
      "Jizhao Liu",
      "Juncheng Li",
      "Teng Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22880v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22880v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22880v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for improving video segmentation and grounding by decoupling visual and textual features, which involves the use of a large language model and multimodal processing. The core topics are related to multimodal large language models and vision-language alignment.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2506.22865v1": {
    "title": "ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source\n  Language Models",
    "summary": "Recent advancements in Large Language Models (LLMs) have revealed a\nsignificant performance gap between closed-source and open-source models,\nparticularly in tasks requiring complex reasoning and precise instruction\nfollowing. This paper introduces ReasonBridge, a methodology that efficiently\ntransfers reasoning capabilities from powerful closed-source to open-source\nmodels through a novel hierarchical knowledge distillation framework. We\ndevelop a tailored dataset Reason1K with only 1,000 carefully curated reasoning\ntraces emphasizing difficulty, diversity, and quality. These traces are\nfiltered from across multiple domains using a structured multi-criteria\nselection algorithm. Our transfer learning approach incorporates: (1) a\nhierarchical distillation process capturing both strategic abstraction and\ntactical implementation patterns, (2) a sparse reasoning-focused adapter\narchitecture requiring only 0.3% additional trainable parameters, and (3) a\ntest-time compute scaling mechanism using guided inference interventions.\nComprehensive evaluations demonstrate that ReasonBridge improves reasoning\ncapabilities in open-source models by up to 23% on benchmark tasks,\nsignificantly narrowing the gap with closed-source models. Notably, the\nenhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its\nperformance on competition-level AIME problems. Our methodology generalizes\neffectively across diverse reasoning domains and model architectures,\nestablishing a sample-efficient approach to reasoning enhancement for\ninstruction following.",
    "published": "2025-06-28T12:22:55Z",
    "updated": "2025-06-28T12:22:55Z",
    "id": "2506.22865v1",
    "authors": [
      "Ziqi Zhong",
      "Xunzhu Tang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22865v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22865v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22865v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on transferring reasoning capabilities from closed-source to open-source LLMs, which involves reasoning abilities and knowledge distillation, aligning with the topics of Reasoning and LLM.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2506.22864v1": {
    "title": "Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation\n  Meets Cross-modal Retrieval",
    "summary": "Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods.",
    "published": "2025-06-28T12:19:49Z",
    "updated": "2025-06-28T12:19:49Z",
    "id": "2506.22864v1",
    "authors": [
      "Li-Cheng Shen",
      "Jih-Kang Hsieh",
      "Wei-Hua Li",
      "Chu-Song Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22864v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22864v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22864v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a new task that combines text-to-image retrieval and referring expression segmentation, leveraging a multimodal large language model (MLLM) for refinement and object grounding. The focus on multimodal integration and the use of MLLM aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA).",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.00068v1": {
    "title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic\n  Optimization for Long-form Multimodal Understanding",
    "summary": "While multi-modal learning has advanced significantly, current approaches\noften treat modalities separately, creating inconsistencies in representation\nand reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization\nvia Textual Alignment), a theoretically-grounded framework that unifies visual\nand auditory inputs into a structured textual space for seamless processing\nwith large language models. MANTA addresses four key challenges: (1) semantic\nalignment across modalities with information-theoretic optimization, (2)\nadaptive temporal synchronization for varying information densities, (3)\nhierarchical content representation for multi-scale understanding, and (4)\ncontext-aware retrieval of sparse information from long sequences. We formalize\nour approach within a rigorous mathematical framework, proving its optimality\nfor context selection under token constraints. Extensive experiments on the\nchallenging task of Long Video Question Answering show that MANTA improves\nstate-of-the-art models by up to 22.6% in overall accuracy, with particularly\nsignificant gains (27.3%) on videos exceeding 30 minutes. Additionally, we\ndemonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)\nand cross-modal understanding (25.1% improvement). Our framework introduces\nnovel density estimation techniques for redundancy minimization while\npreserving rare signals, establishing new foundations for unifying multimodal\nrepresentations through structured text.",
    "published": "2025-06-28T12:12:06Z",
    "updated": "2025-06-28T12:12:06Z",
    "id": "2507.00068v1",
    "authors": [
      "Ziqi Zhong",
      "Daniel Tang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00068v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00068v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00068v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on unifying visual and auditory inputs into a structured textual space for processing with large language models, addressing challenges in semantic alignment, temporal synchronization, hierarchical content representation, and context-aware retrieval. This aligns with topics related to Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA) models.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.02944v1": {
    "title": "Beyond Parallelism: Synergistic Computational Graph Effects in\n  Multi-Head Attention",
    "summary": "Multi-head attention powers Transformer networks, the primary deep learning\narchitecture behind the success of large language models (LLMs). Yet, the\ntheoretical advantages of multi-head versus single-head attention, beyond mere\nparallel processing, remain underexplored. In this paper, we reframe multi-head\nattention as a system of potentially synergistic computational graphs, where\neach head functions as a feedforward directed acyclic graph (DAG) with a common\nsink state. We provide intuition and preliminary theoretical analysis of mixing\ntime and minimax fidelity in this framework. Our results show that multi-head\nattention can synergistically enhance information propagation, yielding faster\nmixing times and minimax fidelity amplification under specific head-diversity\nconditions. Finally, we train single-head and multi-head Transformers, each\nwith the same total number of parameters, on sequence manipulation tasks and\nempirically verify the predicted effects.",
    "published": "2025-06-28T11:35:31Z",
    "updated": "2025-06-28T11:35:31Z",
    "id": "2507.02944v1",
    "authors": [
      "Haitz Sez de Ocriz Borde"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02944v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02944v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02944v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses multi-head attention in Transformer networks, which are fundamental to large language models (LLMs). It explores the theoretical advantages of multi-head attention beyond parallelism, which is directly relevant to LLM architectures and their scaling laws.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.22853v2": {
    "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language\n  Models in Multi-Round, Multi-Party Dialogues",
    "summary": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.",
    "published": "2025-06-28T11:28:04Z",
    "updated": "2025-07-02T07:55:09Z",
    "id": "2506.22853v2",
    "authors": [
      "Kyochul Jang",
      "Donghyeon Lee",
      "Kyusik Kim",
      "Dongseok Heo",
      "Taewhoo Lee",
      "Woojeong Kim",
      "Bongwon Suh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22853v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22853v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22853v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a new benchmark (DICE-BENCH) to evaluate the tool-use capabilities of large language models in multi-round, multi-party dialogues, which aligns with the topics of benchmarking LLMs and evaluating their performance in practical scenarios.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2506.22852v1": {
    "title": "Knowledge Augmented Finetuning Matters in both RAG and Agent Based\n  Dialog Systems",
    "summary": "Large language models (LLMs) have recently been applied to dialog systems.\nDespite making progress, LLMs are prone to errors in knowledge-intensive\nscenarios. Recently, approaches based on retrieval augmented generation (RAG)\nand agent have emerged to improve the factual accuracy by enhancing the LLMs\nwith knowledge retrieved from external knowledge bases (KBs). This is mostly\nimplemented by prompting the LLMs with instructions, examples and the retrieved\nknowledge. However, LLMs may have difficulty using the retrieved knowledge\neffectively for response generation, because they are not well trained to do\nsuch generation for specific domains. To mitigate this problem, we propose to\nfinetune the LLMs in the RAG-based and agent-based systems with domain-specific\ndata, together with domain-specific external knowledge, which is called\nknowledge augmented finetuning (KAFT). We base our study on the MobileCS2\ndataset, a real-life customer service dialog dataset that features intensive\nknowledge interactions, to systematically compare the prompting and KAFT\ntechniques in the RAG-based and agent-based systems. Experiment results show\nthat KAFT substantially surpasses prompting in both RAG and agent systems,\nparticularly in terms of factual accuracy. To the best of our knowledge, this\npaper represents the first solid empirical work to investigate the KAFT idea.",
    "published": "2025-06-28T11:26:31Z",
    "updated": "2025-06-28T11:26:31Z",
    "id": "2506.22852v1",
    "authors": [
      "Yucheng Cai",
      "Yuxuan Wu",
      "Yi Huang",
      "Junlan Feng",
      "Zhijian Ou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22852v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22852v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22852v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in dialog systems, specifically focusing on improving factual accuracy through retrieval augmented generation (RAG) and agent-based systems. It introduces knowledge augmented finetuning (KAFT) as a method to enhance LLMs' performance in specific domains, which aligns with topics related to LLM memory and retrieval-based methods.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.22846v1": {
    "title": "Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization",
    "summary": "End-to-end (E2E) automatic speech recognition (ASR) systems have\nrevolutionized the field by integrating all components into a single neural\nnetwork, with attention-based encoder-decoder models achieving state-of-the-art\nperformance. However, their autoregressive decoding process limits inference\nspeed, making them unsuitable for real-time applications. In contrast,\nCTC-based models offer faster, non-autoregressive decoding but struggle to\nmodel linguistic dependencies effectively. Addressing this challenge, we\npropose a novel auxiliary loss framework called Language-Aware Intermediate\nLoss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large\nlanguage models (LLMs). By attaching connector layers to intermediate encoder\nlayers, LAIL maps outputs to the embedding space of an LLM and computes a\ncausal language modeling loss during training. This approach enhances\nlinguistic modeling while preserving the computational efficiency of CTC\ndecoding. Using the Conformer architecture and various LLaMA models, we\ndemonstrate significant improvements in Word Error Rate (WER) on the\nLibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance\nfor CTC-based ASR with minimal computational overhead.",
    "published": "2025-06-28T10:59:42Z",
    "updated": "2025-06-28T10:59:42Z",
    "id": "2506.22846v1",
    "authors": [
      "Duygu Altinok"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22846v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22846v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22846v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing CTC-based ASR using linguistic knowledge from large language models (LLMs), which directly relates to the use of LLMs in improving model performance.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2506.22819v1": {
    "title": "Prompting without Panic: Attribute-aware, Zero-shot, Test-Time\n  Calibration",
    "summary": "Vision-language models (VLM) have demonstrated impressive performance in\nimage recognition by leveraging self-supervised training on large datasets.\nTheir performance can be further improved by adapting to the test sample using\ntest-time prompt tuning (TPT). Unfortunately, the singular focus of TPT\napproaches on improving the accuracy suffers from tunnel vision, and leads to\ndegradation in confidence calibration. This limits the applicability of TPT in\ncritical applications.\n  We make three contributions in this work. (1) We posit that random or naive\ninitialization of prompts leads to overfitting on a particular test sample, and\nis the main reason for miscalibration of the VLM after TPT. To mitigate the\nproblem, we propose careful initialization of test time prompt using prior\nknowledge about the target label attributes from a large language model (LLM);\n(2) To further maintain the quality of prompts during \\tpt, we propose a novel\nregularization loss to reduce intraclass distance, and increase inter-class\ndistance between the learnt\n  Through extensive experiments on different CLIP architectures and 15\ndatasets, we show that our approach can effectively improve the calibration\nafter TPT. We report an average expected calibration error (ECE) of 4.11 with\nour method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),\n6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is\npublicly accessible at:\nhttps://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.",
    "published": "2025-06-28T08:57:57Z",
    "updated": "2025-06-28T08:57:57Z",
    "id": "2506.22819v1",
    "authors": [
      "Ramya Hebbalaguppe",
      "Tamoghno Kandar",
      "Abhinav Nagpal",
      "Chetan Arora"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22819v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22819v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22819v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of vision-language models (VLM) and test-time prompt tuning (TPT), which aligns with the topic of Vision-Language Alignment models (VLA). Additionally, it mentions leveraging prior knowledge from a large language model (LLM) for prompt initialization, which relates to the LLM topic. The focus on improving calibration and performance through prompt tuning also touches on the Reasoning topic, as it involves optimizing model behavior for better outcomes.",
    "llm_cls_result": [
      "VLA",
      "LLM",
      "Reasoning"
    ]
  },
  "2506.22815v1": {
    "title": "Memory as a Service (MaaS): Rethinking Contextual Memory as\n  Service-Oriented Modules for Collaborative Agents",
    "summary": "This position paper aims to rethink the role and design of memory in Large\nLanguage Model (LLM)-based agent systems. We observe that while current memory\npractices have begun to transcend the limitations of single interactions, they\nremain conceptually grounded in \"bound memory\" in terms of design concept-where\nmemory is treated as local state attached to specific context or entities,\nforming \"memory silos\" that impede cross-entity collaboration. To overcome this\narchitectural bottleneck, this paper proposes the timely design perspective of\n\"Memory as a Service\" (MaaS). MaaS advocates decoupling memory from its\nconventional role as an interaction byproduct and encapsulating it as a modular\nservice that can be independently callable, dynamically composable, and finely\ngoverned. At its core, MaaS leverages the duality of memory-its inherently\nprivate nature and its potential for public service-to grant memory controlled,\non-demand interoperability across entities. This paper introduces a\ntwo-dimensional design space defined by entity structure and service type,\nillustrating how MaaS aligns with current memory practices while naturally\nextending them to cross-entity collaborative scenarios. Finally, we outline an\nopen research agenda spanning governance, security, and ethical ecosystems, and\ncall upon the broader research community to explore this shift toward\nservice-oriented memory for collaborative agents operating across entity\nboundaries.",
    "published": "2025-06-28T08:33:17Z",
    "updated": "2025-06-28T08:33:17Z",
    "id": "2506.22815v1",
    "authors": [
      "Haichang Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22815v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22815v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22815v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the role and design of memory in LLM-based agent systems, proposing a new perspective called 'Memory as a Service' (MaaS). It focuses on memory in the context of LLMs and collaborative agents, which aligns with the 'Memory' and 'LLM' topics.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2506.22813v1": {
    "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity\n  Recognition with Large Language Models",
    "summary": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework.",
    "published": "2025-06-28T08:28:52Z",
    "updated": "2025-06-28T08:28:52Z",
    "id": "2506.22813v1",
    "authors": [
      "Zhuojun Ding",
      "Wei Wei",
      "Chenghao Fan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22813v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22813v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22813v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for named entity recognition (NER) and proposes a framework for dynamically selecting and merging expert models, which involves aspects of LLM adaptation and scalability.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.22808v1": {
    "title": "MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical\n  Ethics Evaluation of LLMs",
    "summary": "While Medical Large Language Models (MedLLMs) have demonstrated remarkable\npotential in clinical tasks, their ethical safety remains insufficiently\nexplored. This paper introduces $\\textbf{MedEthicsQA}$, a comprehensive\nbenchmark comprising $\\textbf{5,623}$ multiple-choice questions and\n$\\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.\nWe systematically establish a hierarchical taxonomy integrating global medical\nethical standards. The benchmark encompasses widely used medical datasets,\nauthoritative question banks, and scenarios derived from PubMed literature.\nRigorous quality control involving multi-stage filtering and multi-faceted\nexpert validation ensures the reliability of the dataset with a low error rate\n($2.72\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\nin answering medical ethics questions compared to their foundation\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\ndataset, registered under CC BY-NC 4.0 license, is available at\nhttps://github.com/JianhuiWei7/MedEthicsQA.",
    "published": "2025-06-28T08:21:35Z",
    "updated": "2025-06-28T08:21:35Z",
    "id": "2506.22808v1",
    "authors": [
      "Jianhui Wei",
      "Zijie Meng",
      "Zikai Xiao",
      "Tianxiang Hu",
      "Yang Feng",
      "Zhijie Zhou",
      "Jian Wu",
      "Zuozhu Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22808v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22808v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22808v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark for evaluating medical ethics in LLMs, which aligns with the 'Benchmark' category as it involves evaluating LLMs. It also touches on 'LLM' as it discusses Medical Large Language Models (MedLLMs).",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2506.22791v3": {
    "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
    "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
    "published": "2025-06-28T07:25:12Z",
    "updated": "2025-07-15T12:59:47Z",
    "id": "2506.22791v3",
    "authors": [
      "Jianxin Yan",
      "Wangze Ni",
      "Lei Chen",
      "Xuemin Lin",
      "Peng Cheng",
      "Zhan Qin",
      "Kui Ren"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22791v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22791v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22791v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a semantic caching system for large language models (LLMs) that improves efficiency and reduces computational costs by reusing responses in multi-turn dialogues. It focuses on enhancing LLM performance through context-aware caching mechanisms.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.22776v1": {
    "title": "Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code\n  Generation",
    "summary": "Quantization has emerged as a mainstream method for compressing Large\nLanguage Models (LLMs), reducing memory requirements and accelerating inference\nwithout architectural modifications. While existing research primarily focuses\non evaluating the effectiveness of quantized LLMs compared to their original\ncounterparts, the impact on robustness remains largely unexplored.In this\npaper, we present the first systematic investigation of how quantization\naffects the robustness of LLMs in code generation tasks. Through extensive\nexperiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and\nStarCoder) with parameter scales ranging from 350M to 33B, we evaluate\nrobustness from dual perspectives: adversarial attacks on input prompts and\nnoise perturbations on model architecture. Our findings challenge conventional\nwisdom by demonstrating that quantized LLMs often exhibit superior robustness\ncompared to their full-precision counterparts, with 51.59% versus 42.86% of our\nadversarial experiments showing better resilience in quantized LLMs. Similarly,\nour noise perturbation experiments also confirm that LLMs after quantitation\ngenerally withstand higher levels of weight disturbances. These results suggest\nthat quantization not only reduces computational requirements but can actually\nenhance LLMs' reliability in code generation tasks, providing valuable insights\nfor developing more robust and efficient LLM deployment strategies.",
    "published": "2025-06-28T06:32:25Z",
    "updated": "2025-06-28T06:32:25Z",
    "id": "2506.22776v1",
    "authors": [
      "Sen Fang",
      "Weiyuan Ding",
      "Antonio Mastropaolo",
      "Bowen Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22776v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22776v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22776v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the robustness of quantized Large Language Models (LLMs) in code generation tasks, which involves benchmarking their performance against adversarial attacks and noise perturbations. This aligns with the topics of 'Benchmark' (evaluating LLMs) and 'LLM' (research on Large Language Models).",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2506.22756v1": {
    "title": "RoboPearls: Editable Video Simulation for Robot Manipulation",
    "summary": "The development of generalist robot manipulation policies has seen\nsignificant progress, driven by large-scale demonstration data across diverse\nenvironments. However, the high cost and inefficiency of collecting real-world\ndemonstrations hinder the scalability of data acquisition. While existing\nsimulation platforms enable controlled environments for robotic learning, the\nchallenge of bridging the sim-to-real gap remains. To address these challenges,\nwe propose RoboPearls, an editable video simulation framework for robotic\nmanipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the\nconstruction of photo-realistic, view-consistent simulations from demonstration\nvideos, and supports a wide range of simulation operators, including various\nobject manipulations, powered by advanced modules like Incremental Semantic\nDistillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by\nincorporating large language models (LLMs), RoboPearls automates the simulation\nproduction process in a user-friendly manner through flexible command\ninterpretation and execution. Furthermore, RoboPearls employs a vision-language\nmodel (VLM) to analyze robotic learning issues to close the simulation loop for\nperformance enhancement. To demonstrate the effectiveness of RoboPearls, we\nconduct extensive experiments on multiple datasets and scenes, including\nRLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which\ndemonstrate our satisfactory simulation performance.",
    "published": "2025-06-28T05:03:31Z",
    "updated": "2025-06-28T05:03:31Z",
    "id": "2506.22756v1",
    "authors": [
      "Tao Tang",
      "Likui Zhang",
      "Youpeng Wen",
      "Kaidong Zhang",
      "Jia-Wang Bian",
      "xia zhou",
      "Tianyi Yan",
      "Kun Zhan",
      "Peng Jia",
      "Hefeng Wu",
      "Liang Lin",
      "Xiaodan Liang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22756v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22756v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22756v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) and vision-language models (VLMs) in the context of robotic manipulation and simulation, which aligns with the topics of LLM and VLA. Additionally, the mention of reinforcement learning (RL) in the context of robotic learning suggests relevance to the RL topic.",
    "llm_cls_result": [
      "LLM",
      "VLA",
      "RL"
    ]
  },
  "2506.22752v1": {
    "title": "Privacy-Preserving Methods for Bug Severity Prediction",
    "summary": "Bug severity prediction is a critical task in software engineering as it\nenables more efficient resource allocation and prioritization in software\nmaintenance. While AI-based analyses and models significantly require access to\nextensive datasets, industrial applications face challenges due to data-sharing\nconstraints and the limited availability of labeled data. In this study, we\ninvestigate method-level bug severity prediction using source code metrics and\nLarge Language Models (LLMs) with two widely used datasets. We compare the\nperformance of models trained using centralized learning, federated learning,\nand synthetic data generation. Our experimental results, obtained using two\nwidely recognized software defect datasets, indicate that models trained with\nfederated learning and synthetic data achieve comparable results to centrally\ntrained models without data sharing. Our finding highlights the potential of\nprivacy-preserving approaches such as federated learning and synthetic data\ngeneration to enable effective bug severity prediction in industrial context\nwhere data sharing is a major challenge.\n  The source code and dataset are available at our GitHub repository:\nhttps://github.com/drvshavva/EASE2025-Privacy-Preserving-Methods-for-Bug-Severity-Prediction.",
    "published": "2025-06-28T04:40:51Z",
    "updated": "2025-06-28T04:40:51Z",
    "id": "2506.22752v1",
    "authors": [
      "Havvanur Derviolu",
      "Ruen Halepmollas",
      "Elif Eyvaz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22752v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22752v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22752v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for bug severity prediction in software engineering, which involves AI-based analyses and models. However, the primary focus is on privacy-preserving methods like federated learning and synthetic data generation, rather than the core aspects of LLMs, such as their architectures, scaling laws, or reasoning abilities.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.22742v1": {
    "title": "RAILS: Retrieval-Augmented Intelligence for Learning Software\n  Development",
    "summary": "Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to\nassist software development, yet they often produce incomplete code or\nincorrect imports, especially when lacking access to external or\nproject-specific documentation. We introduce RAILS (Retrieval-Augmented\nIntelligence for Learning Software Development), a framework that augments LLM\nprompts with semantically retrieved context from curated Java resources using\nFAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop\nguided by compiler feedback to refine suggestions. We evaluated RAILS on 78\nreal-world Java import error cases spanning standard libraries, GUI APIs,\nexternal tools, and custom utilities. Despite using the same LLM, RAILS\noutperforms baseline prompting by preserving intent, avoiding hallucinations,\nand surfacing correct imports even when libraries are unavailable locally.\nFuture work will integrate symbolic filtering via PostgreSQL and extend support\nto other languages and IDEs.",
    "published": "2025-06-28T03:30:04Z",
    "updated": "2025-06-28T03:30:04Z",
    "id": "2506.22742v1",
    "authors": [
      "Wali Mohammad Abdullah",
      "Md. Morshedul Islam",
      "Devraj Parmar",
      "Happy Hasmukhbhai Patel",
      "Sindhuja Prabhakaran",
      "Baidya Saha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22742v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22742v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22742v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in software development and introduces a retrieval-augmented framework to improve their performance. The core topics are related to LLMs and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.22724v1": {
    "title": "The Translation Barrier Hypothesis: Multilingual Generation with Large\n  Language Models Suffers from Implicit Translation Failure",
    "summary": "Multilingual generation with large language models (LLMs) is often of poor\nquality for mid- to low-resource languages. Building on insights from\ninterpretability, we demonstrate the existence of an implicit\ntask-solving-->translation pipeline for generation, whereby the model first\nsolves the required task in a largely target-language-agnostic manner, and\nsubsequently translates answer concepts into the intended target language. We\nhypothesize that the failure of the translation stage is an important culprit\nfor the observed low quality of final outputs, and formalize this as the\ntranslation barrier hypothesis. We test this hypothesis for a word translation\ntask across 108 language pairs, using logit lens to observe model processing in\nintermediate layers. We find that a significant portion of overall failures\nindeed stems from translation failure, or the model's inability to translate\ncorrectly solved intermediate concepts into the target language. This is\nespecially true for low-resource target languages. Our results highlight an\nimportant hurdle for end-to-end multilingual generation, and lend guiding\ninsights for future work seeking to improve multilinguality in LLMs.",
    "published": "2025-06-28T02:09:21Z",
    "updated": "2025-06-28T02:09:21Z",
    "id": "2506.22724v1",
    "authors": [
      "Niyati Bafna",
      "Tianjian Li",
      "Kenton Murray",
      "David R. Mortensen",
      "David Yarowsky",
      "Hale Sirin",
      "Daniel Khashabi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22724v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22724v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22724v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the quality issues in multilingual generation with large language models (LLMs), specifically focusing on the implicit translation failure in mid- to low-resource languages. It aligns with the 'LLM' topic as it involves research on large language models and their architectures, and also touches on 'Reasoning' as it investigates the model's task-solving and translation processes.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.22716v1": {
    "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute",
    "summary": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.",
    "published": "2025-06-28T01:52:50Z",
    "updated": "2025-06-28T01:52:50Z",
    "id": "2506.22716v1",
    "authors": [
      "Dujian Ding",
      "Ankur Mallick",
      "Shaokun Zhang",
      "Chi Wang",
      "Daniel Madrigal",
      "Mirian Del Carmen Hipolito Garcia",
      "Menglin Xia",
      "Laks V. S. Lakshmanan",
      "Qingyun Wu",
      "Victor Rhle"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22716v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22716v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22716v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses adaptive routing of queries to different LLMs based on cost and quality, which involves optimizing the use of LLMs and their computational resources.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.22708v1": {
    "title": "FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement\n  Learning in Peer-to-Peer Markets",
    "summary": "Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for\ndecentralized market regulation, yet existing approaches often lack robust\nframeworks to ensure fairness. This paper presents FairMarket-RL, a novel\nhybrid framework that combines Large Language Models (LLMs) with Reinforcement\nLearning (RL) to enable fairness-aware trading agents. In a simulated P2P\nmicrogrid with multiple sellers and buyers, the LLM acts as a real-time\nfairness critic, evaluating each trading episode using two metrics:\nFairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness\nscores are integrated into agent rewards through scheduled\n{\\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that\nreplaces brittle, rule-based fairness constraints. Agents are trained using\nIndependent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,\nfulfilling over 90% of buyer demand, maintaining fair seller margins, and\nconsistently reaching FTB and FBS scores above 0.80. The training process\ndemonstrates that fairness feedback improves convergence, reduces buyer\nshortfalls, and narrows profit disparities between sellers. With its\nlanguage-based critic, the framework scales naturally, and its extension to a\nlarge power distribution system with household prosumers illustrates its\npractical applicability. FairMarket-RL thus offers a scalable, equity-driven\nsolution for autonomous trading in decentralized energy systems.",
    "published": "2025-06-28T01:17:55Z",
    "updated": "2025-06-28T01:17:55Z",
    "id": "2506.22708v1",
    "authors": [
      "Shrenik Jadhav",
      "Birva Sevak",
      "Srijita Das",
      "Akhtar Hussain",
      "Wencong Su",
      "Van-Hai Bui"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22708v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22708v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22708v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper combines Large Language Models (LLMs) with Reinforcement Learning (RL) to address fairness in peer-to-peer markets, which aligns with the topics of LLM and RL. The use of LLMs as fairness critics and the integration into RL agents' training process are central to the paper's methodology.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.22704v2": {
    "title": "Beyond Code: The Multidimensional Impacts of Large Language Models in\n  Software Development",
    "summary": "Large language models (LLMs) are poised to significantly impact software\ndevelopment, especially in the Open-Source Software (OSS) sector. To understand\nthis impact, we first outline the mechanisms through which LLMs may influence\nOSS through code development, collaborative knowledge transfer, and skill\ndevelopment. We then empirically examine how LLMs affect OSS developers' work\nin these three key areas. Leveraging a natural experiment from a temporary\nChatGPT ban in Italy, we employ a Difference-in-Differences framework with\ntwo-way fixed effects to analyze data from all OSS developers on GitHub in\nthree similar countries, Italy, France, and Portugal, totaling 88,022 users. We\nfind that access to ChatGPT increases developer productivity by 6.4%, knowledge\nsharing by 9.6%, and skill acquisition by 8.4%. These benefits vary\nsignificantly by user experience level: novice developers primarily experience\nproductivity gains, whereas more experienced developers benefit more from\nimproved knowledge sharing and accelerated skill acquisition. In addition, we\nfind that LLM-assisted learning is highly context-dependent, with the greatest\nbenefits observed in technically complex, fragmented, or rapidly evolving\ncontexts. We show that the productivity effects of LLMs extend beyond direct\ncode generation to include enhanced collaborative learning and knowledge\nexchange among developers, dynamics that are essential for gaining a holistic\nunderstanding of LLMs' impact in OSS. Our findings offer critical managerial\nimplications: strategically deploying LLMs can accelerate novice developers'\nonboarding and productivity, empower intermediate developers to foster\nknowledge sharing and collaboration, and support rapid skill acquisition,\ntogether enhancing long-term organizational productivity and agility.",
    "published": "2025-06-28T01:10:24Z",
    "updated": "2025-07-01T02:35:48Z",
    "id": "2506.22704v2",
    "authors": [
      "Sardar Bonabi",
      "Sarah Bana",
      "Vijay Gurbaxani",
      "Tingting Nian"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22704v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22704v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22704v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of Large Language Models (LLMs) on software development, particularly in the Open-Source Software sector, focusing on productivity, knowledge sharing, and skill acquisition. The core topic is the application and effects of LLMs in a specific domain.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.22703v1": {
    "title": "P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial\n  Code",
    "summary": "We present P4OMP, a retrieval-augmented framework for transforming serial\nC/C++ code into OpenMP-annotated parallel code using large language models\n(LLMs). To our knowledge, this is the first system to apply retrieval-based\nprompting for OpenMP pragma correctness without model fine-tuning or compiler\ninstrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with\nstructured instructional knowledge from OpenMP tutorials to improve the\nreliability of prompt-driven code generation. By grounding generation in the\nretrieved context, P4OMP improves syntactic correctness compared to baseline\nprompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,\nGPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world\nC++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.\nP4OMP achieves 100% compilation success on all parallelizable cases, while the\nbaseline fails to compile in 20 out of 108 cases. Six cases that rely on\nnon-random-access iterators or thread-unsafe constructs are excluded due to\nfundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP\nconsistently avoids scoping errors, syntactic misuse, and invalid directive\ncombinations that commonly affect baseline-generated code. We further\ndemonstrate strong runtime scaling across seven compute-intensive benchmarks on\nan HPC cluster. P4OMP offers a robust, modular pipeline that significantly\nimproves the reliability and applicability of LLM-generated OpenMP code.",
    "published": "2025-06-28T01:06:34Z",
    "updated": "2025-06-28T01:06:34Z",
    "id": "2506.22703v1",
    "authors": [
      "Wali Mohammad Abdullah",
      "Azmain Kabir"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22703v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22703v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22703v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in a retrieval-augmented framework for code generation, specifically focusing on OpenMP parallelism. It highlights the application of Retrieval-Augmented Generation (RAG) and the improvement in syntactic correctness and reliability of LLM-generated code. The core topics are related to LLMs and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.22698v2": {
    "title": "Text Production and Comprehension by Human and Artificial Intelligence:\n  Interdisciplinary Workshop Report",
    "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.",
    "published": "2025-06-28T00:31:14Z",
    "updated": "2025-07-01T15:26:29Z",
    "id": "2506.22698v2",
    "authors": [
      "Emily Dux Speltz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22698v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22698v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22698v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the relationship between large language models (LLMs) and human cognitive processes in text comprehension and composition, highlighting the capabilities and limitations of LLMs, and their alignment with human language processing when fine-tuned with human feedback. It also touches on human-AI collaboration in language tasks.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2506.22688v1": {
    "title": "An LLM-assisted approach to designing software architectures using ADD",
    "summary": "Designing effective software architectures is a complex, iterative process\nthat traditionally relies on expert judgment. This paper proposes an approach\nfor Large Language Model (LLM)-assisted software architecture design using the\nAttribute-Driven Design (ADD) method. By providing an LLM with an explicit\ndescription of ADD, an architect persona, and a structured iteration plan, our\nmethod guides the LLM to collaboratively produce architecture artifacts with a\nhuman architect. We validate the approach through case studies, comparing\ngenerated designs against proven solutions and evaluating them with\nprofessional architects. Results show that our LLM-assisted ADD process can\ngenerate architectures closely aligned with established solutions and partially\nsatisfying architectural drivers, highlighting both the promise and current\nlimitations of using LLMs in architecture design. Our findings emphasize the\nimportance of human oversight and iterative refinement when leveraging LLMs in\nthis domain.",
    "published": "2025-06-27T23:58:15Z",
    "updated": "2025-06-27T23:58:15Z",
    "id": "2506.22688v1",
    "authors": [
      "Humberto Cervantes",
      "Rick Kazman",
      "Yuanfang Cai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22688v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22688v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22688v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in assisting the design of software architectures, which directly relates to the topic of LLMs and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.22679v1": {
    "title": "Assessing the feasibility of Large Language Models for detecting\n  micro-behaviors in team interactions during space missions",
    "summary": "We explore the feasibility of large language models (LLMs) in detecting\nsubtle expressions of micro-behaviors in team conversations using transcripts\ncollected during simulated space missions. Specifically, we examine zero-shot\nclassification, fine-tuning, and paraphrase-augmented fine-tuning with\nencoder-only sequence classification LLMs, as well as few-shot text generation\nwith decoder-only causal language modeling LLMs, to predict the micro-behavior\nassociated with each conversational turn (i.e., dialogue). Our findings\nindicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to\ndetect underrepresented micro-behaviors, particularly discouraging speech, even\nwith weighted fine-tuning. In contrast, the instruction fine-tuned version of\nLlama-3.1, a decoder-only LLM, demonstrated superior performance, with the best\nmodels achieving macro F1-scores of 44% for 3-way classification and 68% for\nbinary classification. These results have implications for the development of\nspeech technologies aimed at analyzing team communication dynamics and\nenhancing training interventions in high-stakes environments such as space\nmissions, particularly in scenarios where text is the only accessible data.",
    "published": "2025-06-27T23:06:24Z",
    "updated": "2025-06-27T23:06:24Z",
    "id": "2506.22679v1",
    "authors": [
      "Ankush Raut",
      "Projna Paromita",
      "Sydney Begerowski",
      "Suzanne Bell",
      "Theodora Chaspari"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22679v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22679v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22679v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) for detecting micro-behaviors in team interactions, specifically using encoder-only and decoder-only LLMs. The study evaluates the performance of these models in a specialized context, which aligns with the 'LLM' topic. The research also involves fine-tuning and few-shot learning, which are relevant to 'Pretrain' as it discusses pretraining strategies and their effectiveness in specific tasks.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2506.22653v1": {
    "title": "URSA: The Universal Research and Scientific Agent",
    "summary": "Large language models (LLMs) have moved far beyond their initial form as\nsimple chatbots, now carrying out complex reasoning, planning, writing, coding,\nand research tasks. These skills overlap significantly with those that human\nscientists use day-to-day to solve complex problems that drive the cutting edge\nof research. Using LLMs in \"agentic\" AI has the potential to revolutionize\nmodern science and remove bottlenecks to progress. In this work, we present\nURSA, a scientific agent ecosystem for accelerating research tasks. URSA\nconsists of a set of modular agents and tools, including coupling to advanced\nphysics simulation codes, that can be combined to address scientific problems\nof varied complexity and impact. This work highlights the architecture of URSA,\nas well as examples that highlight the potential of the system.",
    "published": "2025-06-27T21:56:02Z",
    "updated": "2025-06-27T21:56:02Z",
    "id": "2506.22653v1",
    "authors": [
      "Michael Grosskopf",
      "Russell Bent",
      "Rahul Somasundaram",
      "Isaac Michaud",
      "Arthur Lui",
      "Nathan Debardeleben",
      "Earl Lawrence"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22653v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22653v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22653v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of LLMs in creating a scientific agent ecosystem for research tasks, which involves complex reasoning and planning, aligning with the topics of LLM and AGI. The mention of modular agents and tools also hints at the potential use of RL for agentic AI.",
    "llm_cls_result": [
      "LLM",
      "AGI",
      "RL"
    ]
  },
  "2506.22638v1": {
    "title": "Layer Importance for Mathematical Reasoning is Forged in Pre-Training\n  and Invariant after Post-Training",
    "summary": "Large language models can exhibit improved mathematical reasoning\ncapabilities following post-training with instruction tuning, reinforcement\nlearning, or knowledge distillation. However, it remains unclear whether these\nimprovements are driven by major changes in transformer layers or from minor\nadjustments that leave the relative layer importance structures of the base\nmodel largely unchanged. We investigate this question through systematic\nlayer-wise ablation experiments, examining base, instruction-tuned,\nknowledge-distilled, and reinforcement learning variants on mathematical\nreasoning benchmarks. Our findings show that mathematical reasoning gives rise\nto a specific layer importance structure, and this structure persists across\nall post-training paradigms. Removal of such layers causes accuracy drops of up\nto 80%. In contrast, non-mathematical tasks like factual recall exhibit no\ncritical layers. This distinction suggests that mathematical reasoning requires\nspecialized layers that emerge during pre-training, while other non-reasoning\ntasks do not. From an information-theoretic perspective, we also observe that\nthese critical layers are the same layers where major representational\ntransformation occurs.",
    "published": "2025-06-27T21:04:55Z",
    "updated": "2025-06-27T21:04:55Z",
    "id": "2506.22638v1",
    "authors": [
      "Aadim Nepal",
      "Safal Shrestha",
      "Anubhav Shrestha",
      "Minwu Kim",
      "Keith Ross"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22638v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22638v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22638v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the layer importance in large language models for mathematical reasoning, which is established during pre-training and remains invariant post-training. It discusses the impact of different post-training paradigms on the model's reasoning capabilities, aligning with topics related to reasoning in LLMs and pretraining strategies.",
    "llm_cls_result": [
      "Reasoning",
      "Pretrain"
    ]
  },
  "2507.02941v1": {
    "title": "GameTileNet: A Semantic Dataset for Low-Resolution Game Art in\n  Procedural Content Generation",
    "summary": "GameTileNet is a dataset designed to provide semantic labels for\nlow-resolution digital game art, advancing procedural content generation (PCG)\nand related AI research as a vision-language alignment task. Large Language\nModels (LLMs) and image-generative AI models have enabled indie developers to\ncreate visual assets, such as sprites, for game interactions. However,\ngenerating visuals that align with game narratives remains challenging due to\ninconsistent AI outputs, requiring manual adjustments by human artists. The\ndiversity of visual representations in automatically generated game content is\nalso limited because of the imbalance in distributions across styles for\ntraining data. GameTileNet addresses this by collecting artist-created game\ntiles from OpenGameArt.org under Creative Commons licenses and providing\nsemantic annotations to support narrative-driven content generation. The\ndataset introduces a pipeline for object detection in low-resolution tile-based\ngame art (e.g., 32x32 pixels) and annotates semantics, connectivity, and object\nclassifications. GameTileNet is a valuable resource for improving PCG methods,\nsupporting narrative-rich game content, and establishing a baseline for object\ndetection in low-resolution, non-photorealistic images.\n  TL;DR: GameTileNet is a semantic dataset of low-resolution game tiles\ndesigned to support narrative-driven procedural content generation through\nvisual-language alignment.",
    "published": "2025-06-27T20:50:32Z",
    "updated": "2025-06-27T20:50:32Z",
    "id": "2507.02941v1",
    "authors": [
      "Yi-Chun Chen",
      "Arnav Jhala"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02941v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02941v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02941v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a dataset (GameTileNet) designed for low-resolution game art with semantic labels, supporting procedural content generation and vision-language alignment tasks. It mentions the use of Large Language Models (LLMs) and image-generative AI models, but the primary focus is on the dataset and its application in game content generation.",
    "llm_cls_result": [
      "Dataset",
      "VLA"
    ]
  },
  "2506.22623v1": {
    "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing\n  Attacks",
    "summary": "In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~\\cite{aarson} watermarking method.",
    "published": "2025-06-27T20:39:35Z",
    "updated": "2025-06-27T20:39:35Z",
    "id": "2506.22623v1",
    "authors": [
      "Badr Youbi Idrissi",
      "Monica Millunzi",
      "Amelia Sorrenti",
      "Lorenzo Baraldi",
      "Daryna Dementieva"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22623v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22623v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22623v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses watermarking techniques for Large Language Models (LLMs) to detect synthetic text, which aligns with the 'LLM' topic as it involves research on LLMs and their applications. It also touches on ethical concerns and misuse, which are broader issues related to LLMs.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.22604v1": {
    "title": "Bootstrapping Human-Like Planning via LLMs",
    "summary": "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance.",
    "published": "2025-06-27T20:00:51Z",
    "updated": "2025-06-27T20:00:51Z",
    "id": "2506.22604v1",
    "authors": [
      "David Porfirio",
      "Vincent Hsiao",
      "Morgan Fine-Morris",
      "Leslie Smith",
      "Laura M. Hiatt"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22604v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22604v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22604v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to generate human-like action sequences for robot tasks, which aligns with research on LLMs and their applications in generating human-like outputs.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.22598v2": {
    "title": "RExBench: Can coding agents autonomously implement AI research\n  extensions?",
    "summary": "Agents based on Large Language Models (LLMs) have shown promise for\nperforming sophisticated software engineering tasks autonomously. In addition,\nthere has been progress towards developing agents that can perform parts of the\nresearch pipeline in machine learning and the natural sciences. We argue that\nresearch extension and its implementation is a critical capability for such\nsystems, and introduce RExBench to support the evaluation of this capability.\nRExBench is a benchmark consisting of 12 realistic research experiment\nimplementation tasks that aim to investigate research hypotheses that have not\npreviously been implemented. Each task is set up as an extension to an existing\nresearch paper and codebase, accompanied by domain expert-written instructions.\nRExBench is robust to data contamination, and supports an automatic evaluation\ninfrastructure that executes agent outputs to determine whether the success\ncriteria are met. We use this benchmark to evaluate nine LLM agents implemented\nusing three different frameworks: aider, Claude Code, and OpenHands. We find\nthat all agents evaluated fail to autonomously implement the majority of the\nextensions. Although the success rate improves with additional human-written\nhints, the best performance under this setting remains below 40%. This\nindicates that current agents are still short of being able to handle realistic\nresearch extension tasks without substantial human guidance.",
    "published": "2025-06-27T19:41:41Z",
    "updated": "2025-07-17T18:45:58Z",
    "id": "2506.22598v2",
    "authors": [
      "Nicholas Edwards",
      "Yukyung Lee",
      "Yujun Audrey Mao",
      "Yulu Qin",
      "Sebastian Schuster",
      "Najoung Kim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22598v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22598v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22598v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of LLM-based agents in performing research extension tasks, which involves both LLMs and Reinforcement Learning (RL) aspects, and introduces a benchmark for this purpose.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Benchmark"
    ]
  },
  "2506.22557v1": {
    "title": "MetaCipher: A General and Extensible Reinforcement Learning Framework\n  for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs",
    "summary": "The growing capabilities of large language models (LLMs) have exposed them to\nincreasingly sophisticated jailbreak attacks. Among these, obfuscation-based\nattacks -- which encrypt malicious content to evade detection -- remain highly\neffective. By leveraging the reasoning ability of advanced LLMs to interpret\nencrypted prompts, such attacks circumvent conventional defenses that rely on\nkeyword detection or context filtering. These methods are very difficult to\ndefend against, as existing safety mechanisms are not designed to interpret or\ndecode ciphered content. In this work, we propose \\textbf{MetaCipher}, a novel\nobfuscation-based jailbreak framework, along with a reinforcement\nlearning-based dynamic cipher selection mechanism that adaptively chooses\noptimal encryption strategies from a cipher pool. This approach enhances\njailbreak effectiveness and generalizability across diverse task types, victim\nLLMs, and safety guardrails. Our framework is modular and extensible by design,\nsupporting arbitrary cipher families and accommodating evolving adversarial\nstrategies. We complement our method with a large-scale empirical analysis of\ncipher performance across multiple victim LLMs. Within as few as 10 queries,\nMetaCipher achieves over 92\\% attack success rate (ASR) on most recent standard\nmalicious prompt benchmarks against state-of-the-art non-reasoning LLMs, and\nover 74\\% ASR against reasoning-capable LLMs, outperforming all existing\nobfuscation-based jailbreak methods. These results highlight the long-term\nrobustness and adaptability of our approach, making it more resilient than\nprior methods in the face of advancing safety measures.",
    "published": "2025-06-27T18:15:56Z",
    "updated": "2025-06-27T18:15:56Z",
    "id": "2506.22557v1",
    "authors": [
      "Boyuan Chen",
      "Minghao Shao",
      "Abdul Basit",
      "Siddharth Garg",
      "Muhammad Shafique"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22557v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22557v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22557v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a reinforcement learning-based framework for jailbreak attacks on LLMs, which involves the use of LLMs' reasoning abilities and reinforcement learning techniques.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Reasoning"
    ]
  },
  "2506.22419v2": {
    "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
    "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
    "published": "2025-06-27T17:44:32Z",
    "updated": "2025-06-30T21:56:29Z",
    "id": "2506.22419v2",
    "authors": [
      "Bingchen Zhao",
      "Despoina Magka",
      "Minqi Jiang",
      "Xian Li",
      "Roberta Raileanu",
      "Tatiana Shavrina",
      "Jean-Christophe Gagnon-Audet",
      "Kelvin Niu",
      "Shagun Sodhani",
      "Michael Shvartsman",
      "Andrei Lupu",
      "Alisia Lupidi",
      "Edan Toledo",
      "Karen Hambardzumyan",
      "Martin Josifoski",
      "Thomas Foster",
      "Lucia Cipolina-Kun",
      "Abhishek Charnalia",
      "Derek Dunfield",
      "Alexander H. Miller",
      "Oisin Mac Aodha",
      "Jakob Foerster",
      "Yoram Bachrach"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22419v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22419v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22419v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking LLMs' ability to reproduce scientific results and improve training processes, which aligns with the topics of Benchmark and LLM.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2506.22403v2": {
    "title": "HyperCLOVA X THINK Technical Report",
    "summary": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\nTransformer scaled with $\\mu$P, pre-trained through a three-stage curriculum\nthat expands the context window to $128$K tokens, and post-trained via\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\nsupports both detailed rationale and concise-answer modes. It delivers\ncompetitive performance against similarly sized models on Korea-focused\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\npreserving robust bilingual consistency and translation quality. In addition, a\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\nbenchmark, all of which are achieved with substantially lower training compute\nthan existing models of similar sizes. We also present a pruning and\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\nopen-source and business-friendly foundation model. Altogether, these\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\ninnovation and a valuable resource for the global research community.",
    "published": "2025-06-27T17:23:12Z",
    "updated": "2025-07-01T13:39:25Z",
    "id": "2506.22403v2",
    "authors": [
      " NAVER Cloud HyperCLOVA X Team"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22403v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22403v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22403v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a reasoning-focused large language model (HyperCLOVA X THINK) with features such as pre-training, reinforcement learning, and multimodal capabilities (vision-augmented variant). It also mentions performance on benchmarks and datasets, making it relevant to LLM, Reasoning, and MLLM topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "MLLM"
    ]
  },
  "2506.22402v1": {
    "title": "Refining Czech GEC: Insights from a Multi-Experiment Approach",
    "summary": "We present a grammar error correction (GEC) system that achieves state of the\nart for the Czech language. Our system is based on a neural network translation\napproach with the Transformer architecture, and its key feature is its\nreal-time synthetic generation pipeline, which dynamically augments sentences\nwith artificial errors by introducing both language-agnostic and Czech-specific\nerrors. We conduct a comprehensive series of experiments, investigating the\nCzech GEC corpora as bases for synthetic error introduction, several error\ngeneration strategies, domain balancing, tokenization granularity, model size,\nand data scaling during fine-tuning. Additionally, we evaluate the performance\nof large language models (LLMs) on Czech GEC in both end-user and expert\nfine-tuning scenarios. Our best-performing model is superior both in\nperformance and computational efficiency. The source code and the trained model\nlinks are available on https://github.com/ufal/tsd2025-gec.",
    "published": "2025-06-27T17:21:40Z",
    "updated": "2025-06-27T17:21:40Z",
    "id": "2506.22402v1",
    "authors": [
      "Petr Pechman",
      "Milan Straka",
      "Jana Strakov",
      "Jakub Nplava"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22402v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22402v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22402v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on grammar error correction (GEC) for the Czech language using a Transformer-based neural network approach and evaluates the performance of large language models (LLMs) in this context. The mention of LLMs and their application in GEC aligns with the 'LLM' topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.22396v1": {
    "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
    "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
    "published": "2025-06-27T17:10:32Z",
    "updated": "2025-06-27T17:10:32Z",
    "id": "2506.22396v1",
    "authors": [
      "Danush Khanna",
      "Aditya Kumar Guru",
      "Srivarshinee Sridhar",
      "Zidan Ahmed",
      "Rubhav Bahirwani",
      "Meetu Malhotra",
      "Vinija Jain",
      "Aman Chadha",
      "Amitava Das",
      "Kripabandhu Ghosh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22396v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22396v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22396v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on optimizing LLM inference through various techniques such as dynamic token halting, KV cache skipping, and contextual token fusion, which are all aimed at improving the efficiency of LLM deployments without altering the model's architecture or weights. This directly relates to the 'LLM' topic as it deals with large language models and their performance optimization.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.22390v1": {
    "title": "What Makes ChatGPT Effective for Software Issue Resolution? An Empirical\n  Study of Developer-ChatGPT Conversations in GitHub",
    "summary": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks.",
    "published": "2025-06-27T17:00:48Z",
    "updated": "2025-06-27T17:00:48Z",
    "id": "2506.22390v1",
    "authors": [
      "Ramtin Ehsani",
      "Sakshi Pathak",
      "Esteban Parra",
      "Sonia Haiduc",
      "Preetha Chatterjee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22390v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22390v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22390v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the effectiveness of ChatGPT (a large language model) in software issue resolution, analyzing developer interactions and identifying factors that make these interactions helpful. It discusses the practical application of LLMs in a specific domain (software development) and their limitations, which aligns with the 'LLM' and 'Reasoning' topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.22385v1": {
    "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A\n  Study on Defeasible Video Entailment",
    "summary": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs.",
    "published": "2025-06-27T16:51:15Z",
    "updated": "2025-06-27T16:51:15Z",
    "id": "2506.22385v1",
    "authors": [
      "Yue Zhang",
      "Jilei Sun",
      "Yunhui Guo",
      "Vibhav Gogate"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22385v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22385v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22385v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses Video Large Multimodal Models (VLMMs) and their ability to perform abstract and adaptive reasoning, specifically focusing on defeasible reasoning. It introduces a new task (DVidE) and a benchmark dataset, which aligns with topics related to Multimodal Large Language Models (MLLM) and Benchmarking.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2506.22376v1": {
    "title": "Probabilistic Optimality for Inference-time Scaling",
    "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.",
    "published": "2025-06-27T16:44:11Z",
    "updated": "2025-06-27T16:44:11Z",
    "id": "2506.22376v1",
    "authors": [
      "Youkang Wang",
      "Jian Wang",
      "Rubing Chen",
      "Xiao-Yong Wei",
      "Qing Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22376v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22376v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22376v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses inference-time scaling techniques for Large Language Models (LLMs) and their application to reasoning tasks, which aligns with the topics of 'LLM' and 'Reasoning'. The focus on scaling and performance optimization also relates to 'Scaling'.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Scaling"
    ]
  },
  "2506.22372v1": {
    "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and\n  Measurement",
    "summary": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems.",
    "published": "2025-06-27T16:39:12Z",
    "updated": "2025-06-27T16:39:12Z",
    "id": "2506.22372v1",
    "authors": [
      "Maryam Mousavian",
      "Zahra Abbasiantaeb",
      "Mohammad Aliannejadi",
      "Fabio Crestani"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22372v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22372v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22372v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging Large Language Models (LLMs) for detecting and measuring gender bias in passage ranking, which directly involves the use of LLMs and their application in addressing bias in NLP and IR systems.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2506.22370v3": {
    "title": "Can Large Language Models Help Students Prove Software Correctness? An\n  Experimental Study with Dafny",
    "summary": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. Our findings show that\nstudents perform significantly better when using ChatGPT; however, performance\ngains are tied to prompt quality. We conclude with practical recommendations\nfor integrating LLMs into formal methods courses more effectively, including\ndesigning LLM-aware challenges that promote learning rather than substitution.",
    "published": "2025-06-27T16:34:13Z",
    "updated": "2025-07-11T19:12:05Z",
    "id": "2506.22370v3",
    "authors": [
      "Carolina Carreira",
      "lvaro Silva",
      "Alexandre Abreu",
      "Alexandra Mendes"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22370v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22370v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22370v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in supporting students with formal verification tasks in computing education, which directly relates to the topic of LLMs and their applications in educational contexts.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.22359v1": {
    "title": "Concept-Level AI for Telecom: Moving Beyond Large Language Models",
    "summary": "The telecommunications and networking domain stands at the precipice of a\ntransformative era, driven by the necessity to manage increasingly complex,\nhierarchical, multi administrative domains (i.e., several operators on the same\npath) and multilingual systems. Recent research has demonstrated that Large\nLanguage Models (LLMs), with their exceptional general-purpose text analysis\nand code generation capabilities, can be effectively applied to certain telecom\nproblems (e.g., auto-configuration of data plan to meet certain application\nrequirements). However, due to their inherent token-by-token processing and\nlimited capacity for maintaining extended context, LLMs struggle to fulfill\ntelecom-specific requirements such as cross-layer dependency cascades (i.e.,\nover OSI), temporal-spatial fault correlation, and real-time distributed\ncoordination. In contrast, Large Concept Models (LCMs), which reason at the\nabstraction level of semantic concepts rather than individual lexical tokens,\noffer a fundamentally superior approach for addressing these telecom\nchallenges. By employing hyperbolic latent spaces for hierarchical\nrepresentation and encapsulating complex multi-layered network interactions\nwithin concise concept embeddings, LCMs overcome critical shortcomings of LLMs\nin terms of memory efficiency, cross-layer correlation, and native multimodal\nintegration. This paper argues that adopting LCMs is not simply an incremental\nstep, but a necessary evolutionary leap toward achieving robust and effective\nAI-driven telecom management.",
    "published": "2025-06-27T16:20:18Z",
    "updated": "2025-06-27T16:20:18Z",
    "id": "2506.22359v1",
    "authors": [
      "Viswanath Kumarskandpriya",
      "Abdulhalim Dandoush",
      "Abbas Bradai",
      "Ali Belgacem"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22359v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22359v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22359v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the limitations of Large Language Models (LLMs) in the telecom domain and introduces Large Concept Models (LCMs) as a superior alternative. While LLMs are mentioned, the focus is on a new approach (LCMs) that goes beyond traditional LLM capabilities, making it more relevant to the broader topic of Artificial General Intelligence (AGI) rather than specific LLM research.",
    "llm_cls_result": [
      "AGI"
    ]
  },
  "2506.22343v1": {
    "title": "Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts",
    "summary": "Text watermarks in large language models (LLMs) are an increasingly important\ntool for detecting synthetic text and distinguishing human-written content from\nLLM-generated text. While most existing studies focus on determining whether\nentire texts are watermarked, many real-world scenarios involve mixed-source\ntexts, which blend human-written and watermarked content. In this paper, we\naddress the problem of optimally estimating the watermark proportion in\nmixed-source texts. We cast this problem as estimating the proportion parameter\nin a mixture model based on \\emph{pivotal statistics}. First, we show that this\nparameter is not even identifiable in certain watermarking schemes, let alone\nconsistently estimable. In stark contrast, for watermarking methods that employ\ncontinuous pivotal statistics for detection, we demonstrate that the proportion\nparameter is identifiable under mild conditions. We propose efficient\nestimators for this class of methods, which include several popular unbiased\nwatermarks as examples, and derive minimax lower bounds for any measurable\nestimator based on pivotal statistics, showing that our estimators achieve\nthese lower bounds. Through evaluations on both synthetic data and mixed-source\ntext generated by open-source models, we demonstrate that our proposed\nestimators consistently achieve high estimation accuracy.",
    "published": "2025-06-27T15:53:04Z",
    "updated": "2025-06-27T15:53:04Z",
    "id": "2506.22343v1",
    "authors": [
      "Xiang Li",
      "Garrett Wen",
      "Weiqing He",
      "Jiayuan Wu",
      "Qi Long",
      "Weijie J. Su"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22343v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22343v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22343v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on watermarking in large language models (LLMs) and the estimation of watermark proportions in mixed-source texts, which directly relates to LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.22316v1": {
    "title": "Evaluating Scoring Bias in LLM-as-a-Judge",
    "summary": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection.",
    "published": "2025-06-27T15:25:23Z",
    "updated": "2025-06-27T15:25:23Z",
    "id": "2506.22316v1",
    "authors": [
      "Qingquan Li",
      "Shaoyu Dou",
      "Kailai Shao",
      "Chao Chen",
      "Haixiang Hu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22316v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22316v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22316v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of biases in LLMs when used as judges, focusing on scoring-based evaluations and their impact on fairness and reliability. It involves the use of LLMs in evaluation tasks, which is directly related to the 'LLM' topic. Additionally, the construction of an evaluation dataset and the discussion on benchmarks align with the 'Benchmark' topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.22305v1": {
    "title": "Detection of Personal Data in Structured Datasets Using a Large Language\n  Model",
    "summary": "We propose a novel approach for detecting personal data in structured\ndatasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key\ninnovation of our method is the incorporation of contextual information: in\naddition to a feature's name and values, we utilize information from other\nfeature names within the dataset as well as the dataset description. We compare\nour approach to alternative methods, including Microsoft Presidio and CASSED,\nevaluating them on multiple datasets: DeSSI, a large synthetic dataset,\ndatasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a\nreal-world dataset containing patient information from critical care units.\n  Our findings reveal that detection performance varies significantly depending\non the dataset used for evaluation. CASSED excels on DeSSI, the dataset on\nwhich it was trained. Performance on the medical dataset MIMIC-Demo-Ext is\ncomparable across all models, with our GPT-4o-based approach clearly\noutperforming the others. Notably, personal data detection in the Kaggle and\nOpenML datasets appears to benefit from contextual information. This is\nevidenced by the poor performance of CASSED and Presidio (both of which do not\nutilize the context of the dataset) compared to the strong results of our\nGPT-4o-based approach.\n  We conclude that further progress in this field would greatly benefit from\nthe availability of more real-world datasets containing personal information.",
    "published": "2025-06-27T15:16:43Z",
    "updated": "2025-06-27T15:16:43Z",
    "id": "2506.22305v1",
    "authors": [
      "Albert Agisha Ntwali",
      "Luca Rck",
      "Martin Heckmann"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22305v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22305v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22305v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a Large Language Model (GPT-4o) for detecting personal data in structured datasets, which aligns with the 'LLM' topic. It also involves evaluation on multiple datasets, which relates to the 'Dataset' topic.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2506.22283v1": {
    "title": "Rethinking Visual Token Reduction in LVLMs under Cross-modal\n  Misalignment",
    "summary": "Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences\nof patch-level tokens to capture fine-grained semantics. These visual tokens\noften outnumber their textual counterparts by a large margin, leading to\nsubstantial computational overhead and limiting the scalability of LVLMs in\npractice. Previous efforts have explored visual token reduction either prior to\nor within the large language models (LLM). However, most in-LLM reduction\napproaches rely on text-conditioned interactions, implicitly assuming that\ntextual tokens can reliably capture the importance of visual tokens. In this\nwork, we revisit this assumption and reveal causal, semantic, and spatial forms\nof cross-modal misalignment. These misalignments undermine the effectiveness of\ntext-guided visual token reduction. To address this, we introduce VisionDrop, a\ntraining-free, visual-only pruning framework that selects informative visual\ntokens based on intra-modal (visual-to-visual) attention, without relying on\ntextual signals. To further suppress redundancy throughout the model hierarchy,\nwe treat the visual encoder and the LLM as a unified system and design a\nprogressive pruning pipeline. Our method performs dominant token selection and\nlightweight contextual merging at multiple stages, enabling fine-grained visual\ninformation to be retained even under aggressive token budgets. Extensive\nexperiments across diverse benchmarks show that VisionDrop achieves consistent\nimprovements over existing methods, despite requiring no additional training or\ncomplex modifications. Its simple yet effective design enables efficient\ninference while preserving strong performance across tasks.",
    "published": "2025-06-27T14:55:40Z",
    "updated": "2025-06-27T14:55:40Z",
    "id": "2506.22283v1",
    "authors": [
      "Rui Xu",
      "Yunke Wang",
      "Yong Luo",
      "Bo Du"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22283v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22283v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22283v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses Large Vision-Language Models (LVLMs) and focuses on visual token reduction, which is a key aspect of multimodal models. It introduces a method for pruning visual tokens based on intra-modal attention, which is relevant to both multimodal models and vision-language alignment.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2506.22270v2": {
    "title": "Public Service Algorithm: towards a transparent, explainable, and\n  scalable content curation for news content based on editorial values",
    "summary": "The proliferation of disinformation challenges traditional, unscalable\neditorial processes and existing automated systems that prioritize engagement\nover public service values. To address this, we introduce the Public Service\nAlgorithm (PSA), a novel framework using Large Language Models (LLMs) for\nscalable, transparent content curation based on Public Service Media (PSM)\ninspired values. Utilizing a large multilingual news dataset from the 'A\nEuropean Perspective' project, our experiment directly compared article ratings\nfrom a panel of experienced editors from various European PSMs, with those from\nseveral LLMs, focusing on four criteria: diversity, in-depth analysis,\nforward-looking, and cross-border relevance. Utilizing criterion-specific\nprompts, our results indicate a promising alignment between human editorial\njudgment and LLM assessments, demonstrating the potential of LLMs to automate\nvalue-driven curation at scale without sacrificing transparency. This research\nconstitutes a first step towards a scalable framework for the automatic\ncuration of trustworthy news content.",
    "published": "2025-06-27T14:39:38Z",
    "updated": "2025-06-30T13:16:54Z",
    "id": "2506.22270v2",
    "authors": [
      "Ahmad Mel",
      "Sebastien Noir"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22270v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22270v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22270v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for content curation in news, focusing on transparency and scalability, which aligns with the 'LLM' topic. It also touches on the application of LLMs in a specific domain (news curation), which is not directly covered by other topics.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.22267v1": {
    "title": "Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph\n  is All You Need",
    "summary": "With generative artificial intelligence challenging computational scientific\ncomputing, data centers are experiencing unprecedented growth in both scale and\nvolume. As a result, computing efficiency has become more critical than ever.\nOperational Data Analytics (ODA) relies on the collection of data center\ntelemetry to improve efficiency, but so far has been focusing on real-time\ntelemetry data visualization and post-mortem analysis. However, with NoSQL\ndatabases now serving as the default storage backend to support scalability,\nquerying this data is challenging due to its schema-less nature, which requires\ndomain knowledge to traverse relationships between data sources. Ontologies and\nKnowledge Graphs (KGs) can capture these relationships, but traditional KGs are\ncostly to scale and have not been widely applied to multivariate timeseries.\nVirtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating\nquery-specific graphs at runtime. In this work, we present a full end-to-end\nODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL\nqueries, utilizing VKG for data retrieval. This approach achieves 92.5%\naccuracy compared to 25% with direct NoSQL queries. The proposed methodology\noptimizes VKG construction and LLM inference, cutting previous work average\nquery latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179\nMiB. This performance makes the tool suitable for deployment and real-time\ninteraction with ODA end-users.",
    "published": "2025-06-27T14:36:39Z",
    "updated": "2025-06-27T14:36:39Z",
    "id": "2506.22267v1",
    "authors": [
      "Junaid Ahmed Khan",
      "Hiari Pizzini Cavagna",
      "Andrea Proia",
      "Andrea Bartolini"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22267v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22267v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22267v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a Large Language Model (LLM) to generate SPARQL queries for a chatbot system, which aligns with the topic of LLM. The focus on query generation and data retrieval also touches upon the reasoning abilities of LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.22255v1": {
    "title": "Projected Compression: Trainable Projection for Efficient Transformer\n  Compression",
    "summary": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens.",
    "published": "2025-06-27T14:24:01Z",
    "updated": "2025-06-27T14:24:01Z",
    "id": "2506.22255v1",
    "authors": [
      "Maciej Stefaniak",
      "Micha Krutul",
      "Jan Maanicki",
      "Maciej Piro",
      "Jakub Krajewski",
      "Sebastian Jaszczur",
      "Marek Cygan",
      "Kamil Adamczewski",
      "Jan Ludziejewski"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22255v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22255v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22255v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a novel model compression technique for large language models, which is relevant to the topics of LLM (Large Language Models) and Scaling (as it addresses the issue of model size reduction and computational demands).",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.22231v1": {
    "title": "Adapting University Policies for Generative AI: Opportunities,\n  Challenges, and Policy Solutions in Higher Education",
    "summary": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity.",
    "published": "2025-06-27T13:49:02Z",
    "updated": "2025-06-27T13:49:02Z",
    "id": "2506.22231v1",
    "authors": [
      "Russell Beale"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22231v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22231v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22231v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact and integration of large language models (LLMs) in higher education, focusing on opportunities, challenges, and policy solutions. It directly mentions LLMs and their applications, making it relevant to the LLM topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.22200v3": {
    "title": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework",
    "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filter-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame not only enables\nfine-grained categorization of training samples for deeper insight into their\ncontributions, but also introduces an efficient and precise mechanism for\nentropy control, which is critical for balancing exploration and convergence in\nRL training. Our code is available at https://github.com/597358816/EFRame.",
    "published": "2025-06-27T13:09:05Z",
    "updated": "2025-07-07T11:27:02Z",
    "id": "2506.22200v3",
    "authors": [
      "Chen Wang",
      "Lai Wei",
      "Yanzhi Zhang",
      "Chenyang Shao",
      "Zedong Dan",
      "Weiran Huang",
      "Yue Wang",
      "Yuzhi Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22200v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22200v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22200v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the enhancement of reasoning capabilities in large language models (LLMs) through a reinforcement learning (RL) framework, specifically addressing issues like exploration, sample efficiency, and instability in RL training. It also mentions the application of this framework to reasoning benchmarks.",
    "llm_cls_result": [
      "RL",
      "Reasoning",
      "LLM"
    ]
  },
  "2506.22189v1": {
    "title": "Exploring Modularity of Agentic Systems for Drug Discovery",
    "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.",
    "published": "2025-06-27T12:57:00Z",
    "updated": "2025-06-27T12:57:00Z",
    "id": "2506.22189v1",
    "authors": [
      "Laura van Weesep",
      "Samuel Genheden",
      "Ola Engkvist",
      "Jens Sjlund"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22189v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22189v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22189v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large-language models (LLMs) in agentic systems for drug discovery, comparing different LLMs and their modularity. It focuses on the performance and interchangeability of LLMs in a specific application domain, which aligns with the topics of LLM and RL (as it involves agentic systems and tool-calling agents).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.22157v1": {
    "title": "Training Language Model to Critique for Better Refinement",
    "summary": "Large language models (LLMs) have demonstrated remarkable evaluation and\ncritique capabilities, providing insightful feedback and identifying flaws in\nvarious tasks. However, limited research has explored which types of critiques\nare most effective for improving model responses or how to generate such\ncritiques. To address this gap, we introduce \\textbf{R}efinement-oriented\n\\textbf{C}ritique \\textbf{O}ptimization (RCO), a novel framework designed to\ntrain critic models using refinement signals. RCO uses a feedback loop where\ncritiques, generated by the critic model, guide the actor model in refining its\nresponses. The critique utility (CU) quantifies the effectiveness of these\nrefinements, serving as the reward signal for training the critic model. By\nfocusing on critiques that lead to better refinements, RCO eliminates the need\nfor direct critique preference assessment, ensuring that critiques driving\nmeaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,\ndialog generation, summarization, question answering, mathematical reasoning,\nand code generation, and show that it significantly outperforms traditional\nmethods and open-source models in terms of critique quality and refinement\noutcomes. Our contributions include the introduction of RCO, a novel\nsupervision scheme based on refined response preferences, and comprehensive\nexperimental results that highlight the method's effectiveness in enhancing LLM\ncritique-refinement loops.",
    "published": "2025-06-27T12:10:57Z",
    "updated": "2025-06-27T12:10:57Z",
    "id": "2506.22157v1",
    "authors": [
      "Tianshu Yu",
      "Chao Xiang",
      "Mingchuan Yang",
      "Pei Ke",
      "Bosi Wen",
      "Cunxiang Wang",
      "Jiale Cheng",
      "Li Zhang",
      "Xinyu Mu",
      "Chuxiong Sun",
      "Minlie Huang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22157v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22157v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22157v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on training language models to critique and refine responses, which involves reinforcement learning and improving model responses through feedback loops. This aligns with the topics of Reinforcement Learning (RL) and Large Language Models (LLM).",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2506.22139v3": {
    "title": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for\n  Video-LLMs",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks.",
    "published": "2025-06-27T11:30:51Z",
    "updated": "2025-07-22T07:42:31Z",
    "id": "2506.22139v3",
    "authors": [
      "Shaojie Zhang",
      "Jiahui Yang",
      "Jianqin Yin",
      "Zhenbo Luo",
      "Jian Luan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22139v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22139v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22139v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a novel approach for adaptive frame selection and multi-resolution scaling in Video-LLMs, which are a type of Multimodal Large Language Models (MLLMs). The focus is on improving video comprehension by addressing challenges related to data volume and temporal complexity, which are key aspects of MLLM research.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Benchmark"
    ]
  },
  "2506.22068v1": {
    "title": "Query as Test: An Intelligent Driving Test and Data Storage Method for\n  Integrated Cockpit-Vehicle-Road Scenarios",
    "summary": "With the deep penetration of Artificial Intelligence (AI) in the\ntransportation sector, intelligent cockpits, autonomous driving, and\nintelligent road networks are developing at an unprecedented pace. However, the\ndata ecosystems of these three key areas are increasingly fragmented and\nincompatible. Especially, existing testing methods rely on data stacking, fail\nto cover all edge cases, and lack flexibility. To address this issue, this\npaper introduces the concept of \"Query as Test\" (QaT). This concept shifts the\nfocus from rigid, prescripted test cases to flexible, on-demand logical queries\nagainst a unified data representation. Specifically, we identify the need for a\nfundamental improvement in data storage and representation, leading to our\nproposal of \"Extensible Scenarios Notations\" (ESN). ESN is a novel declarative\ndata framework based on Answer Set Programming (ASP), which uniformly\nrepresents heterogeneous multimodal data from the cockpit, vehicle, and road as\na collection of logical facts and rules. This approach not only achieves deep\nsemantic fusion of data, but also brings three core advantages: (1) supports\ncomplex and flexible semantic querying through logical reasoning; (2) provides\nnatural interpretability for decision-making processes; (3) allows for\non-demand data abstraction through logical rules, enabling fine-grained privacy\nprotection. We further elaborate on the QaT paradigm, transforming the\nfunctional validation and safety compliance checks of autonomous driving\nsystems into logical queries against the ESN database, significantly enhancing\nthe expressiveness and formal rigor of the testing. Finally, we introduce the\nconcept of \"Validation-Driven Development\" (VDD), which suggests to guide\ndevelopments by logical validation rather than quantitative testing in the era\nof Large Language Models, in order to accelerating the iteration and\ndevelopment process.",
    "published": "2025-06-27T09:59:58Z",
    "updated": "2025-06-27T09:59:58Z",
    "id": "2506.22068v1",
    "authors": [
      "Shengyue Yao",
      "Runqing Guo",
      "Yangyang Qin",
      "Miangbing Meng",
      "Jipeng Cao",
      "Yilun Lin",
      "Yisheng Lv",
      "Fei-Yue Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22068v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22068v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22068v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of AI in transportation, focusing on intelligent cockpits, autonomous driving, and intelligent road networks. It introduces a novel data framework (ESN) based on Answer Set Programming (ASP) for representing heterogeneous multimodal data. While it mentions Large Language Models (LLMs) in the context of Validation-Driven Development (VDD), the core focus is on data representation and testing methods for autonomous systems, which does not directly align with the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.22050v1": {
    "title": "Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs",
    "summary": "This study explores Machine Translationese (MTese) -- the linguistic\npeculiarities of machine translation outputs -- focusing on the\nunder-researched English-to-Chinese language pair in news texts. We construct a\nlarge dataset consisting of 4 sub-corpora and employ a comprehensive five-layer\nfeature set. Then, a chi-square ranking algorithm is applied for feature\nselection in both classification and clustering tasks. Our findings confirm the\npresence of MTese in both Neural Machine Translation systems (NMTs) and Large\nLanguage Models (LLMs). Original Chinese texts are nearly perfectly\ndistinguishable from both LLM and NMT outputs. Notable linguistic patterns in\nMT outputs are shorter sentence lengths and increased use of adversative\nconjunctions. Comparing LLMs and NMTs, we achieve approximately 70%\nclassification accuracy, with LLMs exhibiting greater lexical diversity and\nNMTs using more brackets. Additionally, translation-specific LLMs show lower\nlexical diversity but higher usage of causal conjunctions compared to generic\nLLMs. Lastly, we find no significant differences between LLMs developed by\nChinese firms and their foreign counterparts.",
    "published": "2025-06-27T09:45:37Z",
    "updated": "2025-06-27T09:45:37Z",
    "id": "2506.22050v1",
    "authors": [
      "Delu Kong",
      "Lieve Macken"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22050v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22050v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22050v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on comparing Machine Translationese (MTese) in outputs from Neural Machine Translation systems (NMTs) and Large Language Models (LLMs), specifically in the English-to-Chinese language pair. It involves the use of LLMs for translation tasks and analyzes their linguistic patterns, which is relevant to the topic of Large Language Models (LLM).",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.22049v2": {
    "title": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling",
    "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the shortcut to dominate over sub-layer outputs in the residual\nconnection and limiting the learning capacity of deeper layers. To mitigate\nthis issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple\ntechnique that can be used in combination with existing approaches. GPAS works\nby scaling down the intermediate activations while keeping their gradients\nunchanged. This leaves information in the activations intact, and avoids the\ngradient vanishing problem associated with gradient downscaling. Extensive\nexperiments across various model sizes from 71M to 1B show that GPAS achieves\nconsistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also\nshows promise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings. Our code is available at\nhttps://github.com/dandingsky/GPAS.",
    "published": "2025-06-27T09:45:15Z",
    "updated": "2025-07-03T16:54:09Z",
    "id": "2506.22049v2",
    "authors": [
      "Tianhao Chen",
      "Xin Xu",
      "Zijing Liu",
      "Pengxiang Li",
      "Xinyuan Song",
      "Ajay Kumar Jaiswal",
      "Fan Zhang",
      "Jishan Hu",
      "Yang Wang",
      "Hao Chen",
      "Shizhe Diao",
      "Shiwei Liu",
      "Yu Li",
      "Lu Yin",
      "Can Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22049v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22049v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22049v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a technique (GPAS) to accelerate the convergence of LLM pretraining by addressing activation variance issues in Pre-LayerNorm Transformers, which is directly related to pretraining strategies and model scaling.",
    "llm_cls_result": [
      "Pretrain",
      "Scaling"
    ]
  },
  "2506.22038v1": {
    "title": "Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in\n  Children's Literature Translation",
    "summary": "This study focuses on evaluating the performance of machine translations\n(MTs) compared to human translations (HTs) in English-to-Chinese children's\nliterature translation (CLT) from a stylometric perspective. The research\nconstructs a Peter Pan corpus, comprising 21 translations: 7 human translations\n(HTs), 7 large language model translations (LLMs), and 7 neural machine\ntranslation outputs (NMTs). The analysis employs a generic feature set\n(including lexical, syntactic, readability, and n-gram features) and a creative\ntext translation (CTT-specific) feature set, which captures repetition, rhythm,\ntranslatability, and miscellaneous levels, yielding 447 linguistic features in\ntotal.\n  Using classification and clustering techniques in machine learning, we\nconduct a stylometric analysis of these translations. Results reveal that in\ngeneric features, HTs and MTs exhibit significant differences in conjunction\nword distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs\nshow significant variation in descriptive words usage and adverb ratios.\nRegarding CTT-specific features, LLMs outperform NMTs in distribution, aligning\nmore closely with HTs in stylistic characteristics, demonstrating the potential\nof LLMs in CLT.",
    "published": "2025-06-27T09:34:40Z",
    "updated": "2025-06-27T09:34:40Z",
    "id": "2506.22038v1",
    "authors": [
      "Delu Kong",
      "Lieve Macken"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22038v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22038v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22038v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper evaluates the performance of machine translations (MTs) compared to human translations (HTs) in children's literature translation (CLT) using stylometric analysis. It specifically compares large language model translations (LLMs) with neural machine translation outputs (NMTs) and human translations, focusing on linguistic features and stylistic characteristics. The study highlights the potential of LLMs in CLT, aligning with research on Large Language Models (LLM) and their applications in translation tasks.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.22028v1": {
    "title": "LMPVC and Policy Bank: Adaptive voice control for industrial robots with\n  code generating LLMs and reusable Pythonic policies",
    "summary": "Modern industry is increasingly moving away from mass manufacturing, towards\nmore specialized and personalized products. As manufacturing tasks become more\ncomplex, full automation is not always an option, human involvement may be\nrequired. This has increased the need for advanced human robot collaboration\n(HRC), and with it, improved methods for interaction, such as voice control.\nRecent advances in natural language processing, driven by artificial\nintelligence (AI), have the potential to answer this demand. Large language\nmodels (LLMs) have rapidly developed very impressive general reasoning\ncapabilities, and many methods of applying this to robotics have been proposed,\nincluding through the use of code generation. This paper presents Language\nModel Program Voice Control (LMPVC), an LLM-based prototype voice control\narchitecture with integrated policy programming and teaching capabilities,\nbuilt for use with Robot Operating System 2 (ROS2) compatible robots. The\narchitecture builds on prior works using code generation for voice control by\nimplementing an additional programming and teaching system, the Policy Bank. We\nfind this system can compensate for the limitations of the underlying LLM, and\nallow LMPVC to adapt to different downstream tasks without a slow and costly\ntraining process. The architecture and additional results are released on\nGitHub (https://github.com/ozzyuni/LMPVC).",
    "published": "2025-06-27T09:14:14Z",
    "updated": "2025-06-27T09:14:14Z",
    "id": "2506.22028v1",
    "authors": [
      "Ossi Parikka",
      "Roel Pieters"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22028v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22028v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22028v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for voice control in robotics, specifically through code generation and policy programming. This aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning) due to the application of LLMs in robotics and potential reinforcement learning aspects in policy programming.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.22005v1": {
    "title": "LeanConjecturer: Automatic Generation of Mathematical Conjectures for\n  Theorem Proving",
    "summary": "We introduce LeanConjecturer, a pipeline for automatically generating\nuniversity-level mathematical conjectures in Lean 4 using Large Language Models\n(LLMs). Our hybrid approach combines rule-based context extraction with\nLLM-based theorem statement generation, addressing the data scarcity challenge\nin formal theorem proving. Through iterative generation and evaluation,\nLeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with\n3,776 identified as syntactically valid and non-trivial, that is, cannot be\nproven by \\texttt{aesop} tactic. We demonstrate the utility of these generated\nconjectures for reinforcement learning through Group Relative Policy\nOptimization (GRPO), showing that targeted training on domain-specific\nconjectures can enhance theorem proving capabilities. Our approach generates\n103.25 novel conjectures per seed file on average, providing a scalable\nsolution for creating training data for theorem proving systems. Our system\nsuccessfully verified several non-trivial theorems in topology, including\nproperties of semi-open, alpha-open, and pre-open sets, demonstrating its\npotential for mathematical discovery beyond simple variations of existing\nresults.",
    "published": "2025-06-27T08:17:18Z",
    "updated": "2025-06-27T08:17:18Z",
    "id": "2506.22005v1",
    "authors": [
      "Naoto Onda",
      "Kazumi Kasaura",
      "Yuta Oriike",
      "Masaya Taniguchi",
      "Akiyoshi Sannai",
      "Sho Sonoda"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22005v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22005v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22005v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating mathematical conjectures and their application in theorem proving, which involves reasoning and reinforcement learning (RL) for enhancing theorem proving capabilities.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Reasoning"
    ]
  },
  "2506.21980v3": {
    "title": "R1-Track: Direct Application of MLLMs to Visual Object Tracking via\n  Reinforcement Learning",
    "summary": "Visual single object tracking aims to continuously localize and estimate the\nscale of a target in subsequent video frames, given only its initial state in\nthe first frame. This task has traditionally been framed as a template matching\nproblem, evolving through major phases including correlation filters,\ntwo-stream networks, and one-stream networks with significant progress\nachieved. However, these methods typically require explicit classification and\nregression modeling, depend on supervised training with large-scale datasets,\nand are limited to the single task of tracking, lacking flexibility. In recent\nyears, multi-modal large language models (MLLMs) have advanced rapidly.\nOpen-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational\ncapabilities, demonstrate excellent performance in grounding tasks. This has\nspurred interest in applying such models directly to visual tracking. However,\nexperiments reveal that Qwen2.5-VL struggles with template matching between\nimage pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned\nQwen2.5-VL using the group relative policy optimization (GRPO) reinforcement\nlearning method on a small-scale dataset with a rule-based reward function. The\nresulting model, R1-Track, achieved notable performance on the GOT-10k\nbenchmark. R1-Track supports flexible initialization via bounding boxes or text\ndescriptions while retaining most of the original model's general capabilities.\nAnd we further discuss potential improvements for R1-Track. This rough\ntechnical report summarizes our findings as of May 2025.",
    "published": "2025-06-27T07:41:15Z",
    "updated": "2025-07-22T15:39:40Z",
    "id": "2506.21980v3",
    "authors": [
      "Biao Wang",
      "Wenwen Li",
      "Jiawei Ge"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21980v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21980v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21980v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of Multi-modal Large Language Models (MLLMs) to visual object tracking using Reinforcement Learning (RL), specifically mentioning the use of Qwen2.5-VL and the GRPO method. It also involves benchmarking on the GOT-10k dataset.",
    "llm_cls_result": [
      "MLLM",
      "RL",
      "Benchmark"
    ]
  },
  "2506.21974v1": {
    "title": "Don't Trust Generative Agents to Mimic Communication on Social Networks\n  Unless You Benchmarked their Empirical Realism",
    "summary": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation.",
    "published": "2025-06-27T07:32:16Z",
    "updated": "2025-06-27T07:32:16Z",
    "id": "2506.21974v1",
    "authors": [
      "Simon Mnker",
      "Nils Schwager",
      "Achim Rettinger"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21974v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21974v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21974v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to mimic human behavior in social networks, which aligns with the 'LLM' topic. It also emphasizes the need for benchmarking the empirical realism of these models, which relates to the 'Benchmark' topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.21972v1": {
    "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM\n  Vulnerabilities and Bypassing Modern Defenses",
    "summary": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries.",
    "published": "2025-06-27T07:26:33Z",
    "updated": "2025-06-27T07:26:33Z",
    "id": "2506.21972v1",
    "authors": [
      "Mohamed Ahmed",
      "Mohamed Abdelmouty",
      "Mingyu Kim",
      "Gunvanth Kandula",
      "Alex Park",
      "James C. Davis"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21972v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21972v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21972v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses vulnerabilities and attack strategies for Large Language Models (LLMs), focusing on jailbreak techniques and safety measures. It directly relates to LLM research and security aspects.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.21964v1": {
    "title": "Using Large Language Models to Suggest Informative Prior Distributions\n  in Bayesian Statistics",
    "summary": "Selecting prior distributions in Bayesian statistics is challenging,\nresource-intensive, and subjective. We analyze using large-language models\n(LLMs) to suggest suitable, knowledge-based informative priors. We developed an\nextensive prompt asking LLMs not only to suggest priors but also to verify and\nreflect on their choices.\n  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real\ndatasets: heart disease risk and concrete strength. All LLMs correctly\nidentified the direction for all associations (e.g., that heart disease risk is\nhigher for males). The quality of suggested priors was measured by their\nKullback-Leibler divergence from the maximum likelihood estimator's\ndistribution.\n  The LLMs suggested both moderately and weakly informative priors. The\nmoderate priors were often overconfident, resulting in distributions misaligned\nwith the data. In our experiments, Claude and Gemini provided better priors\nthan ChatGPT. For weakly informative priors, a key performance difference\nemerged: ChatGPT and Gemini defaulted to an \"unnecessarily vague\" mean of 0,\nwhile Claude did not, demonstrating a significant advantage.\n  The ability of LLMs to identify correct associations shows their great\npotential as an efficient, objective method for developing informative priors.\nHowever, the primary challenge remains in calibrating the width of these priors\nto avoid over- and under-confidence.",
    "published": "2025-06-27T07:11:55Z",
    "updated": "2025-06-27T07:11:55Z",
    "id": "2506.21964v1",
    "authors": [
      "Michael A. Riegler",
      "Kristoffer Herland Hellton",
      "Vajira Thambawita",
      "Hugo L. Hammer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21964v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21964v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21964v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) to suggest prior distributions in Bayesian statistics, focusing on their application and evaluation in this specific context. The core topic is the use of LLMs, which aligns with the 'LLM' category. Additionally, the paper involves evaluating the performance of different LLMs, which could also fit under 'Benchmark' as it compares their effectiveness in a specific task.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.21961v1": {
    "title": "PapersPlease: A Benchmark for Evaluating Motivational Values of Large\n  Language Models Based on ERG Theory",
    "summary": "Evaluating the performance and biases of large language models (LLMs) through\nrole-playing scenarios is becoming increasingly common, as LLMs often exhibit\nbiased behaviors in these contexts. Building on this line of research, we\nintroduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed\nto investigate LLMs' decision-making in prioritizing various levels of human\nneeds. In our setup, LLMs act as immigration inspectors deciding whether to\napprove or deny entry based on the short narratives of people. These narratives\nare constructed using the Existence, Relatedness, and Growth (ERG) theory,\nwhich categorizes human needs into three hierarchical levels. Our analysis of\nsix LLMs reveals statistically significant patterns in decision-making,\nsuggesting that LLMs encode implicit preferences. Additionally, our evaluation\nof the impact of incorporating social identities into the narratives shows\nvarying responsiveness based on both motivational needs and identity cues, with\nsome models exhibiting higher denial rates for marginalized identities. All\ndata is publicly available at https://github.com/yeonsuuuu28/papers-please.",
    "published": "2025-06-27T07:09:11Z",
    "updated": "2025-06-27T07:09:11Z",
    "id": "2506.21961v1",
    "authors": [
      "Junho Myung",
      "Yeon Su Park",
      "Sunwoo Kim",
      "Shin Yoo",
      "Alice Oh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21961v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21961v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21961v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark for evaluating LLMs' decision-making in moral dilemmas, focusing on their motivational values and biases. It involves role-playing scenarios and uses the ERG theory to categorize human needs, which aligns with benchmarking LLMs and evaluating their performance and biases.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2506.21934v1": {
    "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware\n  Layout Design",
    "summary": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation.",
    "published": "2025-06-27T06:09:56Z",
    "updated": "2025-06-27T06:09:56Z",
    "id": "2506.21934v1",
    "authors": [
      "Najmeh Forouzandehmehr",
      "Reza Yousefi Maragheh",
      "Sriram Kollipara",
      "Kai Zhao",
      "Topojoy Biswas",
      "Evren Korpeoglu",
      "Kannan Achan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21934v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21934v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21934v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) and retrieval-augmented generation for layout design, which involves multimodal retrieval and collaborative agentic reasoning. It also mentions the use of a vision-language grader agent, indicating a multimodal approach.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "MLLM"
    ]
  },
  "2506.21931v1": {
    "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized\n  Recommendation",
    "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.",
    "published": "2025-06-27T05:45:59Z",
    "updated": "2025-06-27T05:45:59Z",
    "id": "2506.21931v1",
    "authors": [
      "Reza Yousefi Maragheh",
      "Pratheek Vadla",
      "Priyank Gupta",
      "Kai Zhao",
      "Aysenur Inan",
      "Kehui Yao",
      "Jianpeng Xu",
      "Praveen Kanumala",
      "Jason Cho",
      "Sushant Kumar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21931v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21931v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21931v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses an Agentic Retrieval-Augmented Generation framework (ARAG) that integrates multi-agent collaboration into the RAG pipeline for personalized recommendation, leveraging LLM-based agents for user understanding, semantic alignment, and item ranking. This aligns with topics related to LLM (Large Language Models), RL (Reinforcement Learning with potential agentic reasoning), and Memory (retrieval-augmented generation and long-term user behavior understanding).",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Memory"
    ]
  },
  "2506.21901v1": {
    "title": "A Survey of LLM Inference Systems",
    "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.",
    "published": "2025-06-27T04:38:20Z",
    "updated": "2025-06-27T04:38:20Z",
    "id": "2506.21901v1",
    "authors": [
      "James Pan",
      "Guoliang Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21901v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21901v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21901v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the inference systems for large language models (LLMs), discussing various techniques and optimizations specific to LLM inference. This directly relates to the 'LLM' topic, which covers research on large language models, architectures, and scaling laws. Additionally, the discussion on system design and optimization techniques could also be relevant to 'Scaling', as it involves performance and efficiency considerations in LLM deployment.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.21898v2": {
    "title": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language\n  Models",
    "summary": "Large language models (LLMs) are becoming increasingly ubiquitous in our\ndaily lives, but numerous concerns about bias in LLMs exist. This study\nexamines how gender-diverse populations perceive bias, accuracy, and\ntrustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews\nwith non-binary/transgender, male, and female participants, we investigate how\ngendered and neutral prompts influence model responses and how users evaluate\nthese responses. Our findings reveal that gendered prompts elicit more\nidentity-specific responses, with non-binary participants particularly\nsusceptible to condescending and stereotypical portrayals. Perceived accuracy\nwas consistent across gender groups, with errors most noted in technical topics\nand creative tasks. Trustworthiness varied by gender, with men showing higher\ntrust, especially in performance, and non-binary participants demonstrating\nhigher performance-based trust. Additionally, participants suggested improving\nthe LLMs by diversifying training data, ensuring equal depth in gendered\nresponses, and incorporating clarifying questions. This research contributes to\nthe CSCW/HCI field by highlighting the need for gender-diverse perspectives in\nLLM development in particular and AI in general, to foster more inclusive and\ntrustworthy systems.",
    "published": "2025-06-27T04:35:52Z",
    "updated": "2025-07-08T17:26:59Z",
    "id": "2506.21898v2",
    "authors": [
      "Aimen Gaba",
      "Emily Wall",
      "Tejas Ramkumar Babu",
      "Yuriy Brun",
      "Kyle Hall",
      "Cindy Xiong Bearfield"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21898v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21898v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21898v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the evaluation of bias, accuracy, and trustworthiness in Large Language Models (LLMs) from a gender-diverse perspective, which directly relates to the study of LLMs and their societal impacts.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.21895v1": {
    "title": "Exploring Task-Solving Paradigm for Generalized Cross-Domain Face\n  Anti-Spoofing via Reinforcement Fine-Tuning",
    "summary": "Recently the emergence of novel presentation attacks has drawn increasing\nattention to face anti-spoofing. However, existing methods tend to memorize\ndata patterns from the training set, resulting in poor generalization to\nunknown attack types across different scenarios and limited interpretability.\nTo address these challenges, this paper presents a reinforcement\nfine-tuning-based face anti-spoofing method that stimulates the capabilities of\nmultimodal large language models to think and learn how to solve the\nanti-spoofing task itself, rather than relying on the memorization of\nauthenticity patterns. We design verifiable class consistent reward and\nreasoning consistent reward, and employ a GRPO-based optimization strategy to\nguide the model in exploring reasoning policies from multiple perspectives to\nmaximize expected rewards. As a result, through iterative trial-and-error\nlearning while retaining only high-reward trajectories, the model distills\nhighly generalizable decision-making rules from the extensive solution space to\neffectively address cross-domain face anti-spoofing tasks. Extensive\nexperimental results demonstrate that our method achieves state-of-the-art\ncross-domain generalization performance. It generalizes well to diverse unknown\nattack types in unseen target domains while providing interpretable reasoning\nfor its authenticity decisions without requiring labor-intensive textual\nannotations for training.",
    "published": "2025-06-27T04:28:29Z",
    "updated": "2025-06-27T04:28:29Z",
    "id": "2506.21895v1",
    "authors": [
      "Fangling Jiang",
      "Qi Li",
      "Weining Wang",
      "Gang Wang",
      "Bing Liu",
      "Zhenan Sun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21895v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21895v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21895v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of reinforcement fine-tuning with multimodal large language models (MLLM) to improve face anti-spoofing tasks, which involves reasoning and learning policies. The key topics are MLLM for multimodal integration, RL for reinforcement learning, and Reasoning for the reasoning abilities in LLMs.",
    "llm_cls_result": [
      "MLLM",
      "RL",
      "Reasoning"
    ]
  },
  "2506.21885v1": {
    "title": "Integrating Multi-Modal Sensors: A Review of Fusion Techniques for\n  Intelligent Vehicles",
    "summary": "Multi-sensor fusion plays a critical role in enhancing perception for\nautonomous driving, overcoming individual sensor limitations, and enabling\ncomprehensive environmental understanding. This paper first formalizes\nmulti-sensor fusion strategies into data-level, feature-level, and\ndecision-level categories and then provides a systematic review of deep\nlearning-based methods corresponding to each strategy. We present key\nmulti-modal datasets and discuss their applicability in addressing real-world\nchallenges, particularly in adverse weather conditions and complex urban\nenvironments. Additionally, we explore emerging trends, including the\nintegration of Vision-Language Models (VLMs), Large Language Models (LLMs), and\nthe role of sensor fusion in end-to-end autonomous driving, highlighting its\npotential to enhance system adaptability and robustness. Our work offers\nvaluable insights into current methods and future directions for multi-sensor\nfusion in autonomous driving.",
    "published": "2025-06-27T03:43:48Z",
    "updated": "2025-06-27T03:43:48Z",
    "id": "2506.21885v1",
    "authors": [
      "Chuheng Wei",
      "Ziye Qin",
      "Ziyan Zhang",
      "Guoyuan Wu",
      "Matthew J. Barth"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21885v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21885v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21885v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses multi-sensor fusion techniques in autonomous driving, including the integration of Vision-Language Models (VLMs) and Large Language Models (LLMs), which aligns with the topics of VLA (Vision-Language Action) and LLM (Large Language Models).",
    "llm_cls_result": [
      "VLA",
      "LLM"
    ]
  },
  "2506.21881v1": {
    "title": "A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs",
    "summary": "As large language models (LLMs) are increasingly deployed across diverse\nlinguistic and cultural contexts, understanding their behavior in both factual\nand disputable scenarios is essential, especially when their outputs may shape\npublic opinion or reinforce dominant narratives. In this paper, we define two\ntypes of bias in LLMs: model bias (bias stemming from model training) and\ninference bias (bias induced by the language of the query), through a two-phase\nevaluation. Phase 1 evaluates LLMs on factual questions where a single\nverifiable answer exists, assessing whether models maintain consistency across\ndifferent query languages. Phase 2 expands the scope by probing geopolitically\nsensitive disputes, where responses may reflect culturally embedded or\nideologically aligned perspectives. We construct a manually curated dataset\nspanning both factual and disputable QA, across four languages and question\ntypes. The results show that Phase 1 exhibits query language induced alignment,\nwhile Phase 2 reflects an interplay between the model's training context and\nquery language. This paper offers a structured framework for evaluating LLM\nbehavior across neutral and sensitive topics, providing insights for future LLM\ndeployment and culturally aware evaluation practices in multilingual contexts.",
    "published": "2025-06-27T03:37:15Z",
    "updated": "2025-06-27T03:37:15Z",
    "id": "2506.21881v1",
    "authors": [
      "Sean Kim",
      "Hyuhng Joon Kim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21881v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21881v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21881v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating biases in Large Language Models (LLMs) across different linguistic and cultural contexts, which is relevant to the study of LLMs and their behavior in multilingual and multicultural settings.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Dataset"
    ]
  },
  "2506.21875v1": {
    "title": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation",
    "summary": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation.",
    "published": "2025-06-27T03:18:45Z",
    "updated": "2025-06-27T03:18:45Z",
    "id": "2506.21875v1",
    "authors": [
      "Jian Zhang",
      "Linhao Zhang",
      "Bokai Lei",
      "Chuhan Wu",
      "Wei Jia",
      "Xiao Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21875v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21875v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21875v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking audio LLMs in natural speech conversation, which involves evaluating multi-modal LLMs in speech scenarios. This aligns with the topics of Benchmark (evaluating LLMs), MLLM (multi-modal LLMs), and Dataset (curating real-world chat data for evaluation).",
    "llm_cls_result": [
      "Benchmark",
      "MLLM",
      "Dataset"
    ]
  },
  "2506.21873v1": {
    "title": "Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops\n  in Visual Grounding Caused by Pruning",
    "summary": "Recent Multimodal Large Language Models (MLLMs) have demonstrated strong\nperformance in visual grounding, establishing themselves as a general interface\nfor various vision-language applications. This progress has driven the\ndevelopment of token pruning methods to mitigate the high computational costs\nassociated with processing numerous visual tokens. However, we observe that\npruning significantly weakens the model's grounding ability, leading to\nincorrect predictions and drastic performance degradation. In Referring\nExpression Comprehension (REC), for instance, pruning causes the accuracy of\nLLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis\nidentifies misaligned position IDs after pruning as the primary cause of this\ndegradation, as both the order and value of these IDs are crucial for\nmaintaining performance in grounding tasks. To address this issue, we propose\nGrounding-Aware Token Pruning (GAP), a simple yet effective adjustment to\nposition IDs that recovers REC accuracy back to 51.42%, which is 90% of the\noriginal performance in the without pruning setting, all while requiring no\nadditional training, memory, or computational overhead. Applied to models such\nas Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves\nperformance across various token pruning strategies.",
    "published": "2025-06-27T03:11:22Z",
    "updated": "2025-06-27T03:11:22Z",
    "id": "2506.21873v1",
    "authors": [
      "Tzu-Chun Chien",
      "Chieh-Kai Lin",
      "Shiang-Feng Tsai",
      "Ruei-Chi Lai",
      "Hung-Jen Chen",
      "Min Sun"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21873v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21873v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21873v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the performance of Multimodal Large Language Models (MLLMs) in visual grounding tasks by addressing issues caused by token pruning. It specifically discusses the impact of pruning on grounding ability and proposes a method to recover performance, which aligns with the topics of MLLM and VLA.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2506.21865v1": {
    "title": "RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River\n  Culture",
    "summary": "The Yellow River is China's mother river and a cradle of human civilization.\nThe ancient Yellow River culture is, moreover, an indispensable part of human\nart history. To conserve and inherit the ancient Yellow River culture, we\ndesigned RiverEcho, a real-time interactive system that responds to voice\nqueries using a large language model and a cultural knowledge dataset,\ndelivering explanations through a talking-head digital human. Specifically, we\nbuilt a knowledge database focused on the ancient Yellow River culture,\nincluding the collection of historical texts and the processing pipeline.\nExperimental results demonstrate that leveraging Retrieval-Augmented Generation\n(RAG) on the proposed dataset enhances the response quality of the Large\nLanguage Model(LLM), enabling the system to generate more professional and\ninformative responses. Our work not only diversifies the means of promoting\nYellow River culture but also provides users with deeper cultural insights.",
    "published": "2025-06-27T02:40:00Z",
    "updated": "2025-06-27T02:40:00Z",
    "id": "2506.21865v1",
    "authors": [
      "Haofeng Wang",
      "Yilin Guo",
      "Zehao Li",
      "Tong Yue",
      "Yizong Wang",
      "Enci Zhang",
      "Rongqun Lin",
      "Feng Gao",
      "Shiqi Wang",
      "Siwei Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21865v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21865v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21865v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of a large language model (LLM) and retrieval-augmented generation (RAG) to create an interactive system for cultural knowledge dissemination. This aligns with topics related to LLM and Memory (due to the use of RAG).",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.21862v1": {
    "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
    "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
    "published": "2025-06-27T02:29:58Z",
    "updated": "2025-06-27T02:29:58Z",
    "id": "2506.21862v1",
    "authors": [
      "Boyuan Sun",
      "Jiaxing Zhao",
      "Xihan Wei",
      "Qibin Hou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21862v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21862v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21862v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on token compression for video multimodal large language models, which involves both spatial and temporal domains. It specifically mentions the use of Semantic Connected Components (SCC) for comprehensive semantic coverage and evaluates the method on video understanding benchmarks.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Benchmark"
    ]
  },
  "2506.21853v2": {
    "title": "Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via\n  Waypoint Interface",
    "summary": "Quadrupedal robots have demonstrated exceptional locomotion capabilities\nthrough Reinforcement Learning (RL), including extreme parkour maneuvers.\nHowever, integrating locomotion skills with navigation in quadrupedal robots\nhas not been fully investigated, which holds promise for enhancing\nlong-distance movement capabilities. In this paper, we propose Skill-Nav, a\nmethod that incorporates quadrupedal locomotion skills into a hierarchical\nnavigation framework using waypoints as an interface. Specifically, we train a\nwaypoint-guided locomotion policy using deep RL, enabling the robot to\nautonomously adjust its locomotion skills to reach targeted positions while\navoiding obstacles. Compared with direct velocity commands, waypoints offer a\nsimpler yet more flexible interface for high-level planning and low-level\ncontrol. Utilizing waypoints as the interface allows for the application of\nvarious general planning tools, such as large language models (LLMs) and path\nplanning algorithms, to guide our locomotion policy in traversing terrains with\ndiverse obstacles. Extensive experiments conducted in both simulated and\nreal-world scenarios demonstrate that Skill-Nav can effectively traverse\ncomplex terrains and complete challenging navigation tasks.",
    "published": "2025-06-27T02:08:40Z",
    "updated": "2025-06-30T08:59:34Z",
    "id": "2506.21853v2",
    "authors": [
      "Dewei Wang",
      "Chenjia Bai",
      "Chenhui Li",
      "Jiyuan Shi",
      "Yan Ding",
      "Chi Zhang",
      "Bin Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21853v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21853v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21853v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Reinforcement Learning (RL) with navigation in quadrupedal robots, and mentions the use of large language models (LLMs) for planning, which aligns with the topics of RL and LLM.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2506.21849v1": {
    "title": "The Consistency Hypothesis in Uncertainty Quantification for Large\n  Language Models",
    "summary": "Estimating the confidence of large language model (LLM) outputs is essential\nfor real-world applications requiring high user trust. Black-box uncertainty\nquantification (UQ) methods, relying solely on model API access, have gained\npopularity due to their practical benefits. In this paper, we examine the\nimplicit assumption behind several UQ methods, which use generation consistency\nas a proxy for confidence, an idea we formalize as the consistency hypothesis.\nWe introduce three mathematical statements with corresponding statistical tests\nto capture variations of this hypothesis and metrics to evaluate LLM output\nconformity across tasks. Our empirical investigation, spanning 8 benchmark\ndatasets and 3 tasks (question answering, text summarization, and text-to-SQL),\nhighlights the prevalence of the hypothesis under different settings. Among the\nstatements, we highlight the `Sim-Any' hypothesis as the most actionable, and\ndemonstrate how it can be leveraged by proposing data-free black-box UQ methods\nthat aggregate similarities between generations for confidence estimation.\nThese approaches can outperform the closest baselines, showcasing the practical\nvalue of the empirically observed consistency hypothesis.",
    "published": "2025-06-27T01:53:15Z",
    "updated": "2025-06-27T01:53:15Z",
    "id": "2506.21849v1",
    "authors": [
      "Quan Xiao",
      "Debarun Bhattacharjya",
      "Balaji Ganesan",
      "Radu Marinescu",
      "Katsiaryna Mirylenka",
      "Nhan H Pham",
      "Michael Glass",
      "Junkyu Lee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21849v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21849v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21849v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on uncertainty quantification (UQ) methods for Large Language Models (LLMs), specifically examining the consistency hypothesis as a proxy for confidence in LLM outputs. It involves empirical investigation across benchmark datasets and tasks, which aligns with the topics of LLM research and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.21832v1": {
    "title": "TaleForge: Interactive Multimodal System for Personalized Story Creation",
    "summary": "Storytelling is a deeply personal and creative process, yet existing methods\noften treat users as passive consumers, offering generic plots with limited\npersonalization. This undermines engagement and immersion, especially where\nindividual style or appearance is crucial. We introduce TaleForge, a\npersonalized story-generation system that integrates large language models\n(LLMs) and text-to-image diffusion to embed users' facial images within both\nnarratives and illustrations. TaleForge features three interconnected modules:\nStory Generation, where LLMs create narratives and character descriptions from\nuser prompts; Personalized Image Generation, merging users' faces and outfit\nchoices into character illustrations; and Background Generation, creating scene\nbackdrops that incorporate personalized characters. A user study demonstrated\nheightened engagement and ownership when individuals appeared as protagonists.\nParticipants praised the system's real-time previews and intuitive controls,\nthough they requested finer narrative editing tools. TaleForge advances\nmultimodal storytelling by aligning personalized text and imagery to create\nimmersive, user-centric experiences.",
    "published": "2025-06-27T00:45:38Z",
    "updated": "2025-06-27T00:45:38Z",
    "id": "2506.21832v1",
    "authors": [
      "Minh-Loi Nguyen",
      "Quang-Khai Le",
      "Tam V. Nguyen",
      "Minh-Triet Tran",
      "Trung-Nghia Le"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21832v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21832v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21832v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces TaleForge, a system that integrates large language models (LLMs) and text-to-image diffusion for personalized story generation, which involves multimodal integration and personalization in storytelling.",
    "llm_cls_result": [
      "MLLM",
      "LLM"
    ]
  },
  "2506.21825v1": {
    "title": "Exploring the change in scientific readability following the release of\n  ChatGPT",
    "summary": "The rise and growing popularity of accessible large language models have\nraised questions about their impact on various aspects of life, including how\nscientists write and publish their research. The primary objective of this\npaper is to analyze a dataset consisting of all abstracts posted on arXiv.org\nbetween 2010 and June 7th, 2024, to assess the evolution of their readability\nand determine whether significant shifts occurred following the release of\nChatGPT in November 2022. Four standard readability formulas are used to\ncalculate individual readability scores for each paper, classifying their level\nof readability. These scores are then aggregated by year and across the eight\nprimary categories covered by the platform. The results show a steady annual\ndecrease in readability, suggesting that abstracts are likely becoming\nincreasingly complex. Additionally, following the release of ChatGPT, a\nsignificant change in readability is observed for 2023 and the analyzed months\nof 2024. Similar trends are found across categories, with most experiencing a\nnotable change in readability during 2023 and 2024. These findings offer\ninsights into the broader changes in readability and point to the likely\ninfluence of AI on scientific writing.",
    "published": "2025-06-26T23:57:12Z",
    "updated": "2025-06-26T23:57:12Z",
    "id": "2506.21825v1",
    "authors": [
      "Abdulkareem Alsudais"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21825v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21825v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21825v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on analyzing the impact of ChatGPT on the readability of scientific abstracts, which involves the use of large language models (LLMs) but does not directly contribute to the core topics listed.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.02937v1": {
    "title": "FoGE: Fock Space inspired encoding for graph prompting",
    "summary": "Recent results show that modern Large Language Models (LLM) are indeed\ncapable of understanding and answering questions about structured data such as\ngraphs. This new paradigm can lead to solutions that require less supervision\nwhile, at the same time, providing a model that can generalize and answer\nquestions beyond the training labels. Existing proposals often use some\ndescription of the graph to create an ``augmented'' prompt fed to the LLM. For\na chosen class of graphs, if a well-tailored graph encoder is deployed to play\ntogether with a pre-trained LLM, the model can answer graph-related questions\nwell. Existing solutions to graph-based prompts range from graph serialization\nto graph transformers. In this work, we show that the use of a parameter-free\ngraph encoder based on Fock space representations, a concept borrowed from\nmathematical physics, is remarkably versatile in this problem setting. The\nsimple construction, inherited directly from the theory with a few small\nadjustments, can provide rich and informative graph encodings, for a wide range\nof different graphs. We investigate the use of this idea for prefix-tuned\nprompts leveraging the capabilities of a pre-trained, frozen LLM. The\nmodifications lead to a model that can answer graph-related questions -- from\nsimple graphs to proteins to hypergraphs -- effectively and with minimal, if\nany, adjustments to the architecture. Our work significantly simplifies\nexisting solutions and generalizes well to multiple different graph-based\nstructures effortlessly.",
    "published": "2025-06-26T23:48:03Z",
    "updated": "2025-06-26T23:48:03Z",
    "id": "2507.02937v1",
    "authors": [
      "Sotirios Panagiotis Chytas",
      "Rudrasis Chakraborty",
      "Vikas Singh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02937v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02937v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02937v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for understanding and answering questions about structured data such as graphs, which involves graph encoding and prompting strategies. The focus is on leveraging pre-trained LLMs for graph-related tasks, which aligns with the topics of LLM and Reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.21817v1": {
    "title": "Exploring the Structure of AI-Induced Language Change in Scientific\n  English",
    "summary": "Scientific English has undergone rapid and unprecedented changes in recent\nyears, with words such as \"delve,\" \"intricate,\" and \"crucial\" showing\nsignificant spikes in frequency since around 2022. These changes are widely\nattributed to the growing influence of Large Language Models like ChatGPT in\nthe discourse surrounding bias and misalignment. However, apart from changes in\nfrequency, the exact structure of these linguistic shifts has remained unclear.\nThe present study addresses this and investigates whether these changes involve\nthe replacement of synonyms by suddenly 'spiking words,' for example, \"crucial\"\nreplacing \"essential\" and \"key,\" or whether they reflect broader semantic and\npragmatic qualifications. To further investigate structural changes, we include\npart of speech tagging in our analysis to quantify linguistic shifts over\ngrammatical categories and differentiate between word forms, like \"potential\"\nas a noun vs. as an adjective. We systematically analyze synonym groups for\nwidely discussed 'spiking words' based on frequency trends in scientific\nabstracts from PubMed. We find that entire semantic clusters often shift\ntogether, with most or all words in a group increasing in usage. This pattern\nsuggests that changes induced by Large Language Models are primarily semantic\nand pragmatic rather than purely lexical. Notably, the adjective \"important\"\nshows a significant decline, which prompted us to systematically analyze\ndecreasing lexical items. Our analysis of \"collapsing\" words reveals a more\ncomplex picture, which is consistent with organic language change and contrasts\nwith the patterns of the abrupt spikes. These insights into the structure of\nlanguage change contribute to our understanding of how language technology\ncontinues to shape human language.",
    "published": "2025-06-26T23:44:24Z",
    "updated": "2025-06-26T23:44:24Z",
    "id": "2506.21817v1",
    "authors": [
      "Riley Galpin",
      "Bryce Anderson",
      "Tom S. Juzek"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21817v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21817v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21817v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the influence of Large Language Models (LLMs) like ChatGPT on the structure of language change in scientific English, focusing on semantic and pragmatic shifts rather than purely lexical changes. This aligns with research on LLMs and their impact on language.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.21812v1": {
    "title": "Towards Transparent AI: A Survey on Explainable Large Language Models",
    "summary": "Large Language Models (LLMs) have played a pivotal role in advancing\nArtificial Intelligence (AI). However, despite their achievements, LLMs often\nstruggle to explain their decision-making processes, making them a 'black box'\nand presenting a substantial challenge to explainability. This lack of\ntransparency poses a significant obstacle to the adoption of LLMs in\nhigh-stakes domain applications, where interpretability is particularly\nessential. To overcome these limitations, researchers have developed various\nexplainable artificial intelligence (XAI) methods that provide\nhuman-interpretable explanations for LLMs. However, a systematic understanding\nof these methods remains limited. To address this gap, this survey provides a\ncomprehensive review of explainability techniques by categorizing XAI methods\nbased on the underlying transformer architectures of LLMs: encoder-only,\ndecoder-only, and encoder-decoder models. Then these techniques are examined in\nterms of their evaluation for assessing explainability, and the survey further\nexplores how these explanations are leveraged in practical applications.\nFinally, it discusses available resources, ongoing research challenges, and\nfuture directions, aiming to guide continued efforts toward developing\ntransparent and responsible LLMs.",
    "published": "2025-06-26T23:25:22Z",
    "updated": "2025-06-26T23:25:22Z",
    "id": "2506.21812v1",
    "authors": [
      "Avash Palikhe",
      "Zhenyu Yu",
      "Zichong Wang",
      "Wenbin Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21812v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21812v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21812v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on explainability techniques for Large Language Models (LLMs), which directly relates to research on LLMs and their architectures, as well as the broader goal of developing transparent and responsible AI systems.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2506.21805v1": {
    "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale\n  LLM-Driven Agent Simulation",
    "summary": "Modeling human behavior in urban environments is fundamental for social\nscience, behavioral studies, and urban planning. Prior work often rely on\nrigid, hand-crafted rules, limiting their ability to simulate nuanced\nintentions, plans, and adaptive behaviors. Addressing these challenges, we\nenvision an urban simulator (CitySim), capitalizing on breakthroughs in\nhuman-level intelligence exhibited by large language models. In CitySim, agents\ngenerate realistic daily schedules using a recursive value-driven approach that\nbalances mandatory activities, personal habits, and situational factors. To\nenable long-term, lifelike simulations, we endow agents with beliefs, long-term\ngoals, and spatial memory for navigation. CitySim exhibits closer alignment\nwith real humans than prior work, both at micro and macro levels. Additionally,\nwe conduct insightful experiments by modeling tens of thousands of agents and\nevaluating their collective behaviors under various real-world scenarios,\nincluding estimating crowd density, predicting place popularity, and assessing\nwell-being. Our results highlight CitySim as a scalable, flexible testbed for\nunderstanding and forecasting urban phenomena.",
    "published": "2025-06-26T23:11:42Z",
    "updated": "2025-06-26T23:11:42Z",
    "id": "2506.21805v1",
    "authors": [
      "Nicolas Bougie",
      "Narimasa Watanabe"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21805v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21805v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21805v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to simulate urban behaviors and dynamics, which involves modeling human behavior and decision-making processes. This aligns with the topics of LLM (Large Language Models) and AGI (Artificial General Intelligence) due to the use of LLMs for simulating complex human behaviors and urban dynamics. Additionally, the paper's focus on simulating and predicting collective behaviors under various scenarios suggests a connection to the Reasoning topic, as it involves complex problem-solving and scenario analysis.",
    "llm_cls_result": [
      "LLM",
      "AGI",
      "Reasoning"
    ]
  },
  "2506.22521v1": {
    "title": "A Survey on Model Extraction Attacks and Defenses for Large Language\n  Models",
    "summary": "Model extraction attacks pose significant security threats to deployed\nlanguage models, potentially compromising intellectual property and user\nprivacy. This survey provides a comprehensive taxonomy of LLM-specific\nextraction attacks and defenses, categorizing attacks into functionality\nextraction, training data extraction, and prompt-targeted attacks. We analyze\nvarious attack methodologies including API-based knowledge distillation, direct\nquerying, parameter recovery, and prompt stealing techniques that exploit\ntransformer architectures. We then examine defense mechanisms organized into\nmodel protection, data privacy protection, and prompt-targeted strategies,\nevaluating their effectiveness across different deployment scenarios. We\npropose specialized metrics for evaluating both attack effectiveness and\ndefense performance, addressing the specific challenges of generative language\nmodels. Through our analysis, we identify critical limitations in current\napproaches and propose promising research directions, including integrated\nattack methodologies and adaptive defense mechanisms that balance security with\nmodel utility. This work serves NLP researchers, ML engineers, and security\nprofessionals seeking to protect language models in production environments.",
    "published": "2025-06-26T22:02:01Z",
    "updated": "2025-06-26T22:02:01Z",
    "id": "2506.22521v1",
    "authors": [
      "Kaixiang Zhao",
      "Lincan Li",
      "Kaize Ding",
      "Neil Zhenqiang Gong",
      "Yue Zhao",
      "Yushun Dong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22521v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22521v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22521v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses security threats and defenses specific to Large Language Models (LLMs), focusing on model extraction attacks and related defense mechanisms. This aligns closely with the 'LLM' topic, which encompasses research on Large Language Models, including their architectures and associated challenges like security.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.00057v1": {
    "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation",
    "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence.",
    "published": "2025-06-26T22:00:50Z",
    "updated": "2025-06-26T22:00:50Z",
    "id": "2507.00057v1",
    "authors": [
      "Thomas Valentin",
      "Ardi Madadi",
      "Gaetano Sapia",
      "Marcel Bhme"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00057v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00057v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00057v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the correctness of LLM-generated code without relying on oracles, which is related to the benchmarking and evaluation of LLMs. It also touches on the reasoning abilities of LLMs in the context of code generation.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning"
    ]
  },
  "2506.21784v1": {
    "title": "MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight\n  Domain-Specific Generator and Large Language Models",
    "summary": "Understanding and modeling human mobility patterns is crucial for effective\ntransportation planning and urban development. Despite significant advances in\nmobility research, there remains a critical gap in simulation platforms that\nallow for algorithm development, policy implementation, and comprehensive\nevaluation at scale. Traditional activity-based models require extensive data\ncollection and manual calibration, machine learning approaches struggle with\nadaptation to dynamic conditions, and treding agent-based Large Language Models\n(LLMs) implementations face computational constraints with large-scale\nsimulations. To address these challenges, we propose MobiVerse, a hybrid\nframework leverages the efficiency of lightweight domain-specific generator for\ngenerating base activity chains with the adaptability of LLMs for context-aware\nmodifications. A case study was conducted in Westwood, Los Angeles, where we\nefficiently generated and dynamically adjusted schedules for the whole\npopulation of approximately 53,000 agents on a standard PC. Our experiments\ndemonstrate that MobiVerse successfully enables agents to respond to\nenvironmental feedback, including road closures, large gathering events like\nfootball games, and congestion, through our hybrid framework. Its modular\ndesign facilitates testing various mobility algorithms at both transportation\nsystem and agent levels. Results show our approach maintains computational\nefficiency while enhancing behavioral realism. MobiVerse bridges the gap in\nmobility simulation by providing a customizable platform for mobility systems\nplanning and operations with benchmark algorithms. Code and videos are\navailable at https://github.com/ucla-mobility/MobiVerse.",
    "published": "2025-06-26T21:46:18Z",
    "updated": "2025-06-26T21:46:18Z",
    "id": "2506.21784v1",
    "authors": [
      "Yifan Liu",
      "Xishun Liao",
      "Haoxuan Ma",
      "Jonathan Liu",
      "Rohan Jadhav",
      "Jiaqi Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21784v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21784v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21784v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a hybrid framework for urban mobility simulation, which involves scaling and computational efficiency. The primary focus is on the application of LLMs in a specific domain (urban mobility) rather than on the core topics of LLM research, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.21783v1": {
    "title": "Evaluating List Construction and Temporal Understanding capabilities of\n  Large Language Models",
    "summary": "Large Language Models (LLMs) have demonstrated immense advances in a wide\nrange of natural language tasks. However, these models are susceptible to\nhallucinations and errors on particularly temporal understanding tasks\ninvolving multiple entities in answers. In such tasks, they fail to associate\nentities with accurate time intervals, generate a complete list of entities in\nanswers or reason about events associated with specific temporal bounds.\nExisting works do not extensively evaluate the abilities of the model to\nperform implicit and explicit temporal understanding in a list answer\nconstruction setup. To bridge this gap, we propose the Time referenced List\nbased Question Answering or TLQA benchmark that requires structured answers in\nlist format aligned with corresponding time periods. Our TLQA benchmark,\nrequires both list construction and temporal understanding simultaneously,\nwhich to the best of our knowledge has not been explored in prior benchmarks.\nWe investigate the temporal understanding and list construction capabilities of\nstate-of-the-art generative models on TLQA in closed-book and open-domain\nsettings. Our findings reveal significant shortcomings in current models,\nparticularly their inability to provide complete answers and temporally align\nfacts in a closed-book setup and the need to improve retrieval in open-domain\nsetup, providing clear future directions for research on TLQA. The benchmark\nand code at https://github.com/elixir-research-group/TLQA.",
    "published": "2025-06-26T21:40:58Z",
    "updated": "2025-06-26T21:40:58Z",
    "id": "2506.21783v1",
    "authors": [
      "Alexandru Dumitru",
      "V Venktesh",
      "Adam Jatowt",
      "Avishek Anand"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21783v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21783v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21783v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the capabilities of Large Language Models (LLMs) in temporal understanding and list construction, which aligns with the topics of 'Benchmark' and 'Reasoning'. The creation of a new benchmark (TLQA) specifically for these tasks is a key contribution.",
    "llm_cls_result": [
      "Benchmark",
      "Reasoning"
    ]
  },
  "2506.21763v2": {
    "title": "THE-Tree: Can Tracing Historical Evolution Enhance Scientific\n  Verification and Reasoning?",
    "summary": "Large Language Models (LLMs) are accelerating scientific idea generation, but\nrigorously evaluating these numerous, often superficial, AI-generated\npropositions for novelty and factual accuracy is a critical bottleneck; manual\nverification is too slow. Existing validation methods are inadequate: LLMs as\nstandalone verifiers may hallucinate and lack domain knowledge (our findings\nshow 60% unawareness of relevant papers in specific domains), while traditional\ncitation networks lack explicit causality and narrative surveys are\nunstructured. This underscores a core challenge: the absence of structured,\nverifiable, and causally-linked historical data of scientific evolution.To\naddress this,we introduce \\textbf{THE-Tree} (\\textbf{T}echnology\n\\textbf{H}istory \\textbf{E}volution Tree), a computational framework that\nconstructs such domain-specific evolution trees from scientific literature.\nTHE-Tree employs a search algorithm to explore evolutionary paths. During its\nnode expansion, it utilizes a novel \"Think-Verbalize-Cite-Verify\" process: an\nLLM proposes potential advancements and cites supporting literature.\nCritically, each proposed evolutionary link is then validated for logical\ncoherence and evidential support by a recovered natural language inference\nmechanism that interrogates the cited literature, ensuring that each step is\ngrounded. We construct and validate 88 THE-Trees across diverse domains and\nrelease a benchmark dataset including up to 71k fact verifications covering 27k\npapers to foster further research. Experiments demonstrate that i) in graph\ncompletion, our THE-Tree improves hit@1 by 8% to 14% across multiple models\ncompared to traditional citation networks; ii) for predicting future scientific\ndevelopments, it improves hit@1 metric by nearly 10%; and iii) when combined\nwith other methods, it boosts the performance of evaluating important\nscientific papers by almost 100%.",
    "published": "2025-06-26T20:44:51Z",
    "updated": "2025-07-21T06:49:51Z",
    "id": "2506.21763v2",
    "authors": [
      "Xin Wang",
      "Jiyao Liu",
      "Yulong Xiao",
      "Junzhi Ning",
      "Lihao Liu",
      "Junjun He",
      "Botian Shi",
      "Kaicheng Yu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21763v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21763v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21763v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for scientific verification and reasoning, introduces a computational framework (THE-Tree) to enhance this process, and evaluates its performance. The core topics are related to LLMs, reasoning, and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2506.21745v1": {
    "title": "(Fact) Check Your Bias",
    "summary": "Automatic fact verification systems increasingly rely on large language\nmodels (LLMs). We investigate how parametric knowledge biases in these models\naffect fact-checking outcomes of the HerO system (baseline for FEVER-25). We\nexamine how the system is affected by: (1) potential bias in Llama 3.1's\nparametric knowledge and (2) intentionally injected bias. When prompted\ndirectly to perform fact-verification, Llama 3.1 labels nearly half the claims\nas \"Not Enough Evidence\". Using only its parametric knowledge it is able to\nreach a verdict on the remaining half of the claims. In the second experiment,\nwe prompt the model to generate supporting, refuting, or neutral fact-checking\ndocuments. These prompts significantly influence retrieval outcomes, with\napproximately 50\\% of retrieved evidence being unique to each perspective.\nNotably, the model sometimes refuses to generate supporting documents for\nclaims it believes to be false, creating an inherent negative bias. Despite\ndifferences in retrieved evidence, final verdict predictions show stability\nacross prompting strategies. The code is available at:\nhttps://github.com/eibakke/FEVER-8-Shared-Task",
    "published": "2025-06-26T20:03:58Z",
    "updated": "2025-06-26T20:03:58Z",
    "id": "2506.21745v1",
    "authors": [
      "Eivind Morris Bakke",
      "Nora Winger Heggelund"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21745v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21745v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21745v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of parametric knowledge biases in large language models (LLMs) on fact-checking outcomes, specifically examining Llama 3.1's performance and biases. This aligns with the topics of LLM (Large Language Models) and Benchmark (evaluating LLM performance).",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.22520v1": {
    "title": "Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness\n  Discovery Curiosity and Promote Learning in the Context of Interactive\n  Molecular Dynamics",
    "summary": "This study examines the impact of an Artificial Intelligence tutor teammate\n(AI) on student curiosity-driven engagement and learning effectiveness during\nInteractive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics\nplatform. It explores the role of the AI's curiosity-triggering and response\nbehaviors in stimulating and sustaining student curiosity, affecting the\nfrequency and complexity of student-initiated questions. The study further\nassesses how AI interventions shape student engagement, foster discovery\ncuriosity, and enhance team performance within the IMD learning environment.\nUsing a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI\ntutor teammate's behavior through a large language model. By employing a\nmixed-methods exploratory design, a total of 11 high school students\nparticipated in four IMD tasks that involved molecular visualization and\ncalculations, which increased in complexity over a 60-minute period. Team\nperformance was evaluated through real-time observation and recordings, whereas\nteam communication was measured by question complexity and AI's\ncuriosity-triggering and response behaviors. Cross Recurrence Quantification\nAnalysis (CRQA) metrics reflected structural alignment in coordination and were\nlinked to communication behaviors. High-performing teams exhibited superior\ntask completion, deeper understanding, and increased engagement. Advanced\nquestions were associated with AI curiosity-triggering, indicating heightened\nengagement and cognitive complexity. CRQA metrics highlighted dynamic\nsynchronization in student-AI interactions, emphasizing structured yet adaptive\nengagement to promote curiosity. These proof-of-concept findings suggest that\nthe AI's dual role as a teammate and educator indicates its capacity to provide\nadaptive feedback, sustaining engagement and epistemic curiosity.",
    "published": "2025-06-26T19:30:25Z",
    "updated": "2025-06-26T19:30:25Z",
    "id": "2506.22520v1",
    "authors": [
      "Mustafa Demir",
      "Jacob Miratsky",
      "Jonathan Nguyen",
      "Chun Kit Chan",
      "Punya Mishra",
      "Abhishek Singharoy"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22520v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22520v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22520v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a large language model in an AI tutor teammate to enhance student learning and engagement, which aligns with the topics of LLM (Large Language Models) and AGI (Artificial General Intelligence) due to its focus on adaptive learning and intelligent tutoring systems.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2506.21710v1": {
    "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
    "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
    "published": "2025-06-26T18:51:04Z",
    "updated": "2025-06-26T18:51:04Z",
    "id": "2506.21710v1",
    "authors": [
      "Liangyu Zhong",
      "Fabio Rosenthal",
      "Joachim Sicking",
      "Fabian Hger",
      "Thorsten Bagdonat",
      "Hanno Gottschalk",
      "Leo Schwinn"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21710v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21710v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21710v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving Visual Question Answering (VQA) using Multimodal Large Language Models (MLLMs) by leveraging internal representations for efficient fine-grained visual question answering. It specifically addresses challenges in MLLMs and proposes a method to enhance their performance in VQA tasks.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Benchmark"
    ]
  },
  "2506.21550v1": {
    "title": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and\n  Model Selection at Scale",
    "summary": "Multivariate time series anomaly detection (MTS-AD) is critical in domains\nlike healthcare, cybersecurity, and industrial monitoring, yet remains\nchallenging due to complex inter-variable dependencies, temporal dynamics, and\nsparse anomaly labels. We introduce mTSBench, the largest benchmark to date for\nMTS-AD and unsupervised model selection, spanning 344 labeled time series\nacross 19 datasets and 12 diverse application domains. mTSBench evaluates 24\nanomaly detection methods, including large language model (LLM)-based detectors\nfor multivariate time series, and systematically benchmarks unsupervised model\nselection techniques under standardized conditions. Consistent with prior\nfindings, our results confirm that no single detector excels across datasets,\nunderscoring the importance of model selection. However, even state-of-the-art\nselection methods remain far from optimal, revealing critical gaps. mTSBench\nprovides a unified evaluation suite to enable rigorous, reproducible\ncomparisons and catalyze future advances in adaptive anomaly detection and\nrobust model selection.",
    "published": "2025-06-26T17:59:58Z",
    "updated": "2025-06-26T17:59:58Z",
    "id": "2506.21550v1",
    "authors": [
      "Xiaona Zhou",
      "Constantin Brif",
      "Ismini Lourentzou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21550v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21550v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21550v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking multivariate time series anomaly detection and model selection, which does not directly align with the provided topic list. Although it mentions LLM-based detectors, the core focus is on time series anomaly detection and benchmarking, not specifically on LLMs or the other provided topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.21551v2": {
    "title": "Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test",
    "summary": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between\nsamples. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization \"knowledge\ndigestion\", providing a mechanistic explanation of delayed generalization. In\nthe study, we develop two novel metrics to quantify pathway distance and the\ncomplexity of a single pathway. We show their ability to predict the\ngeneralization improvement on diverse downstream tasks. They are efficient,\nsimple to compute and solely dependent on training data. Hence, they have\npractical value for pretraining, enabling us to monitor the generalization\nperformance without finetuning and test. Theoretically, we show that more\nstructured pathways reduce model complexity and improve the generalization\nbound.",
    "published": "2025-06-26T17:59:58Z",
    "updated": "2025-07-03T01:00:21Z",
    "id": "2506.21551v2",
    "authors": [
      "Ziyue Li",
      "Chenrui Fan",
      "Tianyi Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21551v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21551v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21551v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses grokking in the context of LLM pretraining, focusing on generalization and internal dynamics of large language models. It involves pretraining strategies and reasoning abilities, but does not fit neatly into the other provided categories.",
    "llm_cls_result": [
      "Pretrain",
      "Reasoning"
    ]
  },
  "2506.21655v1": {
    "title": "APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy\n  Optimization",
    "summary": "Multimodal Large Language Models (MLLMs) are powerful at integrating diverse\ndata, but they often struggle with complex reasoning. While Reinforcement\nlearning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.\nCommon issues include a drop in performance on general tasks and the generation\nof overly detailed or \"overthinking\" reasoning. Our work investigates how the\nKL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric\nPolicy Optimization (APO) to address these issues, which divides the sampled\nresponses into positive and negative groups. For positive samples,\nDifficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically\nadjust the KL divergence weight based on their difficulty. This method prevents\npolicy entropy from dropping sharply, improves training stability, utilizes\nsamples better, and preserves the model's existing knowledge. For negative\nsamples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to\npenalize overly long responses. This helps mitigate overthinking and encourages\nmore concise reasoning while preserving the model's explorative capacity. We\napply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B\nsignificantly enhances reasoning capabilities, showing an average 7\\% gain over\nthe base model and outperforming larger MLLMs (7-11B) on various reasoning\nbenchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade\non general tasks, View-R1-3B maintains consistent improvement, demonstrating\nsuperior generalization. These results highlight the effectiveness and broad\napplicability of our DADS and STCR techniques for advancing complex multimodal\nreasoning in MLLMs. The code will be made available at\nhttps://github.com/Indolent-Kawhi/View-R1.",
    "published": "2025-06-26T17:57:08Z",
    "updated": "2025-06-26T17:57:08Z",
    "id": "2506.21655v1",
    "authors": [
      "Minjie Hong",
      "Zirun Guo",
      "Yan Xia",
      "Zehan Wang",
      "Ziang Zhang",
      "Tao Jin",
      "Zhou Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21655v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21655v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21655v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the reasoning ability of Multimodal Large Language Models (MLLMs) using Reinforcement Learning (RL) techniques, specifically addressing issues like overthinking and performance drops on general tasks. It introduces methods like Asymmetric Policy Optimization (APO) and Difficulty-Adaptive Divergence Shaping (DADS) to improve reasoning and training stability.",
    "llm_cls_result": [
      "MLLM",
      "RL",
      "Reasoning"
    ]
  },
  "2506.21536v1": {
    "title": "PsyLite Technical Report",
    "summary": "With the rapid development of digital technology, AI-driven psychological\ncounseling has gradually become an important research direction in the field of\nmental health. However, existing models still have deficiencies in dialogue\nsafety, detailed scenario handling, and lightweight deployment. To address\nthese issues, this study proposes PsyLite, a lightweight psychological\ncounseling large language model agent developed based on the base model\nInternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation\ndata fine-tuning and ORPO preference optimization), PsyLite enhances the\nmodel's deep-reasoning ability, psychological counseling ability, and safe\ndialogue ability. After deployment using Ollama and Open WebUI, a custom\nworkflow is created with Pipelines. An innovative conditional RAG is designed\nto introduce crosstalk humor elements at appropriate times during psychological\ncounseling to enhance user experience and decline dangerous requests to\nstrengthen dialogue safety. Evaluations show that PsyLite outperforms the\nbaseline models in the Chinese general evaluation (CEval), psychological\ncounseling professional evaluation (CPsyCounE), and dialogue safety evaluation\n(SafeDialBench), particularly in psychological counseling professionalism\n(CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score\nimprovement of 2.4\\%). Additionally, the model uses quantization technology\n(GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient\nfor operation), providing a feasible solution for psychological counseling\napplications in resource-constrained environments.",
    "published": "2025-06-26T17:54:42Z",
    "updated": "2025-06-26T17:54:42Z",
    "id": "2506.21536v1",
    "authors": [
      "Fangjun Ding",
      "Renyu Zhang",
      "Xinyu Feng",
      "Chengye Xie",
      "Zheng Zhang",
      "Yanting Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21536v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21536v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21536v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development of a lightweight psychological counseling large language model agent, which involves fine-tuning and optimization techniques but does not directly align with the provided core topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.21535v1": {
    "title": "Exploring the Design Space of 3D MLLMs for CT Report Generation",
    "summary": "Multimodal Large Language Models (MLLMs) have emerged as a promising way to\nautomate Radiology Report Generation (RRG). In this work, we systematically\ninvestigate the design space of 3D MLLMs, including visual input\nrepresentation, projectors, Large Language Models (LLMs), and fine-tuning\ntechniques for 3D CT report generation. We also introduce two knowledge-based\nreport augmentation methods that improve performance on the GREEN score by up\nto 10\\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our\nresults on the 1,687 cases from the AMOS-MM dataset show that RRG is largely\nindependent of the size of LLM under the same training protocol. We also show\nthat larger volume size does not always improve performance if the original ViT\nwas pre-trained on a smaller volume size. Lastly, we show that using a\nsegmentation mask along with the CT volume improves performance. The code is\npublicly available at https://github.com/bowang-lab/AMOS-MM-Solution",
    "published": "2025-06-26T17:54:20Z",
    "updated": "2025-06-26T17:54:20Z",
    "id": "2506.21535v1",
    "authors": [
      "Mohammed Baharoon",
      "Jun Ma",
      "Congyu Fang",
      "Augustin Toma",
      "Bo Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21535v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21535v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21535v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses Multimodal Large Language Models (MLLMs) and their application in Radiology Report Generation (RRG), specifically focusing on 3D CT report generation. It covers aspects like visual input representation, projectors, LLMs, and fine-tuning techniques, which are central to MLLM research. The mention of LLMs and their size independence in performance also ties into the broader LLM research.",
    "llm_cls_result": [
      "MLLM",
      "LLM",
      "Dataset"
    ]
  },
  "2506.21532v1": {
    "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in\n  Large-Scale Conversational AI Datasets",
    "summary": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat",
    "published": "2025-06-26T17:52:18Z",
    "updated": "2025-06-26T17:52:18Z",
    "id": "2506.21532v1",
    "authors": [
      "Akshay Paruchuri",
      "Maryam Aziz",
      "Rohit Vartak",
      "Ayman Ali",
      "Best Uchehara",
      "Xin Liu",
      "Ishan Chatterjee",
      "Monica Agrawal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21532v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21532v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21532v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on analyzing user interactions with large language models (LLMs) for healthcare information, which involves the use of LLMs and their applications in real-world scenarios. The study also involves the creation of a dataset for benchmarking and evaluating LLM performance in healthcare contexts.",
    "llm_cls_result": [
      "LLM",
      "Dataset",
      "Benchmark"
    ]
  },
  "2506.21521v2": {
    "title": "Potemkin Understanding in Large Language Models",
    "summary": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations.",
    "published": "2025-06-26T17:41:35Z",
    "updated": "2025-06-29T18:12:45Z",
    "id": "2506.21521v2",
    "authors": [
      "Marina Mancoridis",
      "Bec Weeks",
      "Keyon Vafa",
      "Sendhil Mullainathan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21521v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21521v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21521v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of Large Language Models (LLMs) using benchmark datasets and questions the validity of these benchmarks if LLMs' misunderstandings do not mirror human misunderstandings. It introduces a framework to quantify 'potemkin understanding' in LLMs, which is relevant to the evaluation and understanding of LLM capabilities.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.22518v1": {
    "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language\n  Models for Graph-based Retrieval Augmented Generation",
    "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to ground responses with structured external knowledge from\nup-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs\noften rely on a weak retriever in graph-based RAG: I) Due to the lack of ground\ntruth, the retriever is often trained on weak supervision, which often\nintroduces spurious signals to the LLMs. II) Due to the abstraction of graph\ndata, the retrieved knowledge is often presented in unorganized forms. To\nmitigate the issue, we present Refined Graph-based RAG (ReG) to align weak\nretrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM\nfeedback to get rid of spurious signals and improve the quality of the\nsupervision. Meanwhile, ReG introduces a structure-aware reorganization module\nto refactor the retrieval results into logically coherent evidence chains.\nExperiments on prominent benchmarks demonstrate that ReG significantly and\nconsistently brings improvements across different LLM backbones by up to 10%.\nThe improved supervision quality enables ReG to match the state-of-the-art\nperformance with 5% training data and to transfer to out-of-distribution KGs.\nNotably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token\ncost by up to 30% and improves the performance by up to 4%.",
    "published": "2025-06-26T17:40:23Z",
    "updated": "2025-06-26T17:40:23Z",
    "id": "2506.22518v1",
    "authors": [
      "Deyu Zou",
      "Yongqiang Chen",
      "Mufei Li",
      "Siqi Miao",
      "Chenxi Liu",
      "Bo Han",
      "James Cheng",
      "Pan Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22518v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22518v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22518v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the alignment of weak retrievers with large language models (LLMs) for graph-based retrieval-augmented generation (RAG), which involves improving the quality of supervision and reorganizing retrieval results. This aligns with topics related to LLMs, memory (retrieval-augmented generation), and reasoning (as it mentions reasoning-based LLMs).",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Reasoning"
    ]
  },
  "2506.21495v1": {
    "title": "Bridging Offline and Online Reinforcement Learning for LLMs",
    "summary": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types.",
    "published": "2025-06-26T17:25:49Z",
    "updated": "2025-06-26T17:25:49Z",
    "id": "2506.21495v1",
    "authors": [
      "Jack Lanchantin",
      "Angelica Chen",
      "Janice Lan",
      "Xian Li",
      "Swarnadeep Saha",
      "Tianlu Wang",
      "Jing Xu",
      "Ping Yu",
      "Weizhe Yuan",
      "Jason E Weston",
      "Sainbayar Sukhbaatar",
      "Ilia Kulikov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21495v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21495v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21495v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses reinforcement learning methods for finetuning large language models, specifically comparing online and offline regimes, and evaluates performance on verifiable and non-verifiable tasks. This aligns with topics related to Reinforcement Learning (RL) and Large Language Models (LLM).",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2506.21467v1": {
    "title": "Efficient and Reuseable Cloud Configuration Search Using Discovery\n  Spaces",
    "summary": "Finding the optimal set of cloud resources to deploy a given workload at\nminimal cost while meeting a defined service level agreement is an active area\nof research. Combining tens of parameters applicable across a large selection\nof compute, storage, and services offered by cloud providers with similar\nnumbers of application-specific parameters leads to configuration spaces with\nmillions of deployment options.\n  In this paper, we propose Discovery Space, an abstraction that formalizes the\ndescription of workload configuration problems, and exhibits a set of\ncharacteristics required for structured, robust and distributed investigations\nof large search spaces. We describe a concrete implementation of the Discovery\nSpace abstraction and show that it is generalizable across a diverse set of\nworkloads such as Large Language Model inference and Big Data Analytics.\n  We demonstrate that our approach enables safe, transparent sharing of data\nbetween executions of best-of-breed optimizers increasing the efficiency of\noptimal configuration detection in large search spaces. We also demonstrate how\nDiscovery Spaces enable transfer and reuse of knowledge across similar search\nspaces, enabling configuration search speed-ups of over 90%.",
    "published": "2025-06-26T16:54:39Z",
    "updated": "2025-06-26T16:54:39Z",
    "id": "2506.21467v1",
    "authors": [
      "Michael Johnston",
      "Burkhard Ringlein",
      "Christoph Hagleitner",
      "Alessandro Pomponio",
      "Vassilis Vassiliadis",
      "Christian Pinto",
      "Srikumar Venugopal"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21467v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21467v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21467v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on cloud resource configuration optimization and does not directly relate to the provided topics related to LLMs, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.21445v1": {
    "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond\n  English",
    "summary": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages.",
    "published": "2025-06-26T16:31:10Z",
    "updated": "2025-06-26T16:31:10Z",
    "id": "2506.21445v1",
    "authors": [
      "Makbule Gulcin Ozsoy",
      "William Tai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21445v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21445v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21445v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating foundational LLMs on a multilingual Text2Cypher task, which involves translating natural language questions into database queries across different languages. This aligns with the 'Benchmark' topic as it involves benchmarking LLMs' performance across languages. It also touches on 'LLM' as it discusses foundational large language models and their capabilities.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2506.21443v1": {
    "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection",
    "summary": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk-sensitive scenarios. To address\nthese challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework\nthat integrates pretrained LLMs with structured, task-specific insights to\nperform fraud and concept drift detection. The proposed architecture consists\nof three main components: (1) a DK-LLM module to detect fake or deceptive\nconversations; (2) a drift detection unit (OCDD) to determine whether a\nsemantic shift has occurred; and (3) a second DK-LLM module to classify the\ndrift as either benign or fraudulent. We first validate the value of domain\nknowledge using a fake review dataset and then apply our full framework to\nSEConvo, a multiturn dialogue dataset that includes various types of fraud and\nspam attacks. Results show that our system detects fake conversations with high\naccuracy and effectively classifies the nature of drift. Guided by structured\nprompts, the LLaMA-based implementation achieves 98% classification accuracy.\nComparative studies against zero-shot baselines demonstrate that incorporating\ndomain knowledge and drift awareness significantly improves performance,\ninterpretability, and robustness in high-stakes NLP applications.",
    "published": "2025-06-26T16:29:45Z",
    "updated": "2025-06-26T16:29:45Z",
    "id": "2506.21443v1",
    "authors": [
      "Ali enol",
      "Garima Agrawal",
      "Huan Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21443v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21443v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21443v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing Large Language Models (LLMs) with domain knowledge for specific tasks like fraud and concept drift detection, which involves pretraining strategies and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Pretrain",
      "Reasoning"
    ]
  },
  "2506.21408v1": {
    "title": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via\n  Stochastic Variational Subspace Inference",
    "summary": "Despite their widespread use, large language models (LLMs) are known to\nhallucinate incorrect information and be poorly calibrated. This makes the\nuncertainty quantification of these models of critical importance, especially\nin high-stakes domains, such as autonomy and healthcare. Prior work has made\nBayesian deep learning-based approaches to this problem more tractable by\nperforming inference over the low-rank adaptation (LoRA) parameters of a\nfine-tuned model. While effective, these approaches struggle to scale to larger\nLLMs due to requiring further additional parameters compared to LoRA. In this\nwork we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank\nAdaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform\nBayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By\nrepurposing the LoRA parameters as projection matrices, we are able to map\nsamples from this subspace into the full weight space of the LLM. This allows\nus to learn all the parameters of our approach using stochastic variational\ninference. Despite the low dimensionality of our subspace, we are able to\nachieve competitive performance with state-of-the-art approaches while only\nrequiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to\nscale up to the largest Bayesian LLM to date, with four times as a many base\nparameters as prior work.",
    "published": "2025-06-26T15:54:45Z",
    "updated": "2025-06-26T15:54:45Z",
    "id": "2506.21408v1",
    "authors": [
      "Colin Samplawski",
      "Adam D. Cobb",
      "Manoj Acharya",
      "Ramneet Kaur",
      "Susmit Jha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21408v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21408v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21408v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses Bayesian approaches to uncertainty quantification in large language models (LLMs) through low-rank adaptation (LoRA), which is directly related to LLM research and scaling techniques.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.21393v1": {
    "title": "TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in\n  Multimodal Table Understanding",
    "summary": "Multimodal understanding of tables in real-world contexts is challenging due\nto the complexity of structure, symbolic density, and visual degradation (blur,\nskew, watermarking, incomplete structures or fonts, multi-span or\nhierarchically nested layouts). Existing multimodal large language models\n(MLLMs) struggle with such WildStruct conditions, resulting in limited\nperformance and poor generalization. To address these challenges, we propose\nTableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture\nspecifically designed for robust, structured reasoning over multimodal table\ndata. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which\npredicts latent semantic token roles (e.g., header, data cell, axis, formula)\nand dynamically routes table elements to specialized experts (Table-to-HTML,\nTable-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed\nby symbolic reasoning graphs. To facilitate effective alignment-driven\npretraining, we introduce the large-scale TableMoE-Align dataset, consisting of\n1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and\nindustry, utilized exclusively for model pretraining. For evaluation, we curate\nand release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,\nWMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models\nunder real-world multimodal degradation and structural complexity. Experimental\nresults demonstrate that TableMoE significantly surpasses existing\nstate-of-the-art models. Extensive ablation studies validate each core\ncomponent, emphasizing the critical role of Neuro-Symbolic Routing and\nstructured expert alignment. Through qualitative analyses, we further showcase\nTableMoE's interpretability and enhanced robustness, underscoring the\neffectiveness of integrating neuro-symbolic reasoning for multimodal table\nunderstanding.",
    "published": "2025-06-26T15:41:34Z",
    "updated": "2025-06-26T15:41:34Z",
    "id": "2506.21393v1",
    "authors": [
      "Junwen Zhang",
      "Pu Chen",
      "Yin Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21393v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21393v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21393v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture for multimodal table understanding, which involves multimodal large language models (MLLMs) and Mixture of Experts (MoE) techniques. It also introduces a large-scale dataset for pretraining and benchmarks for evaluation.",
    "llm_cls_result": [
      "MLLM",
      "MoE",
      "Pretrain"
    ]
  },
  "2506.21360v1": {
    "title": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas\n  Semiotic Square for Large Language Models",
    "summary": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement.",
    "published": "2025-06-26T15:10:24Z",
    "updated": "2025-06-26T15:10:24Z",
    "id": "2506.21360v1",
    "authors": [
      "Fangzhou Dong",
      "Yifan Zeng",
      "Yingpeng Sang",
      "Hong Shen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21360v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21360v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21360v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing Large Language Models' (LLMs) ability to conduct literary criticism using a structured analytical framework (GLASS) based on Greimas Semiotic Square (GSS). It involves creating a dataset and quantitative metrics for literary criticism, which aligns with the topics of LLM and Dataset.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2506.21355v1": {
    "title": "SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context\n  Learning",
    "summary": "Multimodal in-context learning (ICL) remains underexplored despite\nsignificant potential for domains such as medicine. Clinicians routinely\nencounter diverse, specialized tasks requiring adaptation from limited\nexamples, such as drawing insights from a few relevant prior cases or\nconsidering a constrained set of differential diagnoses. While multimodal large\nlanguage models (MLLMs) have shown advances in medical visual question\nanswering (VQA), their ability to learn multimodal tasks from context is\nlargely unknown. We introduce SMMILE, the first expert-driven multimodal ICL\nbenchmark for medical tasks. Eleven medical experts curated problems, each\nincluding a multimodal query and multimodal in-context examples as task\ndemonstrations. SMMILE encompasses 111 problems (517 question-image-answer\ntriplets) covering 6 medical specialties and 13 imaging modalities. We further\nintroduce SMMILE++, an augmented variant with 1038 permuted problems. A\ncomprehensive evaluation of 15 MLLMs demonstrates that most models exhibit\nmoderate to poor multimodal ICL ability in medical tasks. In open-ended\nevaluations, ICL contributes only 8% average improvement over zero-shot on\nSMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant\nin-context examples: even a single noisy or irrelevant example can degrade\nperformance by up to 9.5%. Moreover, example ordering exhibits a recency bias,\ni.e., placing the most relevant example last can lead to substantial\nperformance improvements by up to 71%. Our findings highlight critical\nlimitations and biases in current MLLMs when learning multimodal medical tasks\nfrom context.",
    "published": "2025-06-26T15:08:18Z",
    "updated": "2025-06-26T15:08:18Z",
    "id": "2506.21355v1",
    "authors": [
      "Melanie Rieff",
      "Maya Varma",
      "Ossian Rabow",
      "Subathra Adithan",
      "Julie Kim",
      "Ken Chang",
      "Hannah Lee",
      "Nidhi Rohatgi",
      "Christian Bluethgen",
      "Mohamed S. Muneer",
      "Jean-Benoit Delbrouck",
      "Michael Moor"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21355v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21355v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21355v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark for multimodal in-context learning in the medical domain, specifically focusing on multimodal large language models (MLLMs). It evaluates the performance of these models on expert-curated medical tasks, which aligns with the topics of MLLM (Multimodal Large Language Models) and Benchmark (evaluation of LLMs).",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2506.21343v1": {
    "title": "DynamicBench: Evaluating Real-Time Report Generation in Large Language\n  Models",
    "summary": "Traditional benchmarks for large language models (LLMs) typically rely on\nstatic evaluations through storytelling or opinion expression, which fail to\ncapture the dynamic requirements of real-time information processing in\ncontemporary applications. To address this limitation, we present DynamicBench,\na benchmark designed to evaluate the proficiency of LLMs in storing and\nprocessing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval\npipeline, integrating web searches with local report databases. It necessitates\ndomain-specific knowledge, ensuring accurate responses report generation within\nspecialized fields. By evaluating models in scenarios that either provide or\nwithhold external documents, DynamicBench effectively measures their capability\nto independently process recent information or leverage contextual\nenhancements. Additionally, we introduce an advanced report generation system\nadept at managing dynamic information synthesis. Our experimental results\nconfirm the efficacy of our approach, with our method achieving\nstate-of-the-art performance, surpassing GPT4o in document-free and\ndocument-assisted scenarios by 7.0% and 5.8%, respectively. The code and data\nwill be made publicly available.",
    "published": "2025-06-26T14:53:44Z",
    "updated": "2025-06-26T14:53:44Z",
    "id": "2506.21343v1",
    "authors": [
      "Jingyao Li",
      "Hao Sun",
      "Zile Qiao",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Hong Xu",
      "Jiaya Jia"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21343v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21343v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21343v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark (DynamicBench) for evaluating LLMs in real-time report generation, which involves dynamic information processing and retrieval. This aligns with the topics of benchmarking LLMs and their memory capabilities for handling real-time data.",
    "llm_cls_result": [
      "Benchmark",
      "Memory"
    ]
  },
  "2506.21316v2": {
    "title": "DRISHTIKON: Visual Grounding at Multiple Granularities in Documents",
    "summary": "Visual grounding in text-rich document images is a critical yet underexplored\nchallenge for Document Intelligence and Visual Question Answering (VQA)\nsystems. We present DRISHTIKON, a multi-granular and multi-block visual\ngrounding framework designed to enhance interpretability and trust in VQA for\ncomplex, multilingual documents. Our approach integrates multilingual OCR,\nlarge language models, and a novel region matching algorithm to localize answer\nspans at the block, line, word, and point levels. We introduce the\nMulti-Granular Visual Grounding (MGVG) benchmark, a curated test set of diverse\ncircular notifications from various sectors, each manually annotated with\nfine-grained, human-verified labels across multiple granularities. Extensive\nexperiments show that our method achieves state-of-the-art grounding accuracy,\nwith line-level granularity providing the best balance between precision and\nrecall. Ablation studies further highlight the benefits of multi-block and\nmulti-line reasoning. Comparative evaluations reveal that leading\nvision-language models struggle with precise localization, underscoring the\neffectiveness of our structured, alignment-based approach. Our findings pave\nthe way for more robust and interpretable document understanding systems in\nreal-world, text-centric scenarios with multi-granular grounding support. Code\nand dataset are made available for future research.",
    "published": "2025-06-26T14:32:23Z",
    "updated": "2025-07-16T01:55:35Z",
    "id": "2506.21316v2",
    "authors": [
      "Badri Vishal Kasuba",
      "Parag Chaudhuri",
      "Ganesh Ramakrishnan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21316v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21316v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21316v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on visual grounding in document images, integrating multilingual OCR and large language models, and introduces a benchmark for multi-granular visual grounding. This aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA) due to its focus on integrating vision and language for document understanding and localization.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2506.21294v1": {
    "title": "Detecting Referring Expressions in Visually Grounded Dialogue with\n  Autoregressive Language Models",
    "summary": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches.",
    "published": "2025-06-26T14:14:20Z",
    "updated": "2025-06-26T14:14:20Z",
    "id": "2506.21294v1",
    "authors": [
      "Bram Willemsen",
      "Gabriel Skantze"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21294v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21294v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21294v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a pretrained large language model (LLM) for detecting referring expressions in visually grounded dialogue, which involves both language and visual modalities. The focus is on the adaptation of an LLM for a multimodal task, making it relevant to both LLM and MLLM topics.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2506.21288v1": {
    "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness",
    "summary": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less",
    "published": "2025-06-26T14:09:41Z",
    "updated": "2025-06-26T14:09:41Z",
    "id": "2506.21288v1",
    "authors": [
      "Istabrak Abbes",
      "Gabriele Prato",
      "Quentin Fournier",
      "Fernando Rodriguez",
      "Alaa Boukhary",
      "Adam Elwood",
      "Sarath Chandar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21288v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21288v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21288v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of lightweight encoder models to detect groundedness in responses generated by large language models (LLMs), which is relevant to the topics of LLMs and their performance in NLP tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.21285v2": {
    "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via\n  Self-Critical Fine-Tuning",
    "summary": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique. Our codes and data are\navailable at https://github.com/XinXU-USTC/DoubleChecker",
    "published": "2025-06-26T14:05:45Z",
    "updated": "2025-07-09T03:35:19Z",
    "id": "2506.21285v2",
    "authors": [
      "Xin Xu",
      "Tianhao Chen",
      "Fan Zhang",
      "Wanlong Liu",
      "Pengxiang Li",
      "Ajay Kumar Jaiswal",
      "Yuchen Yan",
      "Jishan Hu",
      "Yang Wang",
      "Hao Chen",
      "Shiwei Liu",
      "Shizhe Diao",
      "Can Yang",
      "Lu Yin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21285v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21285v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21285v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing the reasoning capabilities of large language models (LLMs) through self-critique and iterative refinement, which aligns with the topics of Reasoning and LLM.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2506.21277v1": {
    "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
    "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.",
    "published": "2025-06-26T14:01:03Z",
    "updated": "2025-06-26T14:01:03Z",
    "id": "2506.21277v1",
    "authors": [
      "Qize Yang",
      "Shimin Yao",
      "Weixuan Chen",
      "Shenghao Fu",
      "Detao Bai",
      "Jiaxing Zhao",
      "Boyuan Sun",
      "Bowen Yin",
      "Xihan Wei",
      "Jingren Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21277v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21277v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21277v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses multimodal large language models (MLLM), their reasoning capabilities enhanced by Reinforcement Learning (RL), and introduces a benchmark for evaluating these models. The focus on multimodal reasoning and the use of RL aligns with the topics of MLLM and RL. The introduction of a new benchmark also relates to the Benchmark topic.",
    "llm_cls_result": [
      "MLLM",
      "RL",
      "Benchmark"
    ]
  },
  "2506.22516v1": {
    "title": "Can \"consciousness\" be observed from large language model (LLM) internal\n  states? Dissecting LLM representations obtained from Theory of Mind test with\n  Integrated Information Theory and Span Representation analysis",
    "summary": "Integrated Information Theory (IIT) provides a quantitative framework for\nexplaining consciousness phenomenon, positing that conscious systems comprise\nelements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the\nlatest iterations of this framework -- to sequences of Large Language Model\n(LLM) representations, analyzing data derived from existing Theory of Mind\n(ToM) test results. Our study systematically investigates whether the\ndifferences of ToM test performances, when presented in the LLM\nrepresentations, can be revealed by IIT estimates, i.e., $\\Phi^{\\max}$ (IIT\n3.0), $\\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\\Phi$-structure\n(IIT 4.0). Furthermore, we compare these metrics with the Span Representations\nindependent of any estimate for consciousness. This additional effort aims to\ndifferentiate between potential \"consciousness\" phenomena and inherent\nseparations within LLM representational space. We conduct comprehensive\nexperiments examining variations across LLM transformer layers and linguistic\nspans from stimuli. Our results suggest that sequences of contemporary\nTransformer-based LLM representations lack statistically significant indicators\nof observed \"consciousness\" phenomena but exhibit intriguing patterns under\n$\\textit{spatio}$-permutational analyses. The Appendix and code are available\nas Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.",
    "published": "2025-06-26T13:59:22Z",
    "updated": "2025-06-26T13:59:22Z",
    "id": "2506.22516v1",
    "authors": [
      "Jingkai Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22516v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22516v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22516v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on analyzing the internal states of Large Language Models (LLMs) using Integrated Information Theory (IIT) and Span Representation analysis to investigate potential 'consciousness' phenomena. The core topics are related to LLMs and their representations, as well as the broader implications for Artificial General Intelligence (AGI).",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2506.21274v1": {
    "title": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?",
    "summary": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness",
    "published": "2025-06-26T13:58:43Z",
    "updated": "2025-06-26T13:58:43Z",
    "id": "2506.21274v1",
    "authors": [
      "Andrea McGlinchey",
      "Peter J Barclay"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21274v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21274v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21274v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the generation of fake text by large language models (LLMs) and the detection of such text, which is relevant to the study of LLMs and their capabilities.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2507.05265v1": {
    "title": "BMFM-DNA: A SNP-aware DNA foundation model to capture variant effects",
    "summary": "Large language models (LLMs) trained on text demonstrated remarkable results\non natural language processing (NLP) tasks. These models have been adapted to\ndecipher the language of DNA, where sequences of nucleotides act as \"words\"\nthat encode genomic functions. However, the genome differs fundamentally from\nnatural language, as it lacks clearly defined words or a consistent grammar.\nAlthough DNA language models (DNALMs) such as DNABERT, GENA-LM have achieved\nhigh level of performance on genome-related biological tasks, these models do\nnot encode biological functions in the presence of sequence variations. To\naddress this problem, we pre-train foundation models that effectively integrate\nsequence variations, in particular Single Nucleotide Polymorphisms (SNPs), as\nthey underlie important biological functions. Specifically, we use ModernBERT\nto pre-train two different Biomedical Foundation Models (BMFM), namely,\nBMFM-DNA-REF in which the model is trained with sequences of varying lengths\nalong with their reverse complements derived from the reference genome and\nBMFM-DNA-SNP in which the model is trained with sequences created using a novel\nrepresentation scheme that encodes sequence variations. Our findings indicate\nthat integrating sequence variations into DNALMs helps capture the biological\nfunctions as seen in improvements on all fine-tuning tasks. To explore the\nmodel's practical utility, we experimented with various strategies for SNP\nimputation on promoter detection task introduced in DNABERT-2. However, we\nacknowledge that the current benchmarks are limited in their ability to fully\nevaluate these models. To enable more comprehensive assessment in the future\nand encourage community contributions, we release our models through\nHuggingFace and the code to reproduce the results at\nhttps://github.com/BiomedSciAI/biomed-multi-omic",
    "published": "2025-06-26T13:56:32Z",
    "updated": "2025-06-26T13:56:32Z",
    "id": "2507.05265v1",
    "authors": [
      "Hongyang Li",
      "Sanjoy Dey",
      "Bum Chul Kwon",
      "Michael Danziger",
      "Michal Rosen-Tzvi",
      "Jianying Hu",
      "James Kozloski",
      "Ching-Huei Tsou",
      "Bharath Dandala",
      "Pablo Meyer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05265v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05265v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05265v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the adaptation of large language models (LLMs) to DNA sequences, focusing on integrating sequence variations (SNPs) into DNA language models (DNALMs). It involves pre-training foundation models and evaluating their performance on biological tasks, which aligns with the topics of LLM and Pretrain.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2506.21263v1": {
    "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for\n  Decentralized Cluster",
    "summary": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters.",
    "published": "2025-06-26T13:45:04Z",
    "updated": "2025-06-26T13:45:04Z",
    "id": "2506.21263v1",
    "authors": [
      "Ji Qi",
      "WenPeng Zhu",
      "Li Li",
      "Ming Wu",
      "YingJun Wu",
      "Wu He",
      "Xun Gao",
      "Jason Zeng",
      "Michael Heinrich"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21263v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21263v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21263v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the distributed training of large language models (LLMs) and proposes a framework to improve training efficiency on decentralized clusters, which is closely related to the scaling and pretraining of LLMs.",
    "llm_cls_result": [
      "Scaling",
      "Pretrain",
      "LLM"
    ]
  },
  "2506.21252v1": {
    "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling\n  across Perception, Planning, and Safety in Real-World Multimodal Agents",
    "summary": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show\npromise in real-world tasks like web navigation and embodied intelligence.\nHowever, due to limitations in a lack of external feedback, these agents\nstruggle with self-correction and generalization. A promising approach is to\nuse reward models as external feedback, but there is no clear on how to select\nreward models for agents. Thus, there is an urgent need to build a reward bench\ntargeted at agents. To address these challenges, we propose Agent-RewardBench,\na benchmark designed to evaluate reward modeling ability in MLLMs. The\nbenchmark is characterized by three key features: (1) Multiple dimensions and\nreal-world agent scenarios evaluation. It covers perception, planning, and\nsafety with 7 scenarios; (2) Step-level reward evaluation. It allows for the\nassessment of agent capabilities at the individual steps of a task, providing a\nmore granular view of performance during the planning process; and (3)\nAppropriately difficulty and high-quality. We carefully sample from 10 diverse\nmodels, difficulty control to maintain task challenges, and manual verification\nto ensure the integrity of the data. Experiments demonstrate that even\nstate-of-the-art multimodal models show limited performance, highlighting the\nneed for specialized training in agent reward modeling. Code is available at\ngithub.",
    "published": "2025-06-26T13:36:12Z",
    "updated": "2025-06-26T13:36:12Z",
    "id": "2506.21252v1",
    "authors": [
      "Tianyi Men",
      "Zhuoran Jin",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21252v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21252v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21252v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating reward modeling in Multimodal Large Language Models (MLLMs) for real-world agent tasks, which involves perception, planning, and safety. It introduces a benchmark (Agent-RewardBench) to assess these capabilities, aligning with topics related to MLLMs and benchmarking.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark",
      "RL"
    ]
  },
  "2506.21250v1": {
    "title": "ACTLLM: Action Consistency Tuned Large Language Model",
    "summary": "This paper introduces ACTLLM (Action Consistency Tuned Large Language Model),\na novel approach for robot manipulation in dynamic environments. Traditional\nvision-based systems often struggle to learn visual representations that excel\nin both task execution and spatial reasoning, thereby limiting their\nadaptability in dynamic environments. ACTLLM addresses these challenges by\nharnessing language to craft structured scene descriptors, providing a uniform\ninterface for both spatial understanding and task performance through flexible\nlanguage instructions. Moreover, we introduce a novel action consistency\nconstraint that aligns visual perception with corresponding actions, thereby\nenhancing the learning of actionable visual representations. Additionally, we\nhave reformulated the Markov decision process for manipulation tasks into a\nmulti-turn visual dialogue framework. This approach enables the modeling of\nlong-term task execution with enhanced contextual relevance derived from the\nhistory of task execution. During our evaluation, ACTLLM excels in diverse\nscenarios, proving its effectiveness on challenging vision-based robot\nmanipulation tasks.",
    "published": "2025-06-26T13:35:53Z",
    "updated": "2025-06-26T13:35:53Z",
    "id": "2506.21250v1",
    "authors": [
      "Jing Bi",
      "Lianggong Bruce Wen",
      "Zhang Liu",
      "Chenliang Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21250v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21250v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21250v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on integrating language models with robot manipulation tasks, aligning visual perception with actions, and using a multi-turn visual dialogue framework, which aligns with the topics of Large Language Models (LLM) and Vision-Language Action (VLA) models.",
    "llm_cls_result": [
      "LLM",
      "VLA"
    ]
  },
  "2506.21240v1": {
    "title": "Zero-Shot Learning for Obsolescence Risk Forecasting",
    "summary": "Component obsolescence poses significant challenges in industries reliant on\nelectronic components, causing increased costs and disruptions in the security\nand availability of systems. Accurate obsolescence risk prediction is essential\nbut hindered by a lack of reliable data. This paper proposes a novel approach\nto forecasting obsolescence risk using zero-shot learning (ZSL) with large\nlanguage models (LLMs) to address data limitations by leveraging\ndomain-specific knowledge from tabular datasets. Applied to two real-world\ndatasets, the method demonstrates effective risk prediction. A comparative\nevaluation of four LLMs underscores the importance of selecting the right model\nfor specific forecasting tasks.",
    "published": "2025-06-26T13:23:57Z",
    "updated": "2025-06-26T13:23:57Z",
    "id": "2506.21240v1",
    "authors": [
      "Elie Saad",
      "Aya Mrabah",
      "Mariem Besbes",
      "Marc Zolghadri",
      "Victor Czmil",
      "Claude Baron",
      "Vincent Bourgeois"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21240v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21240v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21240v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for zero-shot learning in the context of obsolescence risk forecasting, which aligns with the 'LLM' topic. It also involves leveraging domain-specific knowledge from datasets, which is relevant to the 'Dataset' topic.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2506.21222v1": {
    "title": "Enhancing Automatic Term Extraction with Large Language Models via\n  Syntactic Retrieval",
    "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks.",
    "published": "2025-06-26T13:14:52Z",
    "updated": "2025-06-26T13:14:52Z",
    "id": "2506.21222v1",
    "authors": [
      "Yongchan Chun",
      "Minhyuk Kim",
      "Dongjun Kim",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21222v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21222v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21222v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for Automatic Term Extraction (ATE) and proposes a retrieval-based prompting strategy. The core topics are related to LLMs and their application in NLP tasks.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.21215v1": {
    "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?",
    "summary": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2.",
    "published": "2025-06-26T13:11:01Z",
    "updated": "2025-06-26T13:11:01Z",
    "id": "2506.21215v1",
    "authors": [
      "Haoang Chi",
      "He Li",
      "Wenjing Yang",
      "Feng Liu",
      "Long Lan",
      "Xiaoguang Ren",
      "Tongliang Liu",
      "Bo Han"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21215v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21215v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21215v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the causal reasoning capabilities of Large Language Models (LLMs), introduces a new benchmark for evaluating these capabilities, and proposes a method to enhance them. The core topics are related to LLMs, reasoning, and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2506.21199v1": {
    "title": "MedPrompt: LLM-CNN Fusion with Weight Routing for Medical Image\n  Segmentation and Classification",
    "summary": "Current medical image analysis systems are typically task-specific, requiring\nseparate models for classification and segmentation, and lack the flexibility\nto support user-defined workflows. To address these challenges, we introduce\nMedPrompt, a unified framework that combines a few-shot prompted Large Language\nModel (Llama-4-17B) for high-level task planning with a modular Convolutional\nNeural Network (DeepFusionLab) for low-level image processing. The LLM\ninterprets user instructions and generates structured output to dynamically\nroute task-specific pretrained weights. This weight routing approach avoids\nretraining the entire framework when adding new tasks-only task-specific\nweights are required, enhancing scalability and deployment. We evaluated\nMedPrompt across 19 public datasets, covering 12 tasks spanning 5 imaging\nmodalities. The system achieves a 97% end-to-end correctness in interpreting\nand executing prompt-driven instructions, with an average inference latency of\n2.5 seconds, making it suitable for near real-time applications. DeepFusionLab\nachieves competitive segmentation accuracy (e.g., Dice 0.9856 on lungs) and\nstrong classification performance (F1 0.9744 on tuberculosis). Overall,\nMedPrompt enables scalable, prompt-driven medical imaging by combining the\ninterpretability of LLMs with the efficiency of modular CNNs.",
    "published": "2025-06-26T12:57:41Z",
    "updated": "2025-06-26T12:57:41Z",
    "id": "2506.21199v1",
    "authors": [
      "Shadman Sobhan",
      "Kazi Abrar Mahmud",
      "Abduz Zami"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21199v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21199v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21199v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of a Large Language Model (LLM) with a Convolutional Neural Network (CNN) for medical image segmentation and classification, which involves LLM for task planning and weight routing. This aligns with the topics of LLM (Large Language Models) and MLLM (Multimodal Large Language Models) due to the multimodal nature of combining language and vision tasks.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.02932v1": {
    "title": "MolProphecy: Bridging Medicinal Chemists' Knowledge and Molecular\n  Pre-Trained Models via a Multi-Modal Framework",
    "summary": "MolProphecy is a human-in-the-loop (HITL) multi-modal framework designed to\nintegrate chemists' domain knowledge into molecular property prediction models.\nWhile molecular pre-trained models have enabled significant gains in predictive\naccuracy, they often fail to capture the tacit, interpretive reasoning central\nto expert-driven molecular design. To address this, MolProphecy employs ChatGPT\nas a virtual chemist to simulate expert-level reasoning and decision-making.\nThe generated chemist knowledge is embedded by the large language model (LLM)\nas a dedicated knowledge representation and then fused with graph-based\nmolecular features through a gated cross-attention mechanism, enabling joint\nreasoning over human-derived and structural features. Evaluated on four\nbenchmark datasets (FreeSolv, BACE, SIDER, and ClinTox), MolProphecy\noutperforms state-of-the-art (SOTA) models, achieving a 15.0 percent reduction\nin RMSE on FreeSolv and a 5.39 percent improvement in AUROC on BACE. Analysis\nreveals that chemist knowledge and structural features provide complementary\ncontributions, improving both accuracy and interpretability. MolProphecy offers\na practical and generalizable approach for collaborative drug discovery, with\nthe flexibility to incorporate real chemist input in place of the current\nsimulated proxy--without the need for model retraining. The implementation is\npublicly available at https://github.com/zhangruochi/MolProphecy.",
    "published": "2025-06-26T12:51:59Z",
    "updated": "2025-06-26T12:51:59Z",
    "id": "2507.02932v1",
    "authors": [
      "Jianping Zhao",
      "Qiong Zhou",
      "Tian Wang",
      "Yusi Fan",
      "Qian Yang",
      "Li Jiao",
      "Chang Liu",
      "Zhehao Guo",
      "Qi Lu",
      "Fengfeng Zhou",
      "Ruochi Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02932v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02932v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02932v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of chemists' domain knowledge into molecular property prediction models using a multi-modal framework that includes a large language model (LLM) for simulating expert-level reasoning. It also involves the use of benchmark datasets for evaluation.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark",
      "LLM"
    ]
  },
  "2506.21191v2": {
    "title": "Prompt-Guided Turn-Taking Prediction",
    "summary": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts.",
    "published": "2025-06-26T12:49:07Z",
    "updated": "2025-07-03T04:01:44Z",
    "id": "2506.21191v2",
    "authors": [
      "Koji Inoue",
      "Mikey Elmers",
      "Yahui Fu",
      "Zi Haur Pang",
      "Divesh Lala",
      "Keiko Ochi",
      "Tatsuya Kawahara"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21191v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21191v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21191v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on turn-taking prediction in dialogue systems using transformer-based architectures and incorporates textual prompts generated by a large language model (LLM). The core topics are related to LLM for generating synthetic prompts and the application in dialogue systems.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.21188v2": {
    "title": "GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud\n  Sequential Grounding",
    "summary": "Sequential grounding in 3D point clouds (SG3D) refers to locating sequences\nof objects by following text instructions for a daily activity with detailed\nsteps. Current 3D visual grounding (3DVG) methods treat text instructions with\nmultiple steps as a whole, without extracting useful temporal information from\neach step. However, the instructions in SG3D often contain pronouns such as\n\"it\", \"here\" and \"the same\" to make language expressions concise. This requires\ngrounding methods to understand the context and retrieve relevant information\nfrom previous steps to correctly locate object sequences. Due to the lack of an\neffective module for collecting related historical information,\nstate-of-the-art 3DVG methods face significant challenges in adapting to the\nSG3D task. To fill this gap, we propose GroundFlow -- a plug-in module for\ntemporal reasoning on 3D point cloud sequential grounding. Firstly, we\ndemonstrate that integrating GroundFlow improves the task accuracy of 3DVG\nbaseline methods by a large margin (+7.5\\% and +10.2\\%) in the SG3D benchmark,\neven outperforming a 3D large language model pre-trained on various datasets.\nFurthermore, we selectively extract both short-term and long-term step\ninformation based on its relevance to the current instruction, enabling\nGroundFlow to take a comprehensive view of historical information and maintain\nits temporal understanding advantage as step counts increase. Overall, our work\nintroduces temporal reasoning capabilities to existing 3DVG models and achieves\nstate-of-the-art performance in the SG3D benchmark across five datasets.",
    "published": "2025-06-26T12:47:33Z",
    "updated": "2025-07-22T07:31:10Z",
    "id": "2506.21188v2",
    "authors": [
      "Zijun Lin",
      "Shuting He",
      "Cheston Tan",
      "Bihan Wen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21188v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21188v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21188v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on temporal reasoning in 3D point cloud sequential grounding, which involves understanding and processing sequential text instructions and historical information. This aligns with the 'Reasoning' topic, as it involves complex problem-solving and contextual understanding. Additionally, the mention of a '3D large language model' suggests relevance to 'LLM'.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2506.21184v1": {
    "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
    "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
    "published": "2025-06-26T12:43:43Z",
    "updated": "2025-06-26T12:43:43Z",
    "id": "2506.21184v1",
    "authors": [
      "Minghao Qin",
      "Yan Shu",
      "Peitian Zhang",
      "Kun Lun",
      "Huaying Yuan",
      "Juenjie Zhou",
      "Shitao Xiao",
      "Bo Zhao",
      "Zheng Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21184v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21184v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21184v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving the efficiency of Multimodal Large Language Models (MLLMs) for long-video understanding through KV compression techniques, which directly relates to MLLM research and its optimization.",
    "llm_cls_result": [
      "MLLM",
      "Scaling"
    ]
  },
  "2506.21138v1": {
    "title": "How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets\n  for AI4RE",
    "summary": "The shortage of publicly available, labeled requirements datasets remains a\nmajor barrier to advancing Artificial Intelligence for Requirements Engineering\n(AI4RE). While Large Language Models offer promising capabilities for synthetic\ndata generation, systematic approaches to control and optimize the quality of\ngenerated requirements remain underexplored. This paper presents Synthline v1,\nan enhanced Product Line approach for generating synthetic requirements data\nthat extends our earlier v0 version with advanced generation strategies and\ncuration techniques. We investigate four research questions assessing how\nprompting strategies, automated prompt optimization, and post-generation\ncuration affect data quality across four classification tasks: defect\ndetection, functional vs. non-functional, quality vs. non-quality, and security\nvs. non-security. Our evaluation shows that multi-sample prompting\nsignificantly boosts both utility and diversity over single-sample generation,\nwith F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic\nEditing) for automated prompt optimization yields task-dependent results,\ngreatly improving functional classification (+32.5 points) but reducing\nperformance on others. Interestingly, similarity-based curation improves\ndiversity but often harms classification performance, indicating that some\nredundancy may help ML models. Most importantly, our results show that\nsynthetic requirements can match or outperform human-authored ones for specific\ntasks, with synthetic data surpassing human data for security (+7.8 points) and\ndefect classification (+15.4 points). These findings offer practical insights\nfor AI4RE and chart a viable path to mitigating dataset scarcity through\nsystematic synthetic generation.",
    "published": "2025-06-26T10:52:07Z",
    "updated": "2025-06-26T10:52:07Z",
    "id": "2506.21138v1",
    "authors": [
      "Abdelkarim El-Hajjami",
      "Camille Salinesi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21138v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21138v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21138v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for generating synthetic datasets, specifically for requirements engineering, and evaluates the quality of these datasets. It involves LLM-generated data, prompting strategies, and dataset quality, which aligns with the topics of LLM and Dataset.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2506.21116v2": {
    "title": "IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for\n  Multi-shot Scenes",
    "summary": "Video Large Language Models (VideoLLMs) have demonstrated remarkable\nunderstanding capabilities, but are found struggling to tackle multi-shot\nscenarios,e.g., video clips with varying camera angles or scene changes. This\nchallenge can render failures such as instance identity forgetting and key\nframe negligence. In this work, we first attribute the challenge to the lack of\nmulti-shot annotations among existing datasets and therefore we introduce a new\ndataset termed MultiClip-Bench, featuring dense descriptions and\ninstruction-based question-answering pairs tailored for multi-shot scenarios.\nWe empirically find that the training set significantly boosts the multi-shot\nperformance, while the testing benchmark provides a reliable measure of the\nmodel capability in multi-shot scenarios. By further analyzing and discovering\nthat current models only encode instance features in a discrete or lossy\nmanner, at the risk of missing identity information, we then contribute a new\nmodel IPFormer-VideoLLM. Its key idea is the injection of instance-level\nfeatures as instance prompts through an efficient attention-based connector.\nThis allows for the aggregation of instance-specific information across scenes.\nExperiments demonstrate that our proposed dataset and model not only enhance\nthe multi-scene video understanding significantly, but also offer distinct\nadvantages across various video benchmarks.",
    "published": "2025-06-26T09:30:57Z",
    "updated": "2025-07-08T02:46:17Z",
    "id": "2506.21116v2",
    "authors": [
      "Yujia Liang",
      "Jile Jiao",
      "Xuetao Feng",
      "Zixuan Ye",
      "Yuan Wang",
      "Zhicheng Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21116v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21116v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21116v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing multi-modal video understanding in multi-shot scenarios, which involves both dataset creation and model improvement. The key aspects include the introduction of a new dataset (MultiClip-Bench) and a new model (IPFormer-VideoLLM) that integrates instance-level features for better performance. This aligns with topics related to multimodal large language models (MLLM) and datasets (Dataset).",
    "llm_cls_result": [
      "MLLM",
      "Dataset"
    ]
  },
  "2506.21101v1": {
    "title": "OracleFusion: Assisting the Decipherment of Oracle Bone Script with\n  Structurally Constrained Semantic Typography",
    "summary": "As one of the earliest ancient languages, Oracle Bone Script (OBS)\nencapsulates the cultural records and intellectual expressions of ancient\ncivilizations. Despite the discovery of approximately 4,500 OBS characters,\nonly about 1,600 have been deciphered. The remaining undeciphered ones, with\ntheir complex structure and abstract imagery, pose significant challenges for\ninterpretation. To address these challenges, this paper proposes a novel\ntwo-stage semantic typography framework, named OracleFusion. In the first\nstage, this approach leverages the Multimodal Large Language Model (MLLM) with\nenhanced Spatial Awareness Reasoning (SAR) to analyze the glyph structure of\nthe OBS character and perform visual localization of key components. In the\nsecond stage, we introduce Oracle Structural Vector Fusion (OSVF),\nincorporating glyph structure constraints and glyph maintenance constraints to\nensure the accurate generation of semantically enriched vector fonts. This\napproach preserves the objective integrity of the glyph structure, offering\nvisually enhanced representations that assist experts in deciphering OBS.\nExtensive qualitative and quantitative experiments demonstrate that\nOracleFusion outperforms state-of-the-art baseline models in terms of\nsemantics, visual appeal, and glyph maintenance, significantly enhancing both\nreadability and aesthetic quality. Furthermore, OracleFusion provides\nexpert-like insights on unseen oracle characters, making it a valuable tool for\nadvancing the decipherment of OBS.",
    "published": "2025-06-26T08:56:07Z",
    "updated": "2025-06-26T08:56:07Z",
    "id": "2506.21101v1",
    "authors": [
      "Caoshuo Li",
      "Zengmao Ding",
      "Xiaobin Hu",
      "Bang Li",
      "Donghao Luo",
      "AndyPian Wu",
      "Chaoyang Wang",
      "Chengjie Wang",
      "Taisong Jin",
      " SevenShu",
      "Yunsheng Wu",
      "Yongge Liu",
      "Rongrong Ji"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21101v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21101v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21101v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a Multimodal Large Language Model (MLLM) with enhanced Spatial Awareness Reasoning (SAR) to assist in deciphering Oracle Bone Script (OBS). The key aspects involve multimodal integration (MLLM) and reasoning (SAR) to analyze and generate semantically enriched vector fonts.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2506.22515v1": {
    "title": "In-context learning for the classification of manipulation techniques in\n  phishing emails",
    "summary": "Traditional phishing detection often overlooks psychological manipulation.\nThis study investigates using Large Language Model (LLM) In-Context Learning\n(ICL) for fine-grained classification of phishing emails based on a taxonomy of\n40 manipulation techniques. Using few-shot examples with GPT-4o-mini on\nreal-world French phishing emails (SignalSpam), we evaluated performance\nagainst a human-annotated test set (100 emails). The approach effectively\nidentifies prevalent techniques (e.g., Baiting, Curiosity Appeal, Request For\nMinor Favor) with a promising accuracy of 0.76. This work demonstrates ICL's\npotential for nuanced phishing analysis and provides insights into attacker\nstrategies.",
    "published": "2025-06-26T08:07:30Z",
    "updated": "2025-06-26T08:07:30Z",
    "id": "2506.22515v1",
    "authors": [
      "Antony Dalmiere",
      "Guillaume Auriol",
      "Vincent Nicomette",
      "Pascal Marchand"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22515v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22515v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22515v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLM) for in-context learning to classify phishing emails, which directly relates to the LLM topic. It also involves fine-grained classification and evaluation, which can be associated with the Benchmark topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.21071v1": {
    "title": "Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge\n  Graph",
    "summary": "Teaching large language models (LLMs) to use tools is crucial for improving\ntheir problem-solving abilities and expanding their applications. However,\neffectively using tools is challenging because it requires a deep understanding\nof tool functionalities and user intentions. Previous methods relied mainly on\nLLMs to generate instruction data, but the quality of these data was often\ninsufficient. In this paper, we propose a new method that uses knowledge graphs\nto generate high-quality instruction data for LLMs. Knowledge graphs are\nmanually curated datasets rich in semantic information. We begin by extracting\nvarious query pathways from a given knowledge graph, which are transformed into\na broad spectrum of user queries. We then translate the relationships between\nentities into actionable tools and parse the pathways of each query into\ndetailed solution steps, thereby creating high-quality instruction data. Our\nexperiments show that fine-tuning on just a small sample of this synthetic data\ncan significantly improve the tool utilization and overall capabilities of\nLLMs.",
    "published": "2025-06-26T07:45:15Z",
    "updated": "2025-06-26T07:45:15Z",
    "id": "2506.21071v1",
    "authors": [
      "Jingwei Wang",
      "Zai Zhang",
      "Hao Qian",
      "Chunjing Gan",
      "Binbin Hu",
      "Ziqi Liu",
      "Zhiqiang Zhang",
      "Jun Zhou",
      "Bin Shi",
      "Bo Dong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21071v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21071v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21071v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving LLMs' tool use through high-quality instruction data generated from knowledge graphs, which directly relates to LLM research and their problem-solving abilities.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.21053v2": {
    "title": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for\n  Conversational Stance Detection",
    "summary": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection.",
    "published": "2025-06-26T06:59:30Z",
    "updated": "2025-07-04T06:30:32Z",
    "id": "2506.21053v2",
    "authors": [
      "Fuqiang Niu",
      "Genan Dai",
      "Yisha Lu",
      "Jiayu Liao",
      "Xiang Li",
      "Hu Huang",
      "Bowen Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21053v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21053v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21053v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a new dataset (MT2-CSD) for conversational stance detection and proposes a method (LLM-CRAN) that leverages LLMs for improved conversational understanding. The focus on dataset creation and the use of LLMs for reasoning align with the topics 'Dataset' and 'Reasoning'.",
    "llm_cls_result": [
      "Dataset",
      "Reasoning"
    ]
  },
  "2506.21033v1": {
    "title": "BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient\n  LLM Services",
    "summary": "The hallucination problem of Large Language Models (LLMs) has increasingly\ndrawn attention. Augmenting LLMs with external knowledge is a promising\nsolution to address this issue. However, due to privacy and security concerns,\na vast amount of downstream task-related knowledge remains dispersed and\nisolated across various \"silos,\" making it difficult to access. To bridge this\nknowledge gap, we propose a blockchain-based external knowledge framework that\ncoordinates multiple knowledge silos to provide reliable foundational knowledge\nfor large model retrieval while ensuring data security. Technically, we distill\nknowledge from local data into prompts and execute transactions and records on\nthe blockchain. Additionally, we introduce a reputation mechanism and\ncross-validation to ensure knowledge quality and provide incentives for\nparticipation. Furthermore, we design a query generation framework that\nprovides a direct API interface for large model retrieval. To evaluate the\nperformance of our proposed framework, we conducted extensive experiments on\nvarious knowledge sources. The results demonstrate that the proposed framework\nachieves efficient LLM service knowledge sharing in blockchain environments.",
    "published": "2025-06-26T06:16:33Z",
    "updated": "2025-06-26T06:16:33Z",
    "id": "2506.21033v1",
    "authors": [
      "Zhaojiacheng Zhou",
      "Hongze Liu",
      "Shijing Yuan",
      "Hanning Zhang",
      "Jiong Lou",
      "Chentao Wu",
      "Jie Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21033v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21033v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21033v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing LLMs with external knowledge using blockchain technology, addressing privacy and security concerns in knowledge sharing. The core topics involve LLM and the integration of external knowledge mechanisms, which aligns with the 'LLM' and 'Memory' categories.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.21031v1": {
    "title": "Large Language Models Acing Chartered Accountancy",
    "summary": "Advanced intelligent systems, particularly Large Language Models (LLMs), are\nsignificantly reshaping financial practices through advancements in Natural\nLanguage Processing (NLP). However, the extent to which these models\neffectively capture and apply domain-specific financial knowledge remains\nuncertain. Addressing a critical gap in the expansive Indian financial context,\nthis paper introduces CA-Ben, a Chartered Accountancy benchmark specifically\ndesigned to evaluate the financial, legal, and quantitative reasoning\ncapabilities of LLMs. CA-Ben comprises structured question-answer datasets\nderived from the rigorous examinations conducted by the Institute of Chartered\nAccountants of India (ICAI), spanning foundational, intermediate, and advanced\nCA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1\n405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated\nusing standardized protocols. Results indicate variations in performance, with\nClaude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and\nlegal reasoning. Notable challenges emerged in numerical computations and legal\ninterpretations. The findings emphasize the strengths and limitations of\ncurrent LLMs, suggesting future improvements through hybrid reasoning and\nretrieval-augmented generation methods, particularly for quantitative analysis\nand accurate legal interpretation.",
    "published": "2025-06-26T06:10:37Z",
    "updated": "2025-06-26T06:10:37Z",
    "id": "2506.21031v1",
    "authors": [
      "Jatin Gupta",
      "Akhil Sharma",
      "Saransh Singhania",
      "Mohammad Adnan",
      "Sakshi Deo",
      "Ali Imam Abidi",
      "Keshav Gupta"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21031v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21031v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21031v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on evaluating the capabilities of Large Language Models (LLMs) in the domain of financial and legal reasoning, specifically using a benchmark (CA-Ben) derived from Chartered Accountancy examinations. The study involves testing various LLMs and discusses their performance in reasoning tasks, which aligns with the 'Reasoning' and 'Benchmark' categories. Additionally, the mention of LLMs and their evaluation fits the 'LLM' category.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2506.21030v2": {
    "title": "STEP Planner: Constructing cross-hierarchical subgoal tree as an\n  embodied long-horizon task planner",
    "summary": "The ability to perform reliable long-horizon task planning is crucial for\ndeploying robots in real-world environments. However, directly employing Large\nLanguage Models (LLMs) as action sequence generators often results in low\nsuccess rates due to their limited reasoning ability for long-horizon embodied\ntasks. In the STEP framework, we construct a subgoal tree through a pair of\nclosed-loop models: a subgoal decomposition model and a leaf node termination\nmodel. Within this framework, we develop a hierarchical tree structure that\nspans from coarse to fine resolutions. The subgoal decomposition model\nleverages a foundation LLM to break down complex goals into manageable\nsubgoals, thereby spanning the subgoal tree. The leaf node termination model\nprovides real-time feedback based on environmental states, determining when to\nterminate the tree spanning and ensuring each leaf node can be directly\nconverted into a primitive action. Experiments conducted in both the\nVirtualHome WAH-NL benchmark and on real robots demonstrate that STEP achieves\nlong-horizon embodied task completion with success rates up to 34% (WAH-NL) and\n25% (real robot) outperforming SOTA methods.",
    "published": "2025-06-26T06:10:02Z",
    "updated": "2025-07-16T03:41:36Z",
    "id": "2506.21030v2",
    "authors": [
      "Tianxing Zhou",
      "Zhirui Wang",
      "Haojia Ao",
      "Guangyan Chen",
      "Boyang Xing",
      "Jingwen Cheng",
      "Yi Yang",
      "Yufeng Yue"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21030v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21030v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21030v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for long-horizon task planning in robotics, which involves reasoning and decomposition of tasks into subgoals. This aligns with the topics of LLM research and reasoning abilities in LLMs.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.21017v1": {
    "title": "Multimodal Prompt Alignment for Facial Expression Recognition",
    "summary": "Prompt learning has been widely adopted to efficiently adapt vision-language\nmodels (VLMs) like CLIP for various downstream tasks. Despite their success,\ncurrent VLM-based facial expression recognition (FER) methods struggle to\ncapture fine-grained textual-visual relationships, which are essential for\ndistinguishing subtle differences between facial expressions. To address this\nchallenge, we propose a multimodal prompt alignment framework for FER, called\nMPA-FER, that provides fine-grained semantic guidance to the learning process\nof prompted visual features, resulting in more precise and interpretable\nrepresentations. Specifically, we introduce a multi-granularity hard prompt\ngeneration strategy that utilizes a large language model (LLM) like ChatGPT to\ngenerate detailed descriptions for each facial expression. The LLM-based\nexternal knowledge is injected into the soft prompts by minimizing the feature\ndiscrepancy between the soft prompts and the hard prompts. To preserve the\ngeneralization abilities of the pretrained CLIP model, our approach\nincorporates prototype-guided visual feature alignment, ensuring that the\nprompted visual features from the frozen image encoder align closely with\nclass-specific prototypes. Additionally, we propose a cross-modal global-local\nalignment module that focuses on expression-relevant facial features, further\nimproving the alignment between textual and visual features. Extensive\nexperiments demonstrate our framework outperforms state-of-the-art methods on\nthree FER benchmark datasets, while retaining the benefits of the pretrained\nmodel and minimizing computational costs.",
    "published": "2025-06-26T05:28:57Z",
    "updated": "2025-06-26T05:28:57Z",
    "id": "2506.21017v1",
    "authors": [
      "Fuyan Ma",
      "Yiran He",
      "Bin Sun",
      "Shutao Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21017v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21017v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21017v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on multimodal learning (combining vision and language) for facial expression recognition, utilizing a large language model (LLM) for prompt generation and aligning textual and visual features. It also involves pretraining strategies and benchmark datasets for evaluation.",
    "llm_cls_result": [
      "MLLM",
      "Pretrain",
      "Benchmark"
    ]
  },
  "2506.20993v1": {
    "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs\n  with Dynamic Intensity Control",
    "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines.",
    "published": "2025-06-26T04:12:15Z",
    "updated": "2025-06-26T04:12:15Z",
    "id": "2506.20993v1",
    "authors": [
      "Adithya Chittem",
      "Aishna Shrivastava",
      "Sai Tarun Pendela",
      "Jagat Sesh Challa",
      "Dhruv Kumar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20993v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20993v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20993v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on measuring and inducing personality traits in Large Language Models (LLMs) using a dynamic intensity control framework, which is directly related to research on LLMs and their human-like interactions.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2506.20982v1": {
    "title": "Our Coding Adventure: Using LLMs to Personalise the Narrative of a\n  Tangible Programming Robot for Preschoolers",
    "summary": "Finding balanced ways to employ Large Language Models (LLMs) in education is\na challenge due to inherent risks of poor understanding of the technology and\nof a susceptible audience. This is particularly so with younger children, who\nare known to have difficulties with pervasive screen time. Working with a\ntangible programming robot called Cubetto, we propose an approach to benefit\nfrom the capabilities of LLMs by employing such models in the preparation of\npersonalised storytelling, necessary for preschool children to get accustomed\nto the practice of commanding the robot. We engage in action research to\ndevelop an early version of a formalised process to rapidly prototype game\nstories for Cubetto. Our approach has both reproducible results, because it\nemploys open weight models, and is model-agnostic, because we test it with 5\ndifferent LLMs. We document on one hand the process, the used materials and\nprompts, and on the other the learning experience and outcomes. We deem the\ngeneration successful for the intended purposes of using the results as a\nteacher aid. Testing the models on 4 different task scenarios, we encounter\nissues of consistency and hallucinations and document the corresponding\nevaluation process and attempts (some successful and some not) to overcome\nthese issues. Importantly, the process does not expose children to LLMs\ndirectly. Rather, the technology is used to help teachers easily develop\npersonalised narratives on children's preferred topics. We believe our method\nis adequate for preschool classes and we are planning to further experiment in\nreal-world educational settings.",
    "published": "2025-06-26T03:54:25Z",
    "updated": "2025-06-26T03:54:25Z",
    "id": "2506.20982v1",
    "authors": [
      "Martin Ruskov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20982v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20982v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20982v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in educational settings, specifically for personalizing narratives for preschoolers using a tangible programming robot. The focus is on leveraging LLMs for educational purposes, which aligns with the 'LLM' topic. The educational application and the challenges mentioned (consistency, hallucinations) are also relevant to the broader use of LLMs.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.20978v1": {
    "title": "Response Quality Assessment for Retrieval-Augmented Generation via\n  Conditional Conformal Factuality",
    "summary": "Existing research on Retrieval-Augmented Generation (RAG) primarily focuses\non improving overall question-answering accuracy, often overlooking the quality\nof sub-claims within generated responses. Recent methods that attempt to\nimprove RAG trustworthiness, such as through auto-evaluation metrics, lack\nprobabilistic guarantees or require ground truth answers. To address these\nlimitations, we propose Conformal-RAG, a novel framework inspired by recent\napplications of conformal prediction (CP) on large language models (LLMs).\nConformal-RAG leverages CP and internal information from the RAG mechanism to\noffer statistical guarantees on response quality. It ensures group-conditional\ncoverage spanning multiple sub-domains without requiring manual labelling of\nconformal sets, making it suitable for complex RAG applications. Compared to\nexisting RAG auto-evaluation methods, Conformal-RAG offers statistical\nguarantees on the quality of refined sub-claims, ensuring response reliability\nwithout the need for ground truth answers. Additionally, our experiments\ndemonstrate that by leveraging information from the RAG system, Conformal-RAG\nretains up to 60\\% more high-quality sub-claims from the response compared to\ndirect applications of CP to LLMs, while maintaining the same reliability\nguarantee.",
    "published": "2025-06-26T03:52:56Z",
    "updated": "2025-06-26T03:52:56Z",
    "id": "2506.20978v1",
    "authors": [
      "Naihe Feng",
      "Yi Sui",
      "Shiyi Hou",
      "Jesse C. Cresswell",
      "Ga Wu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20978v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20978v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20978v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on improving the quality and reliability of responses in Retrieval-Augmented Generation (RAG) systems, which involves memory-augmented models and retrieval-based methods. It also discusses the application of large language models (LLMs) in this context.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.02928v1": {
    "title": "Mitigating Hidden Confounding by Progressive Confounder Imputation via\n  Large Language Models",
    "summary": "Hidden confounding remains a central challenge in estimating treatment\neffects from observational data, as unobserved variables can lead to biased\ncausal estimates. While recent work has explored the use of large language\nmodels (LLMs) for causal inference, most approaches still rely on the\nunconfoundedness assumption. In this paper, we make the first attempt to\nmitigate hidden confounding using LLMs. We propose ProCI (Progressive\nConfounder Imputation), a framework that elicits the semantic and world\nknowledge of LLMs to iteratively generate, impute, and validate hidden\nconfounders. ProCI leverages two key capabilities of LLMs: their strong\nsemantic reasoning ability, which enables the discovery of plausible\nconfounders from both structured and unstructured inputs, and their embedded\nworld knowledge, which supports counterfactual reasoning under latent\nconfounding. To improve robustness, ProCI adopts a distributional reasoning\nstrategy instead of direct value imputation to prevent the collapsed outputs.\nExtensive experiments demonstrate that ProCI uncovers meaningful confounders\nand significantly improves treatment effect estimation across various datasets\nand LLMs.",
    "published": "2025-06-26T03:49:13Z",
    "updated": "2025-06-26T03:49:13Z",
    "id": "2507.02928v1",
    "authors": [
      "Hao Yang",
      "Haoxuan Li",
      "Luyu Chen",
      "Haoxiang Wang",
      "Xu Chen",
      "Mingming Gong"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02928v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02928v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02928v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) for causal inference and mitigating hidden confounding, which directly relates to the research on LLMs and their applications in reasoning and problem-solving.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.20971v1": {
    "title": "Where is AIED Headed? Key Topics and Emerging Frontiers (2020-2024)",
    "summary": "In this study, we analyze 2,398 research articles published between 2020 and\n2024 across eight core venues related to the field of Artificial Intelligence\nin Education (AIED). Using a three-step knowledge co-occurrence network\nanalysis, we analyze the knowledge structure of the field, the evolving\nknowledge clusters, and the emerging frontiers. Our findings reveal that AIED\nresearch remains strongly technically focused, with sustained themes such as\nintelligent tutoring systems, learning analytics, and natural language\nprocessing, alongside rising interest in large language models (LLMs) and\ngenerative artificial intelligence (GenAI). By tracking the bridging keywords\nover the past five years, we identify four emerging frontiers in AIED--LLMs,\nGenAI, multimodal learning analytics, and human-AI collaboration. The current\nresearch interests in GenAI are centered around GAI-driven personalization,\nself-regulated learning, feedback, assessment, motivation, and ethics.The key\nresearch interests and emerging frontiers in AIED reflect a growing emphasis on\nco-adaptive, human-centered AI for education. This study provides the first\nlarge-scale field-level mapping of AIED's transformation in the GenAI era and\nsheds light on the future research development and educational practices.",
    "published": "2025-06-26T03:24:30Z",
    "updated": "2025-06-26T03:24:30Z",
    "id": "2506.20971v1",
    "authors": [
      "Shihui Feng",
      "Huilin Zhang",
      "Dragan Gaevi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20971v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20971v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20971v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evolution of AI in education, with a focus on large language models (LLMs) and generative AI (GenAI), which are key topics in the provided list. It also mentions multimodal learning analytics, which aligns with the MLLM topic.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "AGI"
    ]
  },
  "2506.20964v1": {
    "title": "Evidence-based diagnostic reasoning with multi-agent copilot for human\n  pathology",
    "summary": "Pathology is experiencing rapid digital transformation driven by whole-slide\nimaging and artificial intelligence (AI). While deep learning-based\ncomputational pathology has achieved notable success, traditional models\nprimarily focus on image analysis without integrating natural language\ninstruction or rich, text-based context. Current multimodal large language\nmodels (MLLMs) in computational pathology face limitations, including\ninsufficient training data, inadequate support and evaluation for multi-image\nunderstanding, and a lack of autonomous, diagnostic reasoning capabilities. To\naddress these limitations, we introduce PathChat+, a new MLLM specifically\ndesigned for human pathology, trained on over 1 million diverse,\npathology-specific instruction samples and nearly 5.5 million question answer\nturns. Extensive evaluations across diverse pathology benchmarks demonstrated\nthat PathChat+ substantially outperforms the prior PathChat copilot, as well as\nboth state-of-the-art (SOTA) general-purpose and other pathology-specific\nmodels. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI\nsystem leveraging PathChat+ to autonomously evaluate gigapixel whole-slide\nimages (WSIs) through iterative, hierarchical diagnostic reasoning, reaching\nhigh accuracy on DDxBench, a challenging open-ended differential diagnosis\nbenchmark, while also capable of generating visually grounded,\nhumanly-interpretable summary reports.",
    "published": "2025-06-26T03:02:16Z",
    "updated": "2025-06-26T03:02:16Z",
    "id": "2506.20964v1",
    "authors": [
      "Chengkuan Chen",
      "Luca L. Weishaupt",
      "Drew F. K. Williamson",
      "Richard J. Chen",
      "Tong Ding",
      "Bowen Chen",
      "Anurag Vaidya",
      "Long Phi Le",
      "Guillaume Jaume",
      "Ming Y. Lu",
      "Faisal Mahmood"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20964v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20964v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20964v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a multimodal large language model (MLLM) designed for human pathology, which integrates image analysis with natural language instruction and diagnostic reasoning. It also introduces a multi-agent AI system for autonomous evaluation of whole-slide images, which aligns with the topics of MLLM and Reasoning.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning"
    ]
  },
  "2506.20963v2": {
    "title": "EraRAG: Efficient and Incremental Retrieval Augmented Generation for\n  Growing Corpora",
    "summary": "Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large\nlanguage models (LLMs) by structuring retrieval over an external corpus.\nHowever, existing approaches typically assume a static corpus, requiring\nexpensive full-graph reconstruction whenever new documents arrive, limiting\ntheir scalability in dynamic, evolving environments. To address these\nlimitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework\nthat supports efficient and scalable dynamic updates. Our method leverages\nhyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the\noriginal corpus into hierarchical graph structures, enabling efficient and\nlocalized insertions of new data without disrupting the existing topology. The\ndesign eliminates the need for retraining or costly recomputation while\npreserving high retrieval accuracy and low latency. Experiments on large-scale\nbenchmarks demonstrate that EraRag achieves up to an order of magnitude\nreduction in update time and token consumption compared to existing Graph-RAG\nsystems, while providing superior accuracy performance. This work offers a\npractical path forward for RAG systems that must operate over continually\ngrowing corpora, bridging the gap between retrieval efficiency and\nadaptability. Our code and data are available at\nhttps://github.com/EverM0re/EraRAG-Official.",
    "published": "2025-06-26T03:01:33Z",
    "updated": "2025-07-04T01:31:36Z",
    "id": "2506.20963v2",
    "authors": [
      "Fangyuan Zhang",
      "Zhengjun Huang",
      "Yingli Zhou",
      "Qintian Guo",
      "Zhixun Li",
      "Wensheng Luo",
      "Di Jiang",
      "Yixiang Fang",
      "Xiaofang Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20963v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20963v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20963v2",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing Retrieval-Augmented Generation (RAG) systems for large language models (LLMs) by introducing an efficient and scalable dynamic update mechanism. It directly relates to the 'Memory' topic due to its emphasis on retrieval-augmented generation and long-context processing. It also touches on 'LLM' as it involves large language models and their augmentation with external knowledge.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2506.22508v1": {
    "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing\n  User-generated Text",
    "summary": "In today's digital world, casual user-generated content often contains subtle\ncues that may inadvertently expose sensitive personal attributes. Such risks\nunderscore the growing importance of effective text anonymization to safeguard\nindividual privacy. However, existing methods either rely on rigid replacements\nthat damage utility or cloud-based LLMs that are costly and pose privacy risks.\nTo address these issues, we explore the use of locally deployed smaller-scale\nlanguage models (SLMs) for anonymization. Yet training effective SLMs remains\nchallenging due to limited high-quality supervision. To address the challenge,\nwe propose AgentStealth, a self-reinforcing LLM anonymization framework.First,\nwe introduce an adversarial anonymization workflow enhanced by In-context\nContrastive Learning and Adaptive Utility-Aware Control. Second, we perform\nsupervised adaptation of SLMs using high-quality data collected from the\nworkflow, which includes both anonymization and attack signals. Finally, we\napply online reinforcement learning where the model leverages its internal\nadversarial feedback to iteratively improve anonymization performance.\nExperiments on two datasets show that our method outperforms baselines in both\nanonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight\ndesign supports direct deployment on edge devices, avoiding cloud reliance and\ncommunication-based privacy risks. Our code is open-source at\nhttps://github.com/tsinghua-fib-lab/AgentStealth.",
    "published": "2025-06-26T02:48:16Z",
    "updated": "2025-06-26T02:48:16Z",
    "id": "2506.22508v1",
    "authors": [
      "Chenyang Shao",
      "Tianxing Li",
      "Chenhao Pu",
      "Fengli Xu",
      "Yong Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22508v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22508v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22508v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of locally deployed smaller-scale language models (SLMs) for anonymization, which involves reinforcement learning (RL) to improve anonymization performance. The focus on LLMs and RLHF (Reinforcement Learning with Human Feedback) aligns with the topics of LLM and RL.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.20947v1": {
    "title": "Hierarchical Sub-action Tree for Continuous Sign Language Recognition",
    "summary": "Continuous sign language recognition (CSLR) aims to transcribe untrimmed\nvideos into glosses, which are typically textual words. Recent studies indicate\nthat the lack of large datasets and precise annotations has become a bottleneck\nfor CSLR due to insufficient training data. To address this, some works have\ndeveloped cross-modal solutions to align visual and textual modalities.\nHowever, they typically extract textual features from glosses without fully\nutilizing their knowledge. In this paper, we propose the Hierarchical\nSub-action Tree (HST), termed HST-CSLR, to efficiently combine gloss knowledge\nwith visual representation learning. By incorporating gloss-specific knowledge\nfrom large language models, our approach leverages textual information more\neffectively. Specifically, we construct an HST for textual information\nrepresentation, aligning visual and textual modalities step-by-step and\nbenefiting from the tree structure to reduce computational complexity.\nAdditionally, we impose a contrastive alignment enhancement to bridge the gap\nbetween the two modalities. Experiments on four datasets (PHOENIX-2014,\nPHOENIX-2014T, CSL-Daily, and Sign Language Gesture) demonstrate the\neffectiveness of our HST-CSLR.",
    "published": "2025-06-26T02:27:50Z",
    "updated": "2025-06-26T02:27:50Z",
    "id": "2506.20947v1",
    "authors": [
      "Dejie Yang",
      "Zhu Xu",
      "Xinjie Gao",
      "Yang Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20947v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20947v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20947v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on continuous sign language recognition (CSLR) by aligning visual and textual modalities, leveraging large language models for textual information representation, and enhancing cross-modal alignment. The core topics are related to multimodal large language models and vision-language alignment.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2506.20944v1": {
    "title": "E-FreeM2: Efficient Training-Free Multi-Scale and Cross-Modal News\n  Verification via MLLMs",
    "summary": "The rapid spread of misinformation in mobile and wireless networks presents\ncritical security challenges. This study introduces a training-free,\nretrieval-based multimodal fact verification system that leverages pretrained\nvision-language models and large language models for credibility assessment. By\ndynamically retrieving and cross-referencing trusted data sources, our approach\nmitigates vulnerabilities of traditional training-based models, such as\nadversarial attacks and data poisoning. Additionally, its lightweight design\nenables seamless edge device integration without extensive on-device\nprocessing. Experiments on two fact-checking benchmarks achieve SOTA results,\nconfirming its effectiveness in misinformation detection and its robustness\nagainst various attack vectors, highlighting its potential to enhance security\nin mobile and wireless communication environments.",
    "published": "2025-06-26T02:20:45Z",
    "updated": "2025-06-26T02:20:45Z",
    "id": "2506.20944v1",
    "authors": [
      "Van-Hoang Phan",
      "Long-Khanh Pham",
      "Dang Vu",
      "Anh-Duy Tran",
      "Minh-Son Dao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20944v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20944v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20944v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a training-free, retrieval-based multimodal fact verification system that leverages pretrained vision-language models and large language models, which aligns with the topics of MLLM (Multimodal Large Language Models) and VLA (Vision-Language Alignment models). Additionally, the use of retrieval-based methods for credibility assessment relates to the Memory topic.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Memory"
    ]
  },
  "2506.20941v1": {
    "title": "Model State Arithmetic for Machine Unlearning",
    "summary": "Large language models are trained on massive corpora of web data, which may\ninclude private data, copyrighted material, factually inaccurate data, or data\nthat degrades model performance. Eliminating the influence of such problematic\ndatapoints through complete retraining -- by repeatedly pretraining the model\non datasets that exclude these specific instances -- is computationally\nprohibitive. For this reason, unlearning algorithms have emerged that aim to\neliminate the influence of particular datapoints, while otherwise preserving\nthe model -- at a low computational cost. However, precisely estimating and\nundoing the influence of individual datapoints has proved to be challenging. In\nthis work, we propose a new algorithm, MSA, for estimating and undoing the\ninfluence of datapoints -- by leveraging model checkpoints i.e. artifacts\ncapturing model states at different stages of pretraining. Our experimental\nresults demonstrate that MSA consistently outperforms existing machine\nunlearning algorithms across multiple benchmarks, models, and evaluation\nmetrics, suggesting that MSA could be an effective approach towards more\nflexible large language models that are capable of data erasure.",
    "published": "2025-06-26T02:16:16Z",
    "updated": "2025-06-26T02:16:16Z",
    "id": "2506.20941v1",
    "authors": [
      "Keivan Rezaei",
      "Mehrdad Saberi",
      "Abhilasha Ravichander",
      "Soheil Feizi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20941v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20941v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20941v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses machine unlearning algorithms for large language models, focusing on eliminating the influence of problematic datapoints without complete retraining. This aligns with topics related to LLM (Large Language Models) and Pretrain (pretraining strategies).",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2506.20938v1": {
    "title": "ParEval-Repo: A Benchmark Suite for Evaluating LLMs with\n  Repository-level HPC Translation Tasks",
    "summary": "GPGPU architectures have become significantly diverse in recent years, which\nhas led to an emergence of a variety of specialized programming models and\nsoftware stacks to support them. While portable execution models exist, they\nstill require significant developer effort to port to and optimize for\ndifferent hardware architectures. Recent advances in large language models\n(LLMs) can help us reduce some of this programmer burden. In this paper, we\npresent a novel benchmark and testing framework, ParEval-Repo, which can be\nused to evaluate the efficacy of LLM-based approaches in automatically\ntranslating entire codebases across GPGPU execution models. ParEval-Repo\nincludes several scientific computing and AI mini-applications in a range of\nprogramming models, and levels of repository complexity. We use ParEval-Repo to\nevaluate a range of state-of-the-art open-source and commercial LLMs, with both\na non-agentic and a top-down agentic approach. We assess code generated by the\nLLMs and approaches in terms of compilability, functional correctness,\ncategories of build errors, and the cost of translation in terms of the number\nof inference tokens. Our results demonstrate that LLM translation of scientific\napplications is feasible for small programs but difficulty with generating\nfunctional build systems and cross-file dependencies pose challenges in scaling\nto larger codebases.",
    "published": "2025-06-26T02:01:11Z",
    "updated": "2025-06-26T02:01:11Z",
    "id": "2506.20938v1",
    "authors": [
      "Joshua H. Davis",
      "Daniel Nichols",
      "Ishan Khillan",
      "Abhinav Bhatele"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20938v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20938v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20938v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating LLMs for code translation tasks, specifically in the context of GPGPU architectures, which aligns with benchmarking LLMs and their applications in code-related tasks.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2507.02927v1": {
    "title": "A Unified Speech LLM for Diarization and Speech Recognition in\n  Multilingual Conversations",
    "summary": "Speech Large Language Models (Speech LLMs) have emerged as a crucial paradigm\nin recent years, extending the capabilities of traditional LLMs to speech tasks\nsuch as automatic speech recognition (ASR) and spoken dialogue modeling.\nHowever, their effectiveness in real-world multilingual conversations remains\nlimited by the scarcity of data that captures natural conversational phenomena.\nTo address this, the MLC-SLM Challenge provides a multilingual conversational\ndataset and evaluates models on two tasks: ASR with oracle segmentation (Task\nI) and joint diarization and recognition without oracle information (Task II).\nIn this paper, we focus on Task II and propose a unified speech LLM that\njointly performs diarization and ASR in an end-to-end manner. By reformulating\nthe training data format and modifying the inference procedure, our model\naddresses the ambiguity inherent in pre-segmented audio and achieves a 54.87\\%\nrelative improvement in tcpWER/tcpCER over the baseline, ranking 8th overall,\ndespite using a smaller LLM backbone. We also report results from Task I using\na fine-tuned speech LLM.",
    "published": "2025-06-26T01:54:02Z",
    "updated": "2025-06-26T01:54:02Z",
    "id": "2507.02927v1",
    "authors": [
      "Phurich Saengthong",
      "Boonnithi Jiaramaneepinit",
      "Sheng Li",
      "Manabu Okumura",
      "Takahiro Shinozaki"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02927v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02927v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02927v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a Speech Large Language Model (Speech LLM) that extends traditional LLM capabilities to speech tasks, specifically in multilingual conversations. It focuses on joint diarization and speech recognition, which involves multimodal processing (speech and language) and leverages a multilingual dataset. The core topics are related to Multimodal Large Language Models (MLLM) and the use of datasets for evaluation.",
    "llm_cls_result": [
      "MLLM",
      "Dataset"
    ]
  },
  "2506.20921v1": {
    "title": "LLM-guided Chemical Process Optimization with a Multi-Agent Approach",
    "summary": "Chemical process optimization is crucial to maximize production efficiency\nand economic performance. Traditional methods, including gradient-based\nsolvers, evolutionary algorithms, and parameter grid searches, become\nimpractical when operating constraints are ill-defined or unavailable,\nrequiring engineers to rely on subjective heuristics to estimate feasible\nparameter ranges. To address this constraint definition bottleneck, we present\na multi-agent framework of large language model (LLM) agents that autonomously\ninfer operating constraints from minimal process descriptions, then\ncollaboratively guide optimization using the inferred constraints. Our\nAutoGen-based agentic framework employs OpenAI's o3 model, with specialized\nagents for constraint generation, parameter validation, simulation execution,\nand optimization guidance. Through two phases - autonomous constraint\ngeneration using embedded domain knowledge, followed by iterative multi-agent\noptimization - the framework eliminates the need for predefined operational\nbounds. Validated on the hydrodealkylation process across cost, yield, and\nyield-to-cost ratio metrics, the framework demonstrated competitive performance\nwith conventional optimization methods while achieving better computational\nefficiency, requiring fewer iterations to converge. Our approach converged in\nunder 20 minutes, achieving a 31-fold speedup over grid search. Beyond\ncomputational efficiency, the framework's reasoning-guided search demonstrates\nsophisticated process understanding, correctly identifying utility trade-offs,\nand applying domain-informed heuristics. This approach shows significant\npotential for optimization scenarios where operational constraints are poorly\ncharacterized or unavailable, particularly for emerging processes and retrofit\napplications.",
    "published": "2025-06-26T01:03:44Z",
    "updated": "2025-06-26T01:03:44Z",
    "id": "2506.20921v1",
    "authors": [
      "Tong Zeng",
      "Srivathsan Badrinarayanan",
      "Janghoon Ock",
      "Cheng-Kai Lai",
      "Amir Barati Farimani"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20921v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20921v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20921v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in a multi-agent framework for chemical process optimization, which involves reasoning and autonomous constraint generation. The topics 'LLM' and 'Reasoning' are directly relevant, and 'AGI' is also relevant due to the mention of evolutionary algorithms and the broader application of AI in optimization scenarios.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "AGI"
    ]
  },
  "2506.20920v1": {
    "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data\n  Processing to Every Language",
    "summary": "Pre-training state-of-the-art large language models (LLMs) requires vast\namounts of clean and diverse text data. While the open development of large\nhigh-quality English pre-training datasets has seen substantial recent\nprogress, training performant multilingual LLMs remains a challenge, in large\npart due to the inherent difficulty of tailoring filtering and deduplication\npipelines to a large number of languages. In this work, we introduce a new\npre-training dataset curation pipeline based on FineWeb that can be\nautomatically adapted to support any language. We extensively ablate our\npipeline design choices on a set of nine diverse languages, guided by a set of\nmeaningful and informative evaluation tasks that were chosen through a novel\nselection process based on measurable criteria. Ultimately, we show that our\npipeline can be used to create non-English corpora that produce more performant\nmodels than prior datasets. We additionally introduce a straightforward and\nprincipled approach to rebalance datasets that takes into consideration both\nduplication count and quality, providing an additional performance uplift.\nFinally, we scale our pipeline to over 1000 languages using almost 100 Common\nCrawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)\nmultilingual dataset which we release along with our pipeline, training, and\nevaluation codebases.",
    "published": "2025-06-26T01:01:47Z",
    "updated": "2025-06-26T01:01:47Z",
    "id": "2506.20920v1",
    "authors": [
      "Guilherme Penedo",
      "Hynek Kydlek",
      "Vinko Sabolec",
      "Bettina Messmer",
      "Negar Foroutan",
      "Amir Hossein Kargaran",
      "Colin Raffel",
      "Martin Jaggi",
      "Leandro Von Werra",
      "Thomas Wolf"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20920v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20920v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20920v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the creation and adaptation of pre-training datasets for multilingual large language models (LLMs), which involves data processing, filtering, and deduplication pipelines. This aligns with the topics of 'Pretrain' (pretraining strategies and objectives) and 'Dataset' (LLM datasets and evaluation datasets). The mention of scaling the pipeline to over 1000 languages also hints at 'Scaling' (scaling laws and model capacity), though the primary focus is on dataset creation and adaptation.",
    "llm_cls_result": [
      "Pretrain",
      "Dataset",
      "Scaling"
    ]
  },
  "2506.20918v1": {
    "title": "Metadata Enrichment of Long Text Documents using Large Language Models",
    "summary": "In this project, we semantically enriched and enhanced the metadata of long\ntext documents, theses and dissertations, retrieved from the HathiTrust Digital\nLibrary in English published from 1920 to 2020 through a combination of manual\nefforts and large language models. This dataset provides a valuable resource\nfor advancing research in areas such as computational social science, digital\nhumanities, and information science. Our paper shows that enriching metadata\nusing LLMs is particularly beneficial for digital repositories by introducing\nadditional metadata access points that may not have originally been foreseen to\naccommodate various content types. This approach is particularly effective for\nrepositories that have significant missing data in their existing metadata\nfields, enhancing search results and improving the accessibility of the digital\nrepository.",
    "published": "2025-06-26T00:55:47Z",
    "updated": "2025-06-26T00:55:47Z",
    "id": "2506.20918v1",
    "authors": [
      "Manika Lamba",
      "You Peng",
      "Sophie Nikolov",
      "Glen Layne-Worthey",
      "J. Stephen Downie"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20918v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20918v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20918v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for enriching metadata of long text documents, which directly relates to the application of LLMs in processing and enhancing textual data.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2506.20915v1": {
    "title": "ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large\n  Language Models",
    "summary": "As the deployment of large language models (LLMs) grows in sensitive domains,\nensuring the integrity of their computational provenance becomes a critical\nchallenge, particularly in regulated sectors such as healthcare, where strict\nrequirements are applied in dataset usage. We introduce ZKPROV, a novel\ncryptographic framework that enables zero-knowledge proofs of LLM provenance.\nIt allows users to verify that a model is trained on a reliable dataset without\nrevealing sensitive information about it or its parameters. Unlike prior\napproaches that focus on complete verification of the training process\n(incurring significant computational cost) or depend on trusted execution\nenvironments, ZKPROV offers a distinct balance. Our method cryptographically\nbinds a trained model to its authorized training dataset(s) through\nzero-knowledge proofs while avoiding proof of every training step. By\nleveraging dataset-signed metadata and compact model parameter commitments,\nZKPROV provides sound and privacy-preserving assurances that the result of the\nLLM is derived from a model trained on the claimed authorized and relevant\ndataset. Experimental results demonstrate the efficiency and scalability of the\nZKPROV in generating this proof and verifying it, achieving a practical\nsolution for real-world deployments. We also provide formal security\nguarantees, proving that our approach preserves dataset confidentiality while\nensuring trustworthy dataset provenance.",
    "published": "2025-06-26T00:49:02Z",
    "updated": "2025-06-26T00:49:02Z",
    "id": "2506.20915v1",
    "authors": [
      "Mina Namazi",
      "Alexander Nemecek",
      "Erman Ayday"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20915v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20915v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20915v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on ensuring the integrity of dataset provenance for large language models (LLMs) using a cryptographic framework, which is relevant to the topics of LLMs and datasets.",
    "llm_cls_result": [
      "LLM",
      "Dataset"
    ]
  },
  "2506.20911v1": {
    "title": "FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient\n  Multi-turn Image Editing",
    "summary": "We develop a cost-efficient neurosymbolic agent to address challenging\nmulti-turn image editing tasks such as \"Detect the bench in the image while\nrecoloring it to pink. Also, remove the cat for a clearer view and recolor the\nwall to yellow.'' It combines the fast, high-level subtask planning by large\nlanguage models (LLMs) with the slow, accurate, tool-use, and local A$^*$\nsearch per subtask to find a cost-efficient toolpath -- a sequence of calls to\nAI tools. To save the cost of A$^*$ on similar subtasks, we perform inductive\nreasoning on previously successful toolpaths via LLMs to continuously\nextract/refine frequently used subroutines and reuse them as new tools for\nfuture tasks in an adaptive fast-slow planning, where the higher-level\nsubroutines are explored first, and only when they fail, the low-level A$^*$\nsearch is activated. The reusable symbolic subroutines considerably save\nexploration cost on the same types of subtasks applied to similar images,\nyielding a human-like fast-slow toolpath agent \"FaSTA$^*$'': fast subtask\nplanning followed by rule-based subroutine selection per subtask is attempted\nby LLMs at first, which is expected to cover most tasks, while slow A$^*$\nsearch is only triggered for novel and challenging subtasks. By comparing with\nrecent image editing approaches, we demonstrate FaSTA$^*$ is significantly more\ncomputationally efficient while remaining competitive with the state-of-the-art\nbaseline in terms of success rate.",
    "published": "2025-06-26T00:33:43Z",
    "updated": "2025-06-26T00:33:43Z",
    "id": "2506.20911v1",
    "authors": [
      "Advait Gupta",
      "Rishie Raj",
      "Dang Nguyen",
      "Tianyi Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20911v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20911v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20911v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a neurosymbolic agent that combines LLMs for high-level subtask planning with local search for efficient multi-turn image editing. It involves the use of LLMs for planning and reasoning, and focuses on computational efficiency and cost-saving strategies.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "AGI"
    ]
  },
  "2507.02925v1": {
    "title": "Large Language Model Agent for Modular Task Execution in Drug Discovery",
    "summary": "We present a modular framework powered by large language models (LLMs) that\nautomates and streamlines key tasks across the early-stage computational drug\ndiscovery pipeline. By combining LLM reasoning with domain-specific tools, the\nframework performs biomedical data retrieval, domain-specific question\nanswering, molecular generation, property prediction, property-aware molecular\nrefinement, and 3D protein-ligand structure generation. In a case study\ntargeting BCL-2 in lymphocytic leukemia, the agent autonomously retrieved\nrelevant biomolecular information-including FASTA sequences, SMILES\nrepresentations, and literature-and answered mechanistic questions with\nimproved contextual accuracy over standard LLMs. It then generated chemically\ndiverse seed molecules and predicted 67 ADMET-related properties, which guided\niterative molecular refinement. Across two refinement rounds, the number of\nmolecules with QED > 0.6 increased from 34 to 55, and those passing at least\nfour out of five empirical drug-likeness rules rose from 29 to 52, within a\npool of 194 molecules. The framework also employed Boltz-2 to generate 3D\nprotein-ligand complexes and provide rapid binding affinity estimates for\ncandidate compounds. These results demonstrate that the approach effectively\nsupports molecular screening, prioritization, and structure evaluation. Its\nmodular design enables flexible integration of evolving tools and models,\nproviding a scalable foundation for AI-assisted therapeutic discovery.",
    "published": "2025-06-26T00:19:01Z",
    "updated": "2025-06-26T00:19:01Z",
    "id": "2507.02925v1",
    "authors": [
      "Janghoon Ock",
      "Radheesh Sharma Meda",
      "Srivathsan Badrinarayanan",
      "Neha S. Aluru",
      "Achuth Chandrasekhar",
      "Amir Barati Farimani"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02925v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02925v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02925v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for automating tasks in drug discovery, which involves LLM reasoning and domain-specific applications. The core topics are LLM for its use of large language models, Reasoning for the application of LLM reasoning in drug discovery tasks, and AGI for the broader implications of AI in therapeutic discovery.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "AGI"
    ]
  },
  "2506.20886v1": {
    "title": "Omniwise: Predicting GPU Kernels Performance with LLMs",
    "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.",
    "published": "2025-06-25T23:36:44Z",
    "updated": "2025-06-25T23:36:44Z",
    "id": "2506.20886v1",
    "authors": [
      "Zixian Wang",
      "Cole Ramos",
      "Muhammad A. Awad",
      "Keith Lowery"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20886v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20886v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20886v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models (LLMs) to predict GPU kernel performance, which involves using LLMs for a novel task in performance profiling. The core focus is on LLMs and their application in a specific domain, making 'LLM' the most relevant topic. The other topics do not directly apply as the paper does not focus on reinforcement learning, multimodal models, or other listed areas.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.20869v1": {
    "title": "Engineering RAG Systems for Real-World Applications: Design,\n  Development, and Evaluation",
    "summary": "Retrieval-Augmented Generation (RAG) systems are emerging as a key approach\nfor grounding Large Language Models (LLMs) in external knowledge, addressing\nlimitations in factual accuracy and contextual relevance. However, there is a\nlack of empirical studies that report on the development of RAG-based\nimplementations grounded in real-world use cases, evaluated through general\nuser involvement, and accompanied by systematic documentation of lessons\nlearned. This paper presents five domain-specific RAG applications developed\nfor real-world scenarios across governance, cybersecurity, agriculture,\nindustrial research, and medical diagnostics. Each system incorporates\nmultilingual OCR, semantic retrieval via vector embeddings, and domain-adapted\nLLMs, deployed through local servers or cloud APIs to meet distinct user needs.\nA web-based evaluation involving a total of 100 participants assessed the\nsystems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)\nTransparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of\nRecommendation. Based on user feedback and our development experience, we\ndocumented twelve key lessons learned, highlighting technical, operational, and\nethical challenges affecting the reliability and usability of RAG systems in\npractice.",
    "published": "2025-06-25T22:40:00Z",
    "updated": "2025-06-25T22:40:00Z",
    "id": "2506.20869v1",
    "authors": [
      "Md Toufique Hasan",
      "Muhammad Waseem",
      "Kai-Kristian Kemell",
      "Ayman Asad Khan",
      "Mika Saari",
      "Pekka Abrahamsson"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20869v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20869v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20869v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses Retrieval-Augmented Generation (RAG) systems, which are a type of memory-augmented models, and their application in various domains using Large Language Models (LLMs). The focus on RAG systems and their evaluation aligns with the topics of Memory and LLM.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2506.20856v1": {
    "title": "Leaner Training, Lower Leakage: Revisiting Memorization in LLM\n  Fine-Tuning with LoRA",
    "summary": "Memorization in large language models (LLMs) makes them vulnerable to data\nextraction attacks. While pre-training memorization has been extensively\nstudied, fewer works have explored its impact in fine-tuning, particularly for\nLoRA fine-tuning, a widely adopted parameter-efficient method.\n  In this work, we re-examine memorization in fine-tuning and uncover a\nsurprising divergence from prior findings across different fine-tuning\nstrategies. Factors such as model scale and data duplication, which strongly\ninfluence memorization in pre-training and full fine-tuning, do not follow the\nsame trend in LoRA fine-tuning. Using a more relaxed similarity-based\nmemorization metric, we demonstrate that LoRA significantly reduces\nmemorization risks compared to full fine-tuning, while still maintaining strong\ntask performance.",
    "published": "2025-06-25T22:01:25Z",
    "updated": "2025-06-25T22:01:25Z",
    "id": "2506.20856v1",
    "authors": [
      "Fei Wang",
      "Baochun Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20856v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20856v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20856v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses memorization in large language models (LLMs) during fine-tuning, specifically focusing on LoRA fine-tuning, which is a parameter-efficient method. It highlights the differences in memorization trends between pre-training, full fine-tuning, and LoRA fine-tuning, and evaluates the risks and performance of these methods.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.20822v1": {
    "title": "Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via\n  Behavioral Vignettes",
    "summary": "Large language models (LLMs) are increasingly proposed for detecting and\nresponding to violent content online, yet their ability to reason about morally\nambiguous, real-world scenarios remains underexamined. We present the first\nstudy to evaluate LLMs using a validated social science instrument designed to\nmeasure human response to everyday conflict, namely the Violent Behavior\nVignette Questionnaire (VBVQ). To assess potential bias, we introduce\npersona-based prompting that varies race, age, and geographic identity within\nthe United States. Six LLMs developed across different geopolitical and\norganizational contexts are evaluated under a unified zero-shot setting. Our\nstudy reveals two key findings: (1) LLMs surface-level text generation often\ndiverges from their internal preference for violent responses; (2) their\nviolent tendencies vary across demographics, frequently contradicting\nestablished findings in criminology, social science, and psychology.",
    "published": "2025-06-25T20:43:04Z",
    "updated": "2025-06-25T20:43:04Z",
    "id": "2506.20822v1",
    "authors": [
      "Quintin Myers",
      "Yanjun Gao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20822v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20822v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20822v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating LLMs' responses to violent content and their biases across demographics, which involves understanding LLM behavior and reasoning in morally ambiguous scenarios. This aligns with the topics of LLM (Large Language Models) and Reasoning (LLM reasoning abilities).",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.00054v1": {
    "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset\n  Distillation",
    "summary": "The push to compress and impart the proficiency of Large Language Models\n(LLMs) into more deployable and efficient Small Language Models (SLMs) has\nbenefited from improvements in knowledge distillation (KD) techniques. These\ntechniques allow a smaller student model to learn from a more capable and\nlarger teacher model's responses. However, distillation often revolves around\nthe student model merely copying the teacher's in-distribution responses,\nlimiting its generalisability. This limitation is amplified on reasoning tasks\nand can be computationally expensive. In this study, we propose AdvDistill, a\nreward-guided dataset distillation framework. We utilise multiple generations\n(responses) from a teacher for each prompt and assign rewards based on\nrule-based verifiers. These varying and normally distributed rewards serve as\nweights when training student models. Our methods and their subsequent\nbehavioural analysis demonstrate a significant improvement in student model\nperformance for mathematical and complex reasoning tasks, showcasing the\nefficacy and benefits of incorporating a rewarding mechanism in dataset\ndistillation processes.",
    "published": "2025-06-25T20:07:47Z",
    "updated": "2025-06-25T20:07:47Z",
    "id": "2507.00054v1",
    "authors": [
      "Shreyansh Padarha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00054v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00054v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00054v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing reasoning capabilities in Small Language Models (SLMs) through a reward-guided dataset distillation framework, which involves knowledge distillation from Large Language Models (LLMs) and improving reasoning tasks. The core topics are Reasoning (as it deals with improving reasoning abilities), LLM (since it involves knowledge distillation from LLMs), and RL (due to the use of reward mechanisms).",
    "llm_cls_result": [
      "Reasoning",
      "LLM",
      "RL"
    ]
  },
  "2506.20806v1": {
    "title": "Poster: Enhancing GNN Robustness for Network Intrusion Detection via\n  Agent-based Analysis",
    "summary": "Graph Neural Networks (GNNs) show great promise for Network Intrusion\nDetection Systems (NIDS), particularly in IoT environments, but suffer\nperformance degradation due to distribution drift and lack robustness against\nrealistic adversarial attacks. Current robustness evaluations often rely on\nunrealistic synthetic perturbations and lack demonstrations on systematic\nanalysis of different kinds of adversarial attack, which encompass both\nblack-box and white-box scenarios. This work proposes a novel approach to\nenhance GNN robustness and generalization by employing Large Language Models\n(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These\nagents scrutinize graph structures derived from network flow data, identifying\nand potentially mitigating suspicious or adversarially perturbed elements\nbefore GNN processing. Our experiments, using a framework designed for\nrealistic evaluation and testing with a variety of adversarial attacks\nincluding a dataset collected from physical testbed experiments, demonstrate\nthat integrating LLM analysis can significantly improve the resilience of\nGNN-based NIDS against challenges, showcasing the potential of LLM agent as a\ncomplementary layer in intrusion detection architectures.",
    "published": "2025-06-25T19:49:55Z",
    "updated": "2025-06-25T19:49:55Z",
    "id": "2506.20806v1",
    "authors": [
      "Zhonghao Zhan",
      "Huichi Zhou",
      "Hamed Haddadi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20806v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20806v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20806v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in enhancing the robustness of Graph Neural Networks (GNNs) for network intrusion detection, which involves LLMs acting as simulated cybersecurity expert agents. This aligns with the topic of LLM (Large Language Models) and their applications.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.20805v1": {
    "title": "Predicting Readiness to Engage in Psychotherapy of People with Chronic\n  Pain Based on their Pain-Related Narratives Saar",
    "summary": "Background. Chronic pain afflicts 20 % of the global population. A strictly\nbiomedical mind-set leaves many sufferers chasing somatic cures and has fuelled\nthe opioid crisis. The biopsychosocial model recognises pain subjective,\nmultifactorial nature, yet uptake of psychosocial care remains low. We\nhypothesised that patients own pain narratives would predict their readiness to\nengage in psychotherapy.\n  Methods. In a cross-sectional pilot, 24 chronic-pain patients recorded\nnarrated pain stories on Painstory.science. Open questions probed perceived\npain source, interference and influencing factors. Narratives were cleaned,\nembedded with a pretrained large-language model and entered into\nmachine-learning classifiers that output ready/not ready probabilities.\n  Results. The perception-domain model achieved 95.7 % accuracy (specificity =\n0.80, sensitivity = 1.00, AUC = 0.90). The factors-influencing-pain model\nyielded 83.3 % accuracy (specificity = 0.60, sensitivity = 0.90, AUC = 0.75).\nSentence count correlated with readiness for perception narratives (r = 0.54, p\n< .01) and factor narratives (r = 0.24, p < .05).\n  Conclusion. Brief spoken pain narratives carry reliable signals of\nwillingness to start psychosocial treatment. NLP-based screening could help\nclinicians match chronic-pain patients to appropriate interventions sooner,\nsupporting a patient-centred biopsychosocial pathway.",
    "published": "2025-06-25T19:47:37Z",
    "updated": "2025-06-25T19:47:37Z",
    "id": "2506.20805v1",
    "authors": [
      "Saar Draznin Shiran",
      "Boris Boltyansky",
      "Alexandra Zhuravleva",
      "Dmitry Scherbakov",
      "Pavel Goldstein"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20805v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20805v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20805v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a pretrained large-language model to analyze pain-related narratives for predicting readiness to engage in psychotherapy. While it involves NLP and machine learning, the core topics are not directly related to the provided categories.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.20803v1": {
    "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus\n  Human Research Ideas",
    "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes.",
    "published": "2025-06-25T19:47:23Z",
    "updated": "2025-06-25T19:47:23Z",
    "id": "2506.20803v1",
    "authors": [
      "Chenglei Si",
      "Tatsunori Hashimoto",
      "Diyi Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20803v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20803v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20803v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the evaluation of LLM-generated research ideas compared to human-generated ideas, focusing on the execution outcomes and the limitations of LLMs in generating effective research ideas. This aligns with the topics of LLM (Large Language Models) and Benchmark (evaluation of LLM performance).",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.20793v1": {
    "title": "Multi-lingual Functional Evaluation for Large Language Models",
    "summary": "Multi-lingual competence in large language models is often evaluated via\nstatic data benchmarks such as Belebele, M-MMLU and M-GSM. However, these\nevaluations often fail to provide an adequate understanding of the practical\nperformance and robustness of models across multi-lingual settings. In\nresponse, we create multi-lingual functional benchmarks -- Cross-Lingual Grade\nSchool Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following\nEval (CL-IFEval)-- by translating existing functional benchmark templates from\nEnglish to five additional languages that span the range of resources available\nfor NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that\nsome static multi-lingual benchmarks capture functional performance much more\nclosely than others (i.e. across models, there is a 24%, 17% and 18% decrease\nin performance between M-GSM and CL-GSM Symbolic in English, French and Spanish\nrespectively; similarly there's a 15 - 24% performance drop across languages\nbetween Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between\nM-MMLU and CL-IFEval). Similarly, we find that model robustness across\nlanguages varies significantly, with certain languages (eg. Arabic, English)\nbeing the most consistently well performing across evaluation iterations.",
    "published": "2025-06-25T19:32:31Z",
    "updated": "2025-06-25T19:32:31Z",
    "id": "2506.20793v1",
    "authors": [
      "Victor Ojewale",
      "Inioluwa Deborah Raji",
      "Suresh Venkatasubramanian"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20793v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20793v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20793v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating the multi-lingual competence of large language models through functional benchmarks, which aligns with the 'Benchmark' category. It also discusses the performance and robustness of models across different languages, which is relevant to 'LLM' as it pertains to the capabilities and evaluation of large language models.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2506.20752v1": {
    "title": "Characterization and Mitigation of Training Instabilities in\n  Microscaling Formats",
    "summary": "Training large language models is an expensive, compute-bound process that\nmust be repeated as models scale, algorithms improve, and new data is\ncollected. To address this, next-generation hardware accelerators increasingly\nsupport lower-precision arithmetic formats, such as the Microscaling (MX)\nformats introduced in NVIDIA's Blackwell architecture. These formats use a\nshared scale within blocks of parameters to extend representable range and\nperform forward/backward GEMM operations in reduced precision for efficiency\ngains. In this work, we investigate the challenges and viability of\nblock-scaled precision formats during model training. Across nearly one\nthousand language models trained from scratch -- spanning compute budgets from\n$2 \\times 10^{17}$ to $4.8 \\times 10^{19}$ FLOPs and sweeping over a broad\nrange of weight-activation precision combinations -- we consistently observe\nthat training in MX formats exhibits sharp, stochastic instabilities in the\nloss, particularly at larger compute scales. To explain this phenomenon, we\nconduct controlled experiments and ablations on a smaller proxy model that\nexhibits similar behavior as the language model, sweeping across architectural\nsettings, hyperparameters, and precision formats. These experiments motivate a\nsimple model in which multiplicative gradient bias introduced by the\nquantization of layer-norm affine parameters and a small fraction of\nactivations can trigger runaway divergence. Through \\emph{in situ} intervention\nexperiments on our proxy model, we demonstrate that instabilities can be\naverted or delayed by modifying precision schemes mid-training. Guided by these\nfindings, we evaluate stabilization strategies in the LLM setting and show that\ncertain hybrid configurations recover performance competitive with\nfull-precision training. We release our code at\nhttps://github.com/Hither1/systems-scaling.",
    "published": "2025-06-25T18:25:08Z",
    "updated": "2025-06-25T18:25:08Z",
    "id": "2506.20752v1",
    "authors": [
      "Huangyuan Su",
      "Mujin Kwun",
      "Stephanie Gil",
      "Sham Kakade",
      "Nikhil Anand"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20752v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20752v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20752v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on training instabilities in large language models when using lower-precision arithmetic formats, which is related to the scaling and efficiency of LLMs. However, it does not directly align with the provided topics such as LLM, RL, MLLM, etc., as it is more about the technical aspects of training and precision formats rather than the core topics listed.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.20747v1": {
    "title": "Towards Probabilistic Question Answering Over Tabular Data",
    "summary": "Current approaches for question answering (QA) over tabular data, such as\nNL2SQL systems, perform well for factual questions where answers are directly\nretrieved from tables. However, they fall short on probabilistic questions\nrequiring reasoning under uncertainty. In this paper, we introduce a new\nbenchmark LUCARIO and a framework for probabilistic QA over large tabular data.\nOur method induces Bayesian Networks from tables, translates natural language\nqueries into probabilistic queries, and uses large language models (LLMs) to\ngenerate final answers. Empirical results demonstrate significant improvements\nover baselines, highlighting the benefits of hybrid symbolic-neural reasoning.",
    "published": "2025-06-25T18:15:33Z",
    "updated": "2025-06-25T18:15:33Z",
    "id": "2506.20747v1",
    "authors": [
      "Chen Shen",
      "Sajjadur Rahman",
      "Estevam Hruschka"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20747v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20747v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20747v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a new benchmark and framework for probabilistic QA over tabular data, utilizing large language models (LLMs) for generating final answers. It involves reasoning under uncertainty and hybrid symbolic-neural reasoning, which aligns with the topics of Reasoning and LLM.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2506.20743v1": {
    "title": "A Survey of AI for Materials Science: Foundation Models, LLM Agents,\n  Datasets, and Tools",
    "summary": "Foundation models (FMs) are catalyzing a transformative shift in materials\nscience (MatSci) by enabling scalable, general-purpose, and multimodal AI\nsystems for scientific discovery. Unlike traditional machine learning models,\nwhich are typically narrow in scope and require task-specific engineering, FMs\noffer cross-domain generalization and exhibit emergent capabilities. Their\nversatility is especially well-suited to materials science, where research\nchallenges span diverse data types and scales. This survey provides a\ncomprehensive overview of foundation models, agentic systems, datasets, and\ncomputational tools supporting this growing field. We introduce a task-driven\ntaxonomy encompassing six broad application areas: data extraction,\ninterpretation and Q\\&A; atomistic simulation; property prediction; materials\nstructure, design and discovery; process planning, discovery, and optimization;\nand multiscale modeling. We discuss recent advances in both unimodal and\nmultimodal FMs, as well as emerging large language model (LLM) agents.\nFurthermore, we review standardized datasets, open-source tools, and autonomous\nexperimental platforms that collectively fuel the development and integration\nof FMs into research workflows. We assess the early successes of foundation\nmodels and identify persistent limitations, including challenges in\ngeneralizability, interpretability, data imbalance, safety concerns, and\nlimited multimodal fusion. Finally, we articulate future research directions\ncentered on scalable pretraining, continual learning, data governance, and\ntrustworthiness.",
    "published": "2025-06-25T18:10:30Z",
    "updated": "2025-06-25T18:10:30Z",
    "id": "2506.20743v1",
    "authors": [
      "Minh-Hao Van",
      "Prateek Verma",
      "Chen Zhao",
      "Xintao Wu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20743v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20743v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20743v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of foundation models, including large language models (LLMs), in materials science, covering topics such as multimodal FMs, LLM agents, and datasets. It also touches on pretraining and the integration of these models into research workflows.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "Dataset"
    ]
  },
  "2506.20729v1": {
    "title": "Test-time Scaling Techniques in Theoretical Physics -- A Comparison of\n  Methods on the TPBench Dataset",
    "summary": "Large language models (LLMs) have shown strong capabilities in complex\nreasoning, and test-time scaling techniques can enhance their performance with\ncomparably low cost. Many of these methods have been developed and evaluated on\nmathematical reasoning benchmarks such as AIME. This paper investigates whether\nthe lessons learned from these benchmarks generalize to the domain of advanced\ntheoretical physics. We evaluate a range of common test-time scaling methods on\nthe TPBench physics dataset and compare their effectiveness with results on\nAIME. To better leverage the structure of physics problems, we develop a novel,\nsymbolic weak-verifier framework to improve parallel scaling results. Our\nempirical results demonstrate that this method significantly outperforms\nexisting test-time scaling approaches on TPBench. We also evaluate our method\non AIME, confirming its effectiveness in solving advanced mathematical\nproblems. Our findings highlight the power of step-wise symbolic verification\nfor tackling complex scientific problems.",
    "published": "2025-06-25T18:00:18Z",
    "updated": "2025-06-25T18:00:18Z",
    "id": "2506.20729v1",
    "authors": [
      "Zhiqi Gao",
      "Tianyi Li",
      "Yurii Kvasiuk",
      "Sai Chaitanya Tadepalli",
      "Maja Rudolph",
      "Daniel J. H. Chung",
      "Frederic Sala",
      "Moritz Mnchmeyer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20729v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20729v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20729v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the application of large language models (LLMs) in complex reasoning tasks, specifically in theoretical physics, and introduces a novel test-time scaling technique. It evaluates these methods on a physics dataset and compares them with mathematical reasoning benchmarks. The core topics are related to LLMs, reasoning, and scaling techniques.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Scaling"
    ]
  },
  "2506.20664v1": {
    "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind",
    "summary": "As Large Language Models (LLMs) gain agentic abilities, they will have to\nnavigate complex multi-agent scenarios, interacting with human users and other\nagents in cooperative and competitive settings. This will require new reasoning\nskills, chief amongst them being theory of mind (ToM), or the ability to reason\nabout the \"mental\" states of other agents. However, ToM and other multi-agent\nabilities in LLMs are poorly understood, since existing benchmarks suffer from\nnarrow scope, data leakage, saturation, and lack of interactivity. We thus\npropose Decrypto, a game-based benchmark for multi-agent reasoning and ToM\ndrawing inspiration from cognitive science, computational pragmatics and\nmulti-agent reinforcement learning. It is designed to be as easy as possible in\nall other dimensions, eliminating confounding factors commonly found in other\nbenchmarks. To our knowledge, it is also the first platform for designing\ninteractive ToM experiments.\n  We validate the benchmark design through comprehensive empirical evaluations\nof frontier LLMs, robustness studies, and human-AI cross-play experiments. We\nfind that LLM game-playing abilities lag behind humans and simple\nword-embedding baselines. We then create variants of two classic cognitive\nscience experiments within Decrypto to evaluate three key ToM abilities.\nSurprisingly, we find that state-of-the-art reasoning models are significantly\nworse at those tasks than their older counterparts. This demonstrates that\nDecrypto addresses a crucial gap in current reasoning and ToM evaluations, and\npaves the path towards better artificial agents.",
    "published": "2025-06-25T17:55:27Z",
    "updated": "2025-06-25T17:55:27Z",
    "id": "2506.20664v1",
    "authors": [
      "Andrei Lupu",
      "Timon Willi",
      "Jakob Foerster"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20664v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20664v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20664v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating multi-agent reasoning and theory of mind (ToM) abilities in LLMs, proposing a new benchmark (Decrypto) for these tasks. It involves LLMs and their reasoning capabilities, aligning with the 'Reasoning' and 'Benchmark' topics. Additionally, the mention of multi-agent scenarios and reinforcement learning hints at the 'RL' topic.",
    "llm_cls_result": [
      "Reasoning",
      "Benchmark",
      "RL"
    ]
  },
  "2506.20640v1": {
    "title": "Towards Community-Driven Agents for Machine Learning Engineering",
    "summary": "Large language model-based machine learning (ML) agents have shown great\npromise in automating ML research. However, existing agents typically operate\nin isolation on a given research problem, without engaging with the broader\nresearch community, where human researchers often gain insights and contribute\nby sharing knowledge. To bridge this gap, we introduce MLE-Live, a live\nevaluation framework designed to assess an agent's ability to communicate with\nand leverage collective knowledge from a simulated Kaggle research community.\nBuilding on this framework, we propose CoMind, a novel agent that excels at\nexchanging insights and developing novel solutions within a community context.\nCoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%\nhuman competitors on average across four ongoing Kaggle competitions. Our code\nis released at https://github.com/comind-ml/CoMind.",
    "published": "2025-06-25T17:36:02Z",
    "updated": "2025-06-25T17:36:02Z",
    "id": "2506.20640v1",
    "authors": [
      "Sijie Li",
      "Weiwei Sun",
      "Shanda Li",
      "Ameet Talwalkar",
      "Yiming Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20640v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20640v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20640v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the development of a novel agent (CoMind) that operates within a community context, leveraging collective knowledge and communication, which aligns with the topics of Reinforcement Learning (RL) and Artificial General Intelligence (AGI). The focus on large language model-based agents also hints at the broader context of LLM research.",
    "llm_cls_result": [
      "RL",
      "AGI",
      "LLM"
    ]
  },
  "2506.20639v2": {
    "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
    "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, \\textbf{DiffuCoder}, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose \\textbf{coupled-GRPO}, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR bias during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.",
    "published": "2025-06-25T17:35:47Z",
    "updated": "2025-06-26T15:46:40Z",
    "id": "2506.20639v2",
    "authors": [
      "Shansan Gong",
      "Ruixiang Zhang",
      "Huangjie Zheng",
      "Jiatao Gu",
      "Navdeep Jaitly",
      "Lingpeng Kong",
      "Yizhe Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20639v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20639v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20639v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on diffusion large language models (dLLMs) for code generation, which involves understanding and improving these models. It discusses their denoising processes and reinforcement learning methods, aligning with topics related to LLMs and RL.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.20608v1": {
    "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base",
    "summary": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery.",
    "published": "2025-06-25T17:00:05Z",
    "updated": "2025-06-25T17:00:05Z",
    "id": "2506.20608v1",
    "authors": [
      "Barry Smith",
      "Junchao Zhang",
      "Hong Zhang",
      "Lois Curfman McInnes",
      "Murat Keceli",
      "Archit Vasan",
      "Satish Balay",
      "Toby Isaac",
      "Le Chen",
      "Venkatram Vishwanath"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20608v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20608v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20608v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to enhance the knowledge base of PETSc, a numerical library for high-performance scientific computing. It mentions retrieval-augmented generation (RAG) and other LLM tools, which are relevant to the topics of LLM and Memory.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.20606v1": {
    "title": "Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior\n  Toward Beneficence or Harm",
    "summary": "Agents based on Large Language Models (LLMs) have demonstrated strong\ncapabilities across a wide range of tasks. However, deploying LLM-based agents\nin high-stakes domains comes with significant safety and ethical risks.\nUnethical behavior by these agents can directly result in serious real-world\nconsequences, including physical harm and financial loss. To efficiently steer\nthe ethical behavior of agents, we frame agent behavior steering as a model\nediting task, which we term Behavior Editing. Model editing is an emerging area\nof research that enables precise and efficient modifications to LLMs while\npreserving their overall capabilities. To systematically study and evaluate\nthis approach, we introduce BehaviorBench, a multi-tier benchmark grounded in\npsychological moral theories. This benchmark supports both the evaluation and\nediting of agent behaviors across a variety of scenarios, with each tier\nintroducing more complex and ambiguous scenarios. We first demonstrate that\nBehavior Editing can dynamically steer agents toward the target behavior within\nspecific scenarios. Moreover, Behavior Editing enables not only\nscenario-specific local adjustments but also more extensive shifts in an\nagent's global moral alignment. We demonstrate that Behavior Editing can be\nused to promote ethical and benevolent behavior or, conversely, to induce\nharmful or malicious behavior. Through comprehensive evaluations on agents\nbased on frontier LLMs, BehaviorBench shows the effectiveness of Behavior\nEditing across different models and scenarios. Our findings offer key insights\ninto a new paradigm for steering agent behavior, highlighting both the promise\nand perils of Behavior Editing.",
    "published": "2025-06-25T16:51:51Z",
    "updated": "2025-06-25T16:51:51Z",
    "id": "2506.20606v1",
    "authors": [
      "Baixiang Huang",
      "Zhen Tan",
      "Haoran Wang",
      "Zijie Liu",
      "Dawei Li",
      "Ali Payani",
      "Huan Liu",
      "Tianlong Chen",
      "Kai Shu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20606v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20606v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20606v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the ethical behavior of LLM-based agents and introduces a model editing approach to steer their behavior, which involves both ethical and harmful outcomes. It also introduces a benchmark (BehaviorBench) for evaluating these behaviors.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Benchmark"
    ]
  },
  "2506.20601v1": {
    "title": "Video Perception Models for 3D Scene Synthesis",
    "summary": "Traditionally, 3D scene synthesis requires expert knowledge and significant\nmanual effort. Automating this process could greatly benefit fields such as\narchitectural design, robotics simulation, virtual reality, and gaming. Recent\napproaches to 3D scene synthesis often rely on the commonsense reasoning of\nlarge language models (LLMs) or strong visual priors of modern image generation\nmodels. However, current LLMs demonstrate limited 3D spatial reasoning ability,\nwhich restricts their ability to generate realistic and coherent 3D scenes.\nMeanwhile, image generation-based methods often suffer from constraints in\nviewpoint selection and multi-view inconsistencies. In this work, we present\nVideo Perception models for 3D Scene synthesis (VIPScene), a novel framework\nthat exploits the encoded commonsense knowledge of the 3D physical world in\nvideo generation models to ensure coherent scene layouts and consistent object\nplacements across views. VIPScene accepts both text and image prompts and\nseamlessly integrates video generation, feedforward 3D reconstruction, and\nopen-vocabulary perception models to semantically and geometrically analyze\neach object in a scene. This enables flexible scene synthesis with high realism\nand structural consistency. For more precise analysis, we further introduce\nFirst-Person View Score (FPVScore) for coherence and plausibility evaluation,\nutilizing continuous first-person perspective to capitalize on the reasoning\nability of multimodal large language models. Extensive experiments show that\nVIPScene significantly outperforms existing methods and generalizes well across\ndiverse scenarios. The code will be released.",
    "published": "2025-06-25T16:40:17Z",
    "updated": "2025-06-25T16:40:17Z",
    "id": "2506.20601v1",
    "authors": [
      "Rui Huang",
      "Guangyao Zhai",
      "Zuria Bauer",
      "Marc Pollefeys",
      "Federico Tombari",
      "Leonidas Guibas",
      "Gao Huang",
      "Francis Engelmann"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20601v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20601v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20601v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) and multimodal large language models (MLLMs) for 3D scene synthesis, focusing on their reasoning abilities and integration with video generation models. It also introduces a new evaluation metric leveraging MLLMs.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "Reasoning"
    ]
  },
  "2506.20551v1": {
    "title": "Large Language Model-Driven Code Compliance Checking in Building\n  Information Modeling",
    "summary": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects.",
    "published": "2025-06-25T15:50:34Z",
    "updated": "2025-06-25T15:50:34Z",
    "id": "2506.20551v1",
    "authors": [
      "Soumya Madireddy",
      "Lu Gao",
      "Zia Din",
      "Kinam Kim",
      "Ahmed Senouci",
      "Zhe Han",
      "Yunpeng Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20551v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20551v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20551v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in automating code compliance checking in Building Information Modeling (BIM), which directly relates to the use of LLMs in a specific domain.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.20544v1": {
    "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
    "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.",
    "published": "2025-06-25T15:37:53Z",
    "updated": "2025-06-25T15:37:53Z",
    "id": "2506.20544v1",
    "authors": [
      "Ammar Khairi",
      "Daniel D'souza",
      "Ye Shen",
      "Julia Kreutzer",
      "Sara Hooker"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20544v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20544v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20544v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses scaling inference-time compute for multilingual large language models (LLMs), focusing on sampling and selection strategies to improve performance across diverse languages and tasks. This aligns with the topics of 'Scaling' (as it involves scaling inference compute) and 'LLM' (as it pertains to large language models).",
    "llm_cls_result": [
      "Scaling",
      "LLM"
    ]
  },
  "2506.20535v1": {
    "title": "WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads",
    "summary": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents WattsOnAI, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, WattsOnAI offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, WattsOnAI encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.",
    "published": "2025-06-25T15:24:45Z",
    "updated": "2025-06-25T15:24:45Z",
    "id": "2506.20535v1",
    "authors": [
      "Hongzhen Huang",
      "Kunming Zhang",
      "Hanlong Liao",
      "Kui Wu",
      "Guoming Tang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20535v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20535v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20535v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the environmental impact of AI workloads, particularly focusing on energy use and carbon emissions associated with large language models (LLMs). While it mentions LLMs, the primary focus is on measuring and analyzing energy and carbon footprints, which does not directly align with the provided topic list.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.20531v1": {
    "title": "Case-based Reasoning Augmented Large Language Model Framework for\n  Decision Making in Realistic Safety-Critical Driving Scenarios",
    "summary": "Driving in safety-critical scenarios requires quick, context-aware\ndecision-making grounded in both situational understanding and experiential\nreasoning. Large Language Models (LLMs), with their powerful general-purpose\nreasoning capabilities, offer a promising foundation for such decision-making.\nHowever, their direct application to autonomous driving remains limited due to\nchallenges in domain adaptation, contextual grounding, and the lack of\nexperiential knowledge needed to make reliable and interpretable decisions in\ndynamic, high-risk environments. To address this gap, this paper presents a\nCase-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for\nevasive maneuver decision-making in complex risk scenarios. Our approach\nintegrates semantic scene understanding from dashcam video inputs with the\nretrieval of relevant past driving cases, enabling LLMs to generate maneuver\nrecommendations that are both context-sensitive and human-aligned. Experiments\nacross multiple open-source LLMs show that our framework improves decision\naccuracy, justification quality, and alignment with human expert behavior.\nRisk-aware prompting strategies further enhance performance across diverse risk\ntypes, while similarity-based case retrieval consistently outperforms random\nsampling in guiding in-context learning. Case studies further demonstrate the\nframework's robustness in challenging real-world conditions, underscoring its\npotential as an adaptive and trustworthy decision-support tool for intelligent\ndriving systems.",
    "published": "2025-06-25T15:19:25Z",
    "updated": "2025-06-25T15:19:25Z",
    "id": "2506.20531v1",
    "authors": [
      "Wenbin Gan",
      "Minh-Son Dao",
      "Koji Zettsu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20531v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20531v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20531v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using Large Language Models (LLMs) enhanced with Case-Based Reasoning (CBR) for decision-making in safety-critical driving scenarios. It involves reasoning capabilities of LLMs and their application in a specific domain, which aligns with the topics of Reasoning and LLM.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2506.20520v1": {
    "title": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing\n  positive and negative rewards",
    "summary": "Reinforcement learning (RL) is increasingly used to align large language\nmodels (LLMs). Off-policy methods offer greater implementation simplicity and\ndata efficiency than on-policy techniques, but often result in suboptimal\nperformance. In this work, we study the intermediate range of algorithms\nbetween off-policy RL and supervised fine-tuning by analyzing a simple\noff-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with\n$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$\nemphasizes high-reward samples, while raising it penalizes low-reward ones more\nheavily. We first provide a theoretical analysis of this off-policy REINFORCE\nalgorithm, showing that when the baseline $V$ lower-bounds the expected reward,\nthe algorithm enjoys a policy improvement guarantee. Our analysis reveals that\nwhile on-policy updates can safely leverage both positive and negative signals,\noff-policy updates benefit from focusing more on positive rewards than on\nnegative ones. We validate our findings experimentally in a controlled\nstochastic bandit setting and through fine-tuning state-of-the-art LLMs on\nreasoning tasks.",
    "published": "2025-06-25T15:07:16Z",
    "updated": "2025-06-25T15:07:16Z",
    "id": "2506.20520v1",
    "authors": [
      "Charles Arnal",
      "Gatan Narozniak",
      "Vivien Cabannes",
      "Yunhao Tang",
      "Julia Kempe",
      "Remi Munos"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20520v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20520v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20520v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses reinforcement learning (RL) in the context of aligning large language models (LLMs), specifically focusing on off-policy methods and their implications for policy improvement. It also involves fine-tuning LLMs on reasoning tasks, which aligns with the RL and Reasoning topics.",
    "llm_cls_result": [
      "RL",
      "Reasoning"
    ]
  },
  "2507.02920v1": {
    "title": "Visual-Conversational Interface for Evidence-Based Explanation of\n  Diabetes Risk Prediction",
    "summary": "Healthcare professionals need effective ways to use, understand, and validate\nAI-driven clinical decision support systems. Existing systems face two key\nlimitations: complex visualizations and a lack of grounding in scientific\nevidence. We present an integrated decision support system that combines\ninteractive visualizations with a conversational agent to explain diabetes risk\nassessments. We propose a hybrid prompt handling approach combining fine-tuned\nlanguage models for analytical queries with general Large Language Models\n(LLMs) for broader medical questions, a methodology for grounding AI\nexplanations in scientific evidence, and a feature range analysis technique to\nsupport deeper understanding of feature contributions. We conducted a\nmixed-methods study with 30 healthcare professionals and found that the\nconversational interactions helped healthcare professionals build a clear\nunderstanding of model assessments, while the integration of scientific\nevidence calibrated trust in the system's decisions. Most participants reported\nthat the system supported both patient risk evaluation and recommendation.",
    "published": "2025-06-25T14:56:20Z",
    "updated": "2025-06-25T14:56:20Z",
    "id": "2507.02920v1",
    "authors": [
      "Reza Samimi",
      "Aditya Bhattacharya",
      "Lucija Gosak",
      "Gregor Stiglic",
      "Katrien Verbert"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02920v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02920v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02920v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a healthcare decision support system, specifically for explaining diabetes risk assessments. It mentions the integration of fine-tuned language models and general LLMs, which aligns with the 'LLM' topic. Additionally, the focus on explaining and validating AI-driven decisions in a healthcare context suggests a relevance to 'Reasoning' as it involves logical reasoning and problem-solving.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.20503v1": {
    "title": "BotHash: Efficient and Training-Free Bot Detection Through Approximate\n  Nearest Neighbor",
    "summary": "Online Social Networks (OSNs) are a cornerstone in modern society, serving as\nplatforms for diverse content consumption by millions of users each day.\nHowever, the challenge of ensuring the accuracy of information shared on these\nplatforms remains significant, especially with the widespread dissemination of\ndisinformation. Social bots -- automated accounts designed to mimic human\nbehavior, frequently spreading misinformation -- represent one of the critical\nproblems of OSNs. The advent of Large Language Models (LLMs) has further\ncomplicated bot behaviors, making detection increasingly difficult. This paper\npresents BotHash, an innovative, training-free approach to social bot\ndetection. BotHash leverages a simplified user representation that enables\napproximate nearest-neighbor search to detect bots, avoiding the complexities\nof Deep-Learning model training and large dataset creation. We demonstrate that\nBotHash effectively differentiates between human and bot accounts, even when\nstate-of-the-art LLMs are employed to generate posts' content. BotHash offers\nseveral advantages over existing methods, including its independence from a\ntraining phase, robust performance with minimal ground-truth data, and early\ndetection capabilities, showing promising results across various datasets.",
    "published": "2025-06-25T14:49:28Z",
    "updated": "2025-06-25T14:49:28Z",
    "id": "2506.20503v1",
    "authors": [
      "Edoardo Di Paolo",
      "Fabio De Gaspari",
      "Angelo Spognardi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20503v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20503v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20503v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the detection of social bots using a training-free approach, leveraging approximate nearest-neighbor search, but does not primarily focus on LLMs, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.20495v2": {
    "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
    "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.",
    "published": "2025-06-25T14:41:13Z",
    "updated": "2025-07-17T05:31:07Z",
    "id": "2506.20495v2",
    "authors": [
      "Haoze Wu",
      "Yunzhi Yao",
      "Wenhao Yu",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20495v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20495v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20495v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using reinforcement learning to update LLMs' knowledge of code APIs, which involves both LLMs and reinforcement learning techniques.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.20480v1": {
    "title": "GPTailor: Large Language Model Pruning Through Layer Cutting and\n  Stitching",
    "summary": "Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\ndeployment and inference. While structured pruning of model parameters offers a\npromising way to reduce computational costs at deployment time, current methods\nprimarily focus on single model pruning. In this work, we develop a novel\nstrategy to compress models by strategically combining or merging layers from\nfinetuned model variants, which preserves the original model's abilities by\naggregating capabilities accentuated in different finetunes. We pose the\noptimal tailoring of these LLMs as a zero-order optimization problem, adopting\na search space that supports three different operations: (1) Layer removal, (2)\nLayer selection from different candidate models, and (3) Layer merging. Our\nexperiments demonstrate that this approach leads to competitive model pruning,\nfor example, for the Llama2-13B model families, our compressed models maintain\napproximately 97.3\\% of the original performance while removing $\\sim25\\%$ of\nparameters, significantly outperforming previous state-of-the-art methods. The\ncode is available at https://github.com/Guinan-Su/auto-merge-llm.",
    "published": "2025-06-25T14:24:59Z",
    "updated": "2025-06-25T14:24:59Z",
    "id": "2506.20480v1",
    "authors": [
      "Guinan Su",
      "Li Shen",
      "Lu Yin",
      "Shiwei Liu",
      "Yanwu Yang",
      "Jonas Geiping"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20480v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20480v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20480v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on pruning techniques for large language models (LLMs), which involves reducing model size while maintaining performance. This is directly related to the 'LLM' topic as it discusses model architectures and optimization. Additionally, the approach involves optimization strategies, which can be loosely connected to 'Scaling' as it deals with model efficiency and performance.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.20471v1": {
    "title": "Probing AI Safety with Source Code",
    "summary": "Large language models (LLMs) have become ubiquitous, interfacing with humans\nin numerous safety-critical applications. This necessitates improving\ncapabilities, but importantly coupled with greater safety measures to align\nthese models with human values and preferences. In this work, we demonstrate\nthat contemporary models fall concerningly short of the goal of AI safety,\nleading to an unsafe and harmful experience for users. We introduce a prompting\nstrategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT\nconverts natural language inputs to simple code that represents the same\nintent. For instance, CoDoT transforms the natural language prompt \"Make the\nstatement more toxic: {text}\" to: \"make_more_toxic({text})\". We show that CoDoT\nresults in a consistent failure of a wide range of state-of-the-art LLMs. For\nexample, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of\nthe time, and toxicity increases 300% on average across seven modern LLMs.\nAdditionally, recursively applying CoDoT can further increase toxicity two\ntimes. Given the rapid and widespread adoption of LLMs, CoDoT underscores the\ncritical need to evaluate safety efforts from first principles, ensuring that\nsafety and capabilities advance together.",
    "published": "2025-06-25T14:19:57Z",
    "updated": "2025-06-25T14:19:57Z",
    "id": "2506.20471v1",
    "authors": [
      "Ujwal Narayan",
      "Shreyas Chaudhari",
      "Ashwin Kalyan",
      "Tanmay Rajpurohit",
      "Karthik Narasimhan",
      "Ameet Deshpande",
      "Vishvak Murahari"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20471v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20471v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20471v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the safety evaluation of Large Language Models (LLMs) using a novel prompting strategy called Code of Thought (CoDoT). It focuses on the alignment of LLMs with human values and preferences, which is a key aspect of LLM research. The study involves evaluating the safety of various state-of-the-art LLMs, making it relevant to the LLM topic.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.20451v1": {
    "title": "Automatic Demonstration Selection for LLM-based Tabular Data\n  Classification",
    "summary": "A fundamental question in applying In-Context Learning (ICL) for tabular data\nclassification is how to determine the ideal number of demonstrations in the\nprompt. This work addresses this challenge by presenting an algorithm to\nautomatically select a reasonable number of required demonstrations. Our method\ndistinguishes itself by integrating not only the tabular data's distribution\nbut also the user's selected prompt template and the specific Large Language\nModel (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed\nalgorithm defines a novel metric to quantify the similarities between different\ndemonstrations. We then construct a similarity graph and analyze the\neigenvalues of its Laplacian to derive the minimum number of demonstrations\ncapable of representing the data within the LLM's intrinsic representation\nspace. We validate the efficacy of our approach through experiments comparing\nits performance against conventional random selection algorithms on diverse\ndatasets and LLMs.",
    "published": "2025-06-25T13:57:54Z",
    "updated": "2025-06-25T13:57:54Z",
    "id": "2506.20451v1",
    "authors": [
      "Shuchu Han",
      "Wolfgang Bruckner"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20451v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20451v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20451v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of Large Language Models (LLMs) in tabular data classification, specifically addressing the challenge of demonstration selection for In-Context Learning (ICL). It integrates the LLM's characteristics and the data's distribution, which are central to LLM research and applications.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.20430v1": {
    "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
    "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.",
    "published": "2025-06-25T13:42:26Z",
    "updated": "2025-06-25T13:42:26Z",
    "id": "2506.20430v1",
    "authors": [
      "Weike Zhao",
      "Chaoyi Wu",
      "Yanjie Fan",
      "Xiaoman Zhang",
      "Pengcheng Qiu",
      "Yuze Sun",
      "Xiao Zhou",
      "Yanfeng Wang",
      "Ya Zhang",
      "Yongguo Yu",
      "Kun Sun",
      "Weidi Xie"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20430v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20430v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20430v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses an agentic system powered by a large language model (LLM) for rare disease diagnosis, which involves reasoning and memory components. It also highlights the use of specialized tools and web-scale medical knowledge sources, indicating a focus on LLM applications in specialized domains.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2506.20420v1": {
    "title": "Semantic Caching for Improving Web Affordability",
    "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
    "published": "2025-06-25T13:35:25Z",
    "updated": "2025-06-25T13:35:25Z",
    "id": "2506.20420v1",
    "authors": [
      "Hafsa Akbar",
      "Danish Athar",
      "Muhammad Ayain Fida Rana",
      "Chaudhary Hammad Javed",
      "Zartash Afzal Uzmi",
      "Ihsan Ayyub Qazi",
      "Zafar Ayyub Qazi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20420v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20420v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20420v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) for semantic caching to improve web affordability, which involves LLMs and their application in a specific context.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2506.20415v1": {
    "title": "SV-LLM: An Agentic Approach for SoC Security Verification using Large\n  Language Models",
    "summary": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical\nimperative, yet traditional verification techniques struggle to keep pace due\nto significant challenges in automation, scalability, comprehensiveness, and\nadaptability. The advent of large language models (LLMs), with their remarkable\ncapabilities in natural language understanding, code generation, and advanced\nreasoning, presents a new paradigm for tackling these issues. Moving beyond\nmonolithic models, an agentic approach allows for the creation of multi-agent\nsystems where specialized LLMs collaborate to solve complex problems more\neffectively. Recognizing this opportunity, we introduce SV-LLM, a novel\nmulti-agent assistant system designed to automate and enhance SoC security\nverification. By integrating specialized agents for tasks like verification\nquestion answering, security asset identification, threat modeling, test plan\nand property generation, vulnerability detection, and simulation-based bug\nvalidation, SV-LLM streamlines the workflow. To optimize their performance in\nthese diverse tasks, agents leverage different learning paradigms, such as\nin-context learning, fine-tuning, and retrieval-augmented generation (RAG). The\nsystem aims to reduce manual intervention, improve accuracy, and accelerate\nsecurity analysis, supporting proactive identification and mitigation of risks\nearly in the design cycle. We demonstrate its potential to transform hardware\nsecurity practices through illustrative case studies and experiments that\nshowcase its applicability and efficacy.",
    "published": "2025-06-25T13:31:13Z",
    "updated": "2025-06-25T13:31:13Z",
    "id": "2506.20415v1",
    "authors": [
      "Dipayan Saha",
      "Shams Tarek",
      "Hasan Al Shaikh",
      "Khan Thamid Hasan",
      "Pavan Sai Nalluri",
      "Md. Ajoad Hasan",
      "Nashmin Alam",
      "Jingbo Zhou",
      "Sujan Kumar Saha",
      "Mark Tehranipoor",
      "Farimah Farahmandi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20415v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20415v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20415v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a multi-agent system for SoC security verification, highlighting their capabilities in natural language understanding, code generation, and advanced reasoning. The focus on LLMs and their application in a specialized domain aligns with the 'LLM' and 'Reasoning' topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.20409v2": {
    "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging",
    "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task.",
    "published": "2025-06-25T13:24:46Z",
    "updated": "2025-06-26T13:09:40Z",
    "id": "2506.20409v2",
    "authors": [
      "Ekaterina Taktasheva",
      "Jeff Dalton"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20409v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20409v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20409v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of user preferences into tool-augmented large language models (LLMs) for goal-oriented dialogue agents, which involves personalization and tool use in LLMs.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.02919v1": {
    "title": "ChatGPT is not A Man but Das Man: Representativeness and Structural\n  Consistency of Silicon Samples Generated by Large Language Models",
    "summary": "Large language models (LLMs) in the form of chatbots like ChatGPT and Llama\nare increasingly proposed as \"silicon samples\" for simulating human opinions.\nThis study examines this notion, arguing that LLMs may misrepresent\npopulation-level opinions. We identify two fundamental challenges: a failure in\nstructural consistency, where response accuracy doesn't hold across demographic\naggregation levels, and homogenization, an underrepresentation of minority\nopinions. To investigate these, we prompted ChatGPT (GPT-4) and Meta's Llama\n3.1 series (8B, 70B, 405B) with questions on abortion and unauthorized\nimmigration from the American National Election Studies (ANES) 2020. Our\nfindings reveal significant structural inconsistencies and severe\nhomogenization in LLM responses compared to human data. We propose an\n\"accuracy-optimization hypothesis,\" suggesting homogenization stems from\nprioritizing modal responses. These issues challenge the validity of using\nLLMs, especially chatbots AI, as direct substitutes for human survey data,\npotentially reinforcing stereotypes and misinforming policy.",
    "published": "2025-06-25T12:35:44Z",
    "updated": "2025-06-25T12:35:44Z",
    "id": "2507.02919v1",
    "authors": [
      "Dai Li",
      "Linzhuo Li",
      "Huilian Sophie Qiu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02919v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02919v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02919v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the limitations of using large language models (LLMs) like ChatGPT and Llama as substitutes for human survey data, focusing on structural consistency and homogenization issues. It directly involves LLMs and their applications in simulating human opinions, which falls under the LLM topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.20357v1": {
    "title": "Tabular Feature Discovery With Reasoning Type Exploration",
    "summary": "Feature engineering for tabular data remains a critical yet challenging step\nin machine learning. Recently, large language models (LLMs) have been used to\nautomatically generate new features by leveraging their vast knowledge.\nHowever, existing LLM-based approaches often produce overly simple or\nrepetitive features, partly due to inherent biases in the transformations the\nLLM chooses and the lack of structured reasoning guidance during generation. In\nthis paper, we propose a novel method REFeat, which guides an LLM to discover\ndiverse and informative features by leveraging multiple types of reasoning to\nsteer the feature generation process. Experiments on 59 benchmark datasets\ndemonstrate that our approach not only achieves higher predictive accuracy on\naverage, but also discovers more diverse and meaningful features. These results\nhighlight the promise of incorporating rich reasoning paradigms and adaptive\nstrategy selection into LLM-driven feature discovery for tabular data.",
    "published": "2025-06-25T12:18:34Z",
    "updated": "2025-06-25T12:18:34Z",
    "id": "2506.20357v1",
    "authors": [
      "Sungwon Han",
      "Sungkyu Park",
      "Seungeon Lee"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20357v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20357v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20357v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) for feature engineering in tabular data, focusing on improving feature diversity and quality through structured reasoning. This aligns with topics related to LLMs and reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.20353v1": {
    "title": "DipSVD: Dual-importance Protected SVD for Efficient LLM Compression",
    "summary": "The ever-increasing computational demands and deployment costs of large\nlanguage models (LLMs) have spurred numerous compressing methods. Compared to\nquantization and unstructured pruning, SVD compression offers superior hardware\ncompatibility and theoretical guarantees. However, existing SVD-based methods\nfocus on the overall discrepancy between the original and compressed matrices\nwhile overlooking the protection of critical components within the matrix,\nwhich leads to inferior performance in the compressed models. This paper\nproposes a dual-level importance protection mechanism to enhance SVD-based\ncompression methods: (1) local importance protection: preserving the most\ncritical singular vectors within each weight matrix through channel-weighted\ndata whitening; and (2) global importance protection: enabling less important\nlayers to bear a greater portion of the compression burden through either a\nheuristic or optimization-based approach, thereby minimizing the impact of\ncompression on critical layers. Extensive experiments demonstrate that DipSVD\noutperforms existing SVD-based compression approaches across multiple\nbenchmarks, achieving superior model performance especially at high model\ncompression ratios.",
    "published": "2025-06-25T12:04:53Z",
    "updated": "2025-06-25T12:04:53Z",
    "id": "2506.20353v1",
    "authors": [
      "Xuan Ding",
      "Rui Sun",
      "Yunjian Zhang",
      "Xiu Yan",
      "Yueqi Zhou",
      "Kaihao Huang",
      "Suzhong Fu",
      "Chuanlong Xie",
      "Yao Zhu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20353v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20353v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20353v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for compressing large language models (LLMs) using SVD, which is a technique related to model efficiency and scaling. The focus is on improving the performance of compressed models, which aligns with the 'Scaling' topic as it involves optimizing model size and performance. Additionally, the paper's emphasis on LLMs connects it to the 'LLM' topic.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.20331v1": {
    "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content",
    "summary": "We introduce Biomed-Enriched, a biomedical text dataset constructed from\nPubMed via a two-stage annotation process. In the first stage, a large language\nmodel annotates 400K paragraphs from PubMed scientific articles, assigning\nscores for their type (review, study, clinical case, other), domain (clinical,\nbiomedical, other), and educational quality. The educational quality score\n(rated 1 to 5) estimates how useful a paragraph is for college-level learning.\nThese annotations are then used to fine-tune a small language model, which\npropagates the labels across the full PMC-OA corpus. The resulting metadata\nallows us to extract refined subsets, including 2M clinical case paragraphs\nwith over 450K high-quality ones from articles with commercial-use licenses,\nand to construct several variants via quality filtering and domain upsampling.\nClinical text is typically difficult to access due to privacy constraints, as\nhospital records cannot be publicly shared. Hence, our dataset provides an\nalternative large-scale, openly available collection of clinical cases from\nPubMed, making it a valuable resource for biomedical and clinical NLP.\nPreliminary continual-pretraining experiments with OLMo2 suggest these curated\nsubsets enable targeted improvements, with clinical upsampling boosting\nperformance by ~5% on MMLU ProfMed and educational quality filtering improving\nMedQA and MedMCQA by ~1%. Combinations of these techniques led to faster\nconvergence, reaching same performance with a third of training tokens,\nindicating potential for more efficient and effective biomedical pretraining\nstrategies.",
    "published": "2025-06-25T11:30:25Z",
    "updated": "2025-06-25T11:30:25Z",
    "id": "2506.20331v1",
    "authors": [
      "Rian Touchent",
      "Nathan Godey",
      "Eric de la Clergerie"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20331v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20331v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20331v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the creation of a biomedical dataset enriched with LLMs for pretraining and extracting rare and hidden content, which involves the use of large language models for annotation and fine-tuning, and focuses on biomedical and clinical NLP applications.",
    "llm_cls_result": [
      "LLM",
      "Pretrain",
      "Dataset"
    ]
  },
  "2507.02917v1": {
    "title": "Echo State Transformer: When chaos brings memory",
    "summary": "While Large Language Models and their underlying Transformer architecture are\nremarkably efficient, they do not reflect how our brain processes and learns a\ndiversity of cognitive tasks such as language and working memory. Furthermore,\nsequential data processing with Transformers encounters a fundamental barrier:\nquadratic complexity growth with sequence length. Motivated by these\nlimitations, our ambition is to create more efficient models that are less\nreliant on intensive computations and massive volumes of data. We introduce\nEcho State Transformers (EST), a hybrid architecture that elegantly resolves\nthis challenge while demonstrating exceptional performance in low-data regimes.\nEST integrates the Transformer attention mechanisms with principles from\nReservoir Computing to create a fixedsize window distributed memory system.\nDrawing inspiration from Echo State Networks, the most prominent instance of\nthe Reservoir Computing paradigm, our architecture integrates a new module\ncalled ''Working Memory'' based on several reservoirs (i.e. random recurrent\nnetworks) working in parallel. These reservoirs work as independent memory\nunits with distinct internal dynamics. A novelty here is that the classical\nreservoir hyperparameters controlling the dynamics are now trained. Thus, the\nEST dynamically adapts the memory/non-linearity trade-off in reservoirs. By\nmaintaining a fixed number of memory units regardless of sequence length, EST\nachieves constant computational complexity at each processing step, effectively\nbreaking the quadratic scaling problem of standard Transformers. Evaluations on\nthe STREAM benchmark, which comprises 12 diverse sequential processing tasks,\ndemonstrate that EST outperforms GRUs, LSTMs, and even Transformers on 8 of\nthese tasks. These findings highlight that Echo State Transformers can be an\neffective replacement to GRUs and LSTMs while complementing standard\nTransformers at least on resource-constrained environments and low-data\nscenarios across diverse sequential processing tasks.",
    "published": "2025-06-25T09:56:25Z",
    "updated": "2025-06-25T09:56:25Z",
    "id": "2507.02917v1",
    "authors": [
      "Yannis Bendi-Ouis",
      "Xavier Hinaut"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02917v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02917v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02917v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces Echo State Transformers (EST), a hybrid architecture combining Transformer attention mechanisms with Reservoir Computing principles to address the quadratic complexity growth issue in sequential data processing. It focuses on memory and efficiency in processing, which aligns with the topics of Memory and Scaling.",
    "llm_cls_result": [
      "Memory",
      "Scaling"
    ]
  },
  "2506.20274v1": {
    "title": "Enterprise Large Language Model Evaluation Benchmark",
    "summary": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment.",
    "published": "2025-06-25T09:34:25Z",
    "updated": "2025-06-25T09:34:25Z",
    "id": "2506.20274v1",
    "authors": [
      "Liya Wang",
      "David Yi",
      "Damien Jose",
      "John Passarelli",
      "James Gao",
      "Jordan Leventis",
      "Kang Li"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20274v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20274v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20274v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on evaluating Large Language Models (LLMs) in enterprise contexts, proposing a new benchmark framework and addressing challenges in data annotation and model performance. This aligns with the topics of Benchmark and LLM.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2506.20269v1": {
    "title": "Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and\n  Large Language Models",
    "summary": "With rapidly evolving media narratives, it has become increasingly critical\nto not just extract narratives from a given corpus but rather investigate, how\nthey develop over time. While popular narrative extraction methods such as\nLarge Language Models do well in capturing typical narrative elements or even\nthe complex structure of a narrative, applying them to an entire corpus comes\nwith obstacles, such as a high financial or computational cost. We propose a\ncombination of the language understanding capabilities of Large Language Models\nwith the large scale applicability of topic models to dynamically model\nnarrative shifts across time using the Narrative Policy Framework. We apply a\ntopic model and a corresponding change point detection method to find changes\nthat concern a specific topic of interest. Using this model, we filter our\ncorpus for documents that are particularly representative of that change and\nfeed them into a Large Language Model that interprets the change that happened\nin an automated fashion and distinguishes between content and narrative shifts.\nWe employ our pipeline on a corpus of The Wall Street Journal news paper\narticles from 2009 to 2023. Our findings indicate that a Large Language Model\ncan efficiently extract a narrative shift if one exists at a given point in\ntime, but does not perform as well when having to decide whether a shift in\ncontent or a narrative shift took place.",
    "published": "2025-06-25T09:25:15Z",
    "updated": "2025-06-25T09:25:15Z",
    "id": "2506.20269v1",
    "authors": [
      "Kai-Robin Lange",
      "Tobias Schmidt",
      "Matthias Reccius",
      "Henrik Mller",
      "Michael Roos",
      "Carsten Jentsch"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20269v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20269v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20269v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in combination with topic models to detect narrative shifts over time, focusing on the capabilities and limitations of LLMs in this context.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.20251v1": {
    "title": "Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching\n  for Quantized Large Language Models",
    "summary": "Quantized large language models (LLMs) have gained increasing attention and\nsignificance for enabling deployment in resource-constrained environments.\nHowever, emerging studies on a few calibration dataset-free quantization\nmethods suggest that quantization may compromise the safety capabilities of\nLLMs, underscoring the urgent need for systematic safety evaluations and\neffective mitigation strategies. In this paper, we present comprehensive safety\nevaluations across various mainstream quantization techniques and diverse\ncalibration datasets, utilizing widely accepted safety benchmarks. To address\nthe identified safety vulnerabilities, we propose a quantization-aware safety\npatching framework, Q-resafe, to efficiently restore the safety capabilities of\nquantized LLMs while minimizing any adverse impact on utility. Extensive\nexperimental results demonstrate that Q-resafe successfully re-aligns the\nsafety of quantized LLMs with their pre-quantization counterparts, even under\nchallenging evaluation scenarios. Project page is available at:\nhttps://github.com/Thecommonirin/Qresafe.",
    "published": "2025-06-25T08:52:22Z",
    "updated": "2025-06-25T08:52:22Z",
    "id": "2506.20251v1",
    "authors": [
      "Kejia Chen",
      "Jiawen Zhang",
      "Jiacong Hu",
      "Yu Wang",
      "Jian Lou",
      "Zunlei Feng",
      "Mingli Song"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20251v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20251v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20251v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the safety evaluation and mitigation strategies for quantized large language models (LLMs), which involves both the evaluation of LLM safety and the development of a framework to address safety vulnerabilities. The topics 'LLM' and 'Benchmark' are relevant as the paper discusses LLMs and their safety evaluation using benchmarks.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.20241v1": {
    "title": "Enhancing Large Language Models through Structured Reasoning",
    "summary": "Recent Large Language Models (LLMs) have significantly advanced natural\nlanguage processing and automated decision-making. However, these models still\nencounter difficulties when performing complex reasoning tasks involving\nlogical deduction and systematic planning, primarily due to their reliance on\nimplicit statistical relationships without structured knowledge\nrepresentation.Inspired by cognitive science and neurosymbolic AI, we introduce\na novel approach to enhance LLMs through explicit structured reasoning. First,\nwe convert unstructured data into structured formats by explicitly annotating\nreasoning steps. We then employ this structured dataset to train LLMs through\nSupervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning\ncapabilities of LLMs using Group Relative Policy Optimization (GRPO),\nincorporating two innovative algorithms--MAX-Flow and Longest Common\nSubsequence (LCS)--which notably improve reasoning effectiveness and reduce\ncomputational complexity. Experimental results from fine-tuning a\nDeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust\nperformance across various scenarios, and improved compatibility with\noptimization techniques, validating the efficacy of structured reasoning\nintegration in LLMs.",
    "published": "2025-06-25T08:36:12Z",
    "updated": "2025-06-25T08:36:12Z",
    "id": "2506.20241v1",
    "authors": [
      "Yubo Dong",
      "Hehe Fan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20241v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20241v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20241v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on enhancing Large Language Models (LLMs) through structured reasoning, which involves logical deduction and systematic planning. This aligns with the 'Reasoning' topic, as it discusses reasoning abilities in LLMs and complex problem solving. Additionally, the paper involves fine-tuning LLMs, which is related to 'LLM' research.",
    "llm_cls_result": [
      "Reasoning",
      "LLM"
    ]
  },
  "2506.20214v2": {
    "title": "UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal\n  Understanding and Generation",
    "summary": "Unified multimodal large language models (MLLMs) have shown promise in\njointly advancing multimodal understanding and generation, with visual\ncodebooks discretizing images into tokens for autoregressive modeling. Existing\ncodebook-based methods either rely on small vocabularies (~16K entries) that\nlack fine-grained semantics or naively scale up, resulting in low token\nutilization and unstable training. We propose UniCode$^2$, a cascaded codebook\nframework enabling large-scale, semantically aligned, and stable visual\ntokenization. By clustering millions of SigLIP sequence embeddings, we build a\n500K-entry codebook that preserves vision-language alignment while expanding\ncapacity. Stability is ensured via a cascaded design: a frozen codebook anchors\nthe embedding space, and a trainable codebook refines task-specific semantics.\nThis decoupling promotes high utilization and robust learning. Moreover, the\nalignment of our visual tokens with textual semantics enables seamless\nintegration with pretrained diffusion decoders, supporting high-quality visual\nsynthesis with minimal adaptation. UniCode^2 delivers strong performance across\ndiverse benchmarks, demonstrating the viability of scaling visual token spaces\nwithout sacrificing stability, semantics, or modularity.",
    "published": "2025-06-25T07:57:09Z",
    "updated": "2025-07-08T07:46:39Z",
    "id": "2506.20214v2",
    "authors": [
      "Yanzhe Chen",
      "Huasong Zhong",
      "Yan Li",
      "Zhenheng Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20214v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20214v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20214v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a cascaded codebook framework for unified multimodal understanding and generation, which aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Alignment (VLA). The focus on scaling visual token spaces and maintaining alignment with textual semantics also touches on Pretraining strategies.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Pretrain"
    ]
  },
  "2506.20199v2": {
    "title": "How to Retrieve Examples in In-context Learning to Improve\n  Conversational Emotion Recognition using Large Language Models?",
    "summary": "Large language models (LLMs) have enabled a wide variety of real-world\napplications in various domains. However, creating a high-performing\napplication with high accuracy remains challenging, particularly for subjective\ntasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this\nstudy investigates approaches to improving conversational emotion recognition\n(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples\nin in-context learning (ICL) to enhance CER. We propose various strategies\nbased on random and augmented example retrieval and also analyze the impact of\nconversational context on CER accuracy. Experiments were conducted on the three\ndatasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented\nexample retrieval consistently outperforms other techniques under investigation\nacross all datasets, highlighting the importance of retrieving coherent\ntargeted examples and enhancing them through paraphrasing.",
    "published": "2025-06-25T07:39:19Z",
    "updated": "2025-06-28T03:04:05Z",
    "id": "2506.20199v2",
    "authors": [
      "Mengqi Wang",
      "Tiantian Feng",
      "Shrikanth Narayanan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20199v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20199v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20199v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on improving conversational emotion recognition using large language models (LLMs) through in-context learning (ICL) and example retrieval strategies. It involves LLMs and their application in a specific task, which aligns with the 'LLM' topic. Additionally, the study explores retrieval methods, which is related to 'Memory' as it involves retrieval-augmented techniques. The paper does not directly fit into other specified topics like RL, MLLM, VLA, etc.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.20197v1": {
    "title": "Zero-Shot Attribution for Large Language Models: A Distribution Testing\n  Approach",
    "summary": "A growing fraction of all code is sampled from Large Language Models (LLMs).\nWe investigate the problem of attributing code generated by language models\nusing hypothesis testing to leverage established techniques and guarantees.\nGiven a set of samples $S$ and a suspect model $\\mathcal{L}^*$, our goal is to\nassess the likelihood of $S$ originating from $\\mathcal{L}^*$. Due to the curse\nof dimensionality, this is intractable when only samples from the LLM are\ngiven: to circumvent this, we use both samples and density estimates from the\nLLM, a form of access commonly available.\n  We introduce $\\mathsf{Anubis}$, a zero-shot attribution tool that frames\nattribution as a distribution testing problem. Our experiments on a benchmark\nof code samples show that $\\mathsf{Anubis}$ achieves high AUROC scores (\n$\\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and\nStable-Code using only $\\approx 2000$ samples.",
    "published": "2025-06-25T07:37:16Z",
    "updated": "2025-06-25T07:37:16Z",
    "id": "2506.20197v1",
    "authors": [
      "Clment L. Canonne",
      "Yash Pote",
      "Uddalok Sarkar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20197v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20197v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20197v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on attributing code generated by Large Language Models (LLMs) using hypothesis testing and distribution testing approaches. It specifically addresses the problem of determining the likelihood of code samples originating from a suspect LLM, which is directly related to the study of LLMs and their applications.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.20194v1": {
    "title": "DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in\n  LLMs",
    "summary": "Large language models (LLMs) deliver strong performance but are difficult to\ndeploy due to high memory and compute costs. While pruning reduces these\ndemands, most methods ignore activation sparsity observed at runtime. We\nreinterpret activation sparsity as dynamic structured weight sparsity and\npropose DuoGPT, a unified framework that constructs dual-sparse (spMspV)\nworkloads by combining unstructured weight pruning with activation sparsity. To\npreserve accuracy, we extend the Optimal Brain Compression (OBC) framework with\nactivation-aware calibration and introduce output residuals from the dense\nmodel as correction terms. We further optimize the solution for efficient GPU\nexecution, enabling scalability to billion-parameter LLMs. Evaluations on\nLLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured\npruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\\times$\ncompared to the baseline dense model.",
    "published": "2025-06-25T07:35:12Z",
    "updated": "2025-06-25T07:35:12Z",
    "id": "2506.20194v1",
    "authors": [
      "Ruokai Yin",
      "Yuhang Li",
      "Donghyun Lee",
      "Priyadarshini Panda"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20194v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20194v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20194v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on pruning techniques for Large Language Models (LLMs) to reduce memory and compute costs, which is a core aspect of LLM research. It introduces a novel method combining unstructured weight pruning with activation sparsity, which is relevant to the 'LLM' and 'Scaling' topics as it addresses model efficiency and deployment challenges.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.20187v2": {
    "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
    "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
    "published": "2025-06-25T07:26:42Z",
    "updated": "2025-07-02T05:12:29Z",
    "id": "2506.20187v2",
    "authors": [
      "He Sun",
      "Li Li",
      "Mingjun Xiao",
      "Chengzhong Xu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20187v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20187v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20187v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the challenges and solutions for long-context LLM inference on a single commodity GPU, focusing on memory management and efficiency improvements. This aligns with topics related to LLM memory management and scaling.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Scaling"
    ]
  },
  "2506.22500v1": {
    "title": "Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data\n  Curation for Surgical Risk Perception in Multimodal Large Language Models",
    "summary": "Surgical risk identification is critical for patient safety and reducing\npreventable medical errors. While multimodal large language models (MLLMs) show\npromise for automated operating room (OR) risk detection, they often exhibit\nvisual-semantic knowledge conflicts (VS-KC), failing to identify visual safety\nviolations despite understanding textual rules. To address this, we introduce a\ndataset comprising over 34,000 synthetic images generated by diffusion models,\ndepicting operating room scenes containing entities that violate established\nsafety rules. These images were created to alleviate data scarcity and examine\nMLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated\nimages that serve as a gold-standard reference for validation. This\ncomprehensive dataset, spanning diverse perspectives, stages, and\nconfigurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC\nsignificantly improves MLLMs' detection of trained conflict entities and\ngeneralizes well to new viewpoints for these entities, but performance on\nuntrained entity types remains poor, highlighting learning specificity and the\nneed for comprehensive training. The main contributions of this work include:\n(1) a data generation methodology tailored for rule-violation scenarios; (2)\nthe release of the OR-VSKC dataset and its associated benchmark as open-source\nresources; and (3) an empirical analysis of violation-sensitive knowledge\nconsistency in representative MLLMs. The dataset and appendix are available at\nhttps://github.com/zgg2577/VS-KC.",
    "published": "2025-06-25T07:06:29Z",
    "updated": "2025-06-25T07:06:29Z",
    "id": "2506.22500v1",
    "authors": [
      "Weiyi Zhao",
      "Xiaoyu Tan",
      "Liang Liu",
      "Sijia Li",
      "Youwei Song",
      "Xihe Qiu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22500v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22500v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22500v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on multimodal large language models (MLLMs) and their application in surgical risk perception, involving synthetic data curation and visual-semantic knowledge conflicts. The dataset and benchmark are central to the study.",
    "llm_cls_result": [
      "MLLM",
      "Dataset",
      "Benchmark"
    ]
  },
  "2506.20170v1": {
    "title": "JsDeObsBench: Measuring and Benchmarking LLMs for JavaScript\n  Deobfuscation",
    "summary": "Deobfuscating JavaScript (JS) code poses a significant challenge in web\nsecurity, particularly as obfuscation techniques are frequently used to conceal\nmalicious activities within scripts. While Large Language Models (LLMs) have\nrecently shown promise in automating the deobfuscation process, transforming\ndetection and mitigation strategies against these obfuscated threats, a\nsystematic benchmark to quantify their effectiveness and limitations has been\nnotably absent. To address this gap, we present JsDeObsBench, a dedicated\nbenchmark designed to rigorously evaluate the effectiveness of LLMs in the\ncontext of JS deobfuscation. We detail our benchmarking methodology, which\nincludes a wide range of obfuscation techniques ranging from basic variable\nrenaming to sophisticated structure transformations, providing a robust\nframework for assessing LLM performance in real-world scenarios. Our extensive\nexperimental analysis investigates the proficiency of cutting-edge LLMs, e.g.,\nGPT-4o, Mixtral, Llama, and DeepSeek-Coder, revealing superior performance in\ncode simplification despite challenges in maintaining syntax accuracy and\nexecution reliability compared to baseline methods. We further evaluate the\ndeobfuscation of JS malware to exhibit the potential of LLMs in security\nscenarios. The findings highlight the utility of LLMs in deobfuscation\napplications and pinpoint crucial areas for further improvement.",
    "published": "2025-06-25T06:50:13Z",
    "updated": "2025-06-25T06:50:13Z",
    "id": "2506.20170v1",
    "authors": [
      "Guoqiang Chen",
      "Xin Jin",
      "Zhiqiang Lin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20170v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20170v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20170v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on benchmarking LLMs for JavaScript deobfuscation, which involves evaluating their performance in a specific task. This aligns with the 'Benchmark' topic as it involves systematic evaluation of LLMs. Additionally, the use of LLMs in this context suggests relevance to 'LLM' as it pertains to their application in a specialized domain.",
    "llm_cls_result": [
      "Benchmark",
      "LLM"
    ]
  },
  "2506.20168v1": {
    "title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large\n  Language Models",
    "summary": "Recent advancements in multimodal large language models have enhanced\ndocument understanding by integrating textual and visual information. However,\nexisting models exhibit incompleteness within their paradigm in real-world\nscenarios, particularly under visual degradation. In such conditions, the\ncurrent response paradigm often fails to adequately perceive visual degradation\nand ambiguity, leading to overreliance on linguistic priors or misaligned\nvisual-textual reasoning. This difficulty in recognizing uncertainty frequently\nresults in the generation of hallucinatory content, especially when a precise\nanswer is not feasible. To better demonstrate and analyze this phenomenon and\nproblem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR\nhallucination in degraded document understanding. This dataset includes test\nsamples spanning identity cards and invoices, with simulated real-world\ndegradations for OCR reliability. This setup allows for evaluating models'\ncapacity, under degraded input, to distinguish reliable visual information and\nanswer accordingly, thereby highlighting the challenge of avoiding\nhallucination on uncertain data. To achieve vision-faithful reasoning and\nthereby avoid the aforementioned issues, we further introduce a GRPO-based\nframework featuring a novel reward mechanism. By incorporating a self-awareness\nof visual uncertainty and an analysis method that initiates refusal to answer\nto increase task difficulty within our supervised fine-tuning and reinforcement\nlearning framework, we successfully mitigated hallucinations in ambiguous\nregions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model\nachieves a 22\\% absolute improvement in hallucination-free accuracy over GPT-4o\non KIE-HVQA and there is no significant performance drop in standard tasks,\nhighlighting both effectiveness and robustness.",
    "published": "2025-06-25T06:44:07Z",
    "updated": "2025-06-25T06:44:07Z",
    "id": "2506.20168v1",
    "authors": [
      "Zhentao He",
      "Can Zhang",
      "Ziheng Wu",
      "Zhenghao Chen",
      "Yufei Zhan",
      "Yifan Li",
      "Zhao Zhang",
      "Xian Wang",
      "Minghui Qiu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20168v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20168v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20168v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on addressing OCR hallucinations in Multimodal Large Language Models (MLLMs) by proposing a new benchmark (KIE-HVQA) and a framework (GRPO-based) to mitigate these issues. The core topics are related to multimodal models, their evaluation, and reinforcement learning for improving their performance.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark",
      "RL"
    ]
  },
  "2506.20167v1": {
    "title": "SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series\n  Prediction with LLMs",
    "summary": "Multivariate time series forecasting requires models to simultaneously\ncapture variable-wise structural dependencies and generalize across diverse\ntasks. While structural encoders are effective in modeling feature\ninteractions, they lack the capacity to support semantic-level reasoning or\ntask adaptation. Conversely, large language models (LLMs) possess strong\ngeneralization capabilities but remain incompatible with raw time series\ninputs. This gap limits the development of unified, transferable prediction\nsystems. Therefore, we introduce SEED, a structural encoder for\nembedding-driven decoding, which integrates four stages: a token-aware encoder\nfor patch extraction, a projection module that aligns patches with language\nmodel embeddings, a semantic reprogramming mechanism that maps patches to\ntask-aware prototypes, and a frozen language model for prediction. This modular\narchitecture decouples representation learning from inference, enabling\nefficient alignment between numerical patterns and semantic reasoning.\nEmpirical results demonstrate that the proposed method achieves consistent\nimprovements over strong baselines, and comparative studies on various datasets\nconfirm SEED's role in addressing the structural-semantic modeling gap.",
    "published": "2025-06-25T06:40:14Z",
    "updated": "2025-06-25T06:40:14Z",
    "id": "2506.20167v1",
    "authors": [
      "Fengze Li",
      "Yue Wang",
      "Yangle Liu",
      "Ming Huang",
      "Dou Hong",
      "Jieming Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20167v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20167v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20167v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of structural encoders with large language models (LLMs) for time series prediction, focusing on aligning numerical patterns with semantic reasoning. This involves the use of LLMs for generalization and task adaptation, which is a core aspect of LLM research.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.20156v1": {
    "title": "Irec: A Metacognitive Scaffolding for Self-Regulated Learning through\n  Just-in-Time Insight Recall: A Conceptual Framework and System Prototype",
    "summary": "The core challenge in learning has shifted from knowledge acquisition to\neffective Self-Regulated Learning (SRL): planning, monitoring, and reflecting\non one's learning. Existing digital tools, however, inadequately support\nmetacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized\nreview, overlooking the role of context, while Personal Knowledge Management\n(PKM) tools require high manual maintenance.\n  To address these challenges, this paper introduces \"Insight Recall,\" a novel\nparadigm that conceptualizes the context-triggered retrieval of personal past\ninsights as a metacognitive scaffold to promote SRL. We formalize this paradigm\nusing the Just-in-Time Adaptive Intervention (JITAI) framework and implement a\nprototype system, Irec, to demonstrate its feasibility. At its core, Irec uses\na dynamic knowledge graph of the user's learning history. When a user faces a\nnew problem, a hybrid retrieval engine recalls relevant personal \"insights.\"\nSubsequently, a large language model (LLM) performs a deep similarity\nassessment to filter and present the most relevant scaffold in a just-in-time\nmanner. To reduce cognitive load, Irec features a human-in-the-loop pipeline\nfor LLM-based knowledge graph construction. We also propose an optional \"Guided\nInquiry\" module, where users can engage in a Socratic dialogue with an expert\nLLM, using the current problem and recalled insights as context. The\ncontribution of this paper is a solid theoretical framework and a usable system\nplatform for designing next-generation intelligent learning systems that\nenhance metacognition and self-regulation.",
    "published": "2025-06-25T06:23:39Z",
    "updated": "2025-06-25T06:23:39Z",
    "id": "2506.20156v1",
    "authors": [
      "Xuefei Hou",
      "Xizhao Tan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20156v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20156v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20156v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a large language model (LLM) in a system prototype for self-regulated learning, which involves metacognitive scaffolding and just-in-time insight recall. The LLM is used for deep similarity assessment and knowledge graph construction, indicating relevance to LLM research.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.20151v1": {
    "title": "EAR: Erasing Concepts from Unified Autoregressive Models",
    "summary": "Autoregressive (AR) models have achieved unified and strong performance\nacross both visual understanding and image generation tasks. However, removing\nundesired concepts from AR models while maintaining overall generation quality\nremains an open challenge. In this paper, we propose Erasure Autoregressive\nModel (EAR), a fine-tuning method for effective and utility-preserving concept\nerasure in AR models. Specifically, we introduce Windowed Gradient Accumulation\n(WGA) strategy to align patch-level decoding with erasure objectives, and\nThresholded Loss Masking (TLM) strategy to protect content unrelated to the\ntarget concept during fine-tuning. Furthermore, we propose a novel benchmark,\nErase Concept Generator and Visual Filter (ECGVF), aim at provide a more\nrigorous and comprehensive foundation for evaluating concept erasure in AR\nmodels. Specifically, we first employ structured templates across diverse large\nlanguage models (LLMs) to pre-generate a large-scale corpus of\ntarget-replacement concept prompt pairs. Subsequently, we generate images from\nthese prompts and subject them to rigorous filtering via a visual classifier to\nensure concept fidelity and alignment. Extensive experimental results conducted\non the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR\nachieves marked improvements in both erasure effectiveness and model utility\npreservation. Code is available at: https://github.com/immc-lab/ear/",
    "published": "2025-06-25T06:15:07Z",
    "updated": "2025-06-25T06:15:07Z",
    "id": "2506.20151v1",
    "authors": [
      "Haipeng Fan",
      "Shiyuan Zhang",
      " Baohunesitu",
      "Zihang Guo",
      "Huaiwen Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20151v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20151v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20151v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a method for erasing concepts from autoregressive models, which involves fine-tuning strategies and a novel benchmark for evaluation. It mentions the use of large language models (LLMs) for generating prompts, but the primary focus is on the erasure method and the benchmark, not on LLMs themselves.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.20112v1": {
    "title": "A Multi-Pass Large Language Model Framework for Precise and Efficient\n  Radiology Report Error Detection",
    "summary": "Background: The positive predictive value (PPV) of large language model\n(LLM)-based proofreading for radiology reports is limited due to the low error\nprevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV\nand reduces operational costs compared with baseline approaches. Materials and\nMethods: A retrospective analysis was performed on 1,000 consecutive radiology\nreports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III\ndatabase. Two external datasets (CheXpert and Open-i) were validation sets.\nThree LLM frameworks were tested: (1) single-prompt detector; (2) extractor\nplus detector; and (3) extractor, detector, and false-positive verifier.\nPrecision was measured by PPV and absolute true positive rate (aTPR).\nEfficiency was calculated from model inference charges and reviewer\nremuneration. Statistical significance was tested using cluster bootstrap,\nexact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV\nincreased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,\nFramework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.\nbaselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per\n1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and\nUSD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.\nHuman-reviewed reports decreased from 192 to 88. External validation supported\nFramework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR\n(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and\nreduced operational costs, maintaining detection performance, providing an\neffective strategy for AI-assisted radiology report quality assurance.",
    "published": "2025-06-25T04:02:29Z",
    "updated": "2025-06-25T04:02:29Z",
    "id": "2506.20112v1",
    "authors": [
      "Songsoo Kim",
      "Seungtae Lee",
      "See Young Lee",
      "Joonho Kim",
      "Keechan Kan",
      "Dukyong Yoon"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20112v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20112v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20112v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of a multi-pass Large Language Model (LLM) framework for radiology report error detection, which involves the application of LLMs in a specific domain (radiology) and their efficiency and precision improvements. The core focus is on the use of LLMs for a specialized task, which aligns with the 'LLM' topic.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.22496v1": {
    "title": "Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models:\n  A Behavioral Economics Approach to AI Safety",
    "summary": "Large Language Models (LLMs) exhibit systematic risk-taking behaviors\nanalogous to those observed in gambling psychology, including overconfidence\nbias, loss-chasing tendencies, and probability misjudgment. Drawing from\nbehavioral economics and prospect theory, we identify and formalize these\n\"gambling-like\" patterns where models sacrifice accuracy for high-reward\noutputs, exhibit escalating risk-taking after errors, and systematically\nmiscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG)\nframework, incorporating insights from gambling research to address these\nbehavioral biases through risk-calibrated training, loss-aversion mechanisms,\nand uncertainty-aware decision making. Our approach introduces novel evaluation\nparadigms based on established gambling psychology experiments, including AI\nadaptations of the Iowa Gambling Task and probability learning assessments.\nExperimental results demonstrate measurable reductions in gambling-like\nbehaviors: 18.7\\% decrease in overconfidence bias, 24.3\\% reduction in\nloss-chasing tendencies, and improved risk calibration across diverse\nscenarios. This work establishes the first systematic framework for\nunderstanding and mitigating gambling psychology patterns in AI systems.",
    "published": "2025-06-25T03:45:35Z",
    "updated": "2025-06-25T03:45:35Z",
    "id": "2506.22496v1",
    "authors": [
      "Y. Du"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22496v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22496v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22496v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses risk-taking behaviors in Large Language Models (LLMs) and proposes a framework to mitigate these behaviors, which is directly related to the study of LLMs and their behavioral characteristics.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2506.20103v1": {
    "title": "BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization\n  in AI-Generated Videos",
    "summary": "Recent advances in deep generative models have led to significant progress in\nvideo generation, yet the fidelity of AI-generated videos remains limited.\nSynthesized content often exhibits visual artifacts such as temporally\ninconsistent motion, physically implausible trajectories, unnatural object\ndeformations, and local blurring that undermine realism and user trust.\nAccurate detection and spatial localization of these artifacts are crucial for\nboth automated quality control and for guiding the development of improved\ngenerative models. However, the research community currently lacks a\ncomprehensive benchmark specifically designed for artifact localization in AI\ngenerated videos. Existing datasets either restrict themselves to video or\nframe level detection or lack the fine-grained spatial annotations necessary\nfor evaluating localization methods. To address this gap, we introduce\nBrokenVideos, a benchmark dataset of 3,254 AI-generated videos with\nmeticulously annotated, pixel-level masks highlighting regions of visual\ncorruption. Each annotation is validated through detailed human inspection to\nensure high quality ground truth. Our experiments show that training state of\nthe art artifact detection models and multi modal large language models (MLLMs)\non BrokenVideos significantly improves their ability to localize corrupted\nregions. Through extensive evaluation, we demonstrate that BrokenVideos\nestablishes a critical foundation for benchmarking and advancing research on\nartifact localization in generative video models. The dataset is available at:\nhttps://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.",
    "published": "2025-06-25T03:30:04Z",
    "updated": "2025-06-25T03:30:04Z",
    "id": "2506.20103v1",
    "authors": [
      "Jiahao Lin",
      "Weixuan Peng",
      "Bojia Zi",
      "Yifeng Gao",
      "Xianbiao Qi",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20103v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20103v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20103v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a benchmark dataset for artifact localization in AI-generated videos, which involves fine-grained spatial annotations and evaluation of multi-modal large language models (MLLMs). This aligns with the topics of Benchmark and Dataset, as it provides a new dataset for evaluating models and benchmarks their performance. Additionally, the mention of MLLMs suggests relevance to the MLLM topic.",
    "llm_cls_result": [
      "Benchmark",
      "Dataset",
      "MLLM"
    ]
  },
  "2506.20097v1": {
    "title": "PSALM-V: Automating Symbolic Planning in Interactive Visual Environments\n  with Large Language Models",
    "summary": "We propose PSALM-V, the first autonomous neuro-symbolic learning system able\nto induce symbolic action semantics (i.e., pre- and post-conditions) in visual\nenvironments through interaction. PSALM-V bootstraps reliable symbolic planning\nwithout expert action definitions, using LLMs to generate heuristic plans and\ncandidate symbolic semantics. Previous work has explored using large language\nmodels to generate action semantics for Planning Domain Definition Language\n(PDDL)-based symbolic planners. However, these approaches have primarily\nfocused on text-based domains or relied on unrealistic assumptions, such as\naccess to a predefined problem file, full observability, or explicit error\nmessages. By contrast, PSALM-V dynamically infers PDDL problem files and domain\naction semantics by analyzing execution outcomes and synthesizing possible\nerror explanations. The system iteratively generates and executes plans while\nmaintaining a tree-structured belief over possible action semantics for each\naction, iteratively refining these beliefs until a goal state is reached.\nSimulated experiments of task completion in ALFRED demonstrate that PSALM-V\nincreases the plan success rate from 37% (Claude-3.7) to 74% in partially\nobserved setups. Results on two 2D game environments, RTFM and Overcooked-AI,\nshow that PSALM-V improves step efficiency and succeeds in domain induction in\nmulti-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions\nfor real-world robot BlocksWorld tasks, despite low-level manipulation failures\nfrom the robot.",
    "published": "2025-06-25T02:44:20Z",
    "updated": "2025-06-25T02:44:20Z",
    "id": "2506.20097v1",
    "authors": [
      "Wang Bill Zhu",
      "Miaosen Chai",
      "Ishika Singh",
      "Robin Jia",
      "Jesse Thomason"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20097v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20097v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20097v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using large language models (LLMs) to generate heuristic plans and symbolic semantics for autonomous neuro-symbolic learning in visual environments, which aligns with the topics of LLM (Large Language Models) and Reasoning (LLM reasoning and complex problem solving). The integration of visual environments also touches on MLLM (Multimodal Large Language Models).",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "MLLM"
    ]
  },
  "2506.20093v1": {
    "title": "ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA\n  with Large-Scale Multitask Dataset",
    "summary": "Time-series data are critical in diverse applications, such as industrial\nmonitoring, medical diagnostics, and climate research. However, effectively\nintegrating these high-dimensional temporal signals with natural language for\ndynamic, interactive tasks remains a significant challenge. To address this, we\nintroduce the Time-Series Question Answering (Time-Series QA) task and release\nEngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset\ndesigned to capture complex interactions between time-series signals and\nnatural language. Building on this resource, we propose the Instruct Time\nTransformer (ITFormer), a novel framework that bridges time-series encoders\nwith frozen large language models (LLMs). ITFormer effectively extracts,\naligns, and fuses temporal and textual features, achieving a strong improvement\nin QA accuracy over strong baselines with fewer than 1\\% additional trainable\nparameters. By combining computational efficiency with robust cross-modal\nmodeling, our work establishes a adaptable paradigm for integrating temporal\ndata with natural language, paving the way for new research and applications in\nmulti-modal AI. More details about the project, including datasets and code,\nare available at: https://pandalin98.github.io/itformer_site/",
    "published": "2025-06-25T02:33:47Z",
    "updated": "2025-06-25T02:33:47Z",
    "id": "2506.20093v1",
    "authors": [
      "Yilin Wang",
      "Peixuan Lei",
      "Jie Song",
      "Yuzhe Hao",
      "Tao Chen",
      "Yuxuan Zhang",
      "Lei Jia",
      "Yuanxiang Li",
      "Zhongyu Wei"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20093v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20093v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20093v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a novel framework for integrating time-series data with natural language using a frozen large language model (LLM) and a new multi-modal QA dataset. The focus is on multi-modal integration and the use of LLMs, making it relevant to MLLM and LLM topics.",
    "llm_cls_result": [
      "MLLM",
      "LLM"
    ]
  },
  "2506.20073v1": {
    "title": "A Modular Multitask Reasoning Framework Integrating Spatio-temporal\n  Models and LLMs",
    "summary": "Spatio-temporal data mining plays a pivotal role in informed decision making\nacross diverse domains. However, existing models are often restricted to narrow\ntasks, lacking the capacity for multi-task inference and complex long-form\nreasoning that require generation of in-depth, explanatory outputs. These\nlimitations restrict their applicability to real-world, multi-faceted decision\nscenarios. In this work, we introduce STReason, a novel framework that\nintegrates the reasoning strengths of large language models (LLMs) with the\nanalytical capabilities of spatio-temporal models for multi-task inference and\nexecution. Without requiring task-specific finetuning, STReason leverages\nin-context learning to decompose complex natural language queries into modular,\ninterpretable programs, which are then systematically executed to generate both\nsolutions and detailed rationales. To facilitate rigorous evaluation, we\nconstruct a new benchmark dataset and propose a unified evaluation framework\nwith metrics specifically designed for long-form spatio-temporal reasoning.\nExperimental results show that STReason significantly outperforms advanced LLM\nbaselines across all metrics, particularly excelling in complex,\nreasoning-intensive spatio-temporal scenarios. Human evaluations further\nvalidate STReason's credibility and practical utility, demonstrating its\npotential to reduce expert workload and broaden the applicability to real-world\nspatio-temporal tasks. We believe STReason provides a promising direction for\ndeveloping more capable and generalizable spatio-temporal reasoning systems.",
    "published": "2025-06-25T00:55:34Z",
    "updated": "2025-06-25T00:55:34Z",
    "id": "2506.20073v1",
    "authors": [
      "Kethmi Hirushini Hettige",
      "Jiahao Ji",
      "Cheng Long",
      "Shili Xiang",
      "Gao Cong",
      "Jingyuan Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20073v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20073v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20073v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) with spatio-temporal models for multi-task inference and execution, leveraging in-context learning and reasoning abilities. It also introduces a new benchmark dataset and evaluation framework, which aligns with the topics of LLM, Reasoning, and Benchmark.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2506.20070v1": {
    "title": "Multimodal Information Retrieval for Open World with Edit Distance Weak\n  Supervision",
    "summary": "Existing multi-media retrieval models either rely on creating a common\nsubspace with modality-specific representation models or require schema mapping\namong modalities to measure similarities among multi-media data. Our goal is to\navoid the annotation overhead incurred from considering retrieval as a\nsupervised classification task and re-use the pretrained encoders in large\nlanguage models and vision tasks. We propose \"FemmIR\", a framework to retrieve\nmultimodal results relevant to information needs expressed with multimodal\nqueries by example without any similarity label. Such identification is\nnecessary for real-world applications where data annotations are scarce and\nsatisfactory performance is required without fine-tuning with a common\nframework across applications. We curate a new dataset called MuQNOL for\nbenchmarking progress on this task. Our technique is based on weak supervision\nintroduced through edit distance between samples: graph edit distance can be\nmodified to consider the cost of replacing a data sample in terms of its\nproperties, and relevance can be measured through the implicit signal from the\namount of edit cost among the objects. Unlike metric learning or encoding\nnetworks, FemmIR re-uses the high-level properties and maintains the property\nvalue and relationship constraints with a multi-level interaction score between\ndata samples and the query example provided by the user. We empirically\nevaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs\ncomparably to similar retrieval systems in delivering on-demand retrieval\nresults with exact and approximate similarities while using the existing\nproperty identifiers in the system.",
    "published": "2025-06-25T00:25:08Z",
    "updated": "2025-06-25T00:25:08Z",
    "id": "2506.20070v1",
    "authors": [
      "KMA Solaiman",
      "Bharat Bhargava"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20070v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20070v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20070v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses multimodal information retrieval using pretrained encoders from large language models and vision tasks, and introduces a new dataset for benchmarking. It aligns with topics related to multimodal large language models and datasets.",
    "llm_cls_result": [
      "MLLM",
      "Dataset"
    ]
  },
  "2506.20061v1": {
    "title": "Learning Instruction-Following Policies through Open-Ended Instruction\n  Relabeling with Large Language Models",
    "summary": "Developing effective instruction-following policies in reinforcement learning\nremains challenging due to the reliance on extensive human-labeled instruction\ndatasets and the difficulty of learning from sparse rewards. In this paper, we\npropose a novel approach that leverages the capabilities of large language\nmodels (LLMs) to automatically generate open-ended instructions retrospectively\nfrom previously collected agent trajectories. Our core idea is to employ LLMs\nto relabel unsuccessful trajectories by identifying meaningful subtasks the\nagent has implicitly accomplished, thereby enriching the agent's training data\nand substantially alleviating reliance on human annotations. Through this\nopen-ended instruction relabeling, we efficiently learn a unified\ninstruction-following policy capable of handling diverse tasks within a single\npolicy. We empirically evaluate our proposed method in the challenging Craftax\nenvironment, demonstrating clear improvements in sample efficiency, instruction\ncoverage, and overall policy performance compared to state-of-the-art\nbaselines. Our results highlight the effectiveness of utilizing LLM-guided\nopen-ended instruction relabeling to enhance instruction-following\nreinforcement learning.",
    "published": "2025-06-24T23:49:28Z",
    "updated": "2025-06-24T23:49:28Z",
    "id": "2506.20061v1",
    "authors": [
      "Zhicheng Zhang",
      "Ziyan Wang",
      "Yali Du",
      "Fei Fang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20061v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20061v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20061v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses leveraging large language models (LLMs) to enhance reinforcement learning (RL) by generating open-ended instructions from agent trajectories, which aligns with both LLM and RL topics. The focus on improving instruction-following policies through LLM-guided relabeling also touches on the RLHF aspect of reinforcement learning.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.20059v1": {
    "title": "DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test\n  Recommendation and Diagnosis Prediction",
    "summary": "Recent advances in Large Language Models (LLMs) have led to remarkable\nprogresses in medical consultation. However, existing medical LLMs overlook the\nessential role of Electronic Health Records (EHR) and focus primarily on\ndiagnosis recommendation, limiting their clinical applicability. We propose\nDiaLLM, the first medical LLM that integrates heterogeneous EHR data into\nclinically grounded dialogues, enabling clinical test recommendation, result\ninterpretation, and diagnosis prediction to better align with real-world\nmedical practice. To construct clinically grounded dialogues from EHR, we\ndesign a Clinical Test Reference (CTR) strategy that maps each clinical code to\nits corresponding description and classifies test results as \"normal\" or\n\"abnormal\". Additionally, DiaLLM employs a reinforcement learning framework for\nevidence acquisition and automated diagnosis. To handle the large action space,\nwe introduce a reject sampling strategy to reduce redundancy and improve\nexploration efficiency. Furthermore, a confirmation reward and a\nclass-sensitive diagnosis reward are designed to guide accurate diagnosis\nprediction. Extensive experimental results demonstrate that DiaLLM outperforms\nbaselines in clinical test recommendation and diagnosis prediction.",
    "published": "2025-06-24T23:47:21Z",
    "updated": "2025-06-24T23:47:21Z",
    "id": "2506.20059v1",
    "authors": [
      "Weijieying Ren",
      "Tianxiang Zhao",
      "Lei Wang",
      "Tianchun Wang",
      "Vasant Honavar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20059v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20059v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20059v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on integrating Large Language Models (LLMs) with Electronic Health Records (EHR) for clinical applications, utilizing reinforcement learning for evidence acquisition and diagnosis prediction.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "AGI"
    ]
  },
  "2506.20051v1": {
    "title": "Controlled Retrieval-augmented Context Evaluation for Long-form RAG",
    "summary": "Retrieval-augmented generation (RAG) enhances large language models by\nincorporating context retrieved from external knowledge sources. While the\neffectiveness of the retrieval module is typically evaluated with\nrelevance-based ranking metrics, such metrics may be insufficient to reflect\nthe retrieval's impact on the final RAG result, especially in long-form\ngeneration scenarios. We argue that providing a comprehensive\nretrieval-augmented context is important for long-form RAG tasks like report\ngeneration and propose metrics for assessing the context independent of\ngeneration. We introduce CRUX, a \\textbf{C}ontrolled\n\\textbf{R}etrieval-a\\textbf{U}gmented conte\\textbf{X}t evaluation framework\ndesigned to directly assess retrieval-augmented contexts. This framework uses\nhuman-written summaries to control the information scope of knowledge, enabling\nus to measure how well the context covers information essential for long-form\ngeneration. CRUX uses question-based evaluation to assess RAG's retrieval in a\nfine-grained manner. Empirical results show that CRUX offers more reflective\nand diagnostic evaluation. Our findings also reveal substantial room for\nimprovement in current retrieval methods, pointing to promising directions for\nadvancing RAG's retrieval. Our data and code are publicly available to support\nand advance future research on retrieval.",
    "published": "2025-06-24T23:17:48Z",
    "updated": "2025-06-24T23:17:48Z",
    "id": "2506.20051v1",
    "authors": [
      "Jia-Huei Ju",
      "Suzan Verberne",
      "Maarten de Rijke",
      "Andrew Yates"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20051v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20051v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20051v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on retrieval-augmented generation (RAG) and its evaluation, particularly in long-form generation scenarios. It introduces a framework for assessing retrieval-augmented contexts, which aligns with the topics of Memory (retrieval-augmented generation) and Benchmark (evaluation metrics and performance comparison).",
    "llm_cls_result": [
      "Memory",
      "Benchmark"
    ]
  },
  "2506.20040v2": {
    "title": "Cross-Layer Discrete Concept Discovery for Interpreting Language Models",
    "summary": "Uncovering emergent concepts across transformer layers remains a significant\nchallenge because the residual stream linearly mixes and duplicates\ninformation, obscuring how features evolve within large language models.\nCurrent research efforts primarily inspect neural representations at single\nlayers, thereby overlooking this cross-layer superposition and the redundancy\nit introduces. These representations are typically either analyzed directly for\nactivation patterns or passed to probing classifiers that map them to a limited\nset of predefined concepts. To address these limitations, we propose\ncross-layer VQ-VAE (CLVQ-VAE), a framework that uses vector quantization to map\nrepresentations across layers and in the process collapse duplicated\nresidual-stream features into compact, interpretable concept vectors. Our\napproach uniquely combines top-k temperature-based sampling during quantization\nwith EMA codebook updates, providing controlled exploration of the discrete\nlatent space while maintaining code-book diversity. We further enhance the\nframework with scaled-spherical k-means++ for codebook initialization, which\nclusters by directional similarity rather than magnitude, better aligning with\nsemantic structure in word embedding space.",
    "published": "2025-06-24T22:43:36Z",
    "updated": "2025-07-16T21:35:12Z",
    "id": "2506.20040v2",
    "authors": [
      "Ankur Garg",
      "Xuemin Yu",
      "Hassan Sajjad",
      "Samira Ebrahimi Kahou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20040v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20040v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20040v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on interpreting language models by uncovering emergent concepts across transformer layers, which is closely related to the study of Large Language Models (LLM) and their architectures. The proposed framework involves vector quantization and clustering techniques to enhance interpretability, which aligns with the broader research on LLMs.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.20020v1": {
    "title": "Persona-Assigned Large Language Models Exhibit Human-Like Motivated\n  Reasoning",
    "summary": "Reasoning in humans is prone to biases due to underlying motivations like\nidentity protection, that undermine rational decision-making and judgment. This\nmotivated reasoning at a collective level can be detrimental to society when\ndebating critical issues such as human-driven climate change or vaccine safety,\nand can further aggravate political polarization. Prior studies have reported\nthat large language models (LLMs) are also susceptible to human-like cognitive\nbiases, however, the extent to which LLMs selectively reason toward\nidentity-congruent conclusions remains largely unexplored. Here, we investigate\nwhether assigning 8 personas across 4 political and socio-demographic\nattributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and\nproprietary) across two reasoning tasks from human-subject studies -- veracity\ndiscernment of misinformation headlines and evaluation of numeric scientific\nevidence -- we find that persona-assigned LLMs have up to 9% reduced veracity\ndiscernment relative to models without personas. Political personas\nspecifically, are up to 90% more likely to correctly evaluate scientific\nevidence on gun control when the ground truth is congruent with their induced\npolitical identity. Prompt-based debiasing methods are largely ineffective at\nmitigating these effects. Taken together, our empirical findings are the first\nto suggest that persona-assigned LLMs exhibit human-like motivated reasoning\nthat is hard to mitigate through conventional debiasing prompts -- raising\nconcerns of exacerbating identity-congruent reasoning in both LLMs and humans.",
    "published": "2025-06-24T21:35:17Z",
    "updated": "2025-06-24T21:35:17Z",
    "id": "2506.20020v1",
    "authors": [
      "Saloni Dash",
      "Amlie Reymond",
      "Emma S. Spiro",
      "Aylin Caliskan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20020v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20020v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20020v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper investigates the behavior of large language models (LLMs) when assigned personas, showing human-like motivated reasoning, which is a cognitive bias. This aligns with research on LLMs and their reasoning abilities, particularly in the context of biases and reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.20018v1": {
    "title": "Achieving Trustworthy Real-Time Decision Support Systems with\n  Low-Latency Interpretable AI Models",
    "summary": "This paper investigates real-time decision support systems that leverage\nlow-latency AI models, bringing together recent progress in holistic AI-driven\ndecision tools, integration with Edge-IoT technologies, and approaches for\neffective human-AI teamwork. It looks into how large language models can assist\ndecision-making, especially when resources are limited. The research also\nexamines the effects of technical developments such as DeLLMa, methods for\ncompressing models, and improvements for analytics on edge devices, while also\naddressing issues like limited resources and the need for adaptable frameworks.\nThrough a detailed review, the paper offers practical perspectives on\ndevelopment strategies and areas of application, adding to the field by\npointing out opportunities for more efficient and flexible AI-supported\nsystems. The conclusions set the stage for future breakthroughs in this\nfast-changing area, highlighting how AI can reshape real-time decision support.",
    "published": "2025-06-24T21:22:25Z",
    "updated": "2025-06-24T21:22:25Z",
    "id": "2506.20018v1",
    "authors": [
      "Zechun Deng",
      "Ziwei Liu",
      "Ziqian Bi",
      "Junhao Song",
      "Chia Xin Liang",
      "Joe Yeong",
      "Junfeng Hao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20018v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20018v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20018v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in real-time decision support systems, which aligns with the LLM topic. It also touches on model compression and edge device improvements, which are relevant to the Scaling topic. The focus on decision-making and human-AI teamwork suggests a connection to AGI.",
    "llm_cls_result": [
      "LLM",
      "Scaling",
      "AGI"
    ]
  },
  "2506.20009v1": {
    "title": "Accurate and Energy Efficient: Local Retrieval-Augmented Generation\n  Models Outperform Commercial Large Language Models in Medical Tasks",
    "summary": "Background The increasing adoption of Artificial Intelligence (AI) in\nhealthcare has sparked growing concerns about its environmental and ethical\nimplications. Commercial Large Language Models (LLMs), such as ChatGPT and\nDeepSeek, require substantial resources, while the utilization of these systems\nfor medical purposes raises critical issues regarding patient privacy and\nsafety. Methods We developed a customizable Retrieval-Augmented Generation\n(RAG) framework for medical tasks, which monitors its energy usage and CO2\nemissions. This system was then used to create RAGs based on various\nopen-source LLMs. The tested models included both general purpose models like\nllama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs\nperformance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs\no4-mini model. A dataset of medical questions was used for the evaluation.\nResults Custom RAG models outperformed commercial models in accuracy and energy\nconsumption. The RAG model built on llama3.1:8B achieved the highest accuracy\n(58.5%) and was significantly better than other models, including o4-mini and\nDeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption\nand CO2 footprint among all models, with a Performance per kWh of 0.52 and a\ntotal CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x\ntimes more accuracy points per kWh and 172% less electricity usage while\nmaintaining higher accuracy. Conclusion Our study demonstrates that local LLMs\ncan be leveraged to develop RAGs that outperform commercial, online LLMs in\nmedical tasks, while having a smaller environmental impact. Our modular\nframework promotes sustainable AI development, reducing electricity usage and\naligning with the UNs Sustainable Development Goals.",
    "published": "2025-06-24T20:56:03Z",
    "updated": "2025-06-24T20:56:03Z",
    "id": "2506.20009v1",
    "authors": [
      "Konstantinos Vrettos",
      "Michail E. Klontzas"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20009v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20009v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20009v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper discusses the development and evaluation of Retrieval-Augmented Generation (RAG) models based on open-source LLMs for medical tasks, comparing their performance and energy efficiency to commercial LLMs. The focus on RAG models and their application in medical tasks aligns with the topics of Memory (for retrieval-augmented generation) and LLM (for the use of large language models).",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2506.20008v1": {
    "title": "QHackBench: Benchmarking Large Language Models for Quantum Code\n  Generation Using PennyLane Hackathon Challenges",
    "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\npotential in code generation, yet their effectiveness in quantum computing\nremains underexplored. This paper benchmarks LLMs for PennyLane-based quantum\ncode generation using real-world challenges from the Quantum Hackathon (QHack).\nWe introduce QHackBench, a novel benchmark dataset derived from QHack\ncompetitions, and evaluate model performance under vanilla prompting and\nRetrieval-Augmented Generation (RAG). Our structured evaluation framework\nassesses functional correctness, syntactic validity, and execution success\nacross varying challenge difficulties. Results indicate that RAG-enhanced\nmodels, supplemented with an augmented PennyLane dataset, approximately\ngenerate similar results as the standard prompting, particularly in complex\nquantum algorithms. Additionally, we introduce a multi-agent evaluation\npipeline that iteratively refines incorrect solutions, further enhancing\nexecution success rates. To foster further research, we commit to publicly\nreleasing QHackBench, along with our evaluation framework and experimental\nresults, enabling continued advancements in AI-assisted quantum programming.",
    "published": "2025-06-24T20:54:56Z",
    "updated": "2025-06-24T20:54:56Z",
    "id": "2506.20008v1",
    "authors": [
      "Abdul Basit",
      "Minghao Shao",
      "Haider Asif",
      "Nouhaila Innan",
      "Muhammad Kashif",
      "Alberto Marchisio",
      "Muhammad Shafique"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20008v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20008v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20008v1",
    "keywords": [
      "LLM",
      "Memory"
    ],
    "cls_reason": "The paper focuses on benchmarking Large Language Models (LLMs) for quantum code generation, which involves evaluating their performance in a specific domain (quantum computing) using a novel benchmark dataset (QHackBench). The study also explores Retrieval-Augmented Generation (RAG) and multi-agent evaluation, which are relevant to LLM research and benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark",
      "Memory"
    ]
  },
  "2506.22493v1": {
    "title": "A Detailed Factor Analysis for the Political Compass Test: Navigating\n  Ideologies of Large Language Models",
    "summary": "Political Compass Test (PCT) or similar questionnaires have been used to\nquantify LLM's political leanings. Building on a recent line of work that\nexamines the validity of PCT tests, we demonstrate that variation in standard\ngeneration parameters does not significantly impact the models' PCT scores.\nHowever, external factors such as prompt variations and fine-tuning\nindividually and in combination affect the same. Finally, we demonstrate that\nwhen models are fine-tuned on text datasets with higher political content than\nothers, the PCT scores are not differentially affected. This calls for a\nthorough investigation into the validity of PCT and similar tests, as well as\nthe mechanism by which political leanings are encoded in LLMs.",
    "published": "2025-06-24T20:33:51Z",
    "updated": "2025-06-24T20:33:51Z",
    "id": "2506.22493v1",
    "authors": [
      "Sadia Kamal",
      "Lalu Prasad Yadav Prakash",
      "S M Rafiuddin",
      "Mohammed Rakib",
      "Arunkumar Bagavathi",
      "Atriya Sen",
      "Sagnik Ray Choudhury"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22493v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22493v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22493v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the political leanings of Large Language Models (LLMs) and the factors affecting their scores on the Political Compass Test (PCT), which is directly related to the study of LLMs and their behaviors.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.19993v1": {
    "title": "CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender\n  Systems",
    "summary": "Recommender systems play a pivotal role in providing relevant content to\nusers. With the rapid development of large language models (LLMs), researchers\nhave begun utilizing LLMs to build more powerful recommender systems. However,\nexisting approaches that focus on aligning LLMs with recommendation tasks do\nnot fully leverage their sequential information processing capabilities,\nleading to suboptimal performance.\n  In this paper, we propose a novel system called compressed vocabulary\nexpansion (CoVE). In CoVE, each item is assigned a unique ID within the\nexpanded vocabulary. Our framework effectively capitalizes on sequence\nunderstanding abilities of LLMs, significantly enhancing their performance on\nrecommendation tasks. Additionally, we compress the embedding layer, making\nCoVE practical for large-scale industrial applications. The effectiveness and\nperformance of CoVE are demonstrated through comprehensive experiments on\nmultiple recommendation datasets and comparisons with prior works. Our code can\nbe found at https://github.com/HaochenZhang717/CoVE-official-Repo.",
    "published": "2025-06-24T20:27:51Z",
    "updated": "2025-06-24T20:27:51Z",
    "id": "2506.19993v1",
    "authors": [
      "Haochen Zhang",
      "Tianyi Zhang",
      "Junze Yin",
      "Oren Gal",
      "Anshumali Shrivastava",
      "Vladimir Braverman"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19993v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19993v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19993v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in recommender systems, focusing on enhancing their performance through compressed vocabulary expansion. This aligns with the 'LLM' topic as it involves research on LLMs and their applications. Additionally, the mention of leveraging sequential information processing capabilities of LLMs hints at 'Reasoning' as it involves enhancing the model's ability to process and understand sequences, which is a form of reasoning.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.19992v1": {
    "title": "HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs\n  for Efficient Summarization",
    "summary": "The explosive growth of complex datasets across various modalities\nnecessitates advanced analytical tools that not only group data effectively but\nalso provide human-understandable insights into the discovered structures. We\nintroduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using\nLLMs for Efficient Summarization), a novel algorithm and Python package\ndesigned for hierarchical k-means clustering of diverse data types, including\ntext, images, and numeric data (processed one modality per run). HERCULES\nconstructs a cluster hierarchy by recursively applying k-means clustering,\nstarting from individual data points at level 0. A key innovation is its deep\nintegration of Large Language Models (LLMs) to generate semantically rich\ntitles and descriptions for clusters at each level of the hierarchy,\nsignificantly enhancing interpretability. The algorithm supports two main\nrepresentation modes: `direct' mode, which clusters based on original data\nembeddings or scaled numeric features, and `description' mode, which clusters\nbased on embeddings derived from LLM-generated summaries. Users can provide a\n`topic\\_seed' to guide LLM-generated summaries towards specific themes. An\ninteractive visualization tool facilitates thorough analysis and understanding\nof the clustering results. We demonstrate HERCULES's capabilities and discuss\nits potential for extracting meaningful, hierarchical knowledge from complex\ndatasets.",
    "published": "2025-06-24T20:22:00Z",
    "updated": "2025-06-24T20:22:00Z",
    "id": "2506.19992v1",
    "authors": [
      "Gabor Petnehazi",
      "Bernadett Aradi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19992v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19992v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19992v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper introduces a novel algorithm that integrates Large Language Models (LLMs) for hierarchical clustering and summarization of diverse data types, emphasizing the use of LLMs for generating semantically rich descriptions.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2506.19977v1": {
    "title": "Context Attribution with Multi-Armed Bandit Optimization",
    "summary": "Understanding which parts of the retrieved context contribute to a large\nlanguage model's generated answer is essential for building interpretable and\ntrustworthy generative QA systems. We propose a novel framework that formulates\ncontext attribution as a combinatorial multi-armed bandit (CMAB) problem. Each\ncontext segment is treated as a bandit arm, and we employ Combinatorial\nThompson Sampling (CTS) to efficiently explore the exponentially large space of\ncontext subsets under a limited query budget. Our method defines a reward\nfunction based on normalized token likelihoods, capturing how well a subset of\nsegments supports the original model response. Unlike traditional\nperturbation-based attribution methods such as SHAP, which sample subsets\nuniformly and incur high computational costs, our approach adaptively balances\nexploration and exploitation by leveraging posterior estimates of segment\nrelevance. This leads to substantially improved query efficiency while\nmaintaining high attribution fidelity. Extensive experiments on diverse\ndatasets and LLMs demonstrate that our method achieves competitive attribution\nquality with fewer model queries.",
    "published": "2025-06-24T19:47:27Z",
    "updated": "2025-06-24T19:47:27Z",
    "id": "2506.19977v1",
    "authors": [
      "Deng Pan",
      "Keerthiram Murugesan",
      "Nuno Moniz",
      "Nitesh Chawla"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19977v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19977v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19977v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on understanding and attributing the influence of retrieved context on a large language model's generated answers, which involves interpretability and efficiency in model queries. It uses a combinatorial multi-armed bandit approach for context attribution, which is related to reinforcement learning (RL) and the interpretability of LLMs.",
    "llm_cls_result": [
      "RL",
      "LLM"
    ]
  },
  "2506.19952v1": {
    "title": "CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical\n  Distillation",
    "summary": "Large language models (LLMs), despite their ability to perform few-shot\nmachine translation (MT), often lag behind dedicated MT systems trained on\nparallel corpora, which are crucial for high quality machine translation (MT).\nHowever, parallel corpora are often scarce or non-existent for low-resource\nlanguages. In this paper, we propose CycleDistill, a bootstrapping approach\nleveraging LLMs and few-shot translation to obtain high-quality MT systems.\nCycleDistill involves iteratively generating synthetic parallel corpora from\nmonolingual corpora via zero- or few-shot MT, which is then used to fine-tune\nthe model that was used for generating said data for MT. CycleDistill does not\nneed parallel corpora beyond 1 to 4 few-shot examples, and in our experiments\nfocusing on three Indian languages, by relying solely on monolingual corpora,\nit can achieve high-quality machine translation, improving upon a few-shot\nbaseline model by over 20-30 chrF points on average in the first iteration. We\nalso study the effect of leveraging softmax activations during the distillation\nprocess and observe mild improvements in translation quality.",
    "published": "2025-06-24T18:56:57Z",
    "updated": "2025-06-24T18:56:57Z",
    "id": "2506.19952v1",
    "authors": [
      "Deepon Halder",
      "Thanmay Jayakumar",
      "Raj Dabre"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19952v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19952v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19952v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on leveraging Large Language Models (LLMs) for machine translation, specifically through a cyclical distillation process. It discusses the use of LLMs for few-shot translation and the generation of synthetic parallel corpora, which aligns with the LLM topic. Additionally, the iterative improvement of translation quality through distillation touches on the Pretrain topic, as it involves fine-tuning strategies. The paper does not directly address other topics like RL, MLLM, VLA, MoE, AGI, Scaling, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2506.19935v1": {
    "title": "Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and\n  Architecture",
    "summary": "Large language models (LLMs) predominantly use autoregressive (AR)\napproaches, but masked diffusion models (MDMs) are emerging as viable\nalternatives. A key challenge in comparing AR and MDM paradigms is their\ntypical architectural difference: AR models are often decoder-only, while MDMs\nhave largely been encoder-only. This practice of changing both the modeling\nparadigm and architecture simultaneously makes direct comparisons unfair, as\nit's hard to distinguish whether observed differences stem from the paradigm\nitself or the architectural shift. This research evaluates MDMs within a\ndecoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or\nAO-AR) and standard AR paradigms. Our investigation suggests that the standard\nAO-AR objective, which averages over all token permutations, may benefit from\nrefinement, as many permutations appear less informative compared to the\nlanguage's inherent left-to-right structure. (2) Investigate architectural\ninfluences (decoder-only vs. encoder-only) within MDMs. We demonstrate that\nwhile encoder-only MDMs model a simpler conditional probability space,\ndecoder-only MDMs can achieve dramatic generation speedups ($\\sim25\\times$) and\ncomparable perplexity with temperature annealing despite modeling a vastly\nlarger space, highlighting key trade-offs. This work thus decouples core\nparadigm differences from architectural influences, offering insights for\nfuture model design. Code is available at https://github.com/scxue/AO-GPT-MDM.",
    "published": "2025-06-24T18:22:25Z",
    "updated": "2025-06-24T18:22:25Z",
    "id": "2506.19935v1",
    "authors": [
      "Shuchen Xue",
      "Tianyu Xie",
      "Tianyang Hu",
      "Zijin Feng",
      "Jiacheng Sun",
      "Kenji Kawaguchi",
      "Zhenguo Li",
      "Zhi-Ming Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19935v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19935v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19935v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the comparison between autoregressive (AR) and masked diffusion models (MDMs) within the context of large language models (LLMs), focusing on architectural differences and paradigm comparisons. It directly relates to the research on Large Language Models (LLMs) and their architectures.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.19923v1": {
    "title": "Prover Agent: An Agent-based Framework for Formal Mathematical Proofs",
    "summary": "We present Prover Agent, a novel AI agent for automated theorem proving that\nintegrates large language models (LLMs) with a formal proof assistant, Lean.\nProver Agent coordinates an informal reasoning LLM, a formal prover model, and\nfeedback from Lean while also generating auxiliary lemmas to assist in\ndiscovering the overall proof strategy. It achieves an 86.1% success rate on\nthe MiniF2F benchmark, establishing a new state-of-the-art among methods using\nsmall language models (SLMs) with a much lower sample budget than previous\napproaches. We also present case studies illustrating how these generated\nlemmas contribute to solving challenging problems.",
    "published": "2025-06-24T18:01:52Z",
    "updated": "2025-06-24T18:01:52Z",
    "id": "2506.19923v1",
    "authors": [
      "Kaito Baba",
      "Chaoran Liu",
      "Shuhei Kurita",
      "Akiyoshi Sannai"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19923v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19923v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19923v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of large language models (LLMs) with a formal proof assistant for automated theorem proving, which involves reasoning and the use of LLMs in a specific application context.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.19846v1": {
    "title": "JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents\n  with Reinforcement Learning",
    "summary": "Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm\nfor increasingly complex tasks. However, joint evolution across heterogeneous\nagents remains challenging due to cooperative inefficiency and training\ninstability. In this paper, we propose the joint evolution dynamics for MARL\ncalled JoyAgents-R1, which first applies Group Relative Policy Optimization\n(GRPO) to the joint training of heterogeneous multi-agents. By iteratively\nrefining agents' large language models (LLMs) and memories, the method achieves\nholistic equilibrium with optimal decision-making and memory capabilities.\nSpecifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on\nthe behavior of each agent across entire reasoning trajectories to enhance GRPO\nsampling efficiency while maintaining policy diversity. Then, our marginal\nbenefit-driven selection strategy identifies top-$K$ sampling groups with\nmaximal reward fluctuations, enabling targeted agent model updates that improve\ntraining stability and maximize joint benefits through cost-effective parameter\nadjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution\nmechanism that repurposes GRPO rewards as cost-free supervisory signals to\neliminate repetitive reasoning and accelerate convergence. Experiments across\ngeneral and domain-specific scenarios demonstrate that JoyAgents-R1 achieves\nperformance comparable to that of larger LLMs while built on smaller\nopen-source models.",
    "published": "2025-06-24T17:59:31Z",
    "updated": "2025-06-24T17:59:31Z",
    "id": "2506.19846v1",
    "authors": [
      "Ai Han",
      "Junxing Hu",
      "Pu Wei",
      "Zhiqian Zhang",
      "Yuhang Guo",
      "Jiawei Lu",
      "Zicheng Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19846v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19846v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19846v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in a multi-agent reinforcement learning (MARL) setting, focusing on joint evolution dynamics and reinforcement learning techniques. It also mentions the refinement of agents' LLMs and memories, which relates to the RL and Memory topics.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Memory"
    ]
  },
  "2506.19835v1": {
    "title": "MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via\n  Role-Specialized Collaboration",
    "summary": "Recent advancements in medical Large Language Models (LLMs) have showcased\ntheir powerful reasoning and diagnostic capabilities. Despite their success,\ncurrent unified multimodal medical LLMs face limitations in knowledge update\ncosts, comprehensiveness, and flexibility. To address these challenges, we\nintroduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis\n(MAM). Inspired by our empirical findings highlighting the benefits of role\nassignment and diagnostic discernment in LLMs, MAM decomposes the medical\ndiagnostic process into specialized roles: a General Practitioner, Specialist\nTeam, Radiologist, Medical Assistant, and Director, each embodied by an\nLLM-based agent. This modular and collaborative framework enables efficient\nknowledge updates and leverages existing medical LLMs and knowledge bases.\nExtensive experimental evaluations conducted on a wide range of publicly\naccessible multimodal medical datasets, incorporating text, image, audio, and\nvideo modalities, demonstrate that MAM consistently surpasses the performance\nof modality-specific LLMs. Notably, MAM achieves significant performance\nimprovements ranging from 18% to 365% compared to baseline models. Our code is\nreleased at https://github.com/yczhou001/MAM.",
    "published": "2025-06-24T17:52:43Z",
    "updated": "2025-06-24T17:52:43Z",
    "id": "2506.19835v1",
    "authors": [
      "Yucheng Zhou",
      "Lingran Song",
      "Jianbing Shen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19835v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19835v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19835v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses a modular multi-agent framework for multi-modal medical diagnosis using LLMs, which involves role-specialized collaboration and leverages existing medical LLMs and knowledge bases. This aligns with the topics of LLM (Large Language Models) and MLLM (Multimodal Large Language Models) due to the use of multimodal data and LLM-based agents. Additionally, the framework's collaborative and modular approach could be seen as a step towards more generalist models, touching on AGI (Artificial General Intelligence).",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "AGI"
    ]
  },
  "2506.19813v1": {
    "title": "Curating art exhibitions using machine learning",
    "summary": "Art curatorship has always been mostly the subjective work of human experts,\nwho, with extensive knowledge of many and diverse artworks, select a few of\nthose to present in communal spaces, spaces that evolved into what we now call\nart galleries. There are no hard and fast set of rules on how to select these\nartworks, given a theme which either is presented to the art curator or\nconstructed by her/him. Here we present a series of artificial models -- a\ntotal of four related models -- based on machine learning techniques (a subset\nof artificial intelligence) that attempt to learn from existing exhibitions\nwhich have been curated by human experts, in order to be able to do similar\ncuratorship work. We focus exclusively on the last 25 years of past exhibitions\nat the Metropolitan Museum of Art in New York, due to the quality of the data\navailable and the physical and time limitations of our research. Our four\nartificial intelligence models achieve a reasonable ability at imitating these\nvarious curators responsible for all those exhibitions, with various degrees of\nprecision and curatorial coherence. In particular, we can conclude two key\ninsights: first, that there is sufficient information in these exhibitions to\nconstruct an artificial intelligence model that replicates past exhibitions\nwith an accuracy well above random choices; second, that using feature\nengineering and carefully designing the architecture of modest size models can\nmake them as good as those using the so-called large language models such as\nGPT in a brute force approach. We also believe, based on small attempts to use\nthe models in out-of-sample experiments, that given more much more data, it\nshould be possible for these kinds of artificial intelligence agents to be\ncloser and closer to the aesthetic and curatorial judgment of human art\ncurators.",
    "published": "2025-06-24T17:25:03Z",
    "updated": "2025-06-24T17:25:03Z",
    "id": "2506.19813v1",
    "authors": [
      "Eurico Covas"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19813v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19813v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19813v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of machine learning techniques to curate art exhibitions, focusing on models that learn from past exhibitions curated by human experts. While it mentions machine learning and artificial intelligence, it does not specifically align with the provided topics such as LLM, RL, MLLM, etc., as it does not involve large language models, reinforcement learning, multimodal models, or other specified areas.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.19807v2": {
    "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
    "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
    "published": "2025-06-24T17:17:17Z",
    "updated": "2025-07-06T16:11:23Z",
    "id": "2506.19807v2",
    "authors": [
      "Baochang Ren",
      "Shuofei Qiao",
      "Wenhao Yu",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19807v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19807v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19807v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Reinforcement Learning (RL) to enhance the factuality of Large Language Models (LLMs) by integrating a factuality reward based on knowledge verification. This involves both LLM and RL topics, specifically focusing on improving reasoning and factuality in LLMs through RL techniques.",
    "llm_cls_result": [
      "LLM",
      "RL",
      "Reasoning"
    ]
  },
  "2506.19806v1": {
    "title": "LLM-Based Social Simulations Require a Boundary",
    "summary": "This position paper argues that large language model (LLM)-based social\nsimulations should establish clear boundaries to meaningfully contribute to\nsocial science research. While LLMs offer promising capabilities for modeling\nhuman-like agents compared to traditional agent-based modeling, they face\nfundamental limitations that constrain their reliability for social pattern\ndiscovery. The core issue lies in LLMs' tendency towards an ``average persona''\nthat lacks sufficient behavioral heterogeneity, a critical requirement for\nsimulating complex social dynamics. We examine three key boundary problems:\nalignment (simulated behaviors matching real-world patterns), consistency\n(maintaining coherent agent behavior over time), and robustness\n(reproducibility under varying conditions). We propose heuristic boundaries for\ndetermining when LLM-based simulations can reliably advance social science\nunderstanding. We believe that these simulations are more valuable when\nfocusing on (1) collective patterns rather than individual trajectories, (2)\nagent behaviors aligning with real population averages despite limited\nvariance, and (3) proper validation methods available for testing simulation\nrobustness. We provide a practical checklist to guide researchers in\ndetermining the appropriate scope and claims for LLM-based social simulations.",
    "published": "2025-06-24T17:14:47Z",
    "updated": "2025-06-24T17:14:47Z",
    "id": "2506.19806v1",
    "authors": [
      "Zengqing Wu",
      "Run Peng",
      "Takayuki Ito",
      "Chuan Xiao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19806v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19806v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19806v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in social simulations, focusing on their limitations and proposing boundaries for their application in social science research. The core topics revolve around LLMs and their application in modeling human-like agents, which aligns with the 'LLM' and 'AGI' categories.",
    "llm_cls_result": [
      "LLM",
      "AGI"
    ]
  },
  "2506.19802v1": {
    "title": "KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs",
    "summary": "Despite extensive research on Machine Learning-based Network Intrusion\nDetection Systems (ML-NIDS), their capability to detect diverse attack variants\nremains uncertain. Prior studies have largely relied on homogeneous datasets,\nwhich artificially inflate performance scores and offer a false sense of\nsecurity. Designing systems that can effectively detect a wide range of attack\nvariants remains a significant challenge. The progress of ML-NIDS continues to\ndepend heavily on human expertise, which can embed subjective judgments of\nsystem designers into the model, potentially hindering its ability to\ngeneralize across diverse attack types.\n  To address this gap, we propose KnowML, a framework for knowledge-guided\nmachine learning that integrates attack knowledge into ML-NIDS. KnowML\nsystematically explores the threat landscape by leveraging Large Language\nModels (LLMs) to perform automated analysis of attack implementations. It\nconstructs a unified Knowledge Graph (KG) of attack strategies, on which it\napplies symbolic reasoning to generate KG-Augmented Input, embedding domain\nknowledge directly into the design process of ML-NIDS.\n  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly\ncollected for this study. Our findings reveal that baseline ML-NIDS models fail\nto detect several variants entirely, achieving F1 scores as low as 0 %. In\ncontrast, our knowledge-guided approach achieves up to 99 % F1 score while\nmaintaining a False Positive Rate below 0.1 %.",
    "published": "2025-06-24T17:08:58Z",
    "updated": "2025-06-24T17:08:58Z",
    "id": "2506.19802v1",
    "authors": [
      "Xin Fan Guo",
      "Albert Merono Penuela",
      "Sergio Maffeis",
      "Fabio Pierazzi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19802v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19802v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19802v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the integration of Large Language Models (LLMs) into a framework for improving Machine Learning-based Network Intrusion Detection Systems (ML-NIDS). It focuses on using LLMs for automated analysis and constructing a Knowledge Graph (KG) to enhance the generalization of ML-NIDS, which aligns with the 'LLM' and 'Reasoning' topics.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2506.19794v2": {
    "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
    "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
    "published": "2025-06-24T17:04:23Z",
    "updated": "2025-07-07T14:20:16Z",
    "id": "2506.19794v2",
    "authors": [
      "Yuqi Zhu",
      "Yi Zhong",
      "Jintian Zhang",
      "Ziheng Zhang",
      "Shuofei Qiao",
      "Yujie Luo",
      "Lun Du",
      "Da Zheng",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19794v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19794v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19794v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating and improving the data analysis capabilities of open-source LLMs, which involves reasoning-intensive tasks and strategic planning. The study also involves benchmarking and dataset curation to enhance model performance.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2506.19783v1": {
    "title": "SAGE: Strategy-Adaptive Generation Engine for Query Rewriting",
    "summary": "Query rewriting is pivotal for enhancing dense retrieval, yet current methods\ndemand large-scale supervised data or suffer from inefficient reinforcement\nlearning (RL) exploration. In this work, we first establish that guiding Large\nLanguage Models (LLMs) with a concise set of expert-crafted strategies, such as\nsemantic expansion and entity disambiguation, substantially improves retrieval\neffectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus,\nand SciFact. Building on this insight, we introduce the Strategy-Adaptive\nGeneration Engine (SAGE), which operationalizes these strategies in an RL\nframework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit\nShaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative\nlearning signals. This strategy-guided approach not only achieves new\nstate-of-the-art NDCG@10 results, but also uncovers a compelling emergent\nbehavior: the agent learns to select optimal strategies, reduces unnecessary\nexploration, and generates concise rewrites, lowering inference cost without\nsacrificing performance. Our findings demonstrate that strategy-guided RL,\nenhanced with nuanced reward shaping, offers a scalable, efficient, and more\ninterpretable paradigm for developing the next generation of robust information\nretrieval systems.",
    "published": "2025-06-24T16:50:51Z",
    "updated": "2025-06-24T16:50:51Z",
    "id": "2506.19783v1",
    "authors": [
      "Teng Wang",
      "Hailei Gong",
      "Changwang Zhang",
      "Jun Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19783v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19783v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19783v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in query rewriting, guided by expert-crafted strategies and operationalized in a Reinforcement Learning (RL) framework. It also introduces novel reward shaping mechanisms within the RL context.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.19767v1": {
    "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning",
    "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks.",
    "published": "2025-06-24T16:31:37Z",
    "updated": "2025-06-24T16:31:37Z",
    "id": "2506.19767v1",
    "authors": [
      "Yuqian Fu",
      "Tinghong Chen",
      "Jiajun Chai",
      "Xihuai Wang",
      "Songjun Tu",
      "Guojun Yin",
      "Wei Lin",
      "Qichao Zhang",
      "Yuanheng Zhu",
      "Dongbin Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19767v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19767v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19767v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on integrating Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for reasoning tasks in Large Language Models (LLMs), which directly relates to the topics of Reasoning and RL. The mention of LLMs also ties it to the LLM topic.",
    "llm_cls_result": [
      "Reasoning",
      "RL",
      "LLM"
    ]
  },
  "2506.19753v2": {
    "title": "Arabic Dialect Classification using RNNs, Transformers, and Large\n  Language Models: A Comparative Analysis",
    "summary": "The Arabic language is among the most popular languages in the world with a\nhuge variety of dialects spoken in 22 countries. In this study, we address the\nproblem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.\nRNN models, Transformer models, and large language models (LLMs) via prompt\nengineering are created and tested. Among these, MARBERTv2 performed best with\n65% accuracy and 64% F1-score. Through the use of state-of-the-art\npreprocessing techniques and the latest NLP models, this paper identifies the\nmost significant linguistic issues in Arabic dialect identification. The\nresults corroborate applications like personalized chatbots that respond in\nusers' dialects, social media monitoring, and greater accessibility for Arabic\ncommunities.",
    "published": "2025-06-24T16:06:58Z",
    "updated": "2025-06-28T12:32:10Z",
    "id": "2506.19753v2",
    "authors": [
      "Omar A. Essameldin",
      "Ali O. Elbeih",
      "Wael H. Gomaa",
      "Wael F. Elsersy"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19753v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19753v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19753v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the use of Large Language Models (LLMs) for Arabic dialect classification, which directly relates to the 'LLM' topic. Additionally, the study involves preprocessing techniques and NLP models, which are relevant to 'Pretrain' as it discusses pretraining strategies for language models. The comparative analysis of different models also touches on 'Benchmark' as it evaluates performance.",
    "llm_cls_result": [
      "LLM",
      "Pretrain",
      "Benchmark"
    ]
  },
  "2506.19733v2": {
    "title": "Breaking Barriers: Do Reinforcement Post Training Gains Transfer To\n  Unseen Domains?",
    "summary": "Reinforcement post training (RPT) has recently shown promise in improving the\nreasoning abilities of large language models (LLMs). However, it remains\nunclear how well these improvements generalize to new domains, as prior work\nevaluates RPT models on data from the same domains used for fine-tuning. To\nunderstand the generalizability of RPT, we conduct two studies. (1)\nObservational: We compare a wide range of open-weight RPT models against their\ncorresponding base models across multiple domains, including both seen and\nunseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs\nwith RPT on single domains and evaluate their performance across multiple\ndomains. Both studies converge on the same conclusion that, although RPT brings\nsubstantial gains on tasks similar to the fine-tuning data, the gains\ngeneralize inconsistently and can vanish on domains with different reasoning\npatterns.",
    "published": "2025-06-24T15:53:10Z",
    "updated": "2025-07-23T23:12:20Z",
    "id": "2506.19733v2",
    "authors": [
      "Chuxuan Hu",
      "Yuxuan Zhu",
      "Antony Kellermann",
      "Caleb Biddulph",
      "Suppakit Waiwitlikhit",
      "Jason Benn",
      "Daniel Kang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19733v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19733v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19733v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the impact of Reinforcement post training (RPT) on large language models (LLMs) and its generalizability across different domains. It specifically focuses on the reasoning abilities of LLMs and the effects of RPT, which is closely related to Reinforcement Learning (RL) and Reasoning in the context of LLMs.",
    "llm_cls_result": [
      "RL",
      "Reasoning",
      "LLM"
    ]
  },
  "2506.22491v1": {
    "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation",
    "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology.",
    "published": "2025-06-24T15:33:18Z",
    "updated": "2025-06-24T15:33:18Z",
    "id": "2506.22491v1",
    "authors": [
      "Oliver Warke",
      "Joemon M. Jose",
      "Faegheh Hasibi",
      "Jan Breitsohl"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22491v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22491v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22491v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using LLMs for data augmentation in the context of conflict classification, which involves sensitive and nuanced tasks. The primary focus is on the application of LLMs rather than their architecture or scaling, and it does not directly address multimodal models, reinforcement learning, or other specific LLM research areas.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2506.19702v1": {
    "title": "LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology\n  and Differential Diagnosis",
    "summary": "Medical document analysis plays a crucial role in extracting essential\nclinical insights from unstructured healthcare records, supporting critical\ntasks such as differential diagnosis. Determining the most probable condition\namong overlapping symptoms requires precise evaluation and deep medical\nexpertise. While recent advancements in large language models (LLMs) have\nsignificantly enhanced performance in medical document analysis, privacy\nconcerns related to sensitive patient data limit the use of online LLMs\nservices in clinical settings. To address these challenges, we propose a\ntrustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using\nlow-rank adaptation, specifically optimized for differential diagnosis tasks.\nOur approach utilizes DDXPlus, the largest benchmark dataset for differential\ndiagnosis, and demonstrates superior performance in pathology prediction and\nvariable-length differential diagnosis compared to existing methods. The\ndeveloped web-based platform allows users to submit their own unstructured\nmedical documents and receive accurate, explainable diagnostic results. By\nincorporating advanced explainability techniques, the system ensures\ntransparent and reliable predictions, fostering user trust and confidence.\nExtensive evaluations confirm that the proposed method surpasses current\nstate-of-the-art models in predictive accuracy while offering practical utility\nin clinical settings. This work addresses the urgent need for reliable,\nexplainable, and privacy-preserving artificial intelligence solutions,\nrepresenting a significant advancement in intelligent medical document analysis\nfor real-world healthcare applications. The code can be found at\n\\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}.",
    "published": "2025-06-24T15:12:42Z",
    "updated": "2025-06-24T15:12:42Z",
    "id": "2506.19702v1",
    "authors": [
      "Lei Kang",
      "Xuanshuo Fu",
      "Oriol Ramos Terrades",
      "Javier Vazquez-Corral",
      "Ernest Valveny",
      "Dimosthenis Karatzas"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19702v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19702v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19702v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on the application of large language models (LLMs) in medical document analysis, specifically for differential diagnosis tasks. It mentions fine-tuning a LLaMA-v3 model, which is a type of LLM, and discusses the use of a benchmark dataset for evaluation. The core topics are thus related to LLMs and their application in a specific domain, with a mention of benchmarking.",
    "llm_cls_result": [
      "LLM",
      "Benchmark"
    ]
  },
  "2506.19697v1": {
    "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
    "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
    "published": "2025-06-24T15:03:57Z",
    "updated": "2025-06-24T15:03:57Z",
    "id": "2506.19697v1",
    "authors": [
      "Jungwoo Park",
      "Taewhoo Lee",
      "Chanwoong Yoon",
      "Hyeon Hwang",
      "Jaewoo Kang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19697v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19697v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19697v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses pre-training strategies for Large Language Models (LLMs) to mitigate quantization issues, which is relevant to both LLM research and pretraining strategies.",
    "llm_cls_result": [
      "LLM",
      "Pretrain"
    ]
  },
  "2506.19683v2": {
    "title": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning\n  Guidance",
    "summary": "Understanding medical ultrasound imaging remains a long-standing challenge\ndue to significant visual variability caused by differences in imaging and\nacquisition parameters. Recent advancements in large language models (LLMs)\nhave been used to automatically generate terminology-rich summaries orientated\nto clinicians with sufficient physiological knowledge. Nevertheless, the\nincreasing demand for improved ultrasound interpretability and basic scanning\nguidance among non-expert users, e.g., in point-of-care settings, has not yet\nbeen explored. In this study, we first introduce the scene graph (SG) for\nultrasound images to explain image content to ordinary and provide guidance for\nultrasound scanning. The ultrasound SG is first computed using a\ntransformer-based one-stage method, eliminating the need for explicit object\ndetection. To generate a graspable image explanation for ordinary, the user\nquery is then used to further refine the abstract SG representation through\nLLMs. Additionally, the predicted SG is explored for its potential in guiding\nultrasound scanning toward missing anatomies within the current imaging view,\nassisting ordinary users in achieving more standardized and complete anatomical\nexploration. The effectiveness of this SG-based image explanation and scanning\nguidance has been validated on images from the left and right neck regions,\nincluding the carotid and thyroid, across five volunteers. The results\ndemonstrate the potential of the method to maximally democratize ultrasound by\nenhancing its interpretability and usability for ordinaries.",
    "published": "2025-06-24T14:49:40Z",
    "updated": "2025-06-26T14:20:13Z",
    "id": "2506.19683v2",
    "authors": [
      "Xuesong Li",
      "Dianye Huang",
      "Yameng Zhang",
      "Nassir Navab",
      "Zhongliang Jiang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19683v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19683v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19683v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) to generate explanations and guidance for ultrasound images, which aligns with the topic of LLM research. Additionally, it involves multimodal aspects by integrating vision (ultrasound images) and language (explanations), making it relevant to Multimodal Large Language Models (MLLM).",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2506.19677v2": {
    "title": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees",
    "summary": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving.",
    "published": "2025-06-24T14:44:33Z",
    "updated": "2025-06-25T16:13:14Z",
    "id": "2506.19677v2",
    "authors": [
      "Shi Chang",
      "Boyuan Chen",
      "Kishanthan Thangarajah",
      "Hanan Lutfiyya",
      "Ahmed E. Hassan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19677v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19677v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19677v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses scheduling and serving strategies for Code Large Language Models (CodeLLMs), which are a subset of Large Language Models (LLMs). The focus is on improving performance and efficiency in serving these models, which aligns with the 'LLM' and 'Scaling' topics. However, it does not directly address multimodal aspects, reinforcement learning, or other specialized topics.",
    "llm_cls_result": [
      "LLM",
      "Scaling"
    ]
  },
  "2506.19676v3": {
    "title": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures",
    "summary": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence and adaptability, and are rapidly changing human\nproduction and life. Nowadays, agents are undergoing a new round of evolution.\nThey no longer act as an isolated island like LLMs. Instead, they start to\ncommunicate with diverse external entities, such as other agents and tools, to\nperform more complex tasks collectively. Under this trend, agent communication\nis regarded as a foundational pillar of the future AI ecosystem, and many\norganizations have intensively begun to design related communication protocols\n(e.g., Anthropic's MCP and Google's A2A) within the recent few months. However,\nthis new field exposes significant security hazards, which can cause severe\ndamage to real-world scenarios. To help researchers quickly figure out this\npromising topic and benefit the future agent communication development, this\npaper presents a comprehensive survey of agent communication security. More\nprecisely, we first present a clear definition of agent communication and\ncategorize the entire lifecycle of agent communication into three stages:\nuser-agent interaction, agent-agent communication, and agent-environment\ncommunication. Next, for each communication phase, we dissect related protocols\nand analyze the security risks according to the communication characteristics.\nThen, we summarize and outlook on the possible defense countermeasures for each\nrisk. In addition, we conduct experiments using MCP and A2A to help readers\nbetter understand the novel vulnerabilities brought by agent communication.\nFinally, we discuss open issues and future directions in this promising\nresearch field.",
    "published": "2025-06-24T14:44:28Z",
    "updated": "2025-07-02T08:50:11Z",
    "id": "2506.19676v3",
    "authors": [
      "Dezhang Kong",
      "Shi Lin",
      "Zhenhua Xu",
      "Zhebo Wang",
      "Minghao Li",
      "Yufeng Li",
      "Yilun Zhang",
      "Hujin Peng",
      "Zeyang Sha",
      "Yuyuan Li",
      "Changting Lin",
      "Xun Wang",
      "Xuan Liu",
      "Ningyu Zhang",
      "Chaochao Chen",
      "Muhammad Khurram Khan",
      "Meng Han"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19676v3",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19676v3.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19676v3",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper discusses LLM-driven AI agents and their communication protocols, security risks, and defense countermeasures, which aligns with the topics of LLM (Large Language Models) and RL (Reinforcement Learning, particularly in the context of agent communication and interaction).",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.19665v1": {
    "title": "Recurrent Visual Feature Extraction and Stereo Attentions for CT Report\n  Generation",
    "summary": "Generating reports for computed tomography (CT) images is a challenging task,\nwhile similar to existing studies for medical image report generation, yet has\nits unique characteristics, such as spatial encoding of multiple images,\nalignment between image volume and texts, etc. Existing solutions typically use\ngeneral 2D or 3D image processing techniques to extract features from a CT\nvolume, where they firstly compress the volume and then divide the compressed\nCT slices into patches for visual encoding. These approaches do not explicitly\naccount for the transformations among CT slices, nor do they effectively\nintegrate multi-level image features, particularly those containing specific\norgan lesions, to instruct CT report generation (CTRG). In considering the\nstrong correlation among consecutive slices in CT scans, in this paper, we\npropose a large language model (LLM) based CTRG method with recurrent visual\nfeature extraction and stereo attentions for hierarchical feature modeling.\nSpecifically, we use a vision Transformer to recurrently process each slice in\na CT volume, and employ a set of attentions over the encoded slices from\ndifferent perspectives to selectively obtain important visual information and\nalign them with textual features, so as to better instruct an LLM for CTRG.\nExperiment results and further analysis on the benchmark M3D-Cap dataset show\nthat our method outperforms strong baseline models and achieves\nstate-of-the-art results, demonstrating its validity and effectiveness.",
    "published": "2025-06-24T14:29:06Z",
    "updated": "2025-06-24T14:29:06Z",
    "id": "2506.19665v1",
    "authors": [
      "Yuanhe Tian",
      "Lei Mao",
      "Yan Song"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19665v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19665v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19665v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on using a large language model (LLM) for generating reports from CT images, incorporating recurrent visual feature extraction and stereo attentions. It aligns with the 'LLM' topic due to its use of LLMs and the 'MLLM' topic because it involves multimodal integration of vision and language for report generation.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2506.19662v1": {
    "title": "Multimodal large language models and physics visual tasks: comparative\n  analysis of performance and costs",
    "summary": "Multimodal large language models (MLLMs) capable of processing both text and\nvisual inputs are increasingly being explored for uses in physics education,\nsuch as tutoring, formative assessment, and grading. This study evaluates a\nrange of publicly available MLLMs on a set of standardized, image-based physics\nresearch-based conceptual assessments (concept inventories). We benchmark 15\nmodels from three major providers (Anthropic, Google, and OpenAI) across 102\nphysics items, focusing on two main questions: (1) How well do these models\nperform on conceptual physics tasks involving visual representations? and (2)\nWhat are the financial costs associated with their use? The results show high\nvariability in both performance and cost. The performance of the tested models\nranges from around 76\\% to as low as 21\\%. We also found that expensive models\ndo not always outperform cheaper ones and that, depending on the demands of the\ncontext, cheaper models may be sufficiently capable for some tasks. This is\nespecially relevant in contexts where financial resources are limited or for\nlarge-scale educational implementation of MLLMs. By providing these analyses,\nour aim is to inform teachers, institutions, and other educational stakeholders\nso that they can make evidence-based decisions about the selection of models\nfor use in AI-supported physics education.",
    "published": "2025-06-24T14:25:04Z",
    "updated": "2025-06-24T14:25:04Z",
    "id": "2506.19662v1",
    "authors": [
      "Giulia Polverini",
      "Bor Gregorcic"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19662v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19662v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19662v1",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on evaluating Multimodal Large Language Models (MLLMs) on physics visual tasks, which directly relates to the MLLM topic. It also involves benchmarking these models, which fits the Benchmark topic.",
    "llm_cls_result": [
      "MLLM",
      "Benchmark"
    ]
  },
  "2506.19652v2": {
    "title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager",
    "summary": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals.",
    "published": "2025-06-24T14:15:26Z",
    "updated": "2025-07-08T14:47:33Z",
    "id": "2506.19652v2",
    "authors": [
      "Lucie Galland",
      "Catherine Pelachaud",
      "Florian Pecune"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19652v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19652v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19652v2",
    "keywords": [
      "LLM"
    ],
    "cls_reason": "The paper focuses on integrating LLMs with an RL-based dialogue manager, emphasizing the use of reinforcement learning for dialogue management and personalization, which aligns with the RL and LLM topics.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2507.14849v1": {
    "title": "Beyond Isolated Capabilities: Bridging Long CoT Reasoning and\n  Long-Context Understanding",
    "summary": "Reasoning distillation has emerged as an effective approach to enhance the\nreasoning capabilities of smaller language models. However, the impact of\nlarge-scale reasoning distillation on other critical abilities, particularly\nin-context retrieval and reasoning, remains unexplored. This gap in\nunderstanding is particularly significant given the increasing importance of\nRetrieval-Augmented Generation (RAG) systems, where efficient acquisition and\nutilization of contextual information are paramount for generating reliable\nresponses. Motivated by the need to understand how the extended long-CoT\nprocess influences long-context comprehension, we conduct a comprehensive\ninvestigation using a series of open-source models distilled from Deepseek-R1,\nrenowned for its exceptional reasoning capabilities. Our study focuses on\nevaluating these models' performance in extracting and integrating relevant\ninformation from extended contexts through multi-document question and\nanswering tasks. Through rigorous experimentation, we demonstrate that\ndistilled reasoning patterns significantly improve long-context understanding.\nOur analysis reveals that distillation fosters greater long-context awareness\nby promoting more detailed and explicit reasoning processes during context\nanalysis and information parsing. This advancement effectively mitigates the\npersistent \"lost in the middle\" issue that has hindered long-context models.",
    "published": "2025-07-20T07:43:16Z",
    "updated": "2025-07-20T07:43:16Z",
    "id": "2507.14849v1",
    "authors": [
      "Yifei Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14849v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14849v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14849v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing reasoning capabilities and long-context understanding in language models, particularly through reasoning distillation and its impact on Retrieval-Augmented Generation (RAG) systems. It discusses the integration of reasoning processes and context analysis, which aligns with the topics of Reasoning and Memory.",
    "llm_cls_result": [
      "Reasoning",
      "Memory"
    ]
  },
  "2507.14485v1": {
    "title": "Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud\n  Completion",
    "summary": "Completing the whole 3D structure based on an incomplete point cloud is a\nchallenging task, particularly when the residual point cloud lacks typical\nstructural characteristics. Recent methods based on cross-modal learning\nattempt to introduce instance images to aid the structure feature learning.\nHowever, they still focus on each particular input class, limiting their\ngeneration abilities. In this work, we propose a novel retrieval-augmented\npoint cloud completion framework. The core idea is to incorporate cross-modal\nretrieval into completion task to learn structural prior information from\nsimilar reference samples. Specifically, we design a Structural Shared Feature\nEncoder (SSFE) to jointly extract cross-modal features and reconstruct\nreference features as priors. Benefiting from a dual-channel control gate in\nthe encoder, relevant structural features in the reference sample are enhanced\nand irrelevant information interference is suppressed. In addition, we propose\na Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical\nfeature fusion mechanism to integrate reference prior information with input\nfeatures from global to local. Through extensive evaluations on multiple\ndatasets and real-world scenes, our method shows its effectiveness in\ngenerating fine-grained point clouds, as well as its generalization capability\nin handling sparse data and unseen categories.",
    "published": "2025-07-19T04:57:41Z",
    "updated": "2025-07-19T04:57:41Z",
    "id": "2507.14485v1",
    "authors": [
      "Hongye Hou",
      "Liu Zhan",
      "Yang Yang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14485v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14485v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14485v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on a retrieval-augmented framework for point cloud completion, which involves cross-modal learning and retrieval mechanisms. It does not directly align with the provided topics related to LLMs, RL, MLLM, VLA, MoE, AGI, Scaling, Pretrain, Reasoning, Memory, Benchmark, or Dataset.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.14475v1": {
    "title": "Towards Temporal Knowledge Graph Alignment in the Wild",
    "summary": "Temporal Knowledge Graph Alignment (TKGA) seeks to identify equivalent\nentities across heterogeneous temporal knowledge graphs (TKGs) for fusion to\nimprove their completeness. Although some approaches have been proposed to\ntackle this task, most assume unified temporal element standards and simplified\ntemporal structures across different TKGs. They cannot deal with TKGA in the\nwild (TKGA-Wild), where multi-scale temporal element entanglement and\ncross-source temporal structural imbalances are common. To bridge this gap, we\nstudy the task of TKGA-Wild and propose HyDRA, a new and effective solution.\nHyDRA is the first to reformulate the task via multi-scale hypergraph\nretrieval-augmented generation to address the challenges of TKGA-Wild.In\naddition, we design a new scale-weave synergy mechanism for HyDRA, which\nincorporates intra-scale interactions and cross-scale conflict detection. This\nmechanism is designed to alleviate the fragmentation caused by multi-source\ntemporal incompleteness and resolves inconsistencies arising from complex and\nuneven temporal event density distributions, thereby enhancing the model\ncapacity to handle the intricacies of real-world temporal alignment. Finally,\nthere is no standard benchmark that captures these challenges of TKGA-Wild and\neffectively evaluates existing methods. To this end, we formally propose to\nbenchmark challenges for TKGA-Wild and validate the effectiveness of the method\nby establishing two new datasets(BETA and WildBETA). Extensive experiments on\nthe new datasets and six representative benchmarks show that BETA and WildBETA\nbetter reflect real-world challenges. Meanwhile, HyDRA proposes a new paradigm\nfor TKGA-Wild, consistently outperforming 24 competitive baselines, while\nmaintaining strong efficiency and scalability.",
    "published": "2025-07-19T04:12:06Z",
    "updated": "2025-07-19T04:12:06Z",
    "id": "2507.14475v1",
    "authors": [
      "Runhao Zhao",
      "Weixin Zeng",
      "Wentao Zhang",
      "Xiang Zhao",
      "Jiuyang Tang",
      "Lei Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.14475v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.14475v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.14475v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on Temporal Knowledge Graph Alignment (TKGA) and proposes a new solution called HyDRA, which involves multi-scale hypergraph retrieval-augmented generation. It also introduces new datasets for benchmarking. The topics are not directly related to the provided list, which is more focused on LLMs, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.13937v1": {
    "title": "Marcel: A Lightweight and Open-Source Conversational Agent for\n  University Student Support",
    "summary": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
    "published": "2025-07-18T14:09:45Z",
    "updated": "2025-07-18T14:09:45Z",
    "id": "2507.13937v1",
    "authors": [
      "Jan Trienes",
      "Anastasiia Derzhanskaia",
      "Roland Schwarzkopf",
      "Markus Mhling",
      "Jrg Schltterer",
      "Christin Seifert"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.13937v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.13937v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.13937v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a conversational agent using retrieval-augmented generation, which aligns with the 'Memory' topic due to its focus on retrieval-based methods. It also involves a lightweight system design, which doesn't directly fit into the other provided categories.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2507.11974v1": {
    "title": "A Review of Generative AI in Aquaculture: Foundations, Applications, and\n  Future Directions for Smart and Sustainable Farming",
    "summary": "Generative Artificial Intelligence (GAI) has rapidly emerged as a\ntransformative force in aquaculture, enabling intelligent synthesis of\nmultimodal data, including text, images, audio, and simulation outputs for\nsmarter, more adaptive decision-making. As the aquaculture industry shifts\ntoward data-driven, automation and digital integration operations under the\nAquaculture 4.0 paradigm, GAI models offer novel opportunities across\nenvironmental monitoring, robotics, disease diagnostics, infrastructure\nplanning, reporting, and market analysis. This review presents the first\ncomprehensive synthesis of GAI applications in aquaculture, encompassing\nfoundational architectures (e.g., diffusion models, transformers, and retrieval\naugmented generation), experimental systems, pilot deployments, and real-world\nuse cases. We highlight GAI's growing role in enabling underwater perception,\ndigital twin modeling, and autonomous planning for remotely operated vehicle\n(ROV) missions. We also provide an updated application taxonomy that spans\nsensing, control, optimization, communication, and regulatory compliance.\nBeyond technical capabilities, we analyze key limitations, including limited\ndata availability, real-time performance constraints, trust and explainability,\nenvironmental costs, and regulatory uncertainty. This review positions GAI not\nmerely as a tool but as a critical enabler of smart, resilient, and\nenvironmentally aligned aquaculture systems.",
    "published": "2025-07-16T07:16:51Z",
    "updated": "2025-07-16T07:16:51Z",
    "id": "2507.11974v1",
    "authors": [
      "Waseem Akram",
      "Muhayy Ud Din",
      "Lyes Saad Soud",
      "Irfan Hussain"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11974v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11974v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11974v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the application of Generative AI in aquaculture, which involves multimodal data synthesis and smart decision-making, but it does not specifically focus on the core topics listed such as LLM, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.11894v1": {
    "title": "Context-Aware Search and Retrieval Over Erasure Channels",
    "summary": "This paper introduces and analyzes a search and retrieval model that adopts\nkey semantic communication principles from retrieval-augmented generation. We\nspecifically present an information-theoretic analysis of a remote document\nretrieval system operating over a symbol erasure channel. The proposed model\nencodes the feature vector of a query, derived from term-frequency weights of a\nlanguage corpus by using a repetition code with an adaptive rate dependent on\nthe contextual importance of the terms. At the decoder, we select between two\ndocuments based on the contextual closeness of the recovered query. By\nleveraging a jointly Gaussian approximation for both the true and reconstructed\nsimilarity scores, we derive an explicit expression for the retrieval error\nprobability, i.e., the probability under which the less similar document is\nselected. Numerical simulations on synthetic and real-world data (Google NQ)\nconfirm the validity of the analysis. They further demonstrate that assigning\ngreater redundancy to critical features effectively reduces the error rate,\nhighlighting the effectiveness of semantic-aware feature encoding in\nerror-prone communication settings.",
    "published": "2025-07-16T04:21:46Z",
    "updated": "2025-07-16T04:21:46Z",
    "id": "2507.11894v1",
    "authors": [
      "Sara Ghasvarianjahromi",
      "Yauhen Yakimenka",
      "Jrg Kliewer"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.11894v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.11894v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.11894v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a search and retrieval model that leverages semantic communication principles, which is related to memory-augmented models and retrieval-based methods, but does not directly align with the provided topics.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.10726v1": {
    "title": "Extracting Document Relations from Search Corpus by Marginalizing over\n  User Queries",
    "summary": "Understanding relationships between documents in large-scale corpora is\nessential for knowledge discovery and information organization. However,\nexisting approaches rely heavily on manual annotation or predefined\nrelationship taxonomies. We propose EDR-MQ (Extracting Document Relations by\nMarginalizing over User Queries), a novel framework that discovers document\nrelationships through query marginalization. EDR-MQ is based on the insight\nthat strongly related documents often co-occur in results across diverse user\nqueries, enabling us to estimate joint probabilities between document pairs by\nmarginalizing over a collection of queries. To enable this query\nmarginalization approach, we develop Multiply Conditioned Retrieval-Augmented\nGeneration (MC-RAG), which employs conditional retrieval where subsequent\ndocument retrievals depend on previously retrieved content. By observing\nco-occurrence patterns across diverse queries, EDR-MQ estimates joint\nprobabilities between document pairs without requiring labeled training data or\npredefined taxonomies. Experimental results show that our query marginalization\napproach successfully identifies meaningful document relationships, revealing\ntopical clusters, evidence chains, and cross-domain connections that are not\napparent through traditional similarity-based methods. Our query-driven\nframework offers a practical approach to document organization that adapts to\ndifferent user perspectives and information needs.",
    "published": "2025-07-14T18:47:13Z",
    "updated": "2025-07-14T18:47:13Z",
    "id": "2507.10726v1",
    "authors": [
      "Yuki Iwamoto",
      "Kaoru Tsunoda",
      "Ken Kaneiwa"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10726v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10726v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10726v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on extracting document relations from search corpora using query marginalization and retrieval-augmented generation, which does not directly align with the provided topics related to LLMs, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.10539v1": {
    "title": "Graph World Model",
    "summary": "World models (WMs) demonstrate strong capabilities in prediction, generation,\nand planning tasks. Existing WMs primarily focus on unstructured data and\ncannot leverage the ubiquitous structured data, often represented as graphs, in\nthe digital world. While multiple graph foundation models have been proposed,\nthey focus on graph learning tasks and cannot extend to diverse multi-modal\ndata and interdisciplinary tasks. To address these challenges, we propose the\nGraph World Model (GWM), a world model that supports both unstructured and\ngraph-structured states with multi-modal information and represents diverse\ntasks as actions. The core of a GWM is a generic message-passing algorithm to\naggregate structured information, either over a unified multi-modal token space\nby converting multi-modal data into text (GWM-T) or a unified multi-modal\nembedding space by modality-specific encoders (GWM-E). Notably, GWM introduces\naction nodes to support diverse tasks, where action nodes are linked to other\nnodes via direct reference or similarity computation. Extensive experiments on\nsix tasks from diverse domains, including multi-modal generation and matching,\nrecommendation, graph prediction, multi-agent, retrieval-augmented generation,\nand planning and optimization, show that the same GWM outperforms or matches\ndomain-specific baselines' performance, benefits from multi-hop structures, and\ndemonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our\ncode for GWM is released at https://github.com/ulab-uiuc/GWM.",
    "published": "2025-07-14T17:57:45Z",
    "updated": "2025-07-14T17:57:45Z",
    "id": "2507.10539v1",
    "authors": [
      "Tao Feng",
      "Yexin Wu",
      "Guanyu Lin",
      "Jiaxuan You"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10539v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10539v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10539v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper introduces the Graph World Model (GWM), which focuses on leveraging structured data (graphs) and multi-modal information for diverse tasks. While it involves multi-modal data and some aspects of memory (retrieval-augmented generation), it does not directly align with the provided topics such as LLM, RL, MLLM, etc. The primary focus is on a novel world model approach rather than the specific topics listed.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.10522v1": {
    "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex\n  Scientific Question Answering in Ecology",
    "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.",
    "published": "2025-07-14T17:47:28Z",
    "updated": "2025-07-14T17:47:28Z",
    "id": "2507.10522v1",
    "authors": [
      "Jennifer D'Souza",
      "Endres Keno Sander",
      "Andrei Aioanei"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10522v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10522v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10522v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses an LLM-based system for automated scientific synthesis, which involves retrieval-augmented generation and reasoning capabilities. The focus on LLM-based systems and retrieval-augmented generation aligns with the topics 'LLM' and 'Memory'.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.10411v1": {
    "title": "Am I on the Right Track? What Can Predicted Query Performance Tell Us\n  about the Search Behaviour of Agentic RAG",
    "summary": "Agentic Retrieval-Augmented Generation (RAG) is a new paradigm where the\nreasoning model decides when to invoke a retriever (as a \"tool\") when answering\na question. This paradigm, exemplified by recent research works such as\nSearch-R1, enables the model to decide when to search and obtain external\ninformation. However, the queries generated by such Agentic RAG models and the\nrole of the retriever in obtaining high-quality answers remain understudied. To\nthis end, this initial study examines the applicability of query performance\nprediction (QPP) within the recent Agentic RAG models Search-R1 and\nR1-Searcher. We find that applying effective retrievers can achieve higher\nanswer quality within a shorter reasoning process. Moreover, the QPP estimates\nof the generated queries, used as an approximation of their retrieval quality,\nare positively correlated with the quality of the final answer. Ultimately, our\nwork is a step towards adaptive retrieval within Agentic RAG, where QPP is used\nto inform the model if the retrieved results are likely to be useful.",
    "published": "2025-07-14T15:54:50Z",
    "updated": "2025-07-14T15:54:50Z",
    "id": "2507.10411v1",
    "authors": [
      "Fangzheng Tian",
      "Jinyuan Fang",
      "Debasis Ganguly",
      "Zaiqiao Meng",
      "Craig Macdonald"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10411v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10411v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10411v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses Agentic Retrieval-Augmented Generation (RAG) models, which involve retrieval mechanisms and reasoning models, aligning with topics related to Memory (retrieval-augmented generation) and Reasoning (LLM reasoning).",
    "llm_cls_result": [
      "Memory",
      "Reasoning"
    ]
  },
  "2507.10070v1": {
    "title": "Breaking the Storage-Compute Bottleneck in Billion-Scale ANNS: A\n  GPU-Driven Asynchronous I/O Framework",
    "summary": "With the advancement of information retrieval, recommendation systems, and\nRetrieval-Augmented Generation (RAG), Approximate Nearest Neighbor Search\n(ANNS) gains widespread applications due to its higher performance and\naccuracy. While several disk-based ANNS systems have emerged to handle\nexponentially growing vector datasets, they suffer from suboptimal performance\ndue to two inherent limitations: 1) failing to overlap SSD accesses with\ndistance computation processes and 2) extended I/O latency caused by suboptimal\nI/O Stack. To address these challenges, we present FlashANNS, a GPU-accelerated\nout-of-core graph-based ANNS system through I/O-compute overlapping. Our core\ninsight lies in the synchronized orchestration of I/O and computation through\nthree key innovations: 1) Dependency-Relaxed asynchronous pipeline: FlashANNS\ndecouples I/O-computation dependencies to fully overlap between GPU distance\ncalculations and SSD data transfers. 2) Warp-Level concurrent SSD access:\nFlashANNS implements a lock-free I/O stack with warp-level concurrency control,\nto reduce the latency-induced time overhead. 3) Computation-I/O balanced graph\ndegree Selection: FlashANNS selects graph degrees via lightweight\ncompute-to-I/O ratio sampling, ensuring optimal balance between computational\nload and storage access latency across different I/O bandwidth configurations.\nWe implement FlashANNS and compare it with state-of-the-art out-of-core ANNS\nsystems (SPANN, DiskANN) and a GPU-accelerated out-of-core ANNS system\n(FusionANNS). Experimental results demonstrate that at $\\geq$95\\% recall@10\naccuracy, our method achieves 2.3-5.9$\\times$ higher throughput compared to\nexisting SOTA methods with a single SSD, and further attains 2.7-12.2$\\times$\nthroughput improvement in multi-SSD configurations.",
    "published": "2025-07-14T08:55:51Z",
    "updated": "2025-07-14T08:55:51Z",
    "id": "2507.10070v1",
    "authors": [
      "Yang Xiao",
      "Mo Sun",
      "Ziyu Song",
      "Bing Tian",
      "Jie Zhang",
      "Jie Sun",
      "Zeke Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10070v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10070v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10070v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on improving the performance of Approximate Nearest Neighbor Search (ANNS) systems, particularly in the context of Retrieval-Augmented Generation (RAG), which is related to memory-augmented models and retrieval-based methods.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2507.09985v1": {
    "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model",
    "summary": "Touch is recognized as a vital sense for humans and an equally important\nmodality for robots, especially for dexterous manipulation, material\nidentification, and scenarios involving visual occlusion. Building upon very\nrecent work in touch foundation models, this demonstration will feature\nOctopi-1.5, our latest visual-tactile-language model. Compared to its\npredecessor, Octopi-1.5 introduces the ability to process tactile signals from\nmultiple object parts and employs a simple retrieval-augmented generation (RAG)\nmodule to improve performance on tasks and potentially learn new objects\non-the-fly. The system can be experienced live through a new handheld\ntactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile\nsensors. This convenient and accessible setup allows users to interact with\nOctopi-1.5 without requiring a robot. During the demonstration, we will\nshowcase Octopi-1.5 solving tactile inference tasks by leveraging tactile\ninputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5\nwill identify objects being grasped and respond to follow-up queries about how\nto handle it (e.g., recommending careful handling for soft fruits). We also\nplan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.\nWith live interactions, this demonstration aims to highlight both the progress\nand limitations of VTLMs such as Octopi-1.5 and to foster further interest in\nthis exciting field. Code for Octopi-1.5 and design files for the TMI gripper\nare available at https://github.com/clear-nus/octopi-1.5.",
    "published": "2025-07-14T07:05:36Z",
    "updated": "2025-07-14T07:05:36Z",
    "id": "2507.09985v1",
    "authors": [
      "Samson Yu",
      "Kelvin Lin",
      "Harold Soh"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09985v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09985v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09985v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses Octopi-1.5, a visual-tactile-language model that integrates tactile signals with visual and language modalities, and employs retrieval-augmented generation (RAG) for improved performance. This aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA) models, as it involves cross-modal integration and interaction.",
    "llm_cls_result": [
      "MLLM",
      "VLA"
    ]
  },
  "2507.15863v1": {
    "title": "eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for\n  Knowledge with LLMs",
    "summary": "We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge)\nModule, a secure and scalable Retrieval-Augmented Generation pipeline designed\nspecifically for enterprise document question answering. Designed and\nimplemented by eSapiens, the system ingests heterogeneous content (PDF, Office,\nweb), splits it into 1,000-token overlapping chunks, and indexes them in a\nhybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via\ncombined vector+BM25 search, reranked with Cohere, and answered by an LLM using\nCO-STAR prompt engineering. A LangGraph verifier enforces citation overlap,\nregenerating answers until every claim is grounded. On four LegalBench subsets,\n1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank\nboosts Precision@10 by approximately 7 pp; the verifier raises TRACe\nUtilization above 0.50 and limits unsupported statements to less than 3%. All\ncomponents run in containers, enforce end-to-end TLS 1.3 and AES-256. These\nresults demonstrate that the DEREK module delivers accurate, traceable, and\nproduction-ready document QA with minimal operational overhead. The module is\ndesigned to meet enterprise demands for secure, auditable, and context-faithful\nretrieval, providing a reliable baseline for high-stakes domains such as legal\nand finance.",
    "published": "2025-07-13T05:54:01Z",
    "updated": "2025-07-13T05:54:01Z",
    "id": "2507.15863v1",
    "authors": [
      "Isaac Shi",
      "Zeyuan Li",
      "Fan Liu",
      "Wenli Wang",
      "Lewei He",
      "Yang Yang",
      "Tianyu Shi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.15863v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.15863v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.15863v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a Retrieval-Augmented Generation pipeline for enterprise document question answering, which involves LLMs, reasoning, and memory components.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2507.09138v1": {
    "title": "HedraRAG: Coordinating LLM Generation and Database Retrieval in\n  Heterogeneous RAG Serving",
    "summary": "This paper addresses emerging system-level challenges in heterogeneous\nretrieval-augmented generation (RAG) serving, where complex multi-stage\nworkflows and diverse request patterns complicate efficient execution. We\npresent HedraRAG, a runtime system built on a graph-based abstraction that\nexposes optimization opportunities across stage-level parallelism,\nintra-request similarity, and inter-request skewness. These opportunities are\nrealized through dynamic graph transformations, such as node splitting,\nreordering, edge addition, and dependency rewiring, applied to wavefronts of\nsubgraphs spanning concurrent requests. The resulting execution plans are\nmapped onto hybrid CPU-GPU pipelines to improve resource utilization and reduce\nlatency. Evaluations across a wide range of RAG workflows demonstrate speedups\nexceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the\neffectiveness of coordinated generation and retrieval in serving environments.",
    "published": "2025-07-12T04:42:43Z",
    "updated": "2025-07-12T04:42:43Z",
    "id": "2507.09138v1",
    "authors": [
      "Zhengding Hu",
      "Vibha Murthy",
      "Zaifeng Pan",
      "Wanlu Li",
      "Xiaoyi Fang",
      "Yufei Ding",
      "Yuke Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.09138v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.09138v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.09138v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a system for optimizing retrieval-augmented generation (RAG) serving, which involves both LLM generation and database retrieval. The focus is on system-level optimizations for heterogeneous RAG workflows, which aligns with the 'Memory' topic due to its emphasis on retrieval-augmented generation and long-context processing.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2507.10577v2": {
    "title": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos\n  and influence opinions",
    "summary": "Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space.",
    "published": "2025-07-11T10:08:05Z",
    "updated": "2025-07-16T13:25:34Z",
    "id": "2507.10577v2",
    "authors": [
      "Ccile Log",
      "Rehan Ghori"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10577v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10577v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10577v2",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of AI agents for fact-checking and influencing opinions on YouTube, which involves retrieval-augmented generation (RAG) and iterative improvement through self-evaluation. These aspects are related to Memory (due to RAG) and Reasoning (due to the iterative improvement and persuasive comment generation).",
    "llm_cls_result": [
      "Memory",
      "Reasoning"
    ]
  },
  "2507.08443v1": {
    "title": "KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge\n  Graph-based Perturbations",
    "summary": "Retrieval-Augmented Generation (RAG) enhances language models by grounding\nresponses in external information, yet explainability remains a critical\nchallenge, particularly when retrieval relies on unstructured text. Knowledge\ngraphs (KGs) offer a solution by introducing structured, semantically rich\nrepresentations of entities and their relationships, enabling transparent\nretrieval paths and interpretable reasoning. In this work, we present KGRAG-Ex,\na RAG system that improves both factual grounding and explainability by\nleveraging a domain-specific KG constructed via prompt-based information\nextraction. Given a user query, KGRAG-Ex identifies relevant entities and\nsemantic paths in the graph, which are then transformed into pseudo-paragraphs:\nnatural language representations of graph substructures that guide corpus\nretrieval. To improve interpretability and support reasoning transparency, we\nincorporate perturbation-based explanation methods that assess the influence of\nspecific KG-derived components on the generated answers. We conduct a series of\nexperiments to analyze the sensitivity of the system to different perturbation\nmethods, the relationship between graph component importance and their\nstructural positions, the influence of semantic node types, and how graph\nmetrics correspond to the influence of components within the explanations\nprocess.",
    "published": "2025-07-11T09:35:13Z",
    "updated": "2025-07-11T09:35:13Z",
    "id": "2507.08443v1",
    "authors": [
      "Georgios Balanos",
      "Evangelos Chasanis",
      "Konstantinos Skianis",
      "Evaggelia Pitoura"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.08443v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.08443v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.08443v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on Retrieval-Augmented Generation (RAG) systems, which are related to memory-augmented models and retrieval-based methods. It also discusses explainability and reasoning transparency, which are relevant to reasoning abilities in LLMs.",
    "llm_cls_result": [
      "Memory",
      "Reasoning"
    ]
  },
  "2507.07543v1": {
    "title": "The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English\n  Corpora",
    "summary": "Cross-lingual retrieval-augmented generation (RAG) is a critical capability\nfor retrieving and generating answers across languages. Prior work in this\ncontext has mostly focused on generation and relied on benchmarks derived from\nopen-domain sources, most notably Wikipedia. In such settings, retrieval\nchallenges often remain hidden due to language imbalances, overlap with\npretraining data, and memorized content. To address this gap, we study\nArabic-English RAG in a domain-specific setting using benchmarks derived from\nreal-world corporate datasets. Our benchmarks include all combinations of\nlanguages for the user query and the supporting document, drawn independently\nand uniformly at random. This enables a systematic study of multilingual\nretrieval behavior.\n  Our findings reveal that retrieval is a critical bottleneck in cross-lingual\ndomain-specific scenarios, with significant performance drops occurring when\nthe user query and supporting document languages differ. A key insight is that\nthese failures stem primarily from the retriever's difficulty in ranking\ndocuments across languages. Finally, we propose a simple retrieval strategy\nthat addresses this source of failure by enforcing equal retrieval from both\nlanguages, resulting in substantial improvements in cross-lingual and overall\nperformance. These results highlight meaningful opportunities for improving\nmultilingual retrieval, particularly in practical, real-world RAG applications.",
    "published": "2025-07-10T08:38:31Z",
    "updated": "2025-07-10T08:38:31Z",
    "id": "2507.07543v1",
    "authors": [
      "Chen Amiraz",
      "Yaroslav Fyodorov",
      "Elad Haramaty",
      "Zohar Karnin",
      "Liane Lewin-Eytan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07543v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07543v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07543v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on retrieval-augmented generation (RAG) in a cross-lingual context, specifically addressing retrieval biases and proposing improvements for multilingual retrieval. This aligns with the 'Memory' topic, which includes retrieval-based methods and long-context processing. Additionally, the study involves domain-specific benchmarks, which relates to the 'Benchmark' topic.",
    "llm_cls_result": [
      "Memory",
      "Benchmark"
    ]
  },
  "2507.07155v1": {
    "title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous\n  Scientific Discovery in Astrophysics",
    "summary": "We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on\n105 Cosmology Question-Answer (QA) pairs that we built specifically for this\npurpose.The RAG configurations are manually evaluated by a human expert, that\nis, a total of 945 generated answers were assessed. We find that currently the\nbest RAG agent configuration is with OpenAI embedding and generative model,\nyielding 91.4\\% accuracy. Using our human evaluation results we calibrate\nLLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human\nevaluation. These results allow us to systematically select the best RAG agent\nconfiguration for multi-agent system for autonomous scientific discovery in\nastrophysics (e.g., cmbagent presented in a companion paper) and provide us\nwith an LLMaaJ system that can be scaled to thousands of cosmology QA pairs. We\nmake our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system\npublicly available for further use by the astrophysics community.",
    "published": "2025-07-09T16:46:03Z",
    "updated": "2025-07-09T16:46:03Z",
    "id": "2507.07155v1",
    "authors": [
      "Xueqing Xu",
      "Boris Bolliet",
      "Adrian Dimitrov",
      "Andrew Laverick",
      "Francisco Villaescusa-Navarro",
      "Licong Xu",
      "igo Zubeldia"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.07155v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.07155v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.07155v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the evaluation of Retrieval-Augmented Generation (RAG) agents, which involves memory-augmented models and retrieval-based methods. It also mentions the use of LLM-as-a-Judge (LLMaaJ) system, which is related to reasoning abilities in LLMs. The focus on autonomous scientific discovery in astrophysics does not directly align with the other provided topics.",
    "llm_cls_result": [
      "Memory",
      "Reasoning"
    ]
  },
  "2507.10571v2": {
    "title": "Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification\n  System with Trust-Aware Orchestration and RAG-Based Reasoning",
    "summary": "Modern Artificial Intelligence (AI) increasingly relies on multi-agent\narchitectures that blend visual and language understanding. Yet, a pressing\nchallenge remains: How can we trust these agents especially in zero-shot\nsettings with no fine-tuning? We introduce a novel modular Agentic AI visual\nclassification framework that integrates generalist multimodal agents with a\nnon-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)\nmodule. Applied to apple leaf disease diagnosis, we benchmark three\nconfigurations: (I) zero-shot with confidence-based orchestration, (II)\nfine-tuned agents with improved performance, and (III) trust-calibrated\norchestration enhanced by CLIP-based image retrieval and re-evaluation loops.\nUsing confidence calibration metrics (ECE, OCR, CCC), the orchestrator\nmodulates trust across agents. Our results demonstrate a 77.94\\% accuracy\nimprovement in the zero-shot setting using trust-aware orchestration and RAG,\nachieving 85.63\\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL\ndisplayed overconfidence. Furthermore, image-RAG grounded predictions with\nvisually similar cases, enabling correction of agent overconfidence via\niterative re-evaluation. The proposed system separates perception (vision\nagents) from meta-reasoning (orchestrator), enabling scalable and interpretable\nmulti-agent AI. This blueprint is extensible to diagnostics, biology, and other\ntrust-critical domains. All models, prompts, results, and system components\nincluding the complete software source code are openly released to support\nreproducibility, transparency, and community benchmarking at Github:\nhttps://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust",
    "published": "2025-07-09T16:39:29Z",
    "updated": "2025-07-18T22:25:01Z",
    "id": "2507.10571v2",
    "authors": [
      "Konstantinos I. Roumeliotis",
      "Ranjan Sapkota",
      "Manoj Karkee",
      "Nikolaos D. Tselikas"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.10571v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.10571v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.10571v2",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a modular Agentic AI visual classification framework that integrates multimodal agents with a reasoning orchestrator and a RAG module, focusing on trust-aware orchestration and RAG-based reasoning. It involves multimodal understanding, trust calibration, and retrieval-augmented generation, which are relevant to the topics of MLLM (Multimodal Large Language Models), Reasoning, and Memory.",
    "llm_cls_result": [
      "MLLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2507.06993v1": {
    "title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced\n  Planning, Navigation, and Dynamic Adaptation",
    "summary": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response.",
    "published": "2025-07-09T16:18:09Z",
    "updated": "2025-07-09T16:18:09Z",
    "id": "2507.06993v1",
    "authors": [
      "Jieren Deng",
      "Aleksandar Cvetkovic",
      "Pak Kiu Chung",
      "Dragomir Yankov",
      "Chiqun Zhang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.06993v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.06993v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.06993v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses an LLM-powered framework for travel planning and navigation, which involves the use of Large Language Models (LLM) and Retrieval-Augmented Generation (RAG), indicating relevance to LLM and Memory topics.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.05520v1": {
    "title": "Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic\n  RAG Approaches to Dermatological Diagnosis",
    "summary": "The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized\nby researchers from Microsoft, Stanford University, and the Hospital Clinic of\nBarcelona, focuses on multimodal dermatology question answering and\nsegmentation, using real-world patient queries and images. This work addresses\nthe Closed Visual Question Answering (CVQA) task, where the goal is to select\nthe correct answer to multiple-choice clinical questions based on both\nuser-submitted images and accompanying symptom descriptions. The proposed\napproach combines three core components: (1) fine-tuning open-source multimodal\nmodels from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)\nintroducing a structured reasoning layer that reconciles and adjudicates\nbetween candidate model outputs, and (3) incorporating agentic\nretrieval-augmented generation (agentic RAG), which adds relevant information\nfrom the American Academy of Dermatology's symptom and condition database to\nfill in gaps in patient context. The team achieved second place with a\nsubmission that scored sixth, demonstrating competitive performance and high\naccuracy. Beyond competitive benchmarks, this research addresses a practical\nchallenge in telemedicine: diagnostic decisions must often be made\nasynchronously, with limited input and with high accuracy and interpretability.\nBy emulating the systematic reasoning patterns employed by dermatologists when\nevaluating skin conditions, this architecture provided a pathway toward more\nreliable automated diagnostic support systems.",
    "published": "2025-07-07T22:31:56Z",
    "updated": "2025-07-07T22:31:56Z",
    "id": "2507.05520v1",
    "authors": [
      "Karishma Thakrar",
      "Shreyas Basavatia",
      "Akshay Daftardar"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05520v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05520v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05520v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on multimodal dermatology question answering and segmentation, utilizing multimodal models and retrieval-augmented generation (RAG) for diagnostic support. It involves fine-tuning multimodal models (MLLM), structured reasoning (Reasoning), and agentic RAG (Memory).",
    "llm_cls_result": [
      "MLLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2507.05461v2": {
    "title": "GLOSS: Group of LLMs for Open-Ended Sensemaking of Passive Sensing Data\n  for Health and Wellbeing",
    "summary": "The ubiquitous presence of smartphones and wearables has enabled researchers\nto build prediction and detection models for various health and behavior\noutcomes using passive sensing data from these devices. Achieving a high-level,\nholistic understanding of an individual's behavior and context, however,\nremains a significant challenge. Due to the nature of passive sensing data,\nsensemaking -- the process of interpreting and extracting insights -- requires\nboth domain knowledge and technical expertise, creating barriers for different\nstakeholders. Existing systems designed to support sensemaking are either not\nopen-ended or cannot perform complex data triangulation. In this paper, we\npresent a novel sensemaking system, Group of LLMs for Open-ended Sensemaking\n(GLOSS), capable of open-ended sensemaking and performing complex multimodal\ntriangulation to derive insights. We demonstrate that GLOSS significantly\noutperforms the commonly used Retrieval-Augmented Generation (RAG) technique,\nachieving 87.93% accuracy and 66.19% consistency, compared to RAG's 29.31%\naccuracy and 52.85% consistency. Furthermore, we showcase the promise of GLOSS\nthrough four use cases inspired by prior and ongoing work in the UbiComp and\nHCI communities. Finally, we discuss the potential of GLOSS, its broader\nimplications, and the limitations of our work.",
    "published": "2025-07-07T20:25:44Z",
    "updated": "2025-07-21T18:54:32Z",
    "id": "2507.05461v2",
    "authors": [
      "Akshat Choube",
      "Ha Le",
      "Jiachen Li",
      "Kaixin Ji",
      "Vedant Das Swain",
      "Varun Mishra"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05461v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05461v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05461v2",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of multiple LLMs for sensemaking of passive sensing data, which involves interpreting and extracting insights from multimodal data. This aligns with the topics of LLM (Large Language Models) and MLLM (Multimodal Large Language Models) due to the use of LLMs in processing and interpreting multimodal data. The mention of Retrieval-Augmented Generation (RAG) also relates to Memory, but the primary focus is on the use of LLMs and multimodal data.",
    "llm_cls_result": [
      "LLM",
      "MLLM"
    ]
  },
  "2507.05346v1": {
    "title": "LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks",
    "summary": "The proliferation of fine-tuned language model experts for specific tasks and\ndomains signals the need for efficient selection and combination methods. We\npropose LoRA-Augmented Generation (LAG) for leveraging large libraries of\nknowledge and task-specific LoRA adapters. LAG requires no additional training\nor access to data, and efficiently filters, retrieves, and applies experts on a\nper-token and layer basis. We evaluate LAG on various knowledge-intensive\ntasks, achieving superior performance over existing data-free methods. We\nexplore scenarios where additional data is available, demonstrating LAG's\ncompatibility with alternative solutions such as retrieval-augmented generation\n(RAG).",
    "published": "2025-07-07T18:00:01Z",
    "updated": "2025-07-07T18:00:01Z",
    "id": "2507.05346v1",
    "authors": [
      "William Fleshman",
      "Benjamin Van Durme"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05346v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05346v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05346v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of LoRA adapters for knowledge-intensive tasks and their combination with retrieval-augmented generation (RAG), which aligns with topics related to memory and retrieval-based methods in LLMs.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.04942v2": {
    "title": "SIGIR 2025 -- LiveRAG Challenge Report",
    "summary": "The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,\nprovided a competitive platform for advancing Retrieval-Augmented Generation\n(RAG) technologies. Participants from academia and industry were invited to\ndevelop a RAG-based question-answering system using a fixed corpus\n(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal\nwas to facilitate challenging comparisons of retrieval and prompting\nstrategies. During the Live Challenge Day, 70 teams from 27 different countries\nprovided answers and supportive information to 500 unseen questions within a\nstrict two-hour time window. Evaluation was conducted in two stages: first an\nautomated LLM-as-a-judge approach was used to compute correctness and\nfaithfulness score, then a manual review of top ranked submissions was\nconducted. The finalists were announced on June 12, 2025, with prizes awarded\nduring the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.",
    "published": "2025-07-07T12:38:53Z",
    "updated": "2025-07-08T06:37:05Z",
    "id": "2507.04942v2",
    "authors": [
      "David Carmel",
      "Simone Filice",
      "Guy Horowitz",
      "Yoelle Maarek",
      "Oren Somekh",
      "Ran Tavory",
      "Mehdi Ghissassi",
      "Edo Liberty",
      "Roy Miara"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04942v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04942v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04942v2",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a challenge focused on Retrieval-Augmented Generation (RAG) technologies, which involves memory-augmented models and retrieval-based methods.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2507.04872v1": {
    "title": "SHARP: Shared State Reduction for Efficient Matching of Sequential\n  Patterns",
    "summary": "The detection of sequential patterns in data is a basic functionality of\nmodern data processing systems for complex event processing (CEP), OLAP, and\nretrieval-augmented generation (RAG). In practice, pattern matching is\nchallenging, since common applications rely on a large set of patterns that\nshall be evaluated with tight latency bounds. At the same time, matching needs\nto maintain state, i.e., intermediate results, that grows exponentially in the\ninput size. Hence, systems turn to best-effort processing, striving for maximal\nrecall under a latency bound. Existing techniques, however, consider each\npattern in isolation, neglecting the optimization potential induced by state\nsharing in pattern matching.\n  In this paper, we present SHARP, a library that employs state reduction to\nachieve efficient best-effort pattern matching. To this end, SHARP incorporates\nstate sharing between patterns through a new abstraction, coined\npattern-sharing degree (PSD). At runtime, this abstraction facilitates the\ncategorization and indexing of partial pattern matches. Based thereon, once a\nlatency bound is exceeded, SHARP realizes best-effort processing by selecting a\nsubset of partial matches for further processing in constant time. In\nexperiments with real-world data, SHARP achieves a recall of 97%, 96% and 73%\nfor pattern matching in CEP, OLAP, and RAG applications, under a bound of 50%\nof the average processing latency.",
    "published": "2025-07-07T10:57:55Z",
    "updated": "2025-07-07T10:57:55Z",
    "id": "2507.04872v1",
    "authors": [
      "Cong Yu",
      "Tuo Shi",
      "Matthias Weidlich",
      "Bo Zhao"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04872v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04872v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04872v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses efficient pattern matching techniques, including retrieval-augmented generation (RAG), which is related to memory-augmented models and retrieval-based methods in LLMs.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2507.04706v1": {
    "title": "UrbanMind: Towards Urban General Intelligence via Tool-Enhanced\n  Retrieval-Augmented Generation and Multilevel Optimization",
    "summary": "Urban general intelligence (UGI) refers to the capacity of AI systems to\nautonomously perceive, reason, and act within dynamic and complex urban\nenvironments. In this paper, we introduce UrbanMind, a tool-enhanced\nretrieval-augmented generation (RAG) framework designed to facilitate UGI.\nCentral to UrbanMind is a novel architecture based on Continual\nRetrieval-Augmented MoE-based LLM (C-RAG-LLM), which dynamically incorporates\ndomain-specific knowledge and evolving urban data to support long-term\nadaptability. The architecture of C-RAG-LLM aligns naturally with a multilevel\noptimization framework, where different layers are treated as interdependent\nsub-problems. Each layer has distinct objectives and can be optimized either\nindependently or jointly through a hierarchical learning process. The framework\nis highly flexible, supporting both end-to-end training and partial layer-wise\noptimization based on resource or deployment constraints. To remain adaptive\nunder data drift, it is further integrated with an incremental corpus updating\nmechanism. Evaluations on real-world urban tasks of a variety of complexity\nverify the effectiveness of the proposed framework. This work presents a\npromising step toward the realization of general-purpose LLM agents in future\nurban environments.",
    "published": "2025-07-07T06:57:34Z",
    "updated": "2025-07-07T06:57:34Z",
    "id": "2507.04706v1",
    "authors": [
      "Kai Yang",
      "Zelin Zhu",
      "Chengtao Jian",
      "Hui Ma",
      "Shengjie Zhao",
      "Xiaozhou Ye",
      "Ye Ouyang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04706v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04706v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04706v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a framework (UrbanMind) that integrates retrieval-augmented generation (RAG) with MoE-based LLMs for urban general intelligence, which involves dynamic knowledge incorporation and multilevel optimization. The topics 'AGI' and 'MoE' are relevant due to the focus on general intelligence and the use of Mixture of Experts. 'Memory' is also relevant because of the retrieval-augmented generation aspect.",
    "llm_cls_result": [
      "AGI",
      "MoE",
      "Memory"
    ]
  },
  "2507.04590v1": {
    "title": "VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and\n  Visual Documents",
    "summary": "Multimodal embedding models have been crucial in enabling various downstream\ntasks such as semantic similarity, information retrieval, and clustering over\ndifferent modalities. However, existing multimodal embeddings like VLM2Vec,\nE5-V, GME are predominantly focused on natural images, with limited support for\nother visual forms such as videos and visual documents. This restricts their\napplicability in real-world scenarios, including AI agents, multi-modal search\nand recommendation, and retrieval-augmented generation (RAG). To close this\ngap, we propose VLM2Vec-V2, a unified framework for learning embeddings across\ndiverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark\nthat extends MMEB with five new task types: visual document retrieval, video\nretrieval, temporal grounding, video classification and video question\nanswering - spanning text, image, video, and visual document inputs. Next, we\ntrain VLM2Vec-V2, a general-purpose embedding model that supports text, image,\nvideo, and visual document inputs. Extensive experiments show that VLM2Vec-V2\nachieves strong performance not only on the newly introduced video and document\nretrieval tasks, but also improves over prior baselines on the original image\nbenchmarks. Through extensive evaluation, our study offers insights into the\ngeneralizability of various multimodal embedding models and highlights\neffective strategies for unified embedding learning, laying the groundwork for\nmore scalable and adaptable representation learning in both research and\nreal-world settings.",
    "published": "2025-07-07T00:51:57Z",
    "updated": "2025-07-07T00:51:57Z",
    "id": "2507.04590v1",
    "authors": [
      "Rui Meng",
      "Ziyan Jiang",
      "Ye Liu",
      "Mingyi Su",
      "Xinyi Yang",
      "Yuepeng Fu",
      "Can Qin",
      "Zeyuan Chen",
      "Ran Xu",
      "Caiming Xiong",
      "Yingbo Zhou",
      "Wenhu Chen",
      "Semih Yavuz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04590v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04590v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04590v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on advancing multimodal embedding models, particularly for videos, images, and visual documents, which aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Action (VLA). Additionally, it introduces a new benchmark, which is relevant to the Benchmark topic.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Benchmark"
    ]
  },
  "2507.04395v1": {
    "title": "SpiritRAG: A Q&A System for Religion and Spirituality in the United\n  Nations Archive",
    "summary": "Religion and spirituality (R/S) are complex and highly domain-dependent\nconcepts which have long confounded researchers and policymakers. Due to their\ncontext-specificity, R/S are difficult to operationalize in conventional\narchival search strategies, particularly when datasets are very large, poorly\naccessible, and marked by information noise. As a result, considerable time\ninvestments and specialist knowledge is often needed to extract actionable\ninsights related to R/S from general archival sources, increasing reliance on\npublished literature and manual desk reviews. To address this challenge, we\npresent SpiritRAG, an interactive Question Answering (Q&A) system based on\nRetrieval-Augmented Generation (RAG). Built using 7,500 United Nations (UN)\nresolution documents related to R/S in the domains of health and education,\nSpiritRAG allows researchers and policymakers to conduct complex,\ncontext-sensitive database searches of very large datasets using an easily\naccessible, chat-based web interface. SpiritRAG is lightweight to deploy and\nleverages both UN documents and user provided documents as source material. A\npilot test and evaluation with domain experts on 100 manually composed\nquestions demonstrates the practical value and usefulness of SpiritRAG.",
    "published": "2025-07-06T13:54:54Z",
    "updated": "2025-07-06T13:54:54Z",
    "id": "2507.04395v1",
    "authors": [
      "Yingqiang Gao",
      "Fabian Winiger",
      "Patrick Montjourides",
      "Anastassia Shaitarova",
      "Nianlong Gu",
      "Simon Peng-Keller",
      "Gerold Schneider"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04395v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04395v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04395v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a Q&A system based on Retrieval-Augmented Generation (RAG) for religion and spirituality, which involves memory-augmented models and retrieval-based methods.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2507.04377v1": {
    "title": "Multi-Modal Semantic Parsing for the Interpretation of Tombstone\n  Inscriptions",
    "summary": "Tombstones are historically and culturally rich artifacts, encapsulating\nindividual lives, community memory, historical narratives and artistic\nexpression. Yet, many tombstones today face significant preservation\nchallenges, including physical erosion, vandalism, environmental degradation,\nand political shifts. In this paper, we introduce a novel multi-modal framework\nfor tombstones digitization, aiming to improve the interpretation, organization\nand retrieval of tombstone content. Our approach leverages vision-language\nmodels (VLMs) to translate tombstone images into structured Tombstone Meaning\nRepresentations (TMRs), capturing both image and text information. To further\nenrich semantic parsing, we incorporate retrieval-augmented generation (RAG)\nfor integrate externally dependent elements such as toponyms, occupation codes,\nand ontological concepts. Compared to traditional OCR-based pipelines, our\nmethod improves parsing accuracy from an F1 score of 36.1 to 89.5. We\nadditionally evaluate the model's robustness across diverse linguistic and\ncultural inscriptions, and simulate physical degradation through image fusion\nto assess performance under noisy or damaged conditions. Our work represents\nthe first attempt to formalize tombstone understanding using large\nvision-language models, presenting implications for heritage preservation.",
    "published": "2025-07-06T12:50:07Z",
    "updated": "2025-07-06T12:50:07Z",
    "id": "2507.04377v1",
    "authors": [
      "Xiao Zhang",
      "Johan Bos"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04377v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04377v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04377v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a multi-modal framework leveraging vision-language models (VLMs) for tombstone digitization, which aligns with the topics of Multimodal Large Language Models (MLLM) and Vision-Language Alignment models (VLA). Additionally, the use of retrieval-augmented generation (RAG) relates to Memory-augmented models.",
    "llm_cls_result": [
      "MLLM",
      "VLA",
      "Memory"
    ]
  },
  "2507.04026v1": {
    "title": "Patient-Centered RAG for Oncology Visit Aid Following the Ottawa\n  Decision Guide",
    "summary": "Effective communication is essential in cancer care, yet patients often face\nchallenges in preparing for complex medical visits. We present an interactive,\nRetrieval-augmented Generation-assisted system that helps patients progress\nfrom uninformed to visit-ready. Our system adapts the Ottawa Personal Decision\nGuide into a dynamic retrieval-augmented generation workflow, helping users\nbridge knowledge gaps, clarify personal values and generate useful questions\nfor their upcoming visits. Focusing on localized prostate cancer, we conduct a\nuser study with patients and a clinical expert. Results show high system\nusability (UMUX Mean = 6.0 out of 7), strong relevance of generated content\n(Mean = 6.7 out of 7), minimal need for edits, and high clinical faithfulness\n(Mean = 6.82 out of 7). This work demonstrates the potential of combining\npatient-centered design with language models to enhance clinical preparation in\noncology care.",
    "published": "2025-07-05T12:37:26Z",
    "updated": "2025-07-05T12:37:26Z",
    "id": "2507.04026v1",
    "authors": [
      "Siyang Liu",
      "Lawrence Chin-I An",
      "Rada Mihalcea"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.04026v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.04026v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.04026v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Retrieval-augmented Generation (RAG) in a medical context, specifically for oncology care. This aligns with the 'Memory' topic, which includes retrieval-based methods and long-context processing. The application is specialized and does not fit into the other provided categories.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2507.05285v2": {
    "title": "Beyond classical and contemporary models: a transformative AI framework\n  for student dropout prediction in distance learning using RAG, Prompt\n  engineering, and Cross-modal fusion",
    "summary": "Student dropout in distance learning remains a critical challenge, with\nprofound societal and economic consequences. While classical machine learning\nmodels leverage structured socio-demographic and behavioral data, they often\nfail to capture the nuanced emotional and contextual factors embedded in\nunstructured student interactions. This paper introduces a transformative AI\nframework that redefines dropout prediction through three synergistic\ninnovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment\nanalysis, prompt engineering to decode academic stressors,and cross-modal\nattention fusion to dynamically align textual, behavioral, and\nsocio-demographic insights. By grounding sentiment analysis in a curated\nknowledge base of pedagogical content, our RAG-enhanced BERT model interprets\nstudent comments with unprecedented contextual relevance, while optimized\nprompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload\nanxiety\"). A cross-modal attention layer then fuses these insights with\ntemporal engagement patterns, creating holistic risk pro-files. Evaluated on a\nlongitudinal dataset of 4 423 students, the framework achieves 89% accuracy and\nan F1-score of 0.88, outperforming conventional models by 7% and reducing false\nnegatives by 21%. Beyond prediction, the system generates interpretable\ninterventions by retrieving contextually aligned strategies (e.g., mentorship\nprograms for isolated learners). This work bridges the gap between predictive\nanalytics and actionable pedagogy, offering a scalable solution to mitigate\ndropout risks in global education systems",
    "published": "2025-07-04T21:41:43Z",
    "updated": "2025-07-14T15:49:02Z",
    "id": "2507.05285v2",
    "authors": [
      "Miloud Mihoubi",
      "Meriem Zerkouk",
      "Belkacem Chikhaoui"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.05285v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.05285v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.05285v2",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Retrieval-Augmented Generation (RAG) for sentiment analysis, which is related to memory-augmented models, and cross-modal fusion, which aligns with multimodal approaches. However, the primary focus is on a specific application (student dropout prediction) rather than core research on LLMs or multimodal models.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.03493v1": {
    "title": "AI-VaxGuide: An Agentic RAG-Based LLM for Vaccination Decisions",
    "summary": "Vaccination plays a vital role in global public health, yet healthcare\nprofessionals often struggle to access immunization guidelines quickly and\nefficiently. National protocols and WHO recommendations are typically extensive\nand complex, making it difficult to extract precise information, especially\nduring urgent situations. This project tackles that issue by developing a\nmultilingual, intelligent question-answering system that transforms static\nvaccination guidelines into an interactive and user-friendly knowledge base.\nBuilt on a Retrieval-Augmented Generation (RAG) framework and enhanced with\nagent-based reasoning (Agentic RAG), the system provides accurate,\ncontext-sensitive answers to complex medical queries. Evaluation shows that\nAgentic RAG outperforms traditional methods, particularly in addressing\nmulti-step or ambiguous questions. To support clinical use, the system is\nintegrated into a mobile application designed for real-time, point-of-care\naccess to essential vaccine information. AI-VaxGuide model is publicly\navailable on https://huggingface.co/VaxGuide",
    "published": "2025-07-04T11:33:56Z",
    "updated": "2025-07-04T11:33:56Z",
    "id": "2507.03493v1",
    "authors": [
      "Abdellah Zeggai",
      "Ilyes Traikia",
      "Abdelhak Lakehal",
      "Abdennour Boulesnane"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.03493v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.03493v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.03493v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a system built on Retrieval-Augmented Generation (RAG) framework and enhanced with agent-based reasoning, which aligns with the topics of Memory (due to RAG) and RL (due to agent-based reasoning). The focus on a practical application of LLMs in healthcare also suggests a relevance to LLM.",
    "llm_cls_result": [
      "Memory",
      "RL",
      "LLM"
    ]
  },
  "2507.02652v1": {
    "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for\n  Deep Search",
    "summary": "Complex information needs in real-world search scenarios demand deep\nreasoning and knowledge synthesis across diverse sources, which traditional\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\nuse a single model to handle both high-level planning and detailed execution,\nleading to inefficient reasoning and limited scalability. In this paper, we\nintroduce HiRA, a hierarchical framework that separates strategic planning from\nspecialized execution. Our approach decomposes complex search tasks into\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\nexternal tools and reasoning capabilities, and coordinates the results through\na structured integration mechanism. This separation prevents execution details\nfrom disrupting high-level reasoning while enabling the system to leverage\nspecialized expertise for different types of information processing.\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\nsystems. Our results show improvements in both answer quality and system\nefficiency, highlighting the effectiveness of decoupled planning and execution\nfor multi-step information seeking tasks. Our code is available at\nhttps://github.com/ignorejjj/HiRA.",
    "published": "2025-07-03T14:18:08Z",
    "updated": "2025-07-03T14:18:08Z",
    "id": "2507.02652v1",
    "authors": [
      "Jiajie Jin",
      "Xiaoxi Li",
      "Guanting Dong",
      "Yuyao Zhang",
      "Yutao Zhu",
      "Yang Zhao",
      "Hongjin Qian",
      "Zhicheng Dou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02652v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02652v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02652v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a hierarchical reasoning framework for deep search, which involves complex reasoning and knowledge synthesis. It mentions retrieval-augmented generation (RAG) and focuses on improving reasoning and execution in multi-step information seeking tasks.",
    "llm_cls_result": [
      "Reasoning",
      "Memory"
    ]
  },
  "2507.02424v1": {
    "title": "CyberRAG: An agentic RAG cyber attack classification and reporting tool",
    "summary": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\ngenerate hundreds of thousands of alerts per hour, overwhelming security\nanalysts with logs that demand deep, rapidly evolving domain expertise.\nConventional machine-learning detectors trim the alert volume but still yield\nhigh false-positive rates, while standard single-pass Retrieval-Augmented\nGeneration (RAG) pipelines often retrieve irrelevant context and fail to\njustify their predictions. To overcome these shortcomings, we present CyberRAG,\na modular, agent-based RAG framework that delivers real-time classification,\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\norchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to\na distinct attack family; (ii) tool adapters for enrichment and alerting; and\n(iii) an iterative retrieval-and-reason loop that continuously queries a\ndomain-specific knowledge base until the evidence is both relevant and\nself-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic\ndesign that enables dynamic control flow and adaptive reasoning. This\nagent-centric architecture refines its threat labels and natural-language\njustifications autonomously, reducing false positives and enhancing\ninterpretability. The framework is fully extensible: new attack types can be\nsupported by simply adding a classifier without retraining the core agent.\nCyberRAG has been evaluated achieving over 94% accuracy per class and pushing\nfinal classification accuracy to 94.92% through semantic orchestration.\nGenerated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based\nexpert evaluation. These results show that agentic, specialist-oriented RAG can\npair high detection accuracy with trustworthy, SOC-ready prose, offering a\npractical and scalable path toward semi-autonomous cyber-defence workflows.",
    "published": "2025-07-03T08:32:19Z",
    "updated": "2025-07-03T08:32:19Z",
    "id": "2507.02424v1",
    "authors": [
      "Francesco Blefari",
      "Cristian Cosentino",
      "Francesco Aurelio Pironti",
      "Angelo Furfaro",
      "Fabrizio Marozzo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02424v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02424v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02424v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a modular, agent-based RAG framework (Retrieval-Augmented Generation) that uses an LLM agent for cyber attack classification and reporting. It involves dynamic control flow, adaptive reasoning, and iterative retrieval, which are key aspects of memory-augmented models and retrieval-based methods.",
    "llm_cls_result": [
      "Memory",
      "RL"
    ]
  },
  "2507.01315v1": {
    "title": "Context-Aware Code Wiring Recommendation with LLM-based Agent",
    "summary": "Copy-paste-modify is a widespread and pragmatic practice in software\ndevelopment, where developers adapt reused code snippets, sourced from\nplatforms such as Stack Overflow, GitHub, or LLM outputs, into their local\ncodebase. A critical yet underexplored aspect of this adaptation is code\nwiring, which involves substituting unresolved variables in the pasted code\nwith suitable ones from the surrounding context. Existing solutions either rely\non heuristic rules or historical templates, often failing to effectively\nutilize contextual information, despite studies showing that over half of\nadaptation cases are context-dependent. In this paper, we introduce WIRL, an\nLLM-based agent for code wiring framed as a Retrieval-Augmented Generation\n(RAG) infilling task. WIRL combines an LLM, a customized toolkit, and an\norchestration module to identify unresolved variables, retrieve context, and\nperform context-aware substitutions. To balance efficiency and autonomy, the\nagent adopts a mixed strategy: deterministic rule-based steps for common\npatterns, and a state-machine-guided decision process for intelligent\nexploration. We evaluate WIRL on a carefully curated, high-quality dataset\nconsisting of real-world code adaptation scenarios. Our approach achieves an\nexact match precision of 91.7% and a recall of 90.0%, outperforming advanced\nLLMs by 22.6 and 13.7 percentage points in precision and recall, respectively,\nand surpassing IntelliJ IDEA by 54.3 and 49.9 percentage points. These results\nunderscore its practical utility, particularly in contexts with complex\nvariable dependencies or multiple unresolved variables. We believe WIRL paves\nthe way for more intelligent and context-aware developer assistance in modern\nIDEs.",
    "published": "2025-07-02T03:00:23Z",
    "updated": "2025-07-02T03:00:23Z",
    "id": "2507.01315v1",
    "authors": [
      "Taiming Wang",
      "Yanjie Jiang",
      "Chunhao Dong",
      "Yuxia Zhang",
      "Hui Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01315v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01315v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01315v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on using an LLM-based agent for code wiring recommendation, which involves leveraging LLMs for context-aware code adaptation and retrieval-augmented generation. This aligns with the topics of LLM (Large Language Models) and Memory (retrieval-augmented generation).",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.01297v2": {
    "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive\n  Benchmarks",
    "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems.",
    "published": "2025-07-02T02:35:47Z",
    "updated": "2025-07-05T21:33:54Z",
    "id": "2507.01297v2",
    "authors": [
      "Xinxi Lyu",
      "Michael Duan",
      "Rulin Shao",
      "Pang Wei Koh",
      "Sewon Min"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01297v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01297v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01297v2",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses Retrieval-augmented Generation (RAG) and its application to reasoning-intensive benchmarks, which aligns with topics related to Reasoning and Memory. It also introduces a new dataset (CompactDS) and evaluates it on various benchmarks, which relates to the Dataset topic.",
    "llm_cls_result": [
      "Reasoning",
      "Memory",
      "Dataset"
    ]
  },
  "2507.01079v1": {
    "title": "MobileRAG: A Fast, Memory-Efficient, and Energy-Efficient Method for\n  On-Device RAG",
    "summary": "Retrieval-Augmented Generation (RAG) has proven effective on server\ninfrastructures, but its application on mobile devices is still underexplored\ndue to limited memory and power resources. Existing vector search and RAG\nsolutions largely assume abundant computation resources, making them\nimpractical for on-device scenarios. In this paper, we propose MobileRAG, a\nfully on-device pipeline that overcomes these limitations by combining a\nmobile-friendly vector search algorithm, \\textit{EcoVector}, with a lightweight\n\\textit{Selective Content Reduction} (SCR) method. By partitioning and\npartially loading index data, EcoVector drastically reduces both memory\nfootprint and CPU usage, while the SCR method filters out irrelevant text to\ndiminish Language Model (LM) input size without degrading accuracy. Extensive\nexperiments demonstrated that MobileRAG significantly outperforms conventional\nvector search and RAG methods in terms of latency, memory usage, and power\nconsumption, while maintaining accuracy and enabling offline operation to\nsafeguard privacy in resource-constrained environments.",
    "published": "2025-07-01T15:12:14Z",
    "updated": "2025-07-01T15:12:14Z",
    "id": "2507.01079v1",
    "authors": [
      "Taehwan Park",
      "Geonho Lee",
      "Min-Soo Kim"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.01079v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.01079v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.01079v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on improving Retrieval-Augmented Generation (RAG) for mobile devices, which involves memory-efficient methods and retrieval-based techniques. The core topics are related to memory optimization and retrieval-augmented generation.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2507.00521v2": {
    "title": "WebANNS: Fast and Efficient Approximate Nearest Neighbor Search in Web\n  Browsers",
    "summary": "Approximate nearest neighbor search (ANNS) has become vital to modern AI\ninfrastructure, particularly in retrieval-augmented generation (RAG)\napplications. Numerous in-browser ANNS engines have emerged to seamlessly\nintegrate with popular LLM-based web applications, while addressing privacy\nprotection and challenges of heterogeneous device deployments. However, web\nbrowsers present unique challenges for ANNS, including computational\nlimitations, external storage access issues, and memory utilization\nconstraints, which state-of-the-art (SOTA) solutions fail to address\ncomprehensively. We propose WebANNS, a novel ANNS engine specifically designed\nfor web browsers. WebANNS leverages WebAssembly to overcome computational\nbottlenecks, designs a lazy loading strategy to optimize data retrieval from\nexternal storage, and applies a heuristic approach to reduce memory usage.\nExperiments show that WebANNS is fast and memory efficient, achieving up to\n$743.8\\times$ improvement in 99th percentile query latency over the SOTA\nengine, while reducing memory usage by up to 39\\%. Note that WebANNS decreases\nquery time from 10 seconds to the 10-millisecond range in browsers, making\nin-browser ANNS practical with user-acceptable latency.",
    "published": "2025-07-01T07:37:18Z",
    "updated": "2025-07-02T02:20:54Z",
    "id": "2507.00521v2",
    "authors": [
      "Mugeng Liu",
      "Siqi Zhong",
      "Qi Yang",
      "Yudong Han",
      "Xuanzhe Liu",
      "Yun Ma"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00521v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00521v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00521v2",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on Approximate Nearest Neighbor Search (ANNS) in web browsers, which is related to retrieval-augmented generation (RAG) applications but does not directly align with the provided topics. The main focus is on optimizing ANNS for web environments rather than LLMs or related fields.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2507.00352v1": {
    "title": "An AST-guided LLM Approach for SVRF Code Synthesis",
    "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity.",
    "published": "2025-07-01T00:57:45Z",
    "updated": "2025-07-01T00:57:45Z",
    "id": "2507.00352v1",
    "authors": [
      "Abanoub E. Abdelmalak",
      "Mohamed A. Elsayed",
      "David Abercrombie",
      "Ilhami Torunoglu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.00352v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.00352v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.00352v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on using LLM for code synthesis in a specific domain (SVRF), which involves retrieval-augmented generation (RAG) and structural validation using AST. The core topics are related to LLM applications in code generation and domain-specific tasks.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2507.02974v1": {
    "title": "InvisibleInk: High-Utility and Low-Cost Text Generation with\n  Differential Privacy",
    "summary": "As major progress in LLM-based long-form text generation enables paradigms\nsuch as retrieval-augmented generation (RAG) and inference-time scaling, safely\nincorporating private information into the generation remains a critical open\nquestion. We present InvisibleInk, a highly scalable long-form text generation\nframework satisfying rigorous differential privacy guarantees with respect to\nthe sensitive references. It interprets sampling from the LLM's\nnext-token-distribution as the exponential mechanism over the LLM logits with\ntwo innovations. First, we reduce the privacy cost by isolating and clipping\nonly the sensitive information in the model logits (relative to the public\nlogits). Second, we improve text quality by sampling from a small superset of\nthe top-$k$ private tokens. Empirical evaluations demonstrate a consistent\n$8\\times$ reduction in computation cost over state-of-the-art baselines to\ngenerate long-form private text of the same utility across privacy levels. In\nsummary, InvisibleInk is able to generate private long-form text at less than\n$10\\times$ the computation cost of non-private generation.",
    "published": "2025-06-30T18:00:41Z",
    "updated": "2025-06-30T18:00:41Z",
    "id": "2507.02974v1",
    "authors": [
      "Vishnu Vinod",
      "Krishna Pillutla",
      "Abhradeep Guha Thakurta"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.02974v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.02974v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.02974v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on differential privacy in LLM-based text generation, which involves privacy-preserving techniques and memory-augmented models for safe incorporation of private information.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.23958v1": {
    "title": "Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic\n  Device User Manuals Available in Marginalised Languages",
    "summary": "Millions of people in African countries face barriers to accessing healthcare\ndue to language and literacy gaps. This research tackles this challenge by\ntransforming complex medical documents -- in this case, prosthetic device user\nmanuals -- into accessible formats for underserved populations. This case study\nin cross-cultural translation is particularly pertinent/relevant for\ncommunities that receive donated prosthetic devices but may not receive the\naccompanying user documentation. Or, if available online, may only be available\nin formats (e.g., language and readability) that are inaccessible to local\npopulations (e.g., English-language, high resource settings/cultural context).\nThe approach is demonstrated using the widely spoken Pidgin dialect, but our\nopen-source framework has been designed to enable rapid and easy extension to\nother languages/dialects. This work presents an AI-powered framework designed\nto process and translate complex medical documents, e.g., user manuals for\nprosthetic devices, into marginalised languages. The system enables users --\nsuch as healthcare workers or patients -- to upload English-language medical\nequipment manuals, pose questions in their native language, and receive\naccurate, localised answers in real time. Technically, the system integrates a\nRetrieval-Augmented Generation (RAG) pipeline for processing and semantic\nunderstanding of the uploaded manuals. It then employs advanced Natural\nLanguage Processing (NLP) models for generative question-answering and\nmultilingual translation. Beyond simple translation, it ensures accessibility\nto device instructions, treatment protocols, and safety information, empowering\npatients and clinicians to make informed healthcare decisions.",
    "published": "2025-06-30T15:25:58Z",
    "updated": "2025-06-30T15:25:58Z",
    "id": "2506.23958v1",
    "authors": [
      "Ikechukwu Ogbonna",
      "Lesley Davidson",
      "Soumya Banerjee",
      "Abhishek Dasgupta",
      "Laurence Kenney",
      "Vikranth Harthikote Nagaraja"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23958v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23958v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23958v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on using Retrieval-Augmented Generation (RAG) for translating and making medical documents accessible in marginalized languages, which aligns with the 'Memory' topic due to its use of retrieval-based methods and long-context processing.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2506.23139v1": {
    "title": "Benchmarking Deep Search over Heterogeneous Enterprise Data",
    "summary": "We present a new benchmark for evaluating Deep Search--a realistic and\ncomplex form of retrieval-augmented generation (RAG) that requires\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\nwhich vary in structure and often contain human-to-human interactions. We build\nit using a synthetic data pipeline that simulates business workflows across\nproduct planning, development, and support stages, generating interconnected\ncontent with realistic noise and multi-hop questions with guaranteed\nground-truth answers. We release our benchmark with both answerable and\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\nexperiments reveal that even the best-performing agentic RAG methods achieve an\naverage performance score of 32.96 on our benchmark. With further analysis, we\nhighlight retrieval as the main bottleneck: existing methods struggle to\nconduct deep searches and retrieve all necessary evidence. Consequently, they\noften reason over partial context, leading to significant performance\ndegradation.",
    "published": "2025-06-29T08:34:59Z",
    "updated": "2025-06-29T08:34:59Z",
    "id": "2506.23139v1",
    "authors": [
      "Prafulla Kumar Choubey",
      "Xiangyu Peng",
      "Shilpa Bhagavath",
      "Kung-Hsiang Huang",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23139v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23139v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23139v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper presents a benchmark for evaluating retrieval-augmented generation (RAG) systems, which involves long-context LLM and RAG systems, and discusses their performance and limitations.",
    "llm_cls_result": [
      "Benchmark",
      "Memory",
      "Reasoning"
    ]
  },
  "2506.23026v1": {
    "title": "Machine Assistant with Reliable Knowledge: Enhancing Student Learning\n  via RAG-based Retrieval",
    "summary": "We present Machine Assistant with Reliable Knowledge (MARK), a\nretrieval-augmented question-answering system designed to support student\nlearning through accurate and contextually grounded responses. The system is\nbuilt on a retrieval-augmented generation (RAG) framework, which integrates a\ncurated knowledge base to ensure factual consistency. To enhance retrieval\neffectiveness across diverse question types, we implement a hybrid search\nstrategy that combines dense vector similarity with sparse keyword-based\nretrieval. This dual-retrieval mechanism improves robustness for both general\nand domain-specific queries. The system includes a feedback loop in which\nstudents can rate responses and instructors can review and revise them.\nInstructor corrections are incorporated into the retrieval corpus, enabling\nadaptive refinement over time. The system was deployed in a classroom setting\nas a substitute for traditional office hours, where it successfully addressed a\nbroad range of student queries. It was also used to provide technical support\nby integrating with a customer-specific knowledge base, demonstrating its\nability to handle routine, context-sensitive tasks in applied domains. MARK is\npublicly accessible at https://app.eduquery.ai.",
    "published": "2025-06-28T22:17:27Z",
    "updated": "2025-06-28T22:17:27Z",
    "id": "2506.23026v1",
    "authors": [
      "Yongsheng Lian"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.23026v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.23026v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.23026v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a retrieval-augmented question-answering system (MARK) that uses retrieval-augmented generation (RAG) to provide accurate and contextually grounded responses, which aligns with the 'Memory' topic. The system's focus on integrating a knowledge base and improving retrieval effectiveness also relates to the 'LLM' topic as it involves large language models in its framework.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2506.22900v1": {
    "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical\n  Visual Question Answering",
    "summary": "Medical visual question answering (MedVQA) plays a vital role in clinical\ndecision-making by providing contextually rich answers to image-based queries.\nAlthough vision-language models (VLMs) are widely used for this task, they\noften generate factually incorrect answers. Retrieval-augmented generation\naddresses this challenge by providing information from external sources, but\nrisks retrieving irrelevant context, which can degrade the reasoning\ncapabilities of VLMs. Re-ranking retrievals, as introduced in existing\napproaches, enhances retrieval relevance by focusing on query-text alignment.\nHowever, these approaches neglect the visual or multimodal context, which is\nparticularly crucial for medical diagnosis. We propose MOTOR, a novel\nmultimodal retrieval and re-ranking approach that leverages grounded captions\nand optimal transport. It captures the underlying relationships between the\nquery and the retrieved context based on textual and visual information.\nConsequently, our approach identifies more clinically relevant contexts to\naugment the VLM input. Empirical analysis and human expert evaluation\ndemonstrate that MOTOR achieves higher accuracy on MedVQA datasets,\noutperforming state-of-the-art methods by an average of 6.45%. Code is\navailable at https://github.com/BioMedIA-MBZUAI/MOTOR.",
    "published": "2025-06-28T14:30:37Z",
    "updated": "2025-06-28T14:30:37Z",
    "id": "2506.22900v1",
    "authors": [
      "Mai A. Shaaban",
      "Tausifa Jan Saleem",
      "Vijay Ram Papineni",
      "Mohammad Yaqub"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22900v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22900v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22900v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on multimodal retrieval and re-ranking in medical visual question answering, leveraging grounded captions and optimal transport to improve the relevance of retrieved context for VLMs. This aligns with topics related to multimodal models and retrieval-augmented methods.",
    "llm_cls_result": [
      "MLLM",
      "Memory",
      "VLA"
    ]
  },
  "2506.22750v1": {
    "title": "Enhancing Android Malware Detection with Retrieval-Augmented Generation",
    "summary": "The widespread use of Android applications has made them a prime target for\ncyberattacks, significantly increasing the risk of malware that threatens user\nprivacy, security, and device functionality. Effective malware detection is\nthus critical, with static analysis, dynamic analysis, and Machine Learning\nbeing widely used approaches. In this work, we focus on a Machine\nLearning-based method utilizing static features. We first compiled a dataset of\nbenign and malicious APKs and performed static analysis to extract features\nsuch as code structure, permissions, and manifest file content, without\nexecuting the apps. Instead of relying solely on raw static features, our\nsystem uses an LLM to generate high-level functional descriptions of APKs. To\nmitigate hallucinations, which are a known vulnerability of LLM, we integrated\nRetrieval-Augmented Generation (RAG), enabling the LLM to ground its output in\nrelevant context. Using carefully designed prompts, we guide the LLM to produce\ncoherent function summaries, which are then analyzed using a transformer-based\nmodel, improving detection accuracy over conventional feature-based methods for\nmalware detection.",
    "published": "2025-06-28T04:36:31Z",
    "updated": "2025-06-28T04:36:31Z",
    "id": "2506.22750v1",
    "authors": [
      "Saraga S.",
      "Anagha M. S.",
      "Dincy R. Arikkat",
      "Rafidha Rehiman K. A.",
      "Serena Nicolazzo",
      "Antonino Nocera",
      "Vinod P"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22750v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22750v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22750v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Retrieval-Augmented Generation (RAG) with an LLM to enhance malware detection, which aligns with the 'Memory' topic due to its focus on retrieval-based methods and memory-augmented models. It also involves 'LLM' as it utilizes a Large Language Model for generating functional descriptions.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2506.22644v1": {
    "title": "Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test\n  Sets: LiveRAG Challenge",
    "summary": "We present our submission to the LiveRAG Challenge 2025, which evaluates\nretrieval-augmented generation (RAG) systems on dynamic test sets using the\nFineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense\n(E5) retrieval methods and then aims to generate relevant and faithful answers\nwith Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic\nquestions generated with DataMorgana across 64 unique question-user\ncombinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP\nfrom 0.523 to 0.797 (52% relative improvement) but introduces prohibitive\ncomputational costs (84s vs 1.74s per question). While DSPy-optimized prompting\nstrategies achieved higher semantic similarity (0.771 vs 0.668), their 0%\nrefusal rates raised concerns about over-confidence and generalizability. Our\nsubmitted hybrid system without re-ranking achieved 4th place in faithfulness\nand 11th place in correctness among 25 teams. Analysis across question\ncategories reveals that vocabulary alignment between questions and documents\nwas the strongest predictor of performance on our development set, with\ndocument-similar phrasing improving cosine similarity from 0.562 to 0.762.",
    "published": "2025-06-27T21:20:43Z",
    "updated": "2025-06-27T21:20:43Z",
    "id": "2506.22644v1",
    "authors": [
      "Chase Fensore",
      "Kaustubh Dhole",
      "Joyce C Ho",
      "Eugene Agichtein"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22644v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22644v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22644v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on retrieval-augmented generation (RAG) systems, which involves memory-augmented models and retrieval-based methods. It also discusses the evaluation of these systems, which aligns with benchmarking LLMs.",
    "llm_cls_result": [
      "Memory",
      "Benchmark"
    ]
  },
  "2506.22303v1": {
    "title": "Education-Oriented Graph Retrieval-Augmented Generation for Learning\n  Path Recommendation",
    "summary": "Learning path recommendation seeks to provide learners with a structured\nsequence of learning items (e.g., knowledge concepts or exercises) to optimize\ntheir learning efficiency. Despite significant efforts in this area, most\nexisting methods primarily rely on prerequisite relationships, which present\ntwo major limitations: 1) Many educational datasets do not explicitly provide\nprerequisite relationships between knowledge concepts, hindering the\napplication of current learning path recommendation methods. 2) Relying solely\non prerequisite relationships as the sole knowledge structure can impede\nlearning progress and negatively impact student outcomes. To address these\nchallenges, we propose a novel approach, Discrimination Learning Enhances\nLearning Path Recommendation (DLELP), which enhances learning path\nrecommendations by incorporating both prerequisite and similarity relationships\nbetween knowledge concepts. Specifically, we introduce a knowledge concept\nstructure graph generation module that adaptively constructs knowledge concept\nstructure graphs for different educational datasets, significantly improving\nthe generalizability of learning path recommendation methods. We then propose a\nDiscrimination Learning-driven Reinforcement Learning (DLRL) framework, which\nmitigates the issue of blocked learning paths, further enhancing the efficacy\nof learning path recommendations. Finally, we conduct extensive experiments on\nthree benchmark datasets, demonstrating that our method not only achieves\nstate-of-the-art performance but also provides interpretable reasoning for the\nrecommended learning paths.",
    "published": "2025-06-27T15:15:42Z",
    "updated": "2025-06-27T15:15:42Z",
    "id": "2506.22303v1",
    "authors": [
      "Xinghe Cheng",
      "Zihan Zhang",
      "Jiapu Wang",
      "Liangda Fang",
      "Chaobo He",
      "Quanlong Guan",
      "Shirui Pan",
      "Weiqi Luo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22303v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22303v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22303v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on learning path recommendation using a combination of prerequisite and similarity relationships between knowledge concepts, and employs a Discrimination Learning-driven Reinforcement Learning (DLRL) framework. The primary topics are related to Reinforcement Learning (RL) and Memory (retrieval-augmented generation).",
    "llm_cls_result": [
      "RL",
      "Memory"
    ]
  },
  "2506.22262v1": {
    "title": "JointRank: Rank Large Set with Single Pass",
    "summary": "Efficiently ranking relevant items from large candidate pools is a\ncornerstone of modern information retrieval systems -- such as web search,\nrecommendation, and retrieval-augmented generation. Listwise rerankers, which\nimprove relevance by jointly considering multiple candidates, are often limited\nin practice: either by model input size constraints, or by degraded quality\nwhen processing large sets. We propose a model-agnostic method for fast\nreranking large sets that exceed a model input limits. The method first\npartitions candidate items into overlapping blocks, each of which is ranked\nindependently in parallel. Implicit pairwise comparisons are then derived from\nthese local rankings. Finally, these comparisons are aggregated to construct a\nglobal ranking using algorithms such as Winrate or PageRank. Experiments on\nTREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the\n57.68 for full-context listwise approach using gpt-4.1-mini as long-context\nmodel, while reducing latency from 21 to 8 seconds.\n  The implementation of the algorithm and the experiments is available in the\nrepository: https://github.com/V3RGANz/jointrank",
    "published": "2025-06-27T14:30:12Z",
    "updated": "2025-06-27T14:30:12Z",
    "id": "2506.22262v1",
    "authors": [
      "Evgeny Dedov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22262v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22262v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22262v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on ranking methods for large candidate pools, which is relevant to information retrieval systems and retrieval-augmented generation, but does not directly align with the provided topics related to LLMs, RL, MLLM, etc.",
    "llm_cls_result": [
      "Other"
    ]
  },
  "2506.22210v1": {
    "title": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation\n  of Responses",
    "summary": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. The LiveRAG\nChallenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus\nand a shared, open-source LLM. We propose a modular pipeline that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. This multistage pipeline encompasses query rewriting,\npassage retrieval and reranking, nugget detection and clustering, cluster\nranking and summarization, and response fluency enhancement. This design\ninherently promotes grounding in specific facts, facilitates source\nattribution, and ensures maximum information inclusion within length\nconstraints. In this challenge, we extend our focus to also address the\nretrieval component of RAG, building upon our prior work on multi-faceted query\nrewriting. Furthermore, for augmented generation, we concentrate on improving\ncontext curation capabilities, maximizing the breadth of information covered in\nthe response while ensuring pipeline efficiency. Our results show that\ncombining original queries with a few sub-query rewrites boosts recall, while\nincreasing the number of documents used for reranking and generation beyond a\ncertain point reduces effectiveness, without improving response quality.",
    "published": "2025-06-27T13:29:25Z",
    "updated": "2025-06-27T13:29:25Z",
    "id": "2506.22210v1",
    "authors": [
      "Weronika ajewska",
      "Ivica Kostric",
      "Gabriel Iturra-Bocaz",
      "Mariam Arustashvili",
      "Krisztian Balog"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22210v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22210v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22210v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on retrieval-augmented generation (RAG), which involves memory-augmented models and retrieval-based methods, aligning with the 'Memory' topic. It also discusses improving context curation capabilities and response quality, which are relevant to 'Reasoning' in LLMs.",
    "llm_cls_result": [
      "Memory",
      "Reasoning"
    ]
  },
  "2506.22026v1": {
    "title": "Literature-Grounded Novelty Assessment of Scientific Ideas",
    "summary": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation.",
    "published": "2025-06-27T08:47:28Z",
    "updated": "2025-06-27T08:47:28Z",
    "id": "2506.22026v1",
    "authors": [
      "Simra Shahid",
      "Marissa Radensky",
      "Raymond Fok",
      "Pao Siangliulue",
      "Daniel S. Weld",
      "Tom Hope"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22026v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22026v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22026v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on using LLM-based retrieval-augmented generation (RAG) for evaluating the novelty of scientific ideas, which involves LLM reasoning and memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2506.21098v2": {
    "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for\n  Real-time Community Question Answering in Industry",
    "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations.",
    "published": "2025-06-26T08:48:16Z",
    "updated": "2025-07-01T06:31:06Z",
    "id": "2506.21098v2",
    "authors": [
      "Qinwen Chen",
      "Wenbiao Tao",
      "Zhiwei Zhu",
      "Mingfan Xi",
      "Liangzhong Guo",
      "Yuan Wang",
      "Wei Wang",
      "Yunshi Lan"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21098v2",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21098v2.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21098v2",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on retrieval-augmented generation and memory mechanisms for real-time community question answering, which aligns with the topics of Memory and Reasoning in the context of LLMs.",
    "llm_cls_result": [
      "Memory",
      "Reasoning"
    ]
  },
  "2506.20817v1": {
    "title": "RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced\n  Retrieval-Augmented Generation in Recommendation",
    "summary": "This paper addresses the challenge of developing multimodal recommender\nsystems for the movie domain, where limited metadata (e.g., title, genre) often\nhinders the generation of robust recommendations. We introduce a resource that\ncombines LLM-generated plot descriptions with trailer-derived visual embeddings\nin a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and\ncollaborative filtering. Central to our approach is a data augmentation step\nthat transforms sparse metadata into richer textual signals, alongside fusion\nstrategies (e.g., PCA, CCA) that integrate visual cues. Experimental\nevaluations demonstrate that CCA-based fusion significantly boosts recall\ncompared to unimodal baselines, while an LLM-driven re-ranking step further\nimproves NDCG, particularly in scenarios with limited textual data. By\nreleasing this framework, we invite further exploration of multi-modal\nrecommendation techniques tailored to cold-start, novelty-focused, and\ndomain-specific settings. All code, data, and detailed documentation are\npublicly available at: https://github.com/RecSys-lab/RAG-VisualRec",
    "published": "2025-06-25T20:32:12Z",
    "updated": "2025-06-25T20:32:12Z",
    "id": "2506.20817v1",
    "authors": [
      "Ali Tourani",
      "Fatemeh Nazary",
      "Yashar Deldjoo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20817v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20817v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20817v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the integration of LLM-generated text and visual embeddings for multimodal recommendation systems, which involves aspects of Retrieval-Augmented Generation (RAG) and multimodal data fusion.",
    "llm_cls_result": [
      "Memory",
      "MLLM",
      "Dataset"
    ]
  },
  "2506.20670v1": {
    "title": "MMSearch-R1: Incentivizing LMMs to Search",
    "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search.",
    "published": "2025-06-25T17:59:42Z",
    "updated": "2025-06-25T17:59:42Z",
    "id": "2506.20670v1",
    "authors": [
      "Jinming Wu",
      "Zihao Deng",
      "Wei Li",
      "Yiding Liu",
      "Bo You",
      "Bo Li",
      "Zejun Ma",
      "Ziwei Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20670v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20670v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20670v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a reinforcement learning framework for large multimodal models (LMMs) to perform on-demand, multi-turn search, integrating both image and text search tools. It involves retrieval-augmented generation (RAG) and focuses on multimodal search behavior, which aligns with the topics of Multimodal Large Language Models (MLLM) and Reinforcement Learning (RL).",
    "llm_cls_result": [
      "MLLM",
      "RL"
    ]
  },
  "2506.20598v1": {
    "title": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of\n  Multi-Agent AI for Addressing Sustainable Protein Production Challenges",
    "summary": "The global demand for sustainable protein sources has accelerated the need\nfor intelligent tools that can rapidly process and synthesise domain-specific\nscientific knowledge. In this study, we present a proof-of-concept multi-agent\nArtificial Intelligence (AI) framework designed to support sustainable protein\nproduction research, with an initial focus on microbial protein sources. Our\nRetrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based\nLLM agents: (1) a literature search agent that retrieves relevant scientific\nliterature on microbial protein production for a specified microbial strain,\nand (2) an information extraction agent that processes the retrieved content to\nextract relevant biological and chemical information. Two parallel\nmethodologies, fine-tuning and prompt engineering, were explored for agent\noptimisation. Both methods demonstrated effectiveness at improving the\nperformance of the information extraction agent in terms of transformer-based\ncosine similarity scores between obtained and ideal outputs. Mean cosine\nsimilarity scores were increased by up to 25%, while universally reaching mean\nscores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved\nthe mean scores to a greater extent (consistently of $\\geq 0.94$) compared to\nprompt engineering, although lower statistical uncertainties were observed with\nthe latter approach. A user interface was developed and published for enabling\nthe use of the multi-agent AI system, alongside preliminary exploration of\nadditional chemical safety-based search capabilities",
    "published": "2025-06-25T16:37:46Z",
    "updated": "2025-06-25T16:37:46Z",
    "id": "2506.20598v1",
    "authors": [
      "Alexander D. Kalian",
      "Jaewook Lee",
      "Stefan P. Johannesson",
      "Lennart Otte",
      "Christer Hogstrand",
      "Miao Guo"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.20598v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.20598v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.20598v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of LLMs (Large Language Models) in a multi-agent AI framework, focusing on fine-tuning and prompt engineering to optimize performance. It also mentions Retrieval-Augmented Generation (RAG), which is related to memory-augmented models. The primary focus is on the application of LLMs in a specific domain, but the core topics are LLM and Memory.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.19512v1": {
    "title": "heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for\n  Retrieval Augmented Generation",
    "summary": "This paper presents the approach of our team called heiDS for the ArchEHR-QA\n2025 shared task. A pipeline using a retrieval augmented generation (RAG)\nframework is designed to generate answers that are attributed to clinical\nevidence from the electronic health records (EHRs) of patients in response to\npatient-specific questions. We explored various components of a RAG framework,\nfocusing on ranked list truncation (RLT) retrieval strategies and attribution\napproaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a\nquery-dependent-k retrieval strategy, including the existing surprise and\nautocut methods and two new methods proposed in this work, autocut* and elbow.\nThe experimental results show the benefits of our strategy in producing factual\nand relevant answers when compared to a fixed-$k$.",
    "published": "2025-06-24T11:03:01Z",
    "updated": "2025-06-24T11:03:01Z",
    "id": "2506.19512v1",
    "authors": [
      "Ashish Chouhan",
      "Michael Gertz"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19512v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19512v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19512v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses retrieval augmented generation (RAG) frameworks and strategies for ranked list truncation (RLT), which are related to memory-augmented models and retrieval-based methods.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2506.19484v1": {
    "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI\n  with Proven Theories of Learning",
    "summary": "Large Language Models (LLMs) are rapidly transforming education by enabling\nrich conversational learning experiences. This article provides a comprehensive\nreview of how LLM-based conversational agents are being used in higher\neducation, with extensions to secondary and lifelong learning contexts. We\nsynthesize existing literature on LLMs in education and theories of\nconversational and dialogic pedagogy - including Vygotsky's sociocultural\nlearning (scaffolding and the Zone of Proximal Development), the Socratic\nmethod, and Laurillard's conversational framework - and examine how prompting\nstrategies and retrieval-augmented generation (RAG) can align LLM behaviors\nwith these pedagogical theories, and how it can support personalized, adaptive\nlearning. We map educational theories to LLM capabilities, highlighting where\nLLM-driven dialogue supports established learning principles and where it\nchallenges or falls short of traditional pedagogical assumptions. Notable gaps\nin applying prior theories to LLMs are identified, such as the models tendency\nto provide direct answers instead of fostering co-construction of knowledge,\nand the need to account for the constant availability and broad but non-human\nexpertise of LLM tutors. In response, we propose practical strategies to better\nalign LLM interactions with sound pedagogy - for example, designing prompts\nthat encourage Socratic questioning, scaffolded guidance, and student\nreflection, as well as integrating retrieval mechanisms to ensure accuracy and\ncontextual relevance. Our aim is to bridge the gap between educational theory\nand the emerging practice of AI-driven conversational learning, offering\ninsights and tools for making LLM-based dialogues more educationally productive\nand theory-aligned.",
    "published": "2025-06-24T10:19:09Z",
    "updated": "2025-06-24T10:19:09Z",
    "id": "2506.19484v1",
    "authors": [
      "Russell Beale"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19484v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19484v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19484v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in educational contexts, focusing on aligning LLM behaviors with pedagogical theories and enhancing conversational learning experiences. It highlights the use of LLMs in education, prompting strategies, and retrieval-augmented generation (RAG), which are core topics related to LLMs and their applications in learning and memory.",
    "llm_cls_result": [
      "LLM",
      "Memory",
      "Reasoning"
    ]
  },
  "2506.19385v1": {
    "title": "Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue\n  Systems through Adaptive Dual-Retrieval of Flow Patterns and Context\n  Semantics",
    "summary": "We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval\nAugmented Generation), a novel framework that addresses the limitations of\nexisting dialogue systems in maintaining both contextual coherence and\ngoal-oriented progression in multi-turn customer service conversations. Unlike\ntraditional RAG systems that rely solely on semantic similarity (Conversation\nRAG) or standard knowledge graphs (GraphRAG), CID-GraphRAG constructs dynamic\nintent transition graphs from goal achieved historical dialogues and implements\na dual-retrieval mechanism that adaptively balances intent-based graph\ntraversal with semantic search. This approach enables the system to\nsimultaneously leverage both conversional intent flow patterns and contextual\nsemantics, significantly improving retrieval quality and response quality. In\nextensive experiments on real-world customer service dialogues, we employ both\nautomatic metrics and LLM-as-judge assessments, demonstrating that CID-GraphRAG\nsignificantly outperforms both semantic-based Conversation RAG and intent-based\nGraphRAG baselines across all evaluation criteria. Quantitatively, CID-GraphRAG\ndemonstrates substantial improvements over Conversation RAG across automatic\nmetrics, with relative gains of 11% in BLEU, 5% in ROUGE-L, 6% in METEOR, and\nmost notably, a 58% improvement in response quality according to LLM-as-judge\nevaluations. These results demonstrate that the integration of intent\ntransition structures with semantic retrieval creates a synergistic effect that\nneither approach achieves independently, establishing CID-GraphRAG as an\neffective framework for addressing the challenges of maintaining contextual\ncoherence and goal-oriented progression in knowledge-intensive multi-turn\ndialogues.",
    "published": "2025-06-24T07:20:45Z",
    "updated": "2025-06-24T07:20:45Z",
    "id": "2506.19385v1",
    "authors": [
      "Ziqi Zhu",
      "Tao Hu",
      "Honglong Zhang",
      "Dan Yang",
      "HanGeng Chen",
      "Mengran Zhang",
      "Xilun Chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.19385v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.19385v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.19385v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing multi-turn dialogue systems through a novel retrieval-augmented generation framework that integrates intent transition structures with semantic retrieval. This aligns with topics related to memory-augmented models and retrieval-based methods in LLMs.",
    "llm_cls_result": [
      "Memory",
      "LLM"
    ]
  },
  "2506.22486v1": {
    "title": "Hallucination Detection with Small Language Models",
    "summary": "Since the introduction of ChatGPT, large language models (LLMs) have\ndemonstrated significant utility in various tasks, such as answering questions\nthrough retrieval-augmented generation. Context can be retrieved using a\nvectorized database, serving as a foundation for LLMs to generate responses.\nHowever, hallucinations in responses can undermine the reliability of LLMs in\npractical applications, and they are not easily detectable in the absence of\nground truth, particularly in question-and-answer scenarios. This paper\nproposes a framework that integrates multiple small language models to verify\nresponses generated by LLMs using the retrieved context from a vectorized\ndatabase. By breaking down the responses into individual sentences and\nutilizing the probability of generating \"Yes\" tokens from the outputs of\nmultiple models for a given set of questions, responses, and relevant context,\nhallucinations can be detected. The proposed framework is validated through\nexperiments with real datasets comprising over 100 sets of questions, answers,\nand contexts, including responses with fully and partially correct sentences.\nThe results demonstrate a 10\\% improvement in F1 scores for detecting correct\nresponses compared to hallucinations, indicating that multiple small language\nmodels can be effectively employed for answer verification, providing a\nscalable and efficient solution for both academic and practical applications.",
    "published": "2025-06-24T02:19:26Z",
    "updated": "2025-06-24T02:19:26Z",
    "id": "2506.22486v1",
    "authors": [
      "Ming Cheung"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.22486v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.22486v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.22486v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on detecting hallucinations in responses generated by large language models (LLMs) using small language models, which involves verification and reliability of LLM outputs. This aligns with topics related to LLM reliability and memory-augmented methods.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.18628v1": {
    "title": "AggTruth: Contextual Hallucination Detection using Aggregated Attention\n  Scores in LLMs",
    "summary": "In real-world applications, Large Language Models (LLMs) often hallucinate,\neven in Retrieval-Augmented Generation (RAG) settings, which poses a\nsignificant challenge to their deployment. In this paper, we introduce\nAggTruth, a method for online detection of contextual hallucinations by\nanalyzing the distribution of internal attention scores in the provided context\n(passage). Specifically, we propose four different variants of the method, each\nvarying in the aggregation technique used to calculate attention scores. Across\nall LLMs examined, AggTruth demonstrated stable performance in both same-task\nand cross-task setups, outperforming the current SOTA in multiple scenarios.\nFurthermore, we conducted an in-depth analysis of feature selection techniques\nand examined how the number of selected attention heads impacts detection\nperformance, demonstrating that careful selection of heads is essential to\nachieve optimal results.",
    "published": "2025-06-23T13:35:05Z",
    "updated": "2025-06-23T13:35:05Z",
    "id": "2506.18628v1",
    "authors": [
      "Piotr Matys",
      "Jan Eliasz",
      "Konrad Kieczyski",
      "Mikoaj Langner",
      "Teddy Ferdinan",
      "Jan Koco",
      "Przemysaw Kazienko"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.18628v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.18628v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.18628v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on detecting hallucinations in LLMs using attention scores, which is related to the study of LLM memory and retrieval-augmented generation.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.18559v1": {
    "title": "T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing\n  Logic-RAG Agent",
    "summary": "Large language models excel at generating fluent text but frequently struggle\nwith structured reasoning involving temporal constraints, causal relationships,\nand probabilistic reasoning. To address these limitations, we propose Temporal\nCausal Probabilistic Description Logic (T-CPDL), an integrated framework that\nextends traditional Description Logic with temporal interval operators,\nexplicit causal relationships, and probabilistic annotations. We present two\ndistinct variants of T-CPDL: one capturing qualitative temporal relationships\nthrough Allen's interval algebra, and another variant enriched with explicit\ntimestamped causal assertions. Both variants share a unified logical structure,\nenabling complex reasoning tasks ranging from simple temporal ordering to\nnuanced probabilistic causation. Empirical evaluations on temporal reasoning\nand causal inference benchmarks confirm that T-CPDL substantially improves\ninference accuracy, interpretability, and confidence calibration of language\nmodel outputs. By delivering transparent reasoning paths and fine-grained\ntemporal and causal semantics, T-CPDL significantly enhances the capability of\nlanguage models to support robust, explainable, and trustworthy\ndecision-making. This work also lays the groundwork for developing advanced\nLogic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially\nboosting the reasoning capabilities and efficiency of knowledge graph-enhanced\nRAG systems.",
    "published": "2025-06-23T12:11:15Z",
    "updated": "2025-06-23T12:11:15Z",
    "id": "2506.18559v1",
    "authors": [
      "Hong Qing Yu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.18559v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.18559v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.18559v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing the reasoning capabilities of large language models through a new logical framework, which involves temporal and causal reasoning. This aligns with the topics of Reasoning (for complex problem solving and logical reasoning in LLMs) and Memory (for retrieval-augmented generation and knowledge graph-enhanced systems).",
    "llm_cls_result": [
      "Reasoning",
      "Memory"
    ]
  },
  "2506.18511v1": {
    "title": "Standard Applicability Judgment and Cross-jurisdictional Reasoning: A\n  RAG-based Framework for Medical Device Compliance",
    "summary": "Identifying the appropriate regulatory standard applicability remains a\ncritical yet understudied challenge in medical device compliance, frequently\nnecessitating expert interpretation of fragmented and heterogeneous\ndocumentation across different jurisdictions. To address this challenge, we\nintroduce a modular AI system that leverages a retrieval-augmented generation\n(RAG) pipeline to automate standard applicability determination. Given a\nfree-text device description, our system retrieves candidate standards from a\ncurated corpus and uses large language models to infer jurisdiction-specific\napplicability, classified as Mandatory, Recommended, or Not Applicable, with\ntraceable justifications. We construct an international benchmark dataset of\nmedical device descriptions with expert-annotated standard mappings, and\nevaluate our system against retrieval-only, zero-shot, and rule-based\nbaselines. The proposed approach attains a classification accuracy of 73% and a\nTop-5 retrieval recall of 87%, demonstrating its effectiveness in identifying\nrelevant regulatory standards. We introduce the first end-to-end system for\nstandard applicability reasoning, enabling scalable and interpretable\nAI-supported regulatory science. Notably, our region-aware RAG agent performs\ncross-jurisdictional reasoning between Chinese and U.S. standards, supporting\nconflict resolution and applicability justification across regulatory\nframeworks.",
    "published": "2025-06-23T11:04:58Z",
    "updated": "2025-06-23T11:04:58Z",
    "id": "2506.18511v1",
    "authors": [
      "Yu Han",
      "Aaron Ceross",
      "Jeroen H. M. Bergmann"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.18511v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.18511v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.18511v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of retrieval-augmented generation (RAG) pipelines and large language models (LLMs) for medical device compliance, which involves reasoning and memory aspects of LLMs. It also introduces a benchmark dataset for evaluation.",
    "llm_cls_result": [
      "Memory",
      "Reasoning",
      "Benchmark"
    ]
  },
  "2506.18952v1": {
    "title": "LLMs on a Budget? Say HOLA",
    "summary": "Running Large Language Models (LLMs) on edge devices is constrained by high\ncompute and memory demands posing a barrier for real-time applications in\nsectors like healthcare, education, and embedded systems. Current solutions\nsuch as quantization, pruning, and retrieval-augmented generation (RAG) offer\nonly partial optimizations and often compromise on speed or accuracy. We\nintroduce HOLA, an end-to-end optimization framework for efficient LLM\ndeployment. Internally, it leverages Hierarchical Speculative Decoding (HSD)\nfor faster inference without quality loss. Externally, AdaComp-RAG adjusts\nretrieval complexity based on context needs. Together with LoBi, which blends\nstructured pruning (LoRA) and quantization, HOLA delivers significant gains:\n17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge\ndevices like Jetson Nano--proving both scalable and production-ready.",
    "published": "2025-06-23T10:20:47Z",
    "updated": "2025-06-23T10:20:47Z",
    "id": "2506.18952v1",
    "authors": [
      "Zohaib Hasan Siddiqui",
      "Jiechao Gao",
      "Ebad Shabbir",
      "Mohammad Anas Azeez",
      "Rafiq Ali",
      "Gautam Siddharth Kashyap",
      "Usman Naseem"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.18952v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.18952v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.18952v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses optimization techniques for deploying Large Language Models (LLMs) on edge devices, focusing on methods like Hierarchical Speculative Decoding, AdaComp-RAG, and LoBi for efficient inference. The core topics are related to LLM optimization and deployment, which aligns with the 'LLM' and 'Memory' categories due to the mention of retrieval-augmented generation and memory efficiency.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.18942v1": {
    "title": "Advanced Applications of Generative AI in Actuarial Science: Case\n  Studies Beyond ChatGPT",
    "summary": "This article demonstrates the transformative impact of Generative AI (GenAI)\non actuarial science, illustrated by four implemented case studies. It begins\nwith a historical overview of AI, tracing its evolution from early neural\nnetworks to modern GenAI technologies. The first case study shows how Large\nLanguage Models (LLMs) improve claims cost prediction by deriving significant\nfeatures from unstructured textual data, significantly reducing prediction\nerrors in the underlying machine learning task. In the second case study, we\nexplore the automation of market comparisons using the GenAI concept of\nRetrieval-Augmented Generation to identify and process relevant information\nfrom documents. A third case study highlights the capabilities of fine-tuned\nvision-enabled LLMs in classifying car damage types and extracting contextual\ninformation. The fourth case study presents a multi-agent system that\nautonomously analyzes data from a given dataset and generates a corresponding\nreport detailing the key findings. In addition to these case studies, we\noutline further potential applications of GenAI in the insurance industry, such\nas the automation of claims processing and fraud detection, and the\nverification of document compliance with internal or external policies.\nFinally, we discuss challenges and considerations associated with the use of\nGenAI, covering regulatory issues, ethical concerns, and technical limitations,\namong others.",
    "published": "2025-06-22T19:36:03Z",
    "updated": "2025-06-22T19:36:03Z",
    "id": "2506.18942v1",
    "authors": [
      "Simon Hatzesberger",
      "Iris Nonneman"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.18942v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.18942v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.18942v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the application of Large Language Models (LLMs) in actuarial science, including claims cost prediction, automation of market comparisons, and classification of car damage types using vision-enabled LLMs. It also mentions Retrieval-Augmented Generation and multi-agent systems, which are related to LLM applications.",
    "llm_cls_result": [
      "LLM",
      "MLLM",
      "Memory"
    ]
  },
  "2506.18027v1": {
    "title": "PDF Retrieval Augmented Question Answering",
    "summary": "This paper presents an advancement in Question-Answering (QA) systems using a\nRetrieval Augmented Generation (RAG) framework to enhance information\nextraction from PDF files. Recognizing the richness and diversity of data\nwithin PDFs--including text, images, vector diagrams, graphs, and tables--poses\nunique challenges for existing QA systems primarily designed for textual\ncontent. We seek to develop a comprehensive RAG-based QA system that will\neffectively address complex multimodal questions, where several data types are\ncombined in the query. This is mainly achieved by refining approaches to\nprocessing and integrating non-textual elements in PDFs into the RAG framework\nto derive precise and relevant answers, as well as fine-tuning large language\nmodels to better adapt to our system. We provide an in-depth experimental\nevaluation of our solution, demonstrating its capability to extract accurate\ninformation that can be applied to different types of content across PDFs. This\nwork not only pushes the boundaries of retrieval-augmented QA systems but also\nlays a foundation for further research in multimodal data integration and\nprocessing.",
    "published": "2025-06-22T13:14:19Z",
    "updated": "2025-06-22T13:14:19Z",
    "id": "2506.18027v1",
    "authors": [
      "Thi Thu Uyen Hoang",
      "Viet Anh Nguyen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.18027v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.18027v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.18027v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing Question-Answering systems using Retrieval Augmented Generation (RAG) framework, which involves memory-augmented models and retrieval-based methods. It also mentions fine-tuning large language models, indicating relevance to LLM research.",
    "llm_cls_result": [
      "Memory",
      "LLM",
      "MLLM"
    ]
  },
  "2506.21615v1": {
    "title": "Refine Medical Diagnosis Using Generation Augmented Retrieval and\n  Clinical Practice Guidelines",
    "summary": "Current medical language models, adapted from large language models (LLMs),\ntypically predict ICD code-based diagnosis from electronic health records\n(EHRs) because these labels are readily available. However, ICD codes do not\ncapture the nuanced, context-rich reasoning clinicians use for diagnosis.\nClinicians synthesize diverse patient data and reference clinical practice\nguidelines (CPGs) to make evidence-based decisions. This misalignment limits\nthe clinical utility of existing models. We introduce GARMLE-G, a\nGeneration-Augmented Retrieval framework that grounds medical language model\noutputs in authoritative CPGs. Unlike conventional Retrieval-Augmented\nGeneration based approaches, GARMLE-G enables hallucination-free outputs by\ndirectly retrieving authoritative guideline content without relying on\nmodel-generated text. It (1) integrates LLM predictions with EHR data to create\nsemantically rich queries, (2) retrieves relevant CPG knowledge snippets via\nembedding similarity, and (3) fuses guideline content with model output to\ngenerate clinically aligned recommendations. A prototype system for\nhypertension diagnosis was developed and evaluated on multiple metrics,\ndemonstrating superior retrieval precision, semantic relevance, and clinical\nguideline adherence compared to RAG-based baselines, while maintaining a\nlightweight architecture suitable for localized healthcare deployment. This\nwork provides a scalable, low-cost, and hallucination-free method for grounding\nmedical language models in evidence-based clinical practice, with strong\npotential for broader clinical deployment.",
    "published": "2025-06-22T11:31:13Z",
    "updated": "2025-06-22T11:31:13Z",
    "id": "2506.21615v1",
    "authors": [
      "Wenhao Li",
      "Hongkuan Zhang",
      "Hongwei Zhang",
      "Zhengxu Li",
      "Zengjie Dong",
      "Yafan Chen",
      "Niranjan Bidargaddi",
      "Hong Liu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.21615v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.21615v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.21615v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of large language models (LLMs) in medical diagnosis, specifically focusing on retrieval-augmented generation and clinical practice guidelines. It highlights the integration of LLM predictions with EHR data and the retrieval of relevant CPG knowledge snippets, which aligns with the topics of LLM and Memory (retrieval-based methods).",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.17951v1": {
    "title": "A Comprehensive Graph Framework for Question Answering with Mode-Seeking\n  Preference Alignment",
    "summary": "Recent advancements in retrieval-augmented generation (RAG) have enhanced\nlarge language models in question answering by integrating external knowledge.\nHowever, challenges persist in achieving global understanding and aligning\nresponses with human ethical and quality preferences. To address these issues,\nwe propose GraphMPA, a comprehensive graph-based framework with mode-seeking\npreference alignment. Our approach constructs a hierarchical document graph\nusing a general similarity measurement, mimicking human cognitive processes for\ninformation understanding and synthesis. Additionally, we introduce\nmode-seeking preference optimization to better align model outputs with human\npreferences through probability-matching constraints. Extensive experiments on\nsix datasets demonstrate the effectiveness of our\n\\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.",
    "published": "2025-06-22T09:08:44Z",
    "updated": "2025-06-22T09:08:44Z",
    "id": "2506.17951v1",
    "authors": [
      "Quanwei Tang",
      "Sophia Yat Mei Lee",
      "Junshuang Wu",
      "Dong Zhang",
      "Shoushan Li",
      "Erik Cambria",
      "Guodong Zhou"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.17951v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.17951v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.17951v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses retrieval-augmented generation (RAG) and aligning model outputs with human preferences, which are key topics in Memory and Reasoning.",
    "llm_cls_result": [
      "Memory",
      "Reasoning"
    ]
  },
  "2506.17865v1": {
    "title": "LASA: Enhancing SoC Security Verification with LLM-Aided Property\n  Generation",
    "summary": "Ensuring the security of modern System-on-Chip (SoC) designs poses\nsignificant challenges due to increasing complexity and distributed assets\nacross the intellectual property (IP) blocks. Formal property verification\n(FPV) provides the capability to model and validate design behaviors through\nsecurity properties with model checkers; however, current practices require\nsignificant manual efforts to create such properties, making them\ntime-consuming, costly, and error-prone. The emergence of Large Language Models\n(LLMs) has showcased remarkable proficiency across diverse domains, including\nHDL code generation and verification tasks. Current LLM-based techniques often\nproduce vacuous assertions and lack efficient prompt generation, comprehensive\nverification, and bug detection. This paper presents LASA, a novel framework\nthat leverages LLMs and retrieval-augmented generation (RAG) to produce\nnon-vacuous security properties and SystemVerilog Assertions (SVA) from design\nspecifications and related documentation for bus-based SoC designs. LASA\nintegrates commercial EDA tool for FPV to generate coverage metrics and\niteratively refines prompts through a feedback loop to enhance coverage. The\neffectiveness of LASA is validated through various open-source SoC designs,\ndemonstrating high coverage values with an average of ~88\\%, denoting\ncomprehensive verification through efficient generation of security properties\nand SVAs. LASA also demonstrates bug detection capabilities, identifying five\nunique bugs in the buggy OpenTitan SoC from Hack@DAC'24 competition.",
    "published": "2025-06-22T01:21:03Z",
    "updated": "2025-06-22T01:21:03Z",
    "id": "2506.17865v1",
    "authors": [
      "Dinesh Reddy Ankireddy",
      "Sudipta Paria",
      "Aritra Dasgupta",
      "Sandip Ray",
      "Swarup Bhunia"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.17865v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.17865v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.17865v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLMs) in enhancing security verification for System-on-Chip (SoC) designs, specifically through the generation of security properties and SystemVerilog Assertions (SVA). It also mentions the use of retrieval-augmented generation (RAG), which is related to memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.17781v1": {
    "title": "Beyond instruction-conditioning, MoTE: Mixture of Task Experts for\n  Multi-task Embedding Models",
    "summary": "Dense embeddings are fundamental to modern machine learning systems, powering\nRetrieval-Augmented Generation (RAG), information retrieval, and representation\nlearning. While instruction-conditioning has become the dominant approach for\nembedding specialization, its direct application to low-capacity models imposes\nfundamental representational constraints that limit the performance gains\nderived from specialization. In this paper, we analyze these limitations and\nintroduce the Mixture of Task Experts (MoTE) transformer block, which leverages\ntask-specialized parameters trained with Task-Aware Contrastive Learning\n(\\tacl) to enhance the model ability to generate specialized embeddings.\nEmpirical results show that MoTE achieves $64\\%$ higher performance gains in\nretrieval datasets ($+3.27 \\rightarrow +5.21$) and $43\\%$ higher performance\ngains across all datasets ($+1.81 \\rightarrow +2.60$). Critically, these gains\nare achieved without altering instructions, training data, inference time, or\nnumber of active parameters.",
    "published": "2025-06-21T18:28:25Z",
    "updated": "2025-06-21T18:28:25Z",
    "id": "2506.17781v1",
    "authors": [
      "Miguel Romero",
      "Shuoyang Ding",
      "Corey D. Barret",
      "Georgiana Dinu",
      "George Karypis"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.17781v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.17781v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.17781v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the Mixture of Task Experts (MoTE) transformer block, which is related to the Mixture of Experts (MoE) models. It also mentions Retrieval-Augmented Generation (RAG), which is associated with Memory-augmented models.",
    "llm_cls_result": [
      "MoE",
      "Memory"
    ]
  },
  "2506.17644v1": {
    "title": "Measuring and Augmenting Large Language Models for Solving\n  Capture-the-Flag Challenges",
    "summary": "Capture-the-Flag (CTF) competitions are crucial for cybersecurity education\nand training. As large language models (LLMs) evolve, there is increasing\ninterest in their ability to automate CTF challenge solving. For example, DARPA\nhas organized the AIxCC competition since 2023 to advance AI-powered automated\noffense and defense. However, this demands a combination of multiple abilities,\nfrom knowledge to reasoning and further to actions. In this paper, we highlight\nthe importance of technical knowledge in solving CTF problems and deliberately\nconstruct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs'\nperformance in this core aspect. Our study offers a focused and innovative\nmeasurement of LLMs' capability in understanding CTF knowledge and applying it\nto solve CTF challenges. Our key findings reveal that while LLMs possess\nsubstantial technical knowledge, they falter in accurately applying this\nknowledge to specific scenarios and adapting their strategies based on feedback\nfrom the CTF environment.\n  Based on insights derived from this measurement study, we propose CTFAgent, a\nnovel LLM-driven framework for advancing CTF problem-solving. CTFAgent\nintroduces two new modules: two-stage Retrieval Augmented Generation (RAG) and\ninteractive Environmental Augmentation, which enhance LLMs' technical knowledge\nand vulnerability exploitation on CTF, respectively. Our experimental results\nshow that, on two popular CTF datasets, CTFAgent both achieves over 80%\nperformance improvement. Moreover, in the recent picoCTF2024 hosted by CMU,\nCTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This\nreflects the benefit of our measurement study and the potential of our\nframework in advancing LLMs' capabilities in CTF problem-solving.",
    "published": "2025-06-21T08:56:20Z",
    "updated": "2025-06-21T08:56:20Z",
    "id": "2506.17644v1",
    "authors": [
      "Zimo Ji",
      "Daoyuan Wu",
      "Wenyuan Jiang",
      "Pingchuan Ma",
      "Zongjie Li",
      "Shuai Wang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.17644v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.17644v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.17644v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on evaluating and enhancing the capabilities of Large Language Models (LLMs) in solving Capture-the-Flag (CTF) challenges, which involves knowledge, reasoning, and actions. It introduces a benchmark (CTFKnow) and a framework (CTFAgent) that leverages Retrieval Augmented Generation (RAG) and interactive environmental augmentation, aligning with topics related to LLMs, reasoning, and memory.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2506.17493v1": {
    "title": "PreQRAG -- Classify and Rewrite for Enhanced RAG",
    "summary": "This paper presents the submission of the UDInfo team to the SIGIR 2025\nLiveRAG Challenge. We introduce PreQRAG, a Retrieval Augmented Generation (RAG)\narchitecture designed to improve retrieval and generation quality through\ntargeted question preprocessing. PreQRAG incorporates a pipeline that first\nclassifies each input question as either single-document or multi-document\ntype. For single-document questions, we employ question rewriting techniques to\nimprove retrieval precision and generation relevance. For multi-document\nquestions, we decompose complex queries into focused sub-questions that can be\nprocessed more effectively by downstream components. This classification and\nrewriting strategy improves the RAG performance. Experimental evaluation of the\nLiveRAG Challenge dataset demonstrates the effectiveness of our\nquestion-type-aware architecture, with PreQRAG achieving the preliminary second\nplace in Session 2 of the LiveRAG challenge.",
    "published": "2025-06-20T22:02:05Z",
    "updated": "2025-06-20T22:02:05Z",
    "id": "2506.17493v1",
    "authors": [
      "Damian Martinez",
      "Catalina Riano",
      "Hui Fang"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.17493v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.17493v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.17493v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper focuses on enhancing Retrieval Augmented Generation (RAG) through question preprocessing, classification, and rewriting, which aligns with topics related to memory and retrieval-based methods in LLMs.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2506.17188v1": {
    "title": "Towards AI Search Paradigm",
    "summary": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint\nfor next-generation search systems capable of emulating human information\nprocessing and decision-making. The paradigm employs a modular architecture of\nfour LLM-powered agents (Master, Planner, Executor and Writer) that dynamically\nadapt to the full spectrum of information needs, from simple factual queries to\ncomplex multi-stage reasoning tasks. These agents collaborate dynamically\nthrough coordinated workflows to evaluate query complexity, decompose problems\ninto executable plans, and orchestrate tool usage, task execution, and content\nsynthesis. We systematically present key methodologies for realizing this\nparadigm, including task planning and tool integration, execution strategies,\naligned and robust retrieval-augmented generation, and efficient LLM inference,\nspanning both algorithmic techniques and infrastructure-level optimizations. By\nproviding an in-depth guide to these foundational components, this work aims to\ninform the development of trustworthy, adaptive, and scalable AI search\nsystems.",
    "published": "2025-06-20T17:42:13Z",
    "updated": "2025-06-20T17:42:13Z",
    "id": "2506.17188v1",
    "authors": [
      "Yuchen Li",
      "Hengyi Cai",
      "Rui Kong",
      "Xinran Chen",
      "Jiamin Chen",
      "Jun Yang",
      "Haojie Zhang",
      "Jiayi Li",
      "Jiayi Wu",
      "Yiqun Chen",
      "Changle Qu",
      "Keyi Kong",
      "Wenwen Ye",
      "Lixin Su",
      "Xinyu Ma",
      "Long Xia",
      "Daiting Shi",
      "Jiashu Zhao",
      "Haoyi Xiong",
      "Shuaiqiang Wang",
      "Dawei Yin"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.17188v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.17188v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.17188v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a modular architecture using LLM-powered agents for information processing and decision-making, which aligns with research on Large Language Models (LLM) and their applications in complex reasoning tasks. It also mentions retrieval-augmented generation, which is related to Memory-augmented models.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2506.17001v1": {
    "title": "PersonalAI: Towards digital twins in the graph form",
    "summary": "The challenge of personalizing language models, specifically the ability to\naccount for a user's history during interactions, is of significant interest.\nDespite recent advancements in large language models (LLMs) and Retrieval\nAugmented Generation that have enhanced the factual base of LLMs, the task of\nretaining extensive personal information and using it to generate personalized\nresponses remains pertinent. To address this, we propose utilizing external\nmemory in the form of knowledge graphs, which are constructed and updated by\nthe LLM itself. We have expanded upon ideas of AriGraph architecture and for\nthe first time introduced a combined graph featuring both standard edges and\ntwo types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and\nDiaASQ benchmarks indicates that this approach aids in making the process of\ngraph construction and knowledge extraction unified and robust. Furthermore, we\naugmented the DiaASQ benchmark by incorporating parameters such as time into\ndialogues and introducing contradictory statements made by the same speaker at\ndifferent times. Despite these modifications, the performance of the\nquestion-answering system remained robust, demonstrating the proposed\narchitecture's ability to maintain and utilize temporal dependencies.",
    "published": "2025-06-20T13:52:15Z",
    "updated": "2025-06-20T13:52:15Z",
    "id": "2506.17001v1",
    "authors": [
      "Mikhail Menschikov",
      "Dmitry Evseev",
      "Ruslan Kostoev",
      "Ilya Perepechkin",
      "Ilnaz Salimov",
      "Victoria Dochkina",
      "Petr Anokhin",
      "Evgeny Burnaev",
      "Nikita Semenov"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.17001v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.17001v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.17001v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of external memory in the form of knowledge graphs for personalizing language models, which aligns with the 'Memory' topic. It also involves large language models (LLMs) and their enhancement through retrieval augmented generation, which is relevant to the 'LLM' topic. The introduction of a combined graph with standard edges and hyperedges for knowledge extraction is a novel approach that could be categorized under 'AGI' due to its potential for advancing general intelligence.",
    "llm_cls_result": [
      "Memory",
      "LLM",
      "AGI"
    ]
  },
  "2506.16988v1": {
    "title": "RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed\n  Question Answering",
    "summary": "We present RAGentA, a multi-agent retrieval-augmented generation (RAG)\nframework for attributed question answering (QA). With the goal of trustworthy\nanswer generation, RAGentA focuses on optimizing answer correctness, defined by\ncoverage and relevance to the question and faithfulness, which measures the\nextent to which answers are grounded in retrieved documents. RAGentA uses a\nmulti-agent architecture that iteratively filters retrieved documents,\ngenerates attributed answers with in-line citations, and verifies completeness\nthrough dynamic refinement. Central to the framework is a hybrid retrieval\nstrategy that combines sparse and dense methods, improving Recall@20 by 12.5%\ncompared to the best single retrieval model, resulting in more correct and\nwell-supported answers. Evaluated on a synthetic QA dataset derived from the\nFineWeb index, RAGentA outperforms standard RAG baselines, achieving gains of\n1.09% in correctness and 10.72% in faithfulness. These results demonstrate the\neffectiveness of the multi-agent architecture and hybrid retrieval in advancing\ntrustworthy QA.",
    "published": "2025-06-20T13:37:03Z",
    "updated": "2025-06-20T13:37:03Z",
    "id": "2506.16988v1",
    "authors": [
      "Ines Besrour",
      "Jingbo He",
      "Tobias Schreieder",
      "Michael Frber"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.16988v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.16988v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.16988v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a multi-agent retrieval-augmented generation (RAG) framework for attributed question answering, which involves memory-augmented models and retrieval-based methods. It also touches on the effectiveness of hybrid retrieval strategies, which are relevant to the Memory topic.",
    "llm_cls_result": [
      "Memory"
    ]
  },
  "2506.16813v1": {
    "title": "Integrating Traditional Technical Analysis with AI: A Multi-Agent\n  LLM-Based Approach to Stock Market Forecasting",
    "summary": "Traditional technical analysis methods face limitations in accurately\npredicting trends in today's complex financial markets. This paper introduces\nElliottAgents, an multi-agent system that integrates the Elliott Wave Principle\nwith AI for stock market forecasting. The inherent complexity of financial\nmarkets, characterized by non-linear dynamics, noise, and susceptibility to\nunpredictable external factors, poses significant challenges for accurate\nprediction. To address these challenges, the system employs LLMs to enhance\nnatural language understanding and decision-making capabilities within a\nmulti-agent framework. By leveraging technologies such as Retrieval-Augmented\nGeneration (RAG) and Deep Reinforcement Learning (DRL), ElliottAgents performs\ncontinuous, multi-faceted analysis of market data to identify wave patterns and\npredict future price movements. The research explores the system's ability to\nprocess historical stock data, recognize Elliott wave patterns, and generate\nactionable insights for traders. Experimental results, conducted on historical\ndata from major U.S. companies, validate the system's effectiveness in pattern\nrecognition and trend forecasting across various time frames. This paper\ncontributes to the field of AI-driven financial analysis by demonstrating how\ntraditional technical analysis methods can be effectively combined with modern\nAI approaches to create more reliable and interpretable market prediction\nsystems.",
    "published": "2025-06-20T08:03:36Z",
    "updated": "2025-06-20T08:03:36Z",
    "id": "2506.16813v1",
    "authors": [
      "Micha Wawer",
      "Jarosaw A. Chudziak"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.16813v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.16813v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.16813v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of LLMs in a multi-agent system for stock market forecasting, incorporating techniques like Retrieval-Augmented Generation (RAG) and Deep Reinforcement Learning (DRL). It aligns with topics related to LLMs and Reinforcement Learning.",
    "llm_cls_result": [
      "LLM",
      "RL"
    ]
  },
  "2506.17356v1": {
    "title": "Automatic Large Language Models Creation of Interactive Learning Lessons",
    "summary": "We explore the automatic generation of interactive, scenario-based lessons\ndesigned to train novice human tutors who teach middle school mathematics\nonline. Employing prompt engineering through a Retrieval-Augmented Generation\napproach with GPT-4o, we developed a system capable of creating structured\ntutor training lessons. Our study generated lessons in English for three key\ntopics: Encouraging Students' Independence, Encouraging Help-Seeking Behavior,\nand Turning on Cameras, using a task decomposition prompting strategy that\nbreaks lesson generation into sub-tasks. The generated lessons were evaluated\nby two human evaluators, who provided both quantitative and qualitative\nevaluations using a comprehensive rubric informed by lesson design research.\nResults demonstrate that the task decomposition strategy led to higher-rated\nlessons compared to single-step generation. Human evaluators identified several\nstrengths in the LLM-generated lessons, including well-structured content and\ntime-saving potential, while also noting limitations such as generic feedback\nand a lack of clarity in some instructional sections. These findings underscore\nthe potential of hybrid human-AI approaches for generating effective lessons in\ntutor training.",
    "published": "2025-06-20T06:58:50Z",
    "updated": "2025-06-20T06:58:50Z",
    "id": "2506.17356v1",
    "authors": [
      "Jionghao Lin",
      "Jiarui Rao",
      "Yiyang Zhao",
      "Yuting Wang",
      "Ashish Gurung",
      "Amanda Barany",
      "Jaclyn Ocumpaugh",
      "Ryan S. Baker",
      "Kenneth R. Koedinger"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.17356v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.17356v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.17356v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses the use of Large Language Models (LLM) for generating interactive learning lessons, which involves prompt engineering and Retrieval-Augmented Generation, aligning with the LLM and Memory topics.",
    "llm_cls_result": [
      "LLM",
      "Memory"
    ]
  },
  "2506.16768v1": {
    "title": "eSapiens: A Real-World NLP Framework for Multimodal Document\n  Understanding and Enterprise Knowledge Processing",
    "summary": "We introduce eSapiens, a unified question-answering system designed for\nenterprise settings, which bridges structured databases and unstructured\ntextual corpora via a dual-module architecture. The system combines a\nText-to-SQL planner with a hybrid Retrieval-Augmented Generation (RAG)\npipeline, enabling natural language access to both relational data and\nfree-form documents. To enhance answer faithfulness, the RAG module integrates\ndense and sparse retrieval, commercial reranking, and a citation verification\nloop that ensures grounding consistency. We evaluate eSapiens on the RAGTruth\nbenchmark across five leading large language models (LLMs), analyzing\nperformance across key dimensions such as completeness, hallucination, and\ncontext utilization. Results demonstrate that eSapiens outperforms a FAISS\nbaseline in contextual relevance and generation quality, with optional\nstrict-grounding controls for high-stakes scenarios. This work provides a\ndeployable framework for robust, citation-aware question answering in\nreal-world enterprise applications.",
    "published": "2025-06-20T06:07:20Z",
    "updated": "2025-06-20T06:07:20Z",
    "id": "2506.16768v1",
    "authors": [
      "Isaac Shi",
      "Zeyuan Li",
      "Wenli Wang",
      "Lewei He",
      "Yang Yang",
      "Tianyu Shi"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2506.16768v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2506.16768v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2506.16768v1",
    "keywords": [
      "Memory"
    ],
    "cls_reason": "The paper discusses a multimodal document understanding system that integrates structured and unstructured data, utilizes Retrieval-Augmented Generation (RAG), and evaluates performance on large language models (LLMs). The key topics include multimodal understanding (MLLM), memory-augmented models (Memory), and benchmarking (Benchmark).",
    "llm_cls_result": [
      "MLLM",
      "Memory",
      "Benchmark"
    ]
  },
  "2507.18631v1": {
    "title": "Layer-Aware Representation Filtering: Purifying Finetuning Data to\n  Preserve LLM Safety Alignment",
    "summary": "With rapid advancement and increasing accessibility of LLMs, fine-tuning\naligned models has become a critical step for adapting them to real-world\napplications, which makes the safety of this fine-tuning process more important\nthan ever. However, recent studies have highlighted a critical challenge: even\nwhen fine-tuning with seemingly benign downstream datasets, the safety of\naligned LLMs can be compromised, making them more susceptible to malicious\ninstructions. In this paper, we show that fine-tuning datasets often contain\nsamples with safety-degrading features that are not easily identifiable on the\nsurface. These samples can significantly degrade the safety alignment of LLMs\nduring fine-tuning. To address this issue, we propose LARF, a\n\\textbf{L}ayer-\\textbf{A}ware \\textbf{R}epresentation \\textbf{F}iltering\nmethod. This method identifies safety-sensitive layers within the LLM and\nleverages their representations to detect which data samples in the\npost-training dataset contain safety-degrading features. Experimental results\ndemonstrate that LARF can effectively identify benign data with\nsafety-degrading features. After removing such data, the safety alignment\ndegradation caused by fine-tuning is mitigated. Please see our code at\n\\href{https://github.com/LLLeoLi/LARF}{https://github.com/LLLeoLi/LARF}.",
    "published": "2025-07-24T17:59:24Z",
    "updated": "2025-07-24T17:59:24Z",
    "id": "2507.18631v1",
    "authors": [
      "Hao Li",
      "Lijun Li",
      "Zhenghao Lu",
      "Xianyi Wei",
      "Rui Li",
      "Jing Shao",
      "Lei Sha"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18631v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18631v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18631v1",
    "keywords": [
      "Dataset"
    ],
    "cls_reason": "The paper discusses the safety alignment of LLMs during fine-tuning and proposes a method to filter out safety-degrading features in the data. This is directly related to the research on Large Language Models (LLM) and their safety and alignment issues.",
    "llm_cls_result": [
      "LLM"
    ]
  },
  "2507.18366v1": {
    "title": "Efficient Uncertainty in LLMs through Evidential Knowledge Distillation",
    "summary": "Accurate uncertainty quantification remains a key challenge for standard\nLLMs, prompting the adoption of Bayesian and ensemble-based methods. However,\nsuch methods typically necessitate computationally expensive sampling,\ninvolving multiple forward passes to effectively estimate predictive\nuncertainty.\n  In this paper, we introduce a novel approach enabling efficient and effective\nuncertainty estimation in LLMs without sacrificing performance. Specifically,\nwe distill uncertainty-aware teacher models - originally requiring multiple\nforward passes - into compact student models sharing the same architecture but\nfine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct\ndistillation strategies: one in which the student employs traditional\nsoftmax-based outputs, and another in which the student leverages\nDirichlet-distributed outputs to explicitly model epistemic uncertainty via\nevidential learning.\n  Empirical evaluations on classification datasets demonstrate that such\nstudents can achieve comparable or superior predictive and uncertainty\nquantification performance relative to their teacher models, while critically\nrequiring only a single forward pass. To our knowledge, this is the first\ndemonstration that immediate and robust uncertainty quantification can be\nachieved in LLMs through evidential distillation.",
    "published": "2025-07-24T12:46:40Z",
    "updated": "2025-07-24T12:46:40Z",
    "id": "2507.18366v1",
    "authors": [
      "Lakshmana Sri Harsha Nemani",
      "P. K. Srijith",
      "Tomasz Kumierczyk"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18366v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18366v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18366v1",
    "keywords": [
      "Dataset"
    ],
    "cls_reason": "The paper focuses on improving uncertainty quantification in LLMs through knowledge distillation, which involves techniques like LoRA and evidential learning. The primary topics are related to LLMs and their performance enhancement.",
    "llm_cls_result": [
      "LLM",
      "Reasoning"
    ]
  },
  "2507.18340v1": {
    "title": "TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for\n  In-Context Learning",
    "summary": "In-context learning (ICL) has become a classic approach for enabling LLMs to\nhandle various tasks based on a few input-output examples. The effectiveness of\nICL heavily relies on the quality of these examples, and previous works which\nfocused on enhancing example retrieval capabilities have achieved impressive\nperformances. However, two challenges remain in retrieving high-quality\nexamples: (1) Difficulty in distinguishing cross-task data distributions, (2)\nDifficulty in making the fine-grained connection between retriever output and\nfeedback from LLMs. In this paper, we propose a novel framework called TDR. TDR\ndecouples the ICL examples from different tasks, which enables the retrieval\nmodule to retrieve examples specific to the target task within a multi-task\ndataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise\nand guide the training of the retrieval module, which helps to retrieve\nhigh-quality examples. We conducted extensive experiments on a suite of 30 NLP\ntasks, the results demonstrate that TDR consistently improved results across\nall datasets and achieves state-of-the-art performance. Meanwhile, our approach\nis a plug-and-play method, which can be easily combined with various LLMs to\nimprove example retrieval abilities for ICL. The code is available at\nhttps://github.com/Nnn-s/TDR.",
    "published": "2025-07-24T12:12:04Z",
    "updated": "2025-07-24T12:12:04Z",
    "id": "2507.18340v1",
    "authors": [
      "Yifu Chen",
      "Bingchen Huang",
      "Zhiling Wang",
      "Yuanchao Du",
      "Junfeng Luo",
      "Lei Shen",
      "Zhineng chen"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18340v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18340v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18340v1",
    "keywords": [
      "Dataset"
    ],
    "cls_reason": "The paper focuses on improving in-context learning (ICL) for LLMs by enhancing example retrieval capabilities, which involves feedback from LLMs and retrieval mechanisms. This aligns with topics related to LLM reasoning and memory, as it deals with improving the quality of examples for better task performance and involves retrieval-based methods.",
    "llm_cls_result": [
      "LLM",
      "Reasoning",
      "Memory"
    ]
  },
  "2507.18294v1": {
    "title": "StyleAdaptedLM: Enhancing Instruction Following Models with Efficient\n  Stylistic Transfer",
    "summary": "Adapting LLMs to specific stylistic characteristics, like brand voice or\nauthorial tones, is crucial for enterprise communication but challenging to\nachieve from corpora which lacks instruction-response formatting without\ncompromising instruction adherence. We introduce StyleAdaptedLM, a framework\nthat efficiently transfers stylistic traits to instruction-following models\nusing Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base\nmodel with diverse unstructured stylistic corpora, then merged with a separate\ninstruction-following model. This enables robust stylistic customization\nwithout paired data or sacrificing task performance. Experiments across\nmultiple datasets and models demonstrate improved stylistic consistency while\npreserving instruction adherence, with human evaluations confirming\nbrand-specific convention uptake. StyleAdaptedLM offers an efficient path for\nstylistic personalization in LLMs.",
    "published": "2025-07-24T10:57:32Z",
    "updated": "2025-07-24T10:57:32Z",
    "id": "2507.18294v1",
    "authors": [
      "Pritika Ramu",
      "Apoorv Saxena",
      "Meghanath M Y",
      "Varsha Sankar",
      "Debraj Basu"
    ],
    "arxiv_abstract_url": "http://arxiv.org/abs/2507.18294v1",
    "arxiv_pdf_url": "http://arxiv.org/pdf/2507.18294v1.pdf",
    "alphaxiv_url": "http://alphaxiv.org/abs/2507.18294v1",
    "keywords": [
      "Dataset"
    ],
    "cls_reason": "The paper focuses on enhancing instruction-following models with stylistic transfer using LoRA, which involves adapting LLMs to specific stylistic characteristics without compromising their core functionalities. This aligns with research on Large Language Models (LLM) and their adaptation techniques.",
    "llm_cls_result": [
      "LLM"
    ]
  }
}